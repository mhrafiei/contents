{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lHxobThtCRAo"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "# Upload:\n",
        "# electrical_grid_stability_simulated_data.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyklUgDSwTxL"
      },
      "source": [
        "# 14.1. Combinatory Pattern Recognition Using Supercomputers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "colab": {
          "background_save": true
        },
        "id": "hq_LhZnvwTF8"
      },
      "outputs": [],
      "source": [
        "#@title 14.1.1. One main cell\n",
        "\n",
        "#############################################################\n",
        "# Import some necessary packages\n",
        "#############################################################\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "import itertools # new\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error as mse\n",
        "\n",
        "# ml models\n",
        "from sklearn.svm import SVC as svc\n",
        "from sklearn.neighbors import KNeighborsClassifier as knc\n",
        "from sklearn.tree import DecisionTreeClassifier as dtc\n",
        "from sklearn.ensemble import RandomForestClassifier as rfc\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from multiprocessing import Pool\n",
        "import multiprocessing\n",
        "import time\n",
        "\n",
        "#############################################################\n",
        "# Data processing\n",
        "#############################################################\n",
        "\n",
        "data = pd.read_csv('electrical_grid_stability_simulated_data.csv')\n",
        "\n",
        "datain = data.iloc[:,0:12]\n",
        "dataou = data.iloc[:,-1:]\n",
        "\n",
        "dataou[dataou == 'stable']   = 1\n",
        "dataou[dataou == 'unstable'] = 0\n",
        "\n",
        "datain = datain.values\n",
        "dataou = dataou.values\n",
        "\n",
        "# object to float data\n",
        "mat = np.empty(dataou.shape,dtype=float)\n",
        "for i0 in range(dataou.shape[0]): mat[i0,0] = dataou[i0]\n",
        "dataou = mat\n",
        "\n",
        "# define some simple keys\n",
        "data_keys = list(data.keys())\n",
        "keys = []\n",
        "for i0 in range(len(data_keys)):\n",
        "  keys.append('feature {:02d}'.format(i0+1))\n",
        "\n",
        "# remove the outputs\n",
        "keys = keys[:13]\n",
        "\n",
        "# necessary information for combinatory model\n",
        "num_data      = datain.shape[0]\n",
        "num_variables = datain.shape[1]\n",
        "\n",
        "#############################################################\n",
        "# Some necessary functions\n",
        "#############################################################\n",
        "\n",
        "def fun_combinations(num_data, num_variables):\n",
        "  num_comb = 2**num_variables - 1\n",
        "  comb_bin = np.zeros((num_comb,num_variables))\n",
        "  comb_ind = []\n",
        "\n",
        "  counter  = 0\n",
        "  for i0 in range(num_variables):\n",
        "    ind       = list(itertools.combinations(range(num_variables),i0+1))\n",
        "    comb_ind.append(ind)\n",
        "\n",
        "    for i1 in range(len(ind)):\n",
        "      comb_bin[counter,ind[i1]] = 1\n",
        "      counter                   = counter + 1\n",
        "\n",
        "  comb_bin = comb_bin.astype(dtype = bool)\n",
        "\n",
        "  return comb_bin, comb_ind, num_comb\n",
        "\n",
        "def fun_split(RTT, RRS, num_data):\n",
        "  np.random.seed(42)\n",
        "\n",
        "  index_te = []\n",
        "  index_tr = []\n",
        "\n",
        "  for rtt in RTT:\n",
        "    index_te_tmp = []\n",
        "    index_tr_tmp = []\n",
        "\n",
        "    for rrs in range(RRS):\n",
        "      index = list(range(num_data))\n",
        "      index = np.ndarray.tolist(np.random.permutation(index))\n",
        "\n",
        "      index_te_tmp.append(index[0:int(np.floor(rtt*num_data))])\n",
        "      index_tr_tmp.append(index[int(np.floor(rtt*num_data)):])\n",
        "\n",
        "    index_te.append(index_te_tmp)\n",
        "    index_tr.append(index_tr_tmp)\n",
        "\n",
        "  return index_tr, index_te\n",
        "\n",
        "def fun_prep(datain, dataou, comb_bin, index_tr, index_te, lb, ub):\n",
        "  datain_tr = datain[index_tr, :][:, comb_bin] # this way we avoid \"mismatch\" error in indexing\n",
        "  datain_te = datain[index_te, :][:, comb_bin] # this way we avoid \"mismatch\" error in indexing\n",
        "  dataou_tr = dataou[index_tr,:]\n",
        "  dataou_te = dataou[index_te,:]\n",
        "\n",
        "  if len(datain_tr.shape) == 1:\n",
        "    datain_tr = np.expand_dims(datain_tr, axis = 1)\n",
        "    datain_te = np.expand_dims(datain_te, axis = 1)\n",
        "\n",
        "  if len(dataou_tr.shape) == 1:\n",
        "    dataou_tr = np.expand_dims(dataou_tr, axis = 1)\n",
        "    dataou_te = np.expand_dims(dataou_te, axis = 1)\n",
        "\n",
        "  scalerin = MinMaxScaler(feature_range=(lb,ub))\n",
        "  scalerin.fit(datain_tr)\n",
        "\n",
        "  scalerou = MinMaxScaler(feature_range=(lb,ub))\n",
        "  scalerou.fit(dataou_tr)\n",
        "\n",
        "  datain_tr_calibrated = scalerin.transform(datain_tr)\n",
        "  datain_te_calibrated = scalerin.transform(datain_te)\n",
        "\n",
        "  # dataou_tr_calibrated = scalerou.transform(dataou_tr)\n",
        "  # dataou_te_calibrated = scalerou.transform(dataou_te)\n",
        "\n",
        "  return datain_tr_calibrated, dataou_tr, datain_te_calibrated, dataou_te\n",
        "\n",
        "\n",
        "def fun_ml(datain_tr, dataou_tr, datain_te, dataou_te, model_type):\n",
        "\n",
        "  result = {}\n",
        "\n",
        "  if model_type == 'SVC':\n",
        "    mdl       = svc()\n",
        "  elif model_type == 'KNC':\n",
        "    mdl       = knc()\n",
        "  elif model_type == 'DTC':\n",
        "    mdl       = dtc()\n",
        "  elif model_type == 'RFC':\n",
        "    mdl       = rfc()\n",
        "  else:\n",
        "    print(\"Model type has not been defined!\")\n",
        "\n",
        "  result['model_type'] = model_type\n",
        "  history              = mdl.fit(datain_tr, dataou_tr)\n",
        "  dataes_tr            = mdl.predict(datain_tr)\n",
        "  dataes_te            = mdl.predict(datain_te)\n",
        "\n",
        "  if len(dataes_tr.shape) == 1:\n",
        "    dataes_tr = np.expand_dims(dataes_tr, axis = 1)\n",
        "\n",
        "  if len(dataes_te.shape) == 1:\n",
        "    dataes_te = np.expand_dims(dataes_te, axis = 1)\n",
        "\n",
        "  result['acc_tr']     = fun_accuracy(dataes_tr, dataou_tr)\n",
        "  result['acc_te']     = fun_accuracy(dataes_te, dataou_te)\n",
        "\n",
        "  # result['datain_tr'] = datain_tr\n",
        "  # result['datain_te'] = datain_te\n",
        "\n",
        "  # result['dataes_tr'] = dataes_tr\n",
        "  # result['dataou_tr'] = dataou_tr\n",
        "  # result['dataes_te'] = dataes_te\n",
        "  # result['dataou_te'] = dataou_te\n",
        "\n",
        "  return result\n",
        "\n",
        "def fun_accuracy(dataes, dataou):\n",
        "  num_err = np.count_nonzero(dataes - dataou)\n",
        "  accuracy = 1 - num_err/dataou.shape[0]\n",
        "  return accuracy\n",
        "\n",
        "def fun_rttrrs(i0):\n",
        "  result  = np.empty((len(RTT), RRS, 1, len(model_type)), dtype = dict)\n",
        "  for i1 in range(len(RTT)):\n",
        "    for i2 in range(RRS):\n",
        "      datain_tr, dataou_tr, datain_te, dataou_te = fun_prep(datain, dataou, comb_bin[i0,:], index_tr[i1][i2], index_te[i1][i2], 0, 1)\n",
        "      for i3 in range(len(model_type)):\n",
        "        result[i1,i2,0,i3] = fun_ml(datain_tr, dataou_tr, datain_te, dataou_te, model_type[i3])\n",
        "        print(\"Comb #: {:05d}/{} | RTT #: {:05d}/{} | RRS #: {:05d}/{} | Model: {}\".format(i0+1, num_comb, i1+1, len(RTT), i2+1, RRS, model_type[i3]))\n",
        "  return result\n",
        "\n",
        "def fun_parallel(pool_size):\n",
        "  result = np.empty((len(RTT), RRS, num_comb, len(model_type)), dtype = dict)\n",
        "\n",
        "  # define the loop counters\n",
        "  par_loop = []\n",
        "  for i0 in range(0, num_comb):\n",
        "    par_loop.append((i0,))\n",
        "\n",
        "  #my_pool     = Pool(pool_size) # define the number of cores/workers\n",
        "  my_pool = multiprocessing.get_context('fork').Pool(pool_size)\n",
        "  result_list = my_pool.starmap(fun_rttrrs, par_loop)\n",
        "\n",
        "  # gather results\n",
        "  for i0 in range(len(result_list)):\n",
        "    result[:,:,i0:i0+1,:] = result_list[i0]\n",
        "\n",
        "  return result\n",
        "\n",
        "#############################################################\n",
        "# Main run\n",
        "#############################################################\n",
        "\n",
        "model_type = ['SVC', 'KNC', 'DTC', 'RFC']\n",
        "RTT        = [0.1,0.2,0.3,0.4,0.5]\n",
        "RRS        = 100\n",
        "pool_size  = 2\n",
        "\n",
        "comb_bin, _, num_comb = fun_combinations(num_data, num_variables)\n",
        "index_tr, index_te           = fun_split(RTT, RRS, num_data)\n",
        "\n",
        "comb_index_start = 110\n",
        "comb_index_end   = 115\n",
        "\n",
        "range_list = list(range(comb_index_start,comb_index_end ,1))\n",
        "num_comb   = len(range_list)\n",
        "comb_bin   = comb_bin[range_list,:]\n",
        "\n",
        "time_start = time.time()\n",
        "\n",
        "result    = fun_parallel(pool_size)\n",
        "\n",
        "run_time = time.time() - time_start\n",
        "\n",
        "file_dict = {'comb_bin': comb_bin, 'range': [comb_index_start, comb_index_end], 'run_time':run_time , 'result': result}\n",
        "\n",
        "print(run_time)\n",
        "\n",
        "filename = \"result_{:07d}_{:07d}.npy\".format(comb_index_start, comb_index_end)\n",
        "np.save(filename, file_dict)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "No0sHM2bOGOk"
      },
      "outputs": [],
      "source": [
        "#@title 14.1.2. Run as a .py file\n",
        "#!python module14_a.py"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}