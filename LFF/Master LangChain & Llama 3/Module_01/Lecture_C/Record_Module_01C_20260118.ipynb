{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b978d7f",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Environment Setup**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb958eb",
   "metadata": {},
   "source": [
    ">Last update: 20260118.\n",
    "    \n",
    "By the end of this Lecture, you will be able to:\n",
    "- Set up a reproducible Python environment for LangChain and Llama 3 development. \n",
    "- Organize a project structure suitable for multi-module LangChain applications. \n",
    "- Enable logging and basic debugging tools to inspect LangChain–Llama 3 interactions. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2bd4cb",
   "metadata": {},
   "source": [
    "## **1. LangChain Python Environment**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6c9f57",
   "metadata": {},
   "source": [
    "### **1.1. Python Environment Setup**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353000d7",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master LangChain & Llama 3/Module_01/Lecture_C/image_01_01.jpg?v=1768762647\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Use isolated Python environments for each project\n",
    ">* Avoid version conflicts and keep setups reproducible\n",
    "\n",
    ">* Pick a Python version and isolated environment\n",
    ">* Activate it, install packages, adjust versions safely\n",
    "\n",
    ">* Document versions and setup for easy sharing\n",
    ">* Reproducible environments simplify upgrades and handoffs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118ffe34",
   "metadata": {},
   "source": [
    "### **1.2. Installing LangChain Packages**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2b89f5",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master LangChain & Llama 3/Module_01/Lecture_C/image_01_02.jpg?v=1768762661\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Install LangChain and supporting Llama 3 packages\n",
    ">* Keep tools isolated for consistent, conflict-free development\n",
    "\n",
    ">* Install only essential LangChain packages and pin versions\n",
    ">* Keep environment lean, avoid conflicts, clarify dependencies\n",
    "\n",
    ">* Check version compatibility and test upgrades safely\n",
    ">* Document installs to ensure stable, reproducible environments\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79b128c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Installing LangChain Packages\n",
    "\n",
    "# Demonstrate installing LangChain related packages in a controlled Colab environment.\n",
    "# Show how to pin specific versions for reproducible LangChain and Llama 3 experiments.\n",
    "# Verify installations by importing packages and printing their installed versions.\n",
    "\n",
    "# Install LangChain core package with a specific pinned version using pip.\n",
    "# !pip install langchain==0.3.0\n",
    "\n",
    "# Install a hypothetical Llama 3 client library with pinned version using pip.\n",
    "# !pip install llama3-client==1.2.0\n",
    "\n",
    "# Import the standard subprocess module for checking installed package versions.\n",
    "import subprocess\n",
    "\n",
    "# Define a helper function that returns a package version using pip show command.\n",
    "def get_package_version(package_name):\n",
    "    result = subprocess.run([\n",
    "        \"python\",\n",
    "        \"-m\",\n",
    "        \"pip\",\n",
    "        \"show\",\n",
    "        package_name\n",
    "    ], capture_output=True, text=True)\n",
    "\n",
    "    # Parse the pip show output to find the version line if available.\n",
    "    for line in result.stdout.splitlines():\n",
    "        if line.startswith(\"Version:\"):\n",
    "            return line.split(\":\", 1)[1].strip()\n",
    "\n",
    "    # Return a helpful message when the package is not currently installed.\n",
    "    return \"not installed or not found\"\n",
    "\n",
    "# Create a list describing the core LangChain package and one Llama 3 client.\n",
    "packages_to_check = [\n",
    "    (\"LangChain core library\", \"langchain\"),\n",
    "    (\"Llama 3 client library\", \"llama3-client\")\n",
    "]\n",
    "\n",
    "# Print a short header explaining what information will be displayed below.\n",
    "print(\"Checking selected LangChain related packages and their pinned versions.\")\n",
    "\n",
    "# Loop through each package description and print its current installed version.\n",
    "for description, package_name in packages_to_check:\n",
    "    version = get_package_version(package_name)\n",
    "    print(f\"{description} ({package_name}) version: {version}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833c3eab",
   "metadata": {},
   "source": [
    "### **1.3. Dependency Management Basics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4653a30a",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master LangChain & Llama 3/Module_01/Lecture_C/image_01_03.jpg?v=1768762688\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Manage library versions to keep environments consistent\n",
    ">* Recorded dependencies prevent surprises across machines, time\n",
    "\n",
    ">* Treat dependencies as a clear, documented contract\n",
    ">* Pin versions to keep behavior stable and reversible\n",
    "\n",
    ">* Helps teammates reproduce and extend projects easily\n",
    ">* Supports audits, reduces bugs, strengthens long-term reliability\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d937ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Dependency Management Basics\n",
    "\n",
    "# Demonstrate simple dependency management concepts using Python dictionaries.\n",
    "# Show how version pinning affects reproducible project environments.\n",
    "# Compare two environments and highlight mismatched dependency versions.\n",
    "\n",
    "# Example pip install comment showing pinned versions for reproducibility.\n",
    "# pip install langchain==0.2.0 llama-cpp-python==0.2.90.\n",
    "\n",
    "# Define a dictionary representing your current project dependencies.\n",
    "current_env = {\n",
    "    \"langchain\": \"0.2.0\",\n",
    "    \"llama-cpp-python\": \"0.2.90\",\n",
    "    \"httpx\": \"0.27.0\",\n",
    "}\n",
    "\n",
    "# Define a dictionary representing a colleague environment with different versions.\n",
    "colleague_env = {\n",
    "    \"langchain\": \"0.3.1\",\n",
    "    \"llama-cpp-python\": \"0.2.90\",\n",
    "    \"httpx\": \"0.26.0\",\n",
    "}\n",
    "\n",
    "# Define a function that compares two dependency dictionaries.\n",
    "def compare_dependencies(env_a, env_b):\n",
    "    print(\"Comparing dependency versions between two environments:\")\n",
    "    for package in sorted(set(env_a.keys()) | set(env_b.keys())):\n",
    "        version_a = env_a.get(package, \"not installed\")\n",
    "        version_b = env_b.get(package, \"not installed\")\n",
    "        if version_a == version_b:\n",
    "            print(f\"{package}: SAME version {version_a} in both environments.\")\n",
    "        else:\n",
    "            print(f\"{package}: DIFFERENT versions -> current {version_a}, colleague {version_b}.\")\n",
    "\n",
    "# Call the comparison function to visualize environment differences.\n",
    "compare_dependencies(current_env, colleague_env)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98e1943",
   "metadata": {},
   "source": [
    "## **2. LangChain Project Layout**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359f7b83",
   "metadata": {},
   "source": [
    "### **2.1. Source and Tests Structure**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1bfc99",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master LangChain & Llama 3/Module_01/Lecture_C/image_02_01.jpg?v=1768762718\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Separate entrypoints, reusable components, and matching tests\n",
    ">* Use one src folder with clear subpackages\n",
    "\n",
    ">* Match test folders to source code modules\n",
    ">* Use tests to verify behavior and simplify debugging\n",
    "\n",
    ">* Group domain logic and infrastructure into packages\n",
    ">* Mirror workflows in tests to protect business behavior\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d251a972",
   "metadata": {},
   "source": [
    "### **2.2. Config and secrets handling**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfe3ca2",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master LangChain & Llama 3/Module_01/Lecture_C/image_02_02.jpg?v=1768762735\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Separate public config from private secrets clearly\n",
    ">* Centralize config, load secrets from secure environments\n",
    "\n",
    ">* Use one shared configuration layer across modules\n",
    ">* Prevents mismatched settings and centralizes behavior changes\n",
    "\n",
    ">* Use environment-specific configs and easily rotated secrets\n",
    ">* Support logging, profile visibility, and safe introspection\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a30cadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Config and secrets handling\n",
    "\n",
    "# Demonstrate centralized configuration and secrets handling for LangChain style projects.\n",
    "# Show separation between safe configuration values and sensitive secret values.\n",
    "# Illustrate environment based configuration switching using simple Python structures.\n",
    "\n",
    "# pip install commands would appear here if external libraries were required.\n",
    "\n",
    "# Import operating system module for reading environment variables.\n",
    "import os\n",
    "\n",
    "# Define safe configuration dictionary with non sensitive project settings.\n",
    "safe_config = {\n",
    "    \"model_name\": \"llama3-small\",\n",
    "    \"request_timeout_seconds\": 30,\n",
    "    \"max_context_tokens\": 4096,\n",
    "}\n",
    "\n",
    "# Define function that loads secret values from environment variables.\n",
    "def load_secrets_from_environment():\n",
    "    api_key = os.getenv(\"LLAMA_API_KEY\", \"missing-secret-placeholder\")\n",
    "    db_password = os.getenv(\"VECTOR_DB_PASSWORD\", \"missing-secret-placeholder\")\n",
    "    return {\"llama_api_key\": api_key, \"vector_db_password\": db_password}\n",
    "\n",
    "# Define function that builds environment specific configuration profile.\n",
    "def build_environment_profile(environment_name):\n",
    "    base_profile = {\"environment\": environment_name, \"config\": safe_config.copy()}\n",
    "    if environment_name == \"development\":\n",
    "        base_profile[\"config\"][\"temperature\"] = 0.7\n",
    "        base_profile[\"config\"][\"max_parallel_requests\"] = 2\n",
    "    elif environment_name == \"production\":\n",
    "        base_profile[\"config\"][\"temperature\"] = 0.2\n",
    "        base_profile[\"config\"][\"max_parallel_requests\"] = 10\n",
    "    else:\n",
    "        base_profile[\"config\"][\"temperature\"] = 0.5\n",
    "        base_profile[\"config\"][\"max_parallel_requests\"] = 4\n",
    "    return base_profile\n",
    "\n",
    "# Define function that prints configuration summary without exposing secrets.\n",
    "def print_configuration_summary(profile, secrets):\n",
    "    print(\"Active environment:\", profile[\"environment\"])\n",
    "    print(\"Model name:\", profile[\"config\"][\"model_name\"])\n",
    "    print(\"Temperature setting:\", profile[\"config\"][\"temperature\"])\n",
    "    print(\"Max parallel requests:\", profile[\"config\"][\"max_parallel_requests\"])\n",
    "    print(\"Llama API key present:\", secrets[\"llama_api_key\"] != \"missing-secret-placeholder\")\n",
    "    print(\"Vector database password present:\", secrets[\"vector_db_password\"] != \"missing-secret-placeholder\")\n",
    "\n",
    "# Set environment name using environment variable with default fallback.\n",
    "current_environment = os.getenv(\"APP_ENVIRONMENT\", \"development\")\n",
    "\n",
    "# Build configuration profile and load secrets using helper functions.\n",
    "profile = build_environment_profile(current_environment)\n",
    "secrets = load_secrets_from_environment()\n",
    "\n",
    "# Print configuration summary to demonstrate centralized configuration handling.\n",
    "print_configuration_summary(profile, secrets)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f4345a",
   "metadata": {},
   "source": [
    "### **2.3. Project Data Organization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af2ff63",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master LangChain & Llama 3/Module_01/Lecture_C/image_02_03.jpg?v=1768762765\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Separate raw, processed, and derived project data clearly\n",
    ">* Use directories and names to track data versions\n",
    "\n",
    ">* Match data folders to pipeline lifecycle stages\n",
    ">* Separate raw, processed, and runtime data areas\n",
    "\n",
    ">* Consistent data layout coordinates teams and components\n",
    ">* Predictable structure simplifies experiments, scaling, and migration\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561cbaab",
   "metadata": {},
   "source": [
    "## **3. LangChain Debugging Essentials**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6721c8",
   "metadata": {},
   "source": [
    "### **3.1. Verbose Logging Basics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4c41bd",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master LangChain & Llama 3/Module_01/Lecture_C/image_03_01.jpg?v=1768762782\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Verbose logging reveals hidden steps in pipelines\n",
    ">* Makes debugging easier by exposing where issues start\n",
    "\n",
    ">* Verbose logs show inputs, outputs, and metadata\n",
    ">* They reveal which pipeline step causes wrong behavior\n",
    "\n",
    ">* Verbose logs make model behavior observable, testable\n",
    ">* They improve reliability, safety, and compliance over time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246ec549",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Verbose Logging Basics\n",
    "\n",
    "# Demonstrate simple verbose logging for a tiny text processing pipeline.\n",
    "# Show hidden intermediate steps that usually stay invisible during execution.\n",
    "# Help beginners see how logging supports debugging language style transformations.\n",
    "\n",
    "# Example uses only standard Python libraries available in Google Colab.\n",
    "# No external installations are required for running this simple demonstration.\n",
    "\n",
    "# Import datetime for timestamps in our verbose logging messages.\n",
    "from datetime import datetime\n",
    "\n",
    "# Define a helper function that prints timestamped verbose messages clearly.\n",
    "def log_verbose(step_name, message, enabled=True):\n",
    "    if not enabled:\n",
    "        return\n",
    "    now = datetime.now().strftime('%H:%M:%S')\n",
    "    print(f\"[{now}] {step_name}: {message}\")\n",
    "\n",
    "# Define a simple pipeline that transforms a support question into a model prompt.\n",
    "def build_prompt(user_question, verbose=True):\n",
    "    log_verbose('INPUT', f'Received question: {user_question}', verbose)\n",
    "    system_instruction = 'You are a helpful support assistant.'\n",
    "    log_verbose('SYSTEM', f'Using instruction: {system_instruction}', verbose)\n",
    "\n",
    "    context_snippet = 'Policy: Orders ship within five business days inside continental United States.'\n",
    "    log_verbose('CONTEXT', f'Retrieved context: {context_snippet}', verbose)\n",
    "\n",
    "    prompt = f\"Instruction: {system_instruction}\\nContext: {context_snippet}\\nQuestion: {user_question}\"\n",
    "    log_verbose('PROMPT', f'Final prompt preview: {prompt[:80]}...', verbose)\n",
    "    return prompt\n",
    "\n",
    "# Define a fake model call that pretends to answer using the constructed prompt.\n",
    "def fake_model_call(prompt_text, verbose=True):\n",
    "    log_verbose('MODEL', 'Sending prompt to model backend now.', verbose)\n",
    "    answer = 'Your order will usually ship within five business days after purchase.'\n",
    "    log_verbose('MODEL', f'Model raw answer: {answer}', verbose)\n",
    "    return answer\n",
    "\n",
    "# Run the pipeline twice to compare behavior with and without verbose logging.\n",
    "user_question = 'When will my package probably arrive after I place an order?'\n",
    "\n",
    "# First run with verbose logging enabled to show internal pipeline steps clearly.\n",
    "print('--- Run with verbose logging enabled ---')\n",
    "verbose_prompt = build_prompt(user_question, verbose=True)\n",
    "verbose_answer = fake_model_call(verbose_prompt, verbose=True)\n",
    "print(f'Final answer shown to user: {verbose_answer}')\n",
    "\n",
    "# Second run with verbose logging disabled to show only final visible output.\n",
    "print('\\n--- Run with verbose logging disabled ---')\n",
    "quiet_prompt = build_prompt(user_question, verbose=False)\n",
    "quiet_answer = fake_model_call(quiet_prompt, verbose=False)\n",
    "print(f'Final answer shown to user: {quiet_answer}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e164a41",
   "metadata": {},
   "source": [
    "### **3.2. Inspecting Model Interactions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8167ae",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master LangChain & Llama 3/Module_01/Lecture_C/image_03_02.jpg?v=1768762809\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Trace the request journey through your application\n",
    ">* Inspect prompts, settings, and steps to locate issues\n",
    "\n",
    ">* Check inputs, outputs, and surrounding call context\n",
    ">* Use these layers to diagnose and fix issues\n",
    "\n",
    ">* Shift from fixing failures to monitoring patterns\n",
    ">* Use logs as feedback loop improving reliability\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b5edaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Inspecting Model Interactions\n",
    "\n",
    "# Demonstrate inspecting model interactions using a simple fake model logger.\n",
    "# Show inputs outputs and context for each fake model call.\n",
    "# Help beginners see how logging clarifies model behavior.\n",
    "\n",
    "# pip install langchain-openai langchain-community langchain.\n",
    "\n",
    "# Import required standard library modules for timing and identifiers.\n",
    "import time\n",
    "import uuid\n",
    "\n",
    "# Define a simple fake model that mimics LLM behavior.\n",
    "def fake_llm(prompt_text, temperature_value, model_name):\n",
    "    time.sleep(0.2)\n",
    "    if \"short\" in prompt_text.lower():\n",
    "        return \"Concise answer about the requested topic.\"\n",
    "    return \"Very long detailed answer that might overwhelm the user.\"\n",
    "\n",
    "# Define a helper that logs inputs outputs and context clearly.\n",
    "def logged_model_call(user_message, system_instruction, temperature_value):\n",
    "    request_id = str(uuid.uuid4())\n",
    "    model_name = \"llama3-fake-mini\"\n",
    "    full_prompt = f\"System: {system_instruction}\\nUser: {user_message}\"\n",
    "\n",
    "    start_time = time.time()\n",
    "    model_output = fake_llm(full_prompt, temperature_value, model_name)\n",
    "    end_time = time.time()\n",
    "\n",
    "    print(\"--- Logged interaction summary ---\")\n",
    "    print(f\"Request id: {request_id}\")\n",
    "    print(f\"Model name: {model_name}\")\n",
    "    print(f\"Temperature: {temperature_value}\")\n",
    "    print(f\"Duration seconds: {round(end_time - start_time, 3)}\")\n",
    "    print(\"Input prompt preview:\")\n",
    "    print(full_prompt[:120])\n",
    "    print(\"Model output preview:\")\n",
    "    print(model_output)\n",
    "\n",
    "# Define a scenario where we inspect two different interactions.\n",
    "def run_scenario():\n",
    "    system_instruction = \"You are a helpful assistant giving clear concise answers.\"\n",
    "\n",
    "    print(\"\\nScenario one verbose answer configuration:\\n\")\n",
    "    logged_model_call(\n",
    "        user_message=\"Explain how a home air conditioner works in simple terms.\",\n",
    "        system_instruction=system_instruction,\n",
    "        temperature_value=0.9,\n",
    "    )\n",
    "\n",
    "    print(\"\\nScenario two shorter answer configuration:\\n\")\n",
    "    logged_model_call(\n",
    "        user_message=\"Give a short explanation of how a home air conditioner works.\",\n",
    "        system_instruction=system_instruction,\n",
    "        temperature_value=0.2,\n",
    "    )\n",
    "\n",
    "# Run the scenario so beginners can see logged interactions.\n",
    "run_scenario()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d28b902",
   "metadata": {},
   "source": [
    "### **3.3. Setup Debugging Gotchas**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37883f2e",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master LangChain & Llama 3/Module_01/Lecture_C/image_03_03.jpg?v=1768762852\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Too much debugging output overwhelms logs and performance\n",
    ">* Adjust logging levels selectively for focused investigation\n",
    "\n",
    ">* Separate dev and production debugging configurations carefully\n",
    ">* Document env vars and protect logs, credentials\n",
    "\n",
    ">* Debug logs are easy to misread\n",
    ">* Use hypotheses and targeted tests to interpret\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4beec59d",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Environment Setup**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdee81d",
   "metadata": {},
   "source": [
    "\n",
    "In this lecture, you learned to:\n",
    "- Set up a reproducible Python environment for LangChain and Llama 3 development. \n",
    "- Organize a project structure suitable for multi-module LangChain applications. \n",
    "- Enable logging and basic debugging tools to inspect LangChain–Llama 3 interactions. \n",
    "\n",
    "In the next Module (Module 2), we will go over 'Prompting With Llama 3'"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
