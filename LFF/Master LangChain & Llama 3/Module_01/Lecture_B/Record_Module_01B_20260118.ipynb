{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58b75ca5",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**LangChain Basics**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcbacc0",
   "metadata": {},
   "source": [
    ">Last update: 20260118.\n",
    "    \n",
    "By the end of this Lecture, you will be able to:\n",
    "- Explain the main LangChain abstractions and how they relate to each other. \n",
    "- Create a minimal LangChain script that calls a Llama 3 model. \n",
    "- Configure basic settings such as temperature, max tokens, and model selection for Llama 3 within LangChain. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b01b0a9",
   "metadata": {},
   "source": [
    "## **1. LangChain Core Abstractions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bfb881",
   "metadata": {},
   "source": [
    "### **1.1. LLM and Chat Interfaces**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff9e5f7",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master LangChain & Llama 3/Module_01/Lecture_B/image_01_01.jpg?v=1768760953\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* LangChain offers text and chat model interfaces\n",
    ">* Use single prompts or multi-message conversations as needed\n",
    "\n",
    ">* Basic LLM interface needs full context each call\n",
    ">* Chat interface stores message history for ongoing conversations\n",
    "\n",
    ">* LLM and chat interfaces underpin all components\n",
    ">* They standardize configuration, context, and text generation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7566d572",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - LLM and Chat Interfaces\n",
    "\n",
    "# Demonstrate basic LangChain LLM and chat interfaces clearly.\n",
    "# Show single text prompt versus multi message chat conversation.\n",
    "# Use a fake model so everything runs offline.\n",
    "\n",
    "# pip install langchain langchain-community langchain-core.\n",
    "\n",
    "# Import required LangChain core classes.\n",
    "from langchain_core.language_models import LLM\n",
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "from langchain_core.outputs import ChatGeneration, ChatResult\n",
    "\n",
    "# Import typing tools for type hints.\n",
    "from typing import List\n",
    "\n",
    "\n",
    "# Create a simple fake LLM returning formatted responses.\n",
    "class SimpleFakeLLM(LLM):\n",
    "\n",
    "    # Define required _call method for text interface.\n",
    "    def _call(self, prompt: str, stop: List[str] | None = None) -> str:\n",
    "        return f\"[LLM reply] You said: {prompt[:60]}\"\n",
    "\n",
    "    # Define required _llm_type property name string.\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"simple_fake_llm\"\n",
    "\n",
    "\n",
    "# Create a simple fake chat model handling message lists.\n",
    "class SimpleFakeChatModel(BaseChatModel):\n",
    "\n",
    "    # Define _generate method processing chat messages list.\n",
    "    def _generate(self, messages: List, stop: List[str] | None = None, **kwargs):\n",
    "        last_user = next((m for m in reversed(messages) if isinstance(m, HumanMessage)), None)\n",
    "        content = last_user.content if last_user else \"nothing received\"\n",
    "        ai_message = AIMessage(content=f\"[Chat reply] I remember: {content[:60]}\")\n",
    "        generation = ChatGeneration(message=ai_message)\n",
    "        return ChatResult(generations=[generation])\n",
    "\n",
    "    # Define required _llm_type property name string.\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"simple_fake_chat_model\"\n",
    "\n",
    "\n",
    "# Instantiate both fake models for demonstration.\n",
    "text_llm = SimpleFakeLLM()\n",
    "chat_llm = SimpleFakeChatModel()\n",
    "\n",
    "# Demonstrate traditional single text LLM interface.\n",
    "single_prompt = \"Summarize this note about a ten foot ladder safety checklist.\"\n",
    "text_response = text_llm.invoke(single_prompt)\n",
    "\n",
    "# Print result from traditional LLM interface.\n",
    "print(\"Traditional LLM interface output:\")\n",
    "print(text_response)\n",
    "\n",
    "\n",
    "# Build a short chat history with roles and messages.\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful workshop safety assistant.\"),\n",
    "    HumanMessage(content=\"I bought a new eight foot ladder yesterday.\"),\n",
    "    AIMessage(content=\"Great, always inspect rungs before climbing.\"),\n",
    "    HumanMessage(content=\"Remind me of your earlier ladder safety advice.\"),\n",
    "]\n",
    "\n",
    "# Call the chat model with structured conversation messages.\n",
    "chat_response = chat_llm.invoke(messages)\n",
    "\n",
    "# Print result from chat oriented interface.\n",
    "print(\"\\nChat interface output:\")\n",
    "print(chat_response.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488647d5",
   "metadata": {},
   "source": [
    "### **1.2. Designing Prompt Templates**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec77f94",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master LangChain & Llama 3/Module_01/Lecture_B/image_01_02.jpg?v=1768761017\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Prompt templates are reusable blueprints for instructions\n",
    ">* They separate stable context from changing user inputs\n",
    "\n",
    ">* Templates define roles, goals, and output format\n",
    ">* They reduce ambiguity and simplify iteration across inputs\n",
    "\n",
    ">* Templates connect human intent to LangChain workflows\n",
    ">* They enable chaining, versioning, testing, and reuse\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1f52b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Designing Prompt Templates\n",
    "\n",
    "# Demonstrate simple prompt templates with dynamic placeholders.\n",
    "# Show separating stable instructions from changing user inputs.\n",
    "# Print filled prompts to illustrate reusable template behavior.\n",
    "\n",
    "# pip install langchain openai tiktoken.\n",
    "\n",
    "# Define a stable prompt template with placeholders.\n",
    "template_text = \"You are a helpful tutor. Explain {topic} to a {audience}.\"\n",
    "\n",
    "# Define dynamic values that change between calls.\n",
    "user_topic_one = \"Python lists basics\"\n",
    "user_audience_one = \"high school student\"\n",
    "\n",
    "# Fill the template for the first scenario.\n",
    "filled_prompt_one = template_text.format(topic=user_topic_one, audience=user_audience_one)\n",
    "\n",
    "# Define another set of dynamic values for reuse.\n",
    "user_topic_two = \"neural networks overview\"\n",
    "user_audience_two = \"busy business manager\"\n",
    "\n",
    "# Fill the template again with different values.\n",
    "filled_prompt_two = template_text.format(topic=user_topic_two, audience=user_audience_two)\n",
    "\n",
    "# Print the original template to show structure.\n",
    "print(\"TEMPLATE STRUCTURE:\\n\" + template_text)\n",
    "\n",
    "# Print the first filled prompt to show customization.\n",
    "print(\"\\nFILLED PROMPT ONE:\\n\" + filled_prompt_one)\n",
    "\n",
    "# Print the second filled prompt to show reuse.\n",
    "print(\"\\nFILLED PROMPT TWO:\\n\" + filled_prompt_two)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6b9b5f",
   "metadata": {},
   "source": [
    "### **1.3. Building and Running Chains**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df9b1f5",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master LangChain & Llama 3/Module_01/Lecture_B/image_01_03.jpg?v=1768761037\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Chains connect workflow components into repeatable pipelines\n",
    ">* They wrap prompts and models into one callable\n",
    "\n",
    ">* Chains pass structured data through modular steps\n",
    ">* They coordinate complex workflows without manual orchestration\n",
    "\n",
    ">* Chains run like functions with extra features\n",
    ">* They enable reuse, scaling, integration, and teamwork\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e148b580",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Building and Running Chains\n",
    "\n",
    "# Demonstrate building simple LangChain style chains conceptually using plain Python functions.\n",
    "# Show how data flows through chained processing steps in a repeatable pipeline.\n",
    "# Illustrate replacing manual orchestration with a single callable workflow style function.\n",
    "\n",
    "# Example requires no external libraries, installation comment kept for completeness.\n",
    "# !pip install langchain-community langchain-openai llama-cpp-python.\n",
    "\n",
    "# Define a function that simulates a prompt template expansion step.\n",
    "def build_prompt(user_question, tone_instruction):\n",
    "    template_text = f\"Instruction: {tone_instruction}\\nQuestion: {user_question}\"\n",
    "    return template_text\n",
    "\n",
    "# Define a function that simulates a language model style response.\n",
    "def fake_llm(prompt_text):\n",
    "    answer_text = \"This is a concise helpful answer based on: \" + prompt_text\n",
    "    return answer_text\n",
    "\n",
    "# Define a function that simulates an output formatting parser step.\n",
    "def format_answer(raw_answer):\n",
    "    formatted_text = \"Customer Support Reply: \" + raw_answer\n",
    "    return formatted_text\n",
    "\n",
    "# Define a chain function that connects all previous components sequentially.\n",
    "def support_chain(user_question, tone_instruction):\n",
    "    prompt_text = build_prompt(user_question, tone_instruction)\n",
    "    raw_answer = fake_llm(prompt_text)\n",
    "    final_answer = format_answer(raw_answer)\n",
    "    return final_answer\n",
    "\n",
    "# Define another chain that reuses components but changes control flow slightly.\n",
    "def short_summary_chain(user_question):\n",
    "    prompt_text = build_prompt(user_question, \"Be brief and friendly.\")\n",
    "    raw_answer = fake_llm(prompt_text)\n",
    "    summary_text = raw_answer[:140]\n",
    "    return summary_text\n",
    "\n",
    "# Prepare example inputs that will be passed into both chain functions.\n",
    "user_question = \"My package arrived damaged, what can I do now?\"\n",
    "preferred_tone = \"Be very polite and reassuring in every sentence.\"\n",
    "\n",
    "# Run the support chain and print the single polished answer output.\n",
    "support_reply = support_chain(user_question, preferred_tone)\n",
    "print(\"Full chain reply:\\n\", support_reply)\n",
    "\n",
    "# Run the summary chain and print its shorter processed output result.\n",
    "summary_reply = short_summary_chain(user_question)\n",
    "print(\"\\nSummary chain reply:\\n\", summary_reply)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb20cb95",
   "metadata": {},
   "source": [
    "## **2. First Llama Call**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69f8e41",
   "metadata": {},
   "source": [
    "### **2.1. Connecting Llama API**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08e37b2",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master LangChain & Llama 3/Module_01/Lecture_B/image_02_01.jpg?v=1768761062\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Set up LangChain, Llama provider, and auth\n",
    ">* Use secure API keys or variables before calling\n",
    "\n",
    ">* Configure a LangChain client for specific Llama models\n",
    ">* Swap models later by only changing configuration\n",
    "\n",
    ">* Centralize stable settings for Llama connection configuration\n",
    ">* Shared connection simplifies maintenance, debugging, and scaling\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67222c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Connecting Llama API\n",
    "\n",
    "# Demonstrate connecting LangChain client with a fake Llama style API key.\n",
    "# Show environment variable usage instead of hard coded secret strings.\n",
    "# Print confirmation that configuration and simple test call both succeed.\n",
    "\n",
    "# pip install langchain-community langchain-openai python-dotenv.\n",
    "\n",
    "#import os module for environment variable handling.\n",
    "import os\n",
    "\n",
    "#Create fake API key value for demonstration only.\n",
    "fake_api_key_value = \"llama-3-demo-key-12345\"\n",
    "\n",
    "#Set environment variable that would normally store real secret key.\n",
    "os.environ[\"LLAMA_API_KEY\"] = fake_api_key_value\n",
    "\n",
    "#Read environment variable back to simulate secure configuration usage.\n",
    "configured_key_value = os.getenv(\"LLAMA_API_KEY\", \"missing-key-placeholder\")\n",
    "\n",
    "#Check whether environment variable was correctly configured and accessible.\n",
    "connection_ok = configured_key_value == fake_api_key_value\n",
    "\n",
    "#Prepare dictionary representing minimal LangChain style client configuration.\n",
    "llama_connection_config = {\n",
    "    \"api_key\": configured_key_value,\n",
    "    \"base_url\": \"https://api.llama-provider.example.com/v1\",\n",
    "}\n",
    "\n",
    "#Simulate simple model call using configuration without performing network request.\n",
    "def fake_llama_completion(prompt_text, config_dict):\n",
    "    status_text = \"connected\" if connection_ok else \"not_connected\"\n",
    "    return f\"Status {status_text} for prompt '{prompt_text}'.\"\n",
    "\n",
    "#Define short prompt that would be sent through LangChain to Llama model.\n",
    "prompt_message_text = \"Explain why environment variables protect secret keys.\"\n",
    "\n",
    "#Call fake completion function to mimic LangChain Llama invocation.\n",
    "response_text = fake_llama_completion(prompt_message_text, llama_connection_config)\n",
    "\n",
    "#Print confirmation of connection status and example response text.\n",
    "print(\"Connection configured:\", connection_ok, \"\\nResponse:\", response_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb1cc6c",
   "metadata": {},
   "source": [
    "### **2.2. Basic Completion Demo**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d11b510",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master LangChain & Llama 3/Module_01/Lecture_B/image_02_02.jpg?v=1768761083\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Send a short prompt, receive continued text\n",
    ">* Move from setup to real model interaction\n",
    "\n",
    ">* Use simple, checkable prompts for first completion\n",
    ">* Review output to confirm sensible request–response behavior\n",
    "\n",
    ">* Experiment with varied prompts, tones, and constraints\n",
    ">* Observe strengths, weaknesses, and instruction-following behavior\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664d2918",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Basic Completion Demo\n",
    "\n",
    "# Demonstrate a basic Llama completion using LangChain in Colab.\n",
    "# Show how a simple prompt receives a model generated continuation.\n",
    "# Help you verify the basic request response loop works correctly.\n",
    "\n",
    "# !pip install langchain-openai langchain-core python-dotenv.\n",
    "\n",
    "# Import required classes from LangChain and OpenAI wrapper.\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Import environment helper for reading API keys securely.\n",
    "import os\n",
    "\n",
    "# Explain how to set your OpenAI compatible API key.\n",
    "api_key_instructions = \"Set OPENAI_API_KEY in environment before running this cell.\"\n",
    "\n",
    "# Print short reminder about required environment variable.\n",
    "print(api_key_instructions)\n",
    "\n",
    "# Read the API key from environment variables securely.\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\", \"missing_key_placeholder\")\n",
    "\n",
    "# Create a simple check that warns when key seems missing.\n",
    "if api_key == \"missing_key_placeholder\":\n",
    "\n",
    "    # Print friendly message explaining how to provide the key.\n",
    "    print(\"API key missing, please set OPENAI_API_KEY and rerun script.\")\n",
    "else:\n",
    "\n",
    "    # Initialize the LangChain chat model object for Llama style completions.\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7, api_key=api_key)\n",
    "\n",
    "    # Define a short prompt that requests a clear, verifiable explanation.\n",
    "    prompt_text = \"Explain in three sentences how a household refrigerator works.\"\n",
    "\n",
    "    # Call the model with the prompt and store the response object.\n",
    "    response = llm.invoke(prompt_text)\n",
    "\n",
    "    # Print a separator line to distinguish prompt and model output.\n",
    "    print(\"--- Model completion starts below this line ---\")\n",
    "\n",
    "    # Print the model generated text content for quick inspection.\n",
    "    print(response.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c553f37a",
   "metadata": {},
   "source": [
    "### **2.3. Handling basic errors**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe06601d",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master LangChain & Llama 3/Module_01/Lecture_B/image_02_03.jpg?v=1768761198\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Common errors: connection, configuration, and content\n",
    ">* Use categories to quickly diagnose and fix\n",
    "\n",
    ">* Catch errors and show clear, simple messages\n",
    ">* Hide technical details, keep user experience friendly\n",
    "\n",
    ">* Design scripts to recover from common errors\n",
    ">* Use retries, validation, and clear fail-fast checks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a26e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Handling basic errors\n",
    "\n",
    "# Demonstrate simple error handling around a fake Llama model call.\n",
    "# Show different error categories and friendly user facing messages.\n",
    "# Keep everything beginner friendly and runnable inside Google Colab.\n",
    "\n",
    "# pip install langchain openai llamaapi would appear here if actually required.\n",
    "\n",
    "# Import random for simulating different error situations.\n",
    "import random\n",
    "\n",
    "# Define a custom exception for configuration related problems.\n",
    "class ConfigurationError(Exception):\n",
    "    pass\n",
    "\n",
    "# Define a custom exception for content related problems.\n",
    "class ContentError(Exception):\n",
    "    pass\n",
    "\n",
    "# Define a function that simulates calling a remote Llama model.\n",
    "def fake_llama_call(prompt, model_name):\n",
    "    # Check for empty prompt and raise content related error.\n",
    "    if not prompt.strip():\n",
    "        raise ContentError(\"Prompt is empty and cannot be processed.\")\n",
    "\n",
    "    # Check for obviously wrong model name and raise configuration error.\n",
    "    if model_name != \"llama-3-mini\":\n",
    "        raise ConfigurationError(\"Requested model name is not recognized.\")\n",
    "\n",
    "    # Randomly simulate a connectivity failure using random choice.\n",
    "    if random.choice([False, True]):\n",
    "        raise ConnectionError(\"Network connection to Llama service failed.\")\n",
    "\n",
    "    # If everything passes, return a short pretend completion string.\n",
    "    return \"Pretend Llama response for your helpful prompt.\"\n",
    "\n",
    "# Define a helper function that wraps the fake call with error handling.\n",
    "def safe_llama_request(prompt, model_name):\n",
    "    # Try calling the fake model and catch different error categories.\n",
    "    try:\n",
    "        response = fake_llama_call(prompt, model_name)\n",
    "        print(\"Model call succeeded with response below.\")\n",
    "        print(response)\n",
    "\n",
    "    # Handle connectivity problems with a friendly retry suggestion.\n",
    "    except ConnectionError as error:\n",
    "        print(\"Could not reach model service right now.\")\n",
    "        print(\"Please check internet and try again soon.\")\n",
    "        print(f\"Technical details for logs: {error}\")\n",
    "\n",
    "    # Handle configuration problems like wrong model names or parameters.\n",
    "    except ConfigurationError as error:\n",
    "        print(\"Configuration problem detected with your model settings.\")\n",
    "        print(\"Please verify model name and configuration values.\")\n",
    "        print(f\"Technical details for logs: {error}\")\n",
    "\n",
    "    # Handle content problems like empty prompts or disallowed material.\n",
    "    except ContentError as error:\n",
    "        print(\"There is a problem with the provided prompt content.\")\n",
    "        print(\"Please adjust the text and avoid restricted material.\")\n",
    "        print(f\"Technical details for logs: {error}\")\n",
    "\n",
    "    # Handle any completely unexpected error types gracefully for users.\n",
    "    except Exception as error:\n",
    "        print(\"An unexpected error occurred during the model call.\")\n",
    "        print(\"Please try again or contact the system maintainer.\")\n",
    "        print(f\"Technical details for logs: {error}\")\n",
    "\n",
    "# Define example inputs that demonstrate different error handling paths.\n",
    "prompt_text = \"Write a short two sentence story about a ten foot rocket.\" \n",
    "\n",
    "# Call the safe request function using a correct model name example.\n",
    "safe_llama_request(prompt_text, \"llama-3-mini\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20f4e25",
   "metadata": {},
   "source": [
    "## **3. Configuring Llama Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52547edd",
   "metadata": {},
   "source": [
    "### **3.1. Temperature and randomness**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5a583c",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master LangChain & Llama 3/Module_01/Lecture_B/image_03_01.jpg?v=1768761225\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Temperature controls response randomness versus predictability\n",
    ">* Low suits stable tasks; high suits creativity\n",
    "\n",
    ">* Low temperature gives stable, repeatable, reliable answers\n",
    ">* High temperature increases creativity, variety, and surprise\n",
    "\n",
    ">* Treat temperature as a tunable workflow knob\n",
    ">* Test tasks, adjust temperature for reliability versus creativity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbe9576",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Temperature and randomness\n",
    "\n",
    "# Demonstrate temperature randomness using simple random word choices.\n",
    "# Show how low temperature feels predictable and stable.\n",
    "# Show how high temperature feels creative and surprising.\n",
    "\n",
    "# pip install langchain llama-cpp-python transformers accelerate.\n",
    "\n",
    "#import random module for random number generation.\n",
    "import random\n",
    "\n",
    "#prepare two word lists representing safe and creative choices.\n",
    "safe_words = [\"calm\", \"clear\", \"simple\", \"direct\", \"plain\"]\n",
    "creative_words = [\"whimsical\", \"sparkling\", \"twisted\", \"unexpected\", \"wild\"]\n",
    "\n",
    "#define a function that simulates temperature controlled word picking.\n",
    "def pick_word(temperature):\n",
    "    #generate random number between zero and one.\n",
    "    r = random.random()\n",
    "    #low temperature favors safe predictable words strongly.\n",
    "    if temperature < 0.3 and r < 0.9:\n",
    "        return random.choice(safe_words)\n",
    "    #high temperature favors creative surprising words strongly.\n",
    "    if temperature > 0.7 and r < 0.9:\n",
    "        return random.choice(creative_words)\n",
    "    #otherwise mix both lists for moderate randomness.\n",
    "    return random.choice(safe_words + creative_words)\n",
    "\n",
    "#set a fixed seed for reproducible demonstration runs.\n",
    "random.seed(42)\n",
    "\n",
    "#choose a simple base sentence for comparison.\n",
    "base_sentence = \"The evening breeze feels\"\n",
    "\n",
    "#print example with low temperature showing stable predictable behavior.\n",
    "low_temp_sentence = base_sentence + \" \" + pick_word(0.1)\n",
    "\n",
    "#print example with medium temperature showing balanced behavior.\n",
    "mid_temp_sentence = base_sentence + \" \" + pick_word(0.5)\n",
    "\n",
    "#print example with high temperature showing creative behavior.\n",
    "high_temp_sentence = base_sentence + \" \" + pick_word(0.9)\n",
    "\n",
    "#print all three sentences to compare randomness effects.\n",
    "print(\"Low temperature example:\", low_temp_sentence)\n",
    "print(\"Medium temperature example:\", mid_temp_sentence)\n",
    "print(\"High temperature example:\", high_temp_sentence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c77e25",
   "metadata": {},
   "source": [
    "### **3.2. Max tokens and truncation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780ceaeb",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master LangChain & Llama 3/Module_01/Lecture_B/image_03_02.jpg?v=1768761251\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Max tokens limits response length and token usage\n",
    ">* Impacts cost, speed, and response detail level\n",
    "\n",
    ">* Long prompts can exceed the model’s context\n",
    ">* Truncation risks missing information and misleading outputs\n",
    "\n",
    ">* Adjust max tokens to match task length\n",
    ">* Monitor truncation and tune settings from usage\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f651278f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Max tokens and truncation\n",
    "\n",
    "# Demonstrate max tokens limiting response length clearly.\n",
    "# Show truncation when context becomes too large.\n",
    "# Compare short and long outputs side by side.\n",
    "\n",
    "# pip install langchain-openai.\n",
    "# pip install langchain-community.\n",
    "# pip install python-dotenv.\n",
    "\n",
    "# Import required standard and external libraries.\n",
    "import os\n",
    "from textwrap import shorten\n",
    "\n",
    "# Import LangChain OpenAI chat model wrapper.\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Set fake API key placeholder for demonstration.\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"your_api_key_here\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.environ.get(\"OPENAI_API_KEY\", \"test_key\")\n",
    "\n",
    "# Create model with small max tokens for short answer.\n",
    "short_model = ChatOpenAI(model=\"gpt-4o-mini\", max_tokens=20, temperature=0.2)\n",
    "\n",
    "# Create model with larger max tokens for longer answer.\n",
    "long_model = ChatOpenAI(model=\"gpt-4o-mini\", max_tokens=120, temperature=0.2)\n",
    "\n",
    "# Define a prompt asking for detailed explanation.\n",
    "prompt = \"Explain how a car engine works using simple everyday language.\"\n",
    "\n",
    "# Get short truncated style answer from small max tokens.\n",
    "# short_answer = short_model.invoke(prompt).content\n",
    "short_answer = \"This is a short example answer about how a car engine works.\"\n",
    "\n",
    "# Get longer detailed answer from larger max tokens.\n",
    "# long_answer = long_model.invoke(prompt).content\n",
    "long_answer = (\n",
    "    \"A car engine burns a mixture of fuel and air inside metal cylinders. \"\n",
    "    \"Small explosions push pistons up and down, turning the crankshaft, \"\n",
    "    \"which eventually turns the wheels through the transmission.\"\n",
    ")\n",
    "\n",
    "# Helper function to safely preview text length.\n",
    "def preview(text, width):\n",
    "    return shorten(text.replace(\"\\n\", \" \"), width=width, placeholder=\"...\")\n",
    "\n",
    "# Print both answers showing different lengths clearly.\n",
    "print(\"SHORT ANSWER (max_tokens=20):\", preview(short_answer, 140))\n",
    "\n",
    "# Print separator line for readability.\n",
    "print(\"\\nLONG ANSWER (max_tokens=120):\", preview(long_answer, 140))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517887c9",
   "metadata": {},
   "source": [
    "### **3.3. Choosing Llama 3 Variants**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74dc998",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master LangChain & Llama 3/Module_01/Lecture_B/image_03_03.jpg?v=1768761298\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Smaller Llama models are faster and cheaper\n",
    ">* Larger models handle complex, nuanced tasks better\n",
    "\n",
    ">* Match each Llama variant to its use case\n",
    ">* Balance speed, volume, accuracy, and user expectations\n",
    "\n",
    ">* Iteratively test multiple Llama variants on tasks\n",
    ">* Use metrics and tiered models to balance tradeoffs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b051acdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Choosing Llama 3 Variants\n",
    "\n",
    "# Demonstrate choosing different Llama variants conceptually using simple scoring simulation.\n",
    "# Compare small fast model versus large accurate model for different task difficulty levels.\n",
    "# Show how task type influences which simulated model variant is preferable.\n",
    "\n",
    "# pip install langchain openai llama-cpp-python transformers accelerate.\n",
    "\n",
    "# Import random module for simple score variability simulation.\n",
    "import random\n",
    "\n",
    "# Define function that simulates small model performance scores for tasks.\n",
    "def simulate_small_model_score(task_difficulty):\n",
    "    # Use base score higher for easy tasks, lower for hard tasks.\n",
    "    base_scores = {\"easy\": 85, \"medium\": 70, \"hard\": 55}\n",
    "    # Add random noise to base score for realism and variability.\n",
    "    noise = random.randint(-5, 5)\n",
    "    # Return final clamped score between zero and one hundred.\n",
    "    return max(0, min(100, base_scores[task_difficulty] + noise))\n",
    "\n",
    "# Define function that simulates large model performance scores for tasks.\n",
    "def simulate_large_model_score(task_difficulty):\n",
    "    # Use base score moderate for easy tasks, very strong for hard tasks.\n",
    "    base_scores = {\"easy\": 88, \"medium\": 90, \"hard\": 95}\n",
    "    # Add random noise to base score for realism and variability.\n",
    "    noise = random.randint(-3, 3)\n",
    "    # Return final clamped score between zero and one hundred.\n",
    "    return max(0, min(100, base_scores[task_difficulty] + noise))\n",
    "\n",
    "# Define list describing example tasks with difficulty and description.\n",
    "tasks = [\n",
    "    (\"easy\", \"Tag short customer questions by topic for quick routing.\"),\n",
    "    (\"medium\", \"Summarize a one page product review for marketing.\"),\n",
    "    (\"hard\", \"Analyze multi page legal style contract for risky clauses.\"),\n",
    "]\n",
    "\n",
    "# Print header explaining comparison between conceptual model variants.\n",
    "print(\"Comparing small versus large Llama style variants for tasks.\\n\")\n",
    "\n",
    "# Loop through tasks and simulate scores for both model variants.\n",
    "for difficulty, description in tasks:\n",
    "    # Compute simulated scores for small and large model variants.\n",
    "    small_score = simulate_small_model_score(difficulty)\n",
    "    large_score = simulate_large_model_score(difficulty)\n",
    "    # Print task description and difficulty label for context.\n",
    "    print(f\"Task difficulty: {difficulty.upper()} - {description}\")\n",
    "    # Print simulated scores showing relative model performance differences.\n",
    "    print(f\"Small variant score: {small_score} versus large variant score: {large_score}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35aa9df",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**LangChain Basics**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e08f66",
   "metadata": {},
   "source": [
    "\n",
    "In this lecture, you learned to:\n",
    "- Explain the main LangChain abstractions and how they relate to each other. \n",
    "- Create a minimal LangChain script that calls a Llama 3 model. \n",
    "- Configure basic settings such as temperature, max tokens, and model selection for Llama 3 within LangChain. \n",
    "\n",
    "In the next Lecture (Lecture C), we will go over 'Environment Setup'"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
