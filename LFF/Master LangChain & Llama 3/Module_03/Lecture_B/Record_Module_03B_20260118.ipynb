{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bd77334",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Conversational Memory**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73468b2",
   "metadata": {},
   "source": [
    ">Last update: 20260118.\n",
    "    \n",
    "By the end of this Lecture, you will be able to:\n",
    "- Explain how LangChain memory mechanisms maintain conversational context for Llama 3. \n",
    "- Configure and use different memory types to build a stateful Llama 3 chatbot. \n",
    "- Tune memory behavior to balance context completeness, token limits, and privacy concerns. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f23dec",
   "metadata": {},
   "source": [
    "## **1. Conversational Memory Basics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2114c489",
   "metadata": {},
   "source": [
    "### **1.1. Storing Conversation History**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8e547a",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master LangChain & Llama 3/Module_03/Lecture_B/image_01_01.jpg?v=1768767607\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Memory stores past messages for ongoing context\n",
    ">* Without history, multi-step conversations break down\n",
    "\n",
    ">* LangChain stores role-tagged message pairs chronologically\n",
    ">* Structured history enables recall, summarization, and reuse\n",
    "\n",
    ">* Different apps need different conversation memory strategies\n",
    ">* Organized history creates rich, persistent context for Llama\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f07c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Storing Conversation History\n",
    "\n",
    "# Demonstrate simple conversation history storage conceptually with Python lists.\n",
    "# Show how each conversational turn gets appended and preserved over time.\n",
    "# Print final history to illustrate persistent conversational context clearly.\n",
    "\n",
    "# pip install langchain llama-cpp-python transformers accelerate bitsandbytes.\n",
    "\n",
    "# Define a function that simulates storing conversation history.\n",
    "def simulate_conversation_history():\n",
    "    # Initialize an empty list representing stored conversation history.\n",
    "    history = []\n",
    "    \n",
    "    # Append first user message and assistant reply as a structured pair.\n",
    "    history.append({\"role\": \"user\", \"content\": \"Hi, I need thermostat help.\"})\n",
    "    history.append({\"role\": \"assistant\", \"content\": \"Sure, what temperature currently?\"})\n",
    "    \n",
    "    # Append second user message and assistant reply, continuing the conversation.\n",
    "    history.append({\"role\": \"user\", \"content\": \"House feels cold, around sixty degrees.\"})\n",
    "    history.append({\"role\": \"assistant\", \"content\": \"Try setting seventy two degrees.\"})\n",
    "    \n",
    "    # Append third user message referencing earlier context implicitly.\n",
    "    history.append({\"role\": \"user\", \"content\": \"Same issue again today, still freezing.\"})\n",
    "    history.append({\"role\": \"assistant\", \"content\": \"Let me review previous thermostat details.\"})\n",
    "    \n",
    "    # Return the complete stored history list for later inspection.\n",
    "    return history\n",
    "\n",
    "# Call the simulation function and capture the stored conversation history.\n",
    "conversation_history = simulate_conversation_history()\n",
    "\n",
    "# Print a concise view of the stored conversation history for understanding.\n",
    "for message in conversation_history:\n",
    "    print(f\"Role: {message['role']}, Content: {message['content']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d9f606",
   "metadata": {},
   "source": [
    "### **1.2. Prompt Context Injection**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77019c5",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master LangChain & Llama 3/Module_03/Lecture_B/image_01_02.jpg?v=1768767632\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Memory injects past messages into new prompts\n",
    ">* This lets Llama 3 understand earlier references\n",
    "\n",
    ">* Memory uses templates to structure past messages\n",
    ">* Organized context helps Llama 3 continue conversations\n",
    "\n",
    ">* Memory selects and condenses only relevant history\n",
    ">* Gives Llama 3 focused context for coherent replies\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6c8b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Prompt Context Injection\n",
    "\n",
    "# Demonstrate how stored conversation history becomes injected prompt context.\n",
    "# Show simple memory storing user and assistant conversational turns.\n",
    "# Build final prompt sent into a pretend Llama three model.\n",
    "\n",
    "# pip install langchain langchain-community llama-cpp-python.\n",
    "\n",
    "# Define a simple conversation history list with dictionaries.\n",
    "conversation_history = [\n",
    "    {\"role\": \"user\", \"content\": \"Hi, I need exam help.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Sure, which subject exactly?\"},\n",
    "    {\"role\": \"user\", \"content\": \"Physics and some basic algebra.\"}\n",
    "]\n",
    "\n",
    "# Define a new user message that references earlier conversation.\n",
    "new_user_message = \"Can you quiz me on the topics we listed earlier?\"\n",
    "\n",
    "# Define a simple system instruction describing assistant behavior.\n",
    "system_instruction = \"You are a helpful exam study assistant focusing on clear short questions.\"\n",
    "\n",
    "# Define a function that injects context into a single prompt string.\n",
    "def build_injected_prompt(system_text, history, new_message):\n",
    "    prompt_lines = []\n",
    "    prompt_lines.append(f\"System: {system_text}\")\n",
    "    for turn in history:\n",
    "        role = turn[\"role\"].capitalize()\n",
    "        content = turn[\"content\"]\n",
    "        prompt_lines.append(f\"{role}: {content}\")\n",
    "    prompt_lines.append(f\"User: {new_message}\")\n",
    "    full_prompt = \"\\n\".join(prompt_lines)\n",
    "    return full_prompt\n",
    "\n",
    "# Build the final prompt that would be sent into Llama three.\n",
    "final_prompt = build_injected_prompt(system_instruction, conversation_history, new_user_message)\n",
    "\n",
    "# Print the final injected prompt showing context clearly.\n",
    "print(\"Injected prompt sent into model:\\n\")\n",
    "print(final_prompt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60edad7",
   "metadata": {},
   "source": [
    "### **1.3. Context Limits in Chat**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab8a888",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master LangChain & Llama 3/Module_03/Lecture_B/image_01_03.jpg?v=1768767659\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Chatbots see only a limited context window\n",
    ">* Stored history exceeds what Llama 3 actually reads\n",
    "\n",
    ">* Context limits force LangChain to trim history\n",
    ">* Poor selection can drop key details, hurting continuity\n",
    "\n",
    ">* LangChain compresses chat history into key summaries\n",
    ">* Designers choose what to keep, summarize, discard\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f01a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Context Limits in Chat\n",
    "\n",
    "# Demonstrate chat context limits using a tiny sliding window simulation.\n",
    "# Show how only recent conversation fits inside a fixed context window.\n",
    "# Illustrate why older details disappear when the window becomes full.\n",
    "\n",
    "# !pip install some_required_library_here if external libraries were needed.\n",
    "\n",
    "# Define a simple conversation history as user and assistant message pairs.\n",
    "conversation_history = [\n",
    "    (\"User\", \"Hi, I need travel help to New York.\"),\n",
    "    (\"Assistant\", \"Sure, what dates are you considering for your trip.\"),\n",
    "    (\"User\", \"I prefer late June, maybe around the twenty fifth.\"),\n",
    "    (\"Assistant\", \"Great, do you prefer morning or evening flights.\"),\n",
    "    (\"User\", \"Evening flights only, I hate early mornings.\"),\n",
    "    (\"Assistant\", \"Noted, evening flights only for your New York trip.\"),\n",
    "    (\"User\", \"My budget is eight hundred dollars maximum for everything.\"),\n",
    "    (\"Assistant\", \"Okay, I will search options under eight hundred dollars total.\"),\n",
    "]\n",
    "\n",
    "# Define a tiny context window measured in number of recent messages kept.\n",
    "context_window_size = 4\n",
    "\n",
    "# Function that selects only the most recent messages within the window.\n",
    "def get_visible_context(history, window_size):\n",
    "    visible_slice = history[-window_size:]\n",
    "    return visible_slice\n",
    "\n",
    "# Show full stored history length versus visible context length.\n",
    "print(\"Total stored messages in history:\", len(conversation_history))\n",
    "print(\"Messages visible to model with window:\", context_window_size)\n",
    "\n",
    "# Get the visible context that would be sent inside the prompt.\n",
    "visible_context = get_visible_context(conversation_history, context_window_size)\n",
    "\n",
    "# Print the visible context to show which messages remain accessible.\n",
    "print(\"\\nVisible context before new user question:\")\n",
    "for speaker, message in visible_context:\n",
    "    print(f\"{speaker}: {message}\")\n",
    "\n",
    "# Simulate a new user question that depends on an older forgotten detail.\n",
    "new_user_message = (\"User\", \"Can you remind me my maximum budget again.\")\n",
    "conversation_history.append(new_user_message)\n",
    "\n",
    "# Recalculate visible context after adding the new question message.\n",
    "visible_context_after = get_visible_context(conversation_history, context_window_size)\n",
    "\n",
    "# Print the new visible context and highlight missing earlier budget detail.\n",
    "print(\"\\nVisible context after new question:\")\n",
    "for speaker, message in visible_context_after:\n",
    "    print(f\"{speaker}: {message}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5761f0a1",
   "metadata": {},
   "source": [
    "## **2. Conversational Memory Types**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce6ff38",
   "metadata": {},
   "source": [
    "### **2.1. Buffer Versus Summary Memory**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec0a44b",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master LangChain & Llama 3/Module_03/Lecture_B/image_02_01.jpg?v=1768767686\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Buffer memory stores the full conversation transcript\n",
    ">* Summary memory keeps a compressed narrative of history\n",
    "\n",
    ">* Buffer memory preserves exact wording and tone\n",
    ">* Long buffers increase token use and latency\n",
    "\n",
    ">* Summary memory condenses long chats into evolving notes\n",
    ">* Combining buffer and summary balances detail and scalability\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863f2e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Buffer Versus Summary Memory\n",
    "\n",
    "# Demonstrate buffer memory versus summary memory with simple conversation examples.\n",
    "# Show how full transcripts differ from compact evolving summaries for chatbots.\n",
    "# Help beginners visualize memory tradeoffs without external libraries or real models.\n",
    "\n",
    "# pip install langchain llama-index transformers accelerate bitsandbytes.\n",
    "\n",
    "# Define a simple conversation as ordered user and assistant message pairs.\n",
    "conversation = [\n",
    "    (\"User\", \"Hi, I need help planning a road trip.\"),\n",
    "    (\"Assistant\", \"Great, where are you starting from today.\"),\n",
    "    (\"User\", \"I live in Denver and I drive a small SUV.\"),\n",
    "    (\"Assistant\", \"Noted, Denver start with a small SUV vehicle detail.\"),\n",
    "    (\"User\", \"I want to visit national parks within five hundred miles.\"),\n",
    "    (\"Assistant\", \"Okay, we will focus on nearby national parks within that driving range.\"),\n",
    "]\n",
    "\n",
    "# Implement buffer memory that keeps the full detailed transcript unchanged.\n",
    "def build_buffer_memory(conversation_pairs):\n",
    "    buffer_lines = []\n",
    "    for speaker, text in conversation_pairs:\n",
    "        buffer_lines.append(f\"{speaker}: {text}\")\n",
    "    return \"\\n\".join(buffer_lines)\n",
    "\n",
    "# Implement summary memory that stores only key facts and preferences.\n",
    "def build_summary_memory(conversation_pairs):\n",
    "    summary = \"User plans a Denver road trip using a small SUV vehicle.\"  \\\n",
    "              \" User prefers national parks within roughly five hundred miles driving distance.\"\n",
    "    return summary\n",
    "\n",
    "# Build both memory representations from the same conversation history.\n",
    "buffer_memory = build_buffer_memory(conversation)\n",
    "summary_memory = build_summary_memory(conversation)\n",
    "\n",
    "# Print a clear header and the buffer memory transcript content.\n",
    "print(\"BUFFER MEMORY TRANSCRIPT:\\n\")\n",
    "print(buffer_memory)\n",
    "\n",
    "# Print a separator line to visually distinguish the two memory styles.\n",
    "print(\"\\n\" + \"-\" * 40 + \"\\n\")\n",
    "\n",
    "# Print a clear header and the summary memory compact description.\n",
    "print(\"SUMMARY MEMORY DESCRIPTION:\\n\")\n",
    "print(summary_memory)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ed7acc",
   "metadata": {},
   "source": [
    "### **2.2. Key Value Memory Patterns**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e94ad5",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master LangChain & Llama 3/Module_03/Lecture_B/image_02_02.jpg?v=1768767711\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Store facts as labeled keys for retrieval\n",
    ">* Use selected values in prompts for stateful chatbots\n",
    "\n",
    ">* Profile-style keys store and update user preferences\n",
    ">* Structured memory enables personalized, debuggable chatbot responses\n",
    "\n",
    ">* Choose scoped, lasting keys for user state\n",
    ">* Balance helpful memory with privacy and auditability\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6b672e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Key Value Memory Patterns\n",
    "\n",
    "# Demonstrate simple key value memory for chatbot preferences.\n",
    "# Show how user profile updates across multiple conversation turns.\n",
    "# Print final profile and personalized response using stored values.\n",
    "# pip install langchain llama-cpp-python transformers accelerate.\n",
    "\n",
    "# Create an empty dictionary representing user memory profile.\n",
    "user_memory_profile = {\"user_name\": None, \"home_airport\": None, \"seat_preference\": None}\n",
    "\n",
    "# Define a function that updates memory using extracted key value pairs.\n",
    "def update_memory_with_facts(memory_dict, new_facts_dict):\n",
    "    for fact_key, fact_value in new_facts_dict.items():\n",
    "        memory_dict[fact_key] = fact_value\n",
    "\n",
    "# Simulate first conversation turn where user shares basic information.\n",
    "turn_one_facts = {\"user_name\": \"Alex\", \"home_airport\": \"JFK\", \"seat_preference\": \"aisle\"}\n",
    "\n",
    "# Update memory profile using first turn extracted facts.\n",
    "update_memory_with_facts(user_memory_profile, turn_one_facts)\n",
    "\n",
    "# Simulate second conversation turn where user changes seat preference.\n",
    "turn_two_facts = {\"seat_preference\": \"window\"}\n",
    "\n",
    "# Update memory profile again, overwriting outdated preference value.\n",
    "update_memory_with_facts(user_memory_profile, turn_two_facts)\n",
    "\n",
    "# Build a prompt snippet using only relevant memory keys.\n",
    "prompt_context = f\"Traveler {user_memory_profile['user_name']} flies from {user_memory_profile['home_airport']} preferring {user_memory_profile['seat_preference']} seats.\"\n",
    "\n",
    "# Print the structured memory profile dictionary for inspection.\n",
    "print(\"Current user memory profile:\", user_memory_profile)\n",
    "\n",
    "# Print the prompt context that would be sent into Llama model.\n",
    "print(\"Prompt context for Llama three:\", prompt_context)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc28ff9",
   "metadata": {},
   "source": [
    "### **2.3. Selecting Memory Approaches**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e53fd6",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master LangChain & Llama 3/Module_03/Lecture_B/image_02_03.jpg?v=1768767736\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Match memory design to chatbot interaction needs\n",
    ">* Separate short-term context from longer-term user details\n",
    "\n",
    ">* Match memory type to conversation length, structure\n",
    ">* Use buffer, summary, or key-value based risks\n",
    "\n",
    ">* Combine buffer, summary, and key-value memories thoughtfully\n",
    ">* Tune memory settings using constraints, testing, and privacy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2cc940",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Selecting Memory Approaches\n",
    "\n",
    "# Demonstrate choosing memory approaches for different chatbot interaction needs.\n",
    "# Compare buffer, summary, and key value memory behaviors in one script.\n",
    "# Show how mixing memory types changes chatbot responses and stored information.\n",
    "# pip install langchain langchain-community langchain-openai.\n",
    "\n",
    "# Import required standard libraries for simple chatbot simulation.\n",
    "import textwrap\n",
    "\n",
    "# Define a simple buffer memory storing recent conversational turns exactly.\n",
    "class BufferMemory:\n",
    "    def __init__(self, max_turns):\n",
    "        self.max_turns = max_turns\n",
    "        self.turns = []\n",
    "\n",
    "    # Add a new turn and keep only recent turns within limit.\n",
    "    def add_turn(self, user, bot):\n",
    "        self.turns.append((user, bot))\n",
    "        self.turns = self.turns[-self.max_turns :]\n",
    "\n",
    "    # Build a context string showing recent conversation history.\n",
    "    def get_context(self):\n",
    "        lines = [f\"User: {u} | Bot: {b}\" for u, b in self.turns]\n",
    "        return \" | \".join(lines)\n",
    "\n",
    "# Define a simple summary memory storing compressed conversation description.\n",
    "class SummaryMemory:\n",
    "    def __init__(self):\n",
    "        self.summary = \"\"\n",
    "\n",
    "    # Update summary using a naive rule based concatenation approach.\n",
    "    def update(self, user, bot):\n",
    "        snippet = f\"User asked about {user[:20]} and bot replied.\"\n",
    "        if not self.summary:\n",
    "            self.summary = snippet\n",
    "        else:\n",
    "            self.summary += \" Then \" + snippet\n",
    "\n",
    "    # Return current summary describing earlier conversation parts.\n",
    "    def get_summary(self):\n",
    "        return self.summary\n",
    "\n",
    "# Define a key value memory storing stable user attributes explicitly.\n",
    "class KeyValueMemory:\n",
    "    def __init__(self):\n",
    "        self.store = {}\n",
    "\n",
    "    # Set a stable attribute like name or preference value.\n",
    "    def set(self, key, value):\n",
    "        self.store[key] = value\n",
    "\n",
    "    # Retrieve a stored attribute with default fallback value.\n",
    "    def get(self, key, default=None):\n",
    "        return self.store.get(key, default)\n",
    "\n",
    "# Define a simple rule based chatbot using different memory types.\n",
    "class SimpleChatbot:\n",
    "    def __init__(self, buffer_memory, summary_memory, kv_memory):\n",
    "        self.buffer = buffer_memory\n",
    "        self.summary = summary_memory\n",
    "        self.kv = kv_memory\n",
    "\n",
    "    # Generate a response using memory choices and simple rules.\n",
    "    def respond(self, user_message):\n",
    "        lower = user_message.lower()\n",
    "        if \"my name is\" in lower:\n",
    "            name = user_message.split(\"is\")[-1].strip()\n",
    "            self.kv.set(\"name\", name)\n",
    "            bot = f\"Nice meeting you {name}, I will remember your name.\"\n",
    "        elif \"favorite job\" in lower:\n",
    "            name = self.kv.get(\"name\", \"there\")\n",
    "            bot = f\"{name}, your favorite job seems related to helping people grow.\"\n",
    "        elif \"recap\" in lower:\n",
    "            bot = f\"Conversation summary so far: {self.summary.get_summary()}\"\n",
    "        else:\n",
    "            bot = \"I am a simple coach, tell me goals or ask career questions.\"\n",
    "        self.buffer.add_turn(user_message, bot)\n",
    "        self.summary.update(user_message, bot)\n",
    "        return bot\n",
    "\n",
    "# Helper function printing a titled block with wrapped text lines.\n",
    "def print_block(title, text):\n",
    "    print(f\"\\n{title}:\")\n",
    "    for line in textwrap.wrap(text, width=70):\n",
    "        print(line)\n",
    "\n",
    "# Create memory objects representing different time horizons and structures.\n",
    "buffer_memory = BufferMemory(max_turns=2)\n",
    "summary_memory = SummaryMemory()\n",
    "kv_memory = KeyValueMemory()\n",
    "\n",
    "# Create chatbot instance combining buffer, summary, and key value memories.\n",
    "chatbot = SimpleChatbot(buffer_memory, summary_memory, kv_memory)\n",
    "\n",
    "# Simulate several user messages representing a longer coaching interaction.\n",
    "messages = [\n",
    "    \"My name is Alex.\",\n",
    "    \"I want a new job helping people learn skills.\",\n",
    "    \"What would be my favorite job path maybe.\",\n",
    "    \"Can you recap our conversation so far briefly please.\",\n",
    "]\n",
    "\n",
    "# Run conversation, capturing last response and printing key memory views.\n",
    "last_response = \"\"\n",
    "for msg in messages:\n",
    "    last_response = chatbot.respond(msg)\n",
    "\n",
    "# Show how buffer memory keeps only recent conversational turns exactly.\n",
    "print_block(\"Buffer memory recent turns\", buffer_memory.get_context())\n",
    "\n",
    "# Show how summary memory keeps compressed longer horizon conversational information.\n",
    "print_block(\"Summary memory narrative\", summary_memory.get_summary())\n",
    "\n",
    "# Show how key value memory keeps stable user attributes like remembered name.\n",
    "print_block(\"Key value memory snapshot\", str(kv_memory.store))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be7d675",
   "metadata": {},
   "source": [
    "## **3. Optimizing Chat Memory**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908bb969",
   "metadata": {},
   "source": [
    "### **3.1. Context Window Tradeoffs**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c8cb75",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master LangChain & Llama 3/Module_03/Lecture_B/image_03_01.jpg?v=1768767770\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Choosing context window size shapes conversation quality\n",
    ">* More history improves coherence but increases token costs\n",
    "\n",
    ">* Short chats can include nearly all history\n",
    ">* Long chats need selective history and summaries\n",
    "\n",
    ">* Prioritize the most useful past details first\n",
    ">* Balance context depth with tokens, privacy, compliance\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c7a02b",
   "metadata": {},
   "source": [
    "### **3.2. Conversation Summarization Strategies**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b65e763",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master LangChain & Llama 3/Module_03/Lecture_B/image_03_02.jpg?v=1768767788\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Summaries replace full transcripts to save context\n",
    ">* Keep key goals and decisions within token limits\n",
    "\n",
    ">* Use incremental or hierarchical summaries to track context\n",
    ">* Give summaries clear focus on key user details\n",
    "\n",
    ">* Continuously update summaries to reflect latest information\n",
    ">* Separate stable facts, session details, and discard small talk\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694425bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Conversation Summarization Strategies\n",
    "\n",
    "# Demonstrate simple conversation summarization strategies with incremental updates.\n",
    "# Show how summaries stay compact while preserving important conversational information.\n",
    "# Compare full conversation history versus evolving concise summaries for a chatbot.\n",
    "# pip install langchain llama-index transformers accelerate bitsandbytes.\n",
    "\n",
    "# Define a sample conversation with several user and assistant turns.\n",
    "conversation_turns = [\n",
    "    (\"user\", \"I want a road trip from Boston to New York this weekend.\"),\n",
    "    (\"assistant\", \"Great, do you prefer fast highways or scenic back roads.\"),\n",
    "    (\"user\", \"Scenic back roads please, and I hate driving at night.\"),\n",
    "    (\"assistant\", \"Got it, scenic daytime route, avoiding late night driving as requested.\"),\n",
    "    (\"user\", \"My budget is around three hundred dollars for gas and motels.\"),\n",
    "    (\"assistant\", \"Okay, I will keep costs under three hundred dollars including motels.\"),\n",
    "    (\"user\", \"Actually, I can spend more because this is our anniversary trip.\"),\n",
    "    (\"assistant\", \"Understood, anniversary trip, comfort prioritized over strict budget constraints now.\"),\n",
    "]\n",
    "\n",
    "# Define a function that creates a compact summary from conversation turns.\n",
    "def summarize_conversation(turns, previous_summary=None):\n",
    "    important_preferences = []\n",
    "    goals = []\n",
    "    constraints = []\n",
    "\n",
    "    # Extract simple goals, preferences, and constraints from user messages.\n",
    "    for role, message in turns:\n",
    "        if role == \"user\":\n",
    "            lower_message = message.lower()\n",
    "            if \"road trip\" in lower_message:\n",
    "                goals.append(\"Plan pleasant Boston to New York weekend road trip.\")\n",
    "            if \"scenic\" in lower_message:\n",
    "                important_preferences.append(\"Prefers scenic back roads over fastest highways.\")\n",
    "            if \"hate driving at night\" in lower_message:\n",
    "                constraints.append(\"Avoid night driving whenever reasonably possible.\")\n",
    "            if \"budget\" in lower_message and \"three hundred\" in lower_message:\n",
    "                constraints.append(\"Initial budget around three hundred dollars for trip costs.\")\n",
    "            if \"anniversary\" in lower_message:\n",
    "                important_preferences.append(\"Trip is anniversary celebration, comfort more important than savings.\")\n",
    "\n",
    "    # Remove outdated budget constraint when anniversary preference appears.\n",
    "    if any(\"anniversary\" in p.lower() for p in important_preferences):\n",
    "        constraints = [c for c in constraints if \"Initial budget\" not in c]\n",
    "\n",
    "    # Build a new concise summary string from extracted elements.\n",
    "    summary_parts = []\n",
    "    if goals:\n",
    "        summary_parts.append(\"Goal: \" + \" \".join(sorted(set(goals))))\n",
    "    if important_preferences:\n",
    "        summary_parts.append(\"Preferences: \" + \" \".join(sorted(set(important_preferences))))\n",
    "    if constraints:\n",
    "        summary_parts.append(\"Constraints: \" + \" \".join(sorted(set(constraints))))\n",
    "\n",
    "    new_summary = \" | \".join(summary_parts) if summary_parts else \"No important details captured yet.\"\n",
    "\n",
    "    # Combine with previous summary to simulate incremental summarization behavior.\n",
    "    if previous_summary and previous_summary != new_summary:\n",
    "        combined = previous_summary + \" THEN UPDATED TO \" + new_summary\n",
    "        return combined\n",
    "    return new_summary\n",
    "\n",
    "# Simulate incremental summarization after several conversation turns.\n",
    "running_summary = None\n",
    "for index in range(2, len(conversation_turns) + 1, 2):\n",
    "    current_slice = conversation_turns[:index]\n",
    "    running_summary = summarize_conversation(current_slice, running_summary)\n",
    "\n",
    "# Show final full conversation length versus compact summary length.\n",
    "full_history_text = \" \".join(message for _, message in conversation_turns)\n",
    "print(\"Full history characters:\", len(full_history_text))\n",
    "print(\"Summary characters:\", len(running_summary))\n",
    "print(\"\\nFinal incremental summary:\\n\", running_summary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a23c59",
   "metadata": {},
   "source": [
    "### **3.3. Protecting Sensitive Chat Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f69b49",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master LangChain & Llama 3/Module_03/Lecture_B/image_03_03.jpg?v=1768767823\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Chat memory quietly stores sensitive personal information\n",
    ">* Treat memory like regulated, long-term data storage\n",
    "\n",
    ">* Store only essential context, discard unnecessary details\n",
    ">* Use summaries and expirations to limit sensitive data\n",
    "\n",
    ">* Use technical controls to secure stored memories\n",
    ">* Set clear policies for consent, retention, deletion\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5e79cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Protecting Sensitive Chat Data\n",
    "\n",
    "# Demonstrates simple redaction of sensitive chat fields before storing memory.\n",
    "# Shows how to keep helpful context while masking private identifiers.\n",
    "# Illustrates safer conversational memory design using basic Python structures.\n",
    "# pip install commands are not required because this script uses only standard libraries.\n",
    "\n",
    "# Define example chat messages containing potentially sensitive information.\n",
    "messages = [\n",
    "    \"Hi, my name is Alice Johnson and my email is alice@example.com.\",\n",
    "    \"My phone number is 555-123-4567 and I live near 10 Oak Street.\",\n",
    "    \"Here is my project code name: Phoenix-42, please keep this confidential.\",\n",
    "]\n",
    "\n",
    "# Define simple redaction rules for names, emails, phones, and project codenames.\n",
    "redaction_rules = {\n",
    "    \"email\": \"[EMAIL REDACTED]\",\n",
    "    \"phone\": \"[PHONE REDACTED]\",\n",
    "    \"name\": \"[NAME REDACTED]\",\n",
    "    \"codename\": \"[CODENAME REDACTED]\",\n",
    "}\n",
    "\n",
    "# Define helper function that masks obvious email patterns using string operations.\n",
    "def redact_email(text):\n",
    "    if \"@\" in text and \".\" in text:\n",
    "        return redaction_rules[\"email\"]\n",
    "    return text\n",
    "\n",
    "# Define helper function that masks simple phone patterns using digit counting logic.\n",
    "def redact_phone(text):\n",
    "    digits = sum(ch.isdigit() for ch in text)\n",
    "    if digits >= 7:\n",
    "        return redaction_rules[\"phone\"]\n",
    "    return text\n",
    "\n",
    "# Define helper function that masks likely full names using space separated tokens.\n",
    "def redact_name(text):\n",
    "    parts = text.split()\n",
    "    if len(parts) >= 2 and parts[0].istitle() and parts[1].istitle():\n",
    "        return text.replace(\" \".join(parts[:2]), redaction_rules[\"name\"])\n",
    "    return text\n",
    "\n",
    "# Define helper function that masks project codenames using simple keyword detection.\n",
    "def redact_codename(text):\n",
    "    if \"code name\" in text.lower() or \"codename\" in text.lower():\n",
    "        return redaction_rules[\"codename\"]\n",
    "    return text\n",
    "\n",
    "# Define function that applies all redaction helpers to a single message string.\n",
    "def redact_message(message):\n",
    "    step_one = redact_name(message)\n",
    "    step_two = redact_email(step_one)\n",
    "    step_three = redact_phone(step_two)\n",
    "    final_text = redact_codename(step_three)\n",
    "    return final_text\n",
    "\n",
    "# Build safe memory list by redacting each message before simulated storage.\n",
    "safe_memory = []\n",
    "for msg in messages:\n",
    "    safe_memory.append(redact_message(msg))\n",
    "\n",
    "# Print original messages and redacted versions to compare stored memory safety.\n",
    "for original, redacted in zip(messages, safe_memory):\n",
    "    print(\"ORIGINAL:\", original)\n",
    "    print(\"STORED  :\", redacted)\n",
    "    print(\"-\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7817fbcd",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Conversational Memory**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0e8ec7",
   "metadata": {},
   "source": [
    "\n",
    "In this lecture, you learned to:\n",
    "- Explain how LangChain memory mechanisms maintain conversational context for Llama 3. \n",
    "- Configure and use different memory types to build a stateful Llama 3 chatbot. \n",
    "- Tune memory behavior to balance context completeness, token limits, and privacy concerns. \n",
    "\n",
    "In the next Module (Module 4), we will go over 'Tools And Retrieval'"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
