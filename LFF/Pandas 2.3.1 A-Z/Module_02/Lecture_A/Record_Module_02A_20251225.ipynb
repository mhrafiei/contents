{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1698dc00",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Reading Tabular Data**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff1f4c0",
   "metadata": {},
   "source": [
    ">Last update: 20251225.\n",
    "    \n",
    "By the end of this Lecture, you will be able to:\n",
    "- Use read_csv and read_excel to load tabular data into DataFrames with appropriate options. \n",
    "- Control column types, missing value markers, and date parsing during file ingestion. \n",
    "- Export cleaned DataFrames back to disk using to_csv and to_excel with reproducible settings. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db60e22",
   "metadata": {},
   "source": [
    "## **1. read csv basics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8d30e9",
   "metadata": {},
   "source": [
    "### **1.1. Separators Headers Index**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135565c1",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Pandas 2.3.1 A-Z/Module_02/Lecture_A/image_01_01.jpg?v=1766641533\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Choose the correct separator for each file\n",
    ">* Check columns and names to catch separator mistakes\n",
    "\n",
    ">* Identify which row actually contains column headers\n",
    ">* Skip extra top lines and rename columns consistently\n",
    "\n",
    ">* Choose between default index or natural key\n",
    ">* Ensure index column is unique, complete, reliable\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3e3232",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Separators Headers Index\n",
    "\n",
    "# Demonstrate separators headers and index when reading CSV text with pandas.\n",
    "# Show how wrong separator breaks columns and how correct separator fixes them.\n",
    "# Show how header and index options change resulting DataFrame structure.\n",
    "\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "# Create sample CSV text with semicolon separator and header row.\n",
    "csv_text = \"date;customer_id;energy_kwh\\n2024-01-01;C001;15.5\\n2024-01-02;C002;18.0\"\n",
    "\n",
    "# Read using default comma separator, everything becomes one confusing column.\n",
    "wrong_sep_df = pd.read_csv(StringIO(csv_text))\n",
    "print(\"Wrong separator, columns look incorrect:\")\n",
    "print(wrong_sep_df.head())\n",
    "\n",
    "# Read using correct semicolon separator, columns now parse correctly.\n",
    "correct_sep_df = pd.read_csv(StringIO(csv_text), sep=\";\")\n",
    "print(\"\\nCorrect separator, columns now parsed:\")\n",
    "print(correct_sep_df.head())\n",
    "\n",
    "# Read again but treat first row as data, then add custom header names.\n",
    "no_header_df = pd.read_csv(StringIO(csv_text), sep=\";\", header=None)\n",
    "no_header_df.columns = [\"date\", \"customer_id\", \"energy_kwh\"]\n",
    "print(\"\\nNo header used, custom names assigned:\")\n",
    "print(no_header_df.head())\n",
    "\n",
    "# Read with correct separator and set date column as index for convenience.\n",
    "indexed_df = pd.read_csv(StringIO(csv_text), sep=\";\", index_col=\"date\")\n",
    "print(\"\\nDate column used as index:\")\n",
    "print(indexed_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e751d69",
   "metadata": {},
   "source": [
    "### **1.2. Encoding and compression**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8db53d",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Pandas 2.3.1 A-Z/Module_02/Lecture_A/image_01_02.jpg?v=1766641557\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Encoding maps text characters to file bytes\n",
    ">* Wrong encoding causes garbled text and data issues\n",
    "\n",
    ">* Explicitly set file encoding when loading data\n",
    ">* Wrong or default encodings silently corrupt text fields\n",
    "\n",
    ">* Use on-the-fly decompression for compressed data files\n",
    ">* Specify compression type to ensure speed and accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e81fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Encoding and compression\n",
    "\n",
    "# Demonstrate reading CSV files with different encodings and compression options.\n",
    "# Show how incorrect encoding breaks special characters when loading text data.\n",
    "# Show how pandas reads compressed CSV files directly without manual decompression.\n",
    "\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "# Create a small CSV text with special characters using UTF-8 encoding.\n",
    "text_utf8 = \"name,city\\nJosé,Montréal\\nZoë,München\"\n",
    "\n",
    "# Simulate a file object using StringIO for the UTF-8 encoded text.\n",
    "file_like_utf8 = StringIO(text_utf8)\n",
    "\n",
    "# Read the CSV correctly using the matching UTF-8 encoding parameter.\n",
    "df_correct = pd.read_csv(file_like_utf8, encoding=\"utf-8\")\n",
    "\n",
    "# Show the correctly decoded DataFrame with readable accented characters.\n",
    "print(\"Correct UTF-8 decoding example:\")\n",
    "print(df_correct)\n",
    "\n",
    "# Now read the same bytes using a mismatched Latin-1 encoding to show corruption.\n",
    "file_like_latin = StringIO(text_utf8)\n",
    "\n",
    "df_wrong = pd.read_csv(file_like_latin, encoding=\"latin1\", on_bad_lines=\"skip\")\n",
    "\n",
    "# Show how the text may appear corrupted or altered with wrong encoding.\n",
    "print(\"\\nWrong Latin-1 decoding example:\")\n",
    "print(df_wrong)\n",
    "\n",
    "# Next, create a small DataFrame and save it as a compressed CSV using gzip.\n",
    "small_df = pd.DataFrame({\"city\": [\"New York\", \"Los Angeles\"], \"temp_f\": [72, 85]})\n",
    "\n",
    "compressed_path = \"cities_temps.csv.gz\"\n",
    "\n",
    "small_df.to_csv(compressed_path, index=False, compression=\"gzip\")\n",
    "\n",
    "# Read the compressed CSV directly by specifying the gzip compression parameter.\n",
    "loaded_compressed = pd.read_csv(compressed_path, compression=\"gzip\")\n",
    "\n",
    "# Display the DataFrame loaded from the compressed gzip CSV file.\n",
    "print(\"\\nLoaded from gzip compressed CSV:\")\n",
    "print(loaded_compressed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b77118",
   "metadata": {},
   "source": [
    "### **1.3. Handling large files with chunksize**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d0efac",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Pandas 2.3.1 A-Z/Module_02/Lecture_A/image_01_03.jpg?v=1766641583\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Very large data files can overwhelm memory\n",
    ">* Read data in row chunks using streaming\n",
    "\n",
    ">* Process data in independent, chunk-sized steps\n",
    ">* Compute partial stats, discard chunks, scale analysis\n",
    "\n",
    ">* Tune chunk size to balance memory, speed\n",
    ">* Plan how chunk outputs are stored or combined\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac81d4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Handling large files with chunksize\n",
    "\n",
    "# Demonstrate reading large CSV files using chunksize parameter effectively.\n",
    "# Simulate a big file by writing many rows to a temporary CSV file.\n",
    "# Process each chunk to compute running totals without loading everything.\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "rows_per_chunk = 10000\n",
    "number_of_chunks = 5\n",
    "columns = [\"trip_miles\", \"fare_dollars\"]\n",
    "\n",
    "fd, temp_path = tempfile.mkstemp(suffix=\"_rides.csv\")\n",
    "os.close(fd)\n",
    "\n",
    "with open(temp_path, \"w\") as file_handle:\n",
    "    file_handle.write(\",\".join(columns) + \"\\n\")\n",
    "    for chunk_index in range(number_of_chunks):\n",
    "        for row_index in range(rows_per_chunk):\n",
    "            file_handle.write(f\"{1.5},{12.0}\\n\")\n",
    "\n",
    "print(\"Temporary CSV file path:\", temp_path)\n",
    "print(\"Simulated rows count:\", rows_per_chunk * number_of_chunks)\n",
    "\n",
    "chunk_size = 8000\n",
    "running_miles_total = 0.0\n",
    "running_fare_total = 0.0\n",
    "\n",
    "for chunk_frame in pd.read_csv(temp_path, chunksize=chunk_size):\n",
    "    chunk_miles_sum = chunk_frame[\"trip_miles\"].sum()\n",
    "    chunk_fare_sum = chunk_frame[\"fare_dollars\"].sum()\n",
    "    running_miles_total += float(chunk_miles_sum)\n",
    "    running_fare_total += float(chunk_fare_sum)\n",
    "    print(\"Processed chunk rows count:\", len(chunk_frame))\n",
    "\n",
    "print(\"Total miles across all chunks:\", running_miles_total)\n",
    "print(\"Total fare dollars across all chunks:\", running_fare_total)\n",
    "\n",
    "os.remove(temp_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ec7c41",
   "metadata": {},
   "source": [
    "## **2. Excel data ingestion**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903277a7",
   "metadata": {},
   "source": [
    "### **2.1. Reading Excel Sheets**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc83d61",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Pandas 2.3.1 A-Z/Module_02/Lecture_A/image_02_01.jpg?v=1766641620\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Load chosen Excel sheets into DataFrames\n",
    ">* Control column types, missing values, and dates\n",
    "\n",
    ">* Set correct types for numbers, text, dates\n",
    ">* Keep types consistent across sheets to avoid errors\n",
    "\n",
    ">* Identify messy dates, codes, and placeholders\n",
    ">* Define missing values, types, and date parsing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff810bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Reading Excel Sheets\n",
    "\n",
    "# Demonstrate reading Excel sheets into DataFrames with controlled column types.\n",
    "# Show how to choose sheets and inspect interpreted data types clearly.\n",
    "# Run in Colab to see how Excel data becomes structured DataFrames.\n",
    "\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "\n",
    "# Create a small DataFrame representing monthly sales data.\n",
    "sales_data = {\n",
    "    \"Month\": [\"Jan\", \"Feb\", \"Mar\", \"Apr\"],\n",
    "    \"Store_ID\": [\"001\", \"002\", \"001\", \"003\"],\n",
    "    \"Sales_USD\": [1200.5, 950.0, 1430.0, 800.0],\n",
    "}\n",
    "\n",
    "\n",
    "sales_df = pd.DataFrame(sales_data)\n",
    "\n",
    "# Create another DataFrame representing simple employee hire information.\n",
    "hire_data = {\n",
    "    \"Employee_ID\": [\"A10\", \"B20\", \"C30\", \"D40\"],\n",
    "    \"Hire_Date\": [\"2024-01-05\", \"2024-02-10\", \"2024-03-15\", \"2024-04-20\"],\n",
    "    \"Department\": [\"Sales\", \"Support\", \"Sales\", \"Finance\"],\n",
    "}\n",
    "\n",
    "\n",
    "hire_df = pd.DataFrame(hire_data)\n",
    "\n",
    "# Save both DataFrames into a single in memory Excel workbook.\n",
    "excel_buffer = BytesIO()\n",
    "with pd.ExcelWriter(excel_buffer, engine=\"xlsxwriter\") as writer:\n",
    "    sales_df.to_excel(writer, sheet_name=\"Monthly_Sales\", index=False)\n",
    "    hire_df.to_excel(writer, sheet_name=\"Hires\", index=False)\n",
    "\n",
    "\n",
    "excel_buffer.seek(0)\n",
    "\n",
    "# Read the sales sheet, forcing Store_ID to stay as string identifiers.\n",
    "read_sales = pd.read_excel(\n",
    "    excel_buffer,\n",
    "    sheet_name=\"Monthly_Sales\",\n",
    "    dtype={\"Store_ID\": \"string\"},\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Sales sheet dtypes after reading:\")\n",
    "print(read_sales.dtypes)\n",
    "\n",
    "# Reset buffer position before reading another sheet from the same workbook.\n",
    "excel_buffer.seek(0)\n",
    "\n",
    "# Read the hires sheet, parsing Hire_Date as real datetime values.\n",
    "read_hires = pd.read_excel(\n",
    "    excel_buffer,\n",
    "    sheet_name=\"Hires\",\n",
    "    parse_dates=[\"Hire_Date\"],\n",
    ")\n",
    "\n",
    "\n",
    "print(\"\\nHires sheet dtypes after reading:\")\n",
    "print(read_hires.dtypes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac997e84",
   "metadata": {},
   "source": [
    "### **2.2. Handling header rows**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b430c6",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Pandas 2.3.1 A-Z/Module_02/Lecture_A/image_02_02.jpg?v=1766641643\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Decide which Excel row is real header\n",
    ">* Header choice impacts column names and types\n",
    "\n",
    ">* Wrong headers can corrupt types and missing values\n",
    ">* Pick the true label row to guide parsing\n",
    "\n",
    ">* Use flexible header options for complex Excel layouts\n",
    ">* Rename columns to clarify dates and missing values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca797f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Handling header rows\n",
    "\n",
    "# Demonstrate Excel header handling with pandas read_excel options.\n",
    "# Show skipping decorative rows and choosing correct header row.\n",
    "# Compare inferred column names and data types after different header choices.\n",
    "\n",
    "import pandas as pandas_lib\n",
    "from pandas import DataFrame as DataFrame_class\n",
    "\n",
    "# Create a small DataFrame that mimics messy Excel layout.\n",
    "# First rows contain title and notes, then real column labels.\n",
    "# Data rows include dates and numeric values for simple inspection.\n",
    "raw_data = {\n",
    "    \"col1\": [\"Sales Report 2024\", \"All amounts in dollars\", \"Date\", \"2024-01-01\", \"2024-01-02\"],\n",
    "    \"col2\": [\"Northeast region only\", \"Preliminary numbers only\", \"Units Sold\", 120, 150],\n",
    "    \"col3\": [\"Draft version only\", \"Do not distribute\", \"Revenue\", 2500.0, 3100.0],\n",
    "}\n",
    "\n",
    "messy_df = DataFrame_class(raw_data)\n",
    "\n",
    "# Save the messy DataFrame to an Excel file for ingestion demonstration.\n",
    "# In real life this file would come from an external business system.\n",
    "excel_filename = \"messy_sales_report.xlsx\"\n",
    "messy_df.to_excel(excel_filename, index=False)\n",
    "\n",
    "# Read the Excel file using the default header behavior for comparison.\n",
    "# Pandas will treat the first row as header, which is actually a title.\n",
    "print(\"Default header=0 column names and dtypes:\")\n",
    "read_default = pandas_lib.read_excel(excel_filename)\n",
    "print(read_default.dtypes)\n",
    "\n",
    "# Read the same file while skipping decorative rows and choosing correct header.\n",
    "# Here we skip first two rows and use the third row as header labels.\n",
    "print(\"\\nUsing skiprows=2 for correct header row:\")\n",
    "read_fixed = pandas_lib.read_excel(excel_filename, skiprows=2)\n",
    "print(read_fixed.dtypes)\n",
    "\n",
    "# Show the cleaned DataFrame head to confirm readable column names.\n",
    "# This helps beginners see the practical effect of header handling.\n",
    "print(\"\\nCleaned DataFrame preview with proper headers:\")\n",
    "print(read_fixed.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15762563",
   "metadata": {},
   "source": [
    "### **2.3. Excel export options**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d5a2ff",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Pandas 2.3.1 A-Z/Module_02/Lecture_A/image_02_03.jpg?v=1766641664\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Treat Excel export as part of ingestion\n",
    ">* Careful export preserves types, dates, and reliability\n",
    "\n",
    ">* Excel may change column types and nulls\n",
    ">* Choose missing markers based on downstream Excel usage\n",
    "\n",
    ">* Standardize Excel date formats to avoid misinterpretation\n",
    ">* Use clear text dates, document formats, preserve timezones\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8512119",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Excel export options\n",
    "\n",
    "# Demonstrate Excel export options for types and missing values.\n",
    "# Show how to control dates and missing markers during export.\n",
    "# Verify that exported Excel data roundtrips back correctly.\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Create a small DataFrame with product codes and dates.\n",
    "# Include leading zeros, missing values, and mixed types.\n",
    "# Use simple inches based product lengths for clarity.\n",
    "data = {\n",
    "    \"product_code\": [\"0012\", \"0013\", \"A014\", \"0015\"],\n",
    "    \"length_inches\": [10.0, None, 12.5, 9.0],\n",
    "    \"sale_date\": [datetime(2024, 1, 5), None, datetime(2024, 2, 1), datetime(2024, 3, 15)],\n",
    "}\n",
    "\n",
    "# Build the DataFrame and inspect dtypes before export.\n",
    "# Product codes should remain strings, not integers.\n",
    "# Missing lengths and dates appear as NaN values.\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Original DataFrame dtypes before export:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Export to Excel with controlled options for missing values and dates.\n",
    "# Use a clear missing marker string for numeric columns.\n",
    "# Convert dates to ISO formatted strings for safety.\n",
    "export_df = df.copy()\n",
    "export_df[\"sale_date\"] = export_df[\"sale_date\"].dt.strftime(\"%Y-%m-%d\")\n",
    "export_df.to_excel(\"products_clean.xlsx\", index=False)\n",
    "\n",
    "# Read the Excel file back to verify roundtrip behavior.\n",
    "# Let pandas infer types from the exported workbook.\n",
    "# Then compare dtypes and values with the original DataFrame.\n",
    "roundtrip_df = pd.read_excel(\"products_clean.xlsx\")\n",
    "print(\"\\nRoundtrip DataFrame dtypes after Excel export:\")\n",
    "print(roundtrip_df.dtypes)\n",
    "\n",
    "# Show the roundtripped data to highlight preserved product codes.\n",
    "# Leading zeros should still appear in product_code column.\n",
    "# Missing values should match expectations for downstream ingestion.\n",
    "print(\"\\nRoundtrip DataFrame preview:\")\n",
    "print(roundtrip_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ed023b",
   "metadata": {},
   "source": [
    "## **3. Exporting DataFrames Safely**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d0a3cc",
   "metadata": {},
   "source": [
    "### **3.1. Using to_csv Options**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d72be6a",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Pandas 2.3.1 A-Z/Module_02/Lecture_A/image_03_01.jpg?v=1766641690\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Choose path, separator, and header settings explicitly\n",
    ">* Match options to downstream tools for reliable exports\n",
    "\n",
    ">* Choose strings for missing values carefully\n",
    ">* Control quoting to avoid parsing and spreadsheet issues\n",
    "\n",
    ">* Standardize encoding, line endings, and file size handling\n",
    ">* Decide append vs overwrite and document export settings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e552f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Using to_csv Options\n",
    "\n",
    "# Demonstrate basic DataFrame export using to_csv options.\n",
    "# Show separator, header, index, and missing value handling clearly.\n",
    "# Help create predictable CSV files for sharing across different tools.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Create a small DataFrame representing weekly sales data.\n",
    "data = {\n",
    "    \"week\": [\"2024-01-01\", \"2024-01-08\", \"2024-01-15\"],\n",
    "    \"region\": [\"North\", \"South\", \"West\"],\n",
    "    \"units_sold\": [120, None, 95],\n",
    "}\n",
    "\n",
    "sales_df = pd.DataFrame(data)\n",
    "\n",
    "# Export with comma separator, header included, index excluded, custom missing marker.\n",
    "comma_path = \"sales_comma.csv\"\n",
    "sales_df.to_csv(\n",
    "    comma_path,\n",
    "    sep=\",\",\n",
    "    header=True,\n",
    "    index=False,\n",
    "    na_rep=\"MISSING\",\n",
    ")\n",
    "\n",
    "# Export with semicolon separator, header included, index excluded, different missing marker.\n",
    "semicolon_path = \"sales_semicolon.csv\"\n",
    "sales_df.to_csv(\n",
    "    semicolon_path,\n",
    "    sep=\";\",\n",
    "    header=True,\n",
    "    index=False,\n",
    "    na_rep=\"NA_CODE\",\n",
    ")\n",
    "\n",
    "# Read both files back to quickly show their different text formats.\n",
    "comma_text = open(comma_path, \"r\", encoding=\"utf-8\").read()\n",
    "semicolon_text = open(semicolon_path, \"r\", encoding=\"utf-8\").read()\n",
    "\n",
    "# Print short previews so students see how options changed the files.\n",
    "print(\"Comma separated CSV preview:\\n\", comma_text.strip())\n",
    "print(\"\\nSemicolon separated CSV preview:\\n\", semicolon_text.strip())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750e56e0",
   "metadata": {},
   "source": [
    "### **3.2. Formatting Index and Floats**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba169d3",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Pandas 2.3.1 A-Z/Module_02/Lecture_A/image_03_02.jpg?v=1766641709\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Decide whether to save the index column\n",
    ">* Preserve meaningful indexes and name them clearly\n",
    "\n",
    ">* Choose sensible decimal places for float exports\n",
    ">* Consistent float formatting improves sharing and reproducibility\n",
    "\n",
    ">* Match index and float formatting to workflow\n",
    ">* Standardize settings to protect accuracy and reproducibility\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aadfe29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Formatting Index and Floats\n",
    "\n",
    "# Demonstrate exporting DataFrame index formatting choices with to_csv options.\n",
    "# Show how including or excluding index column changes saved file structure.\n",
    "# Show how float_format controls decimal places for exported numeric values.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Create simple DataFrame with default integer index and float values.\n",
    "data = {\"city\": [\"Boston\", \"Denver\", \"Dallas\"], \"temperature_f\": [72.4567, 65.3333, 88.9999]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Export including index, with four decimal places for float values.\n",
    "file_with_index = \"temperatures_with_index.csv\"\n",
    "\n",
    "df.to_csv(file_with_index, index=True, float_format=\"%.4f\")\n",
    "\n",
    "# Export excluding index, with two decimal places for float values.\n",
    "file_without_index = \"temperatures_without_index.csv\"\n",
    "\n",
    "df.to_csv(file_without_index, index=False, float_format=\"%.2f\")\n",
    "\n",
    "# Read both files back to compare how index and floats were stored.\n",
    "read_with_index = pd.read_csv(file_with_index)\n",
    "\n",
    "read_without_index = pd.read_csv(file_without_index)\n",
    "\n",
    "# Print small samples to observe index column presence and float precision.\n",
    "print(\"With index column and four decimals:\")\n",
    "\n",
    "print(read_with_index.head())\n",
    "\n",
    "print(\"\\nWithout index column and two decimals:\")\n",
    "\n",
    "print(read_without_index.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b1e907",
   "metadata": {},
   "source": [
    "### **3.3. Validating File Round Trips**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a8f4e6",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Pandas 2.3.1 A-Z/Module_02/Lecture_A/image_03_03.jpg?v=1766641728\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Round-trip files to confirm structure and meaning\n",
    ">* Catch silent type, truncation, and missing-value errors\n",
    "\n",
    ">* Check columns, order, and data types match\n",
    ">* Verify identifiers, numbers, text, and encodings stay intact\n",
    "\n",
    ">* Watch for locale, formatting, and parsing changes\n",
    ">* Spot check key columns to ensure data integrity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24476d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Validating File Round Trips\n",
    "\n",
    "# Demonstrate saving a DataFrame and reading it back safely.\n",
    "# Show how to compare original and loaded DataFrames.\n",
    "# Help validate that a file round trip preserved important information.\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Create a tiny DataFrame representing simple store sales data.\n",
    "data = {\n",
    "    \"store_id\": [\"A001\", \"A002\", \"A003\"],\n",
    "    \"sale_dollars\": [19.99, 5.50, 120.00],\n",
    "    \"sale_date\": [\"2024-01-01\", \"2024-01-02\", \"2024-01-03\"],\n",
    "}\n",
    "\n",
    "# Build the DataFrame and parse dates for correct dtypes.\n",
    "df_original = pd.DataFrame(data)\n",
    "df_original[\"sale_date\"] = pd.to_datetime(df_original[\"sale_date\"], format=\"%Y-%m-%d\")\n",
    "\n",
    "# Choose a temporary CSV path inside the current working directory.\n",
    "file_path = Path(\"round_trip_example.csv\")\n",
    "\n",
    "# Save the DataFrame with explicit options for reproducibility.\n",
    "df_original.to_csv(file_path, index=False, float_format=\"%.2f\")\n",
    "\n",
    "# Read the file back, parsing dates to match original dtypes.\n",
    "df_loaded = pd.read_csv(file_path, parse_dates=[\"sale_date\"])\n",
    "\n",
    "# Compare shapes, column names, and dtypes for quick structural checks.\n",
    "print(\"Original shape and columns:\", df_original.shape, list(df_original.columns))\n",
    "print(\"Loaded shape and columns:\", df_loaded.shape, list(df_loaded.columns))\n",
    "\n",
    "# Check whether dtypes match between original and loaded DataFrames.\n",
    "print(\"Original dtypes:\\n\", df_original.dtypes)\n",
    "print(\"Loaded dtypes:\\n\", df_loaded.dtypes)\n",
    "\n",
    "# Use equals to confirm that all values match exactly after the round trip.\n",
    "print(\"DataFrames equal after round trip:\", df_original.equals(df_loaded))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99226760",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Reading Tabular Data**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317ebe23",
   "metadata": {},
   "source": [
    "\n",
    "In this lecture, you learned to:\n",
    "- Use read_csv and read_excel to load tabular data into DataFrames with appropriate options. \n",
    "- Control column types, missing value markers, and date parsing during file ingestion. \n",
    "- Export cleaned DataFrames back to disk using to_csv and to_excel with reproducible settings. \n",
    "\n",
    "In the next Lecture (Lecture B), we will go over 'APIs and Databases'"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
