{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c75e51ca",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Text Pipelines**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c8d64a",
   "metadata": {},
   "source": [
    ">Last update: 20260130.\n",
    "    \n",
    "By the end of this Lecture, you will be able to:\n",
    "- Tokenize raw text into sequences of tokens suitable for neural models. \n",
    "- Build vocabularies and map tokens to integer indices, handling unknown and padding tokens. \n",
    "- Create PyTorch datasets and dataloaders that yield padded text batches and labels. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e39fe6",
   "metadata": {},
   "source": [
    "## **1. Core Text Tokenization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c736c49",
   "metadata": {},
   "source": [
    "### **1.1. Word and Subword Choices**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c30c96",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_06/Lecture_A/image_01_01.jpg?v=1769753278\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Word-level tokenization is intuitive and often effective\n",
    ">* Large sparse vocabularies hurt learning and efficiency\n",
    "\n",
    ">* Subword tokenization splits words into reusable pieces\n",
    ">* Improves rare word handling, vocabulary size, and generalization\n",
    "\n",
    ">* Choice depends on language, task, and constraints\n",
    ">* Words aid clarity; subwords boost robustness and coverage\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961e06a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Word and Subword Choices\n",
    "\n",
    "# This script compares word and subword tokenization.\n",
    "# It uses simple examples for clear intuition.\n",
    "# Run cells to see printed tokenization outputs.\n",
    "\n",
    "# Required external installs would be placed here.\n",
    "# No extra libraries are needed for this script.\n",
    "\n",
    "# Define a small list of example sentences.\n",
    "texts = [\n",
    "    \"unbelievable movie with unbelievable acting\",\n",
    "    \"believer in believable stories\",\n",
    "    \"newproductX is unbelievably cool\",\n",
    "]\n",
    "\n",
    "# Show the raw texts so learners see inputs.\n",
    "print(\"Raw texts:\")\n",
    "for t in texts:\n",
    "    print(\"  -\", t)\n",
    "\n",
    "# Implement a simple whitespace word tokenizer.\n",
    "def word_tokenize(text):\n",
    "    tokens = text.strip().split()\n",
    "    return tokens\n",
    "\n",
    "# Build a basic word vocabulary from the texts.\n",
    "word_vocab = {\"<unk>\": 0}\n",
    "for text in texts:\n",
    "    for tok in word_tokenize(text):\n",
    "        if tok not in word_vocab:\n",
    "            word_vocab[tok] = len(word_vocab)\n",
    "\n",
    "# Print the small word vocabulary mapping.\n",
    "print(\"\\nWord vocabulary:\")\n",
    "print(word_vocab)\n",
    "\n",
    "# Tokenize one sentence using word level tokens.\n",
    "example_text = texts[2]\n",
    "word_tokens = word_tokenize(example_text)\n",
    "word_ids = [word_vocab.get(tok, 0) for tok in word_tokens]\n",
    "\n",
    "# Show word tokens and their integer ids.\n",
    "print(\"\\nWord tokens for example:\")\n",
    "print(list(zip(word_tokens, word_ids)))\n",
    "\n",
    "# Implement a simple subword tokenizer using suffixes.\n",
    "def subword_tokenize(text, suffixes):\n",
    "    tokens = []\n",
    "    for word in text.strip().split():\n",
    "        matched = False\n",
    "        for suf in suffixes:\n",
    "            if word.endswith(suf) and len(word) > len(suf):\n",
    "                stem = word[: -len(suf)]\n",
    "                tokens.append(stem)\n",
    "                tokens.append(\"_\" + suf)\n",
    "                matched = True\n",
    "                break\n",
    "        if not matched:\n",
    "            tokens.append(word)\n",
    "    return tokens\n",
    "\n",
    "# Define a few hand crafted subword suffixes.\n",
    "subword_suffixes = [\"able\", \"ably\", \"er\"]\n",
    "\n",
    "# Build a subword vocabulary from all texts.\n",
    "subword_vocab = {\"<unk>\": 0}\n",
    "for text in texts:\n",
    "    for tok in subword_tokenize(text, subword_suffixes):\n",
    "        if tok not in subword_vocab:\n",
    "            subword_vocab[tok] = len(subword_vocab)\n",
    "\n",
    "# Print the small subword vocabulary mapping.\n",
    "print(\"\\nSubword vocabulary:\")\n",
    "print(subword_vocab)\n",
    "\n",
    "# Tokenize the same sentence using subword tokens.\n",
    "sub_tokens = subword_tokenize(example_text, subword_suffixes)\n",
    "sub_ids = [subword_vocab.get(tok, 0) for tok in sub_tokens]\n",
    "\n",
    "# Show subword tokens and their integer ids.\n",
    "print(\"\\nSubword tokens for example:\")\n",
    "print(list(zip(sub_tokens, sub_ids)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11167874",
   "metadata": {},
   "source": [
    "### **1.2. Text normalization basics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807ac522",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_06/Lecture_A/image_01_02.jpg?v=1769753324\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Normalize messy text to reduce surface variation\n",
    ">* Choose case handling based on downstream task needs\n",
    "\n",
    ">* Normalize characters, accents, and scripts consistently\n",
    ">* Standardize Unicode and whitespace to stabilize tokenization\n",
    "\n",
    ">* Normalize numbers, symbols, and informal spellings carefully\n",
    ">* Balance simplification with preserving task-relevant signals\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9804dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Text normalization basics\n",
    "\n",
    "# This script demonstrates basic text normalization.\n",
    "# We focus on simple rules before tokenization.\n",
    "# Run cells to see each transformation clearly.\n",
    "\n",
    "# Required external libraries would be installed like this.\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Define a small list of raw example sentences.\n",
    "raw_texts = [\n",
    "    \"Apple   released   the NEW iPhone!!!\",\n",
    "    \"I am soooo HAPPY about this cafÃ©.\",\n",
    "    \"OK!!! This   PDF   text has\tweird spaces...\",\n",
    "]\n",
    "\n",
    "# Print the original raw texts for comparison.\n",
    "print(\"Original texts:\")\n",
    "for t in raw_texts:\n",
    "    print(\"-\", t)\n",
    "\n",
    "# Import regular expressions for simple text cleaning.\n",
    "import re\n",
    "\n",
    "# Define a function to lowercase and normalize whitespace.\n",
    "def basic_normalize(text: str) -> str:\n",
    "    # Convert all characters to lowercase form.\n",
    "    lowered = text.lower()\n",
    "    # Replace any whitespace sequence with single space.\n",
    "    spaced = re.sub(r\"\\s+\", \" \", lowered)\n",
    "    # Strip leading and trailing spaces safely.\n",
    "    cleaned = spaced.strip()\n",
    "    return cleaned\n",
    "\n",
    "# Define a function to compress repeated punctuation marks.\n",
    "def compress_punctuation(text: str) -> str:\n",
    "    # Replace three or more exclamation marks with one.\n",
    "    text = re.sub(r\"!{2,}\", \"!\", text)\n",
    "    # Replace three or more dots with single period.\n",
    "    text = re.sub(r\"\\.{3,}\", \".\", text)\n",
    "    return text\n",
    "\n",
    "# Define a function to normalize elongated words like soooo.\n",
    "def normalize_elongation(text: str) -> str:\n",
    "    # Compress three or more repeated letters to two.\n",
    "    return re.sub(r\"(\\w)\\1{2,}\", r\"\\1\\1\", text)\n",
    "\n",
    "# Apply each normalization step in sequence to examples.\n",
    "normalized_texts = []\n",
    "for t in raw_texts:\n",
    "    # First apply punctuation compression.\n",
    "    step1 = compress_punctuation(t)\n",
    "    # Then normalize elongated character sequences.\n",
    "    step2 = normalize_elongation(step1)\n",
    "    # Finally lowercase and normalize whitespace.\n",
    "    step3 = basic_normalize(step2)\n",
    "    normalized_texts.append(step3)\n",
    "\n",
    "# Print the normalized texts to observe differences.\n",
    "print(\"\\nNormalized texts:\")\n",
    "for t in normalized_texts:\n",
    "    print(\"-\", t)\n",
    "\n",
    "# Show a simple whitespace tokenization after normalization.\n",
    "print(\"\\nTokens after normalization:\")\n",
    "for t in normalized_texts:\n",
    "    # Split on spaces to get rough tokens.\n",
    "    tokens = t.split(\" \")\n",
    "    # Print tokens list for each sentence.\n",
    "    print(tokens)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e808380",
   "metadata": {},
   "source": [
    "### **1.3. Punctuation in Tokenization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15983165",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_06/Lecture_A/image_01_03.jpg?v=1769753368\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Punctuation encodes structure, emphasis, and sentence type\n",
    ">* Tokenization choices affect meaning and model performance\n",
    "\n",
    ">* Inconsistent punctuation creates noisy, sparse token vocabularies\n",
    ">* Normalize patterns while preserving emotional and structural signals\n",
    "\n",
    ">* Punctuation strategy must match language and task\n",
    ">* Treat domain-specific symbols as meaningful special tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8f4c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Punctuation in Tokenization\n",
    "\n",
    "# This script explores punctuation handling in tokenization.\n",
    "# It shows how different rules change token sequences.\n",
    "# Use it to compare simple and custom tokenizers.\n",
    "\n",
    "# Optional install for regex if using external packages.\n",
    "# In this script we only use Python standard library.\n",
    "\n",
    "# Import regular expressions for custom tokenization.\n",
    "import re\n",
    "\n",
    "# Define a tiny corpus with expressive punctuation.\n",
    "texts = [\n",
    "    \"Wait, what? That's unbelievable!\",\n",
    "    \"What???!!! This is soooo good...\",\n",
    "    \"Hello :) This product is okay.\",\n",
    "]\n",
    "\n",
    "# Show the raw texts for reference.\n",
    "print(\"Raw texts:\")\n",
    "for t in texts:\n",
    "    print(\"-\", t)\n",
    "\n",
    "# Define a very naive whitespace tokenizer.\n",
    "def whitespace_tokenize(text):\n",
    "    return text.split()\n",
    "\n",
    "# Define a regex tokenizer that separates punctuation.\n",
    "pattern = re.compile(r\"\\w+|[:;()]+|[!?\\.]+|[^\\w\\s]\")\n",
    "\n",
    "# Tokenize text using the regex pattern.\n",
    "def regex_tokenize(text):\n",
    "    return pattern.findall(text)\n",
    "\n",
    "# Normalize repeated question and exclamation marks.\n",
    "def normalize_punct(tokens):\n",
    "    normalized = []\n",
    "    for tok in tokens:\n",
    "        if tok.count(\"?\") > 1:\n",
    "            normalized.append(\"?\")\n",
    "        elif tok.count(\"!\") > 1:\n",
    "            normalized.append(\"!\")\n",
    "        elif tok in (\":)\", \":(\"):\n",
    "            normalized.append(\"<EMOTICON>\")\n",
    "        else:\n",
    "            normalized.append(tok)\n",
    "    return normalized\n",
    "\n",
    "# Print a header for comparison output.\n",
    "print(\"\\nTokenization comparison (first two texts):\")\n",
    "\n",
    "# Loop over first two texts to keep output short.\n",
    "for idx, text in enumerate(texts[:2]):\n",
    "    print(f\"\\nExample {idx + 1}:\")\n",
    "    ws_tokens = whitespace_tokenize(text)\n",
    "    rg_tokens = regex_tokenize(text)\n",
    "    norm_tokens = normalize_punct(rg_tokens)\n",
    "    print(\"Whitespace:\", ws_tokens)\n",
    "    print(\"Regex:\", rg_tokens)\n",
    "    print(\"Normalized:\", norm_tokens)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa405f4",
   "metadata": {},
   "source": [
    "## **2. Vocabulary and Indices**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a512bc",
   "metadata": {},
   "source": [
    "### **2.1. Frequency Based Vocabulary**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab1988d",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_06/Lecture_A/image_02_01.jpg?v=1769753409\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Count token occurrences and sort by frequency\n",
    ">* Keep frequent tokens, discard most extremely rare ones\n",
    "\n",
    ">* Control vocab size with max and min frequency\n",
    ">* Trade off detail versus memory and speed\n",
    "\n",
    ">* Vocabulary reflects domain-specific token frequencies\n",
    ">* Manually include rare but important domain tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc48f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Frequency Based Vocabulary\n",
    "\n",
    "# This script demonstrates frequency based vocabularies.\n",
    "# We build a tiny corpus and count token frequencies.\n",
    "# Then we map tokens to indices with special handling.\n",
    "\n",
    "# Required external libraries would be installed like this.\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Define a tiny example corpus of short sentences.\n",
    "corpus = [\n",
    "    \"pytorch makes deep learning easier\",\n",
    "    \"deep learning powers many modern applications\",\n",
    "    \"pytorch is popular for nlp tasks\",\n",
    "    \"nlp models need a good vocabulary\",\n",
    "]\n",
    "\n",
    "# Define a simple whitespace tokenizer function.\n",
    "def simple_tokenize(text):\n",
    "    return text.lower().strip().split()\n",
    "\n",
    "# Tokenize each sentence into a list of tokens.\n",
    "tokenized_corpus = [simple_tokenize(sentence) for sentence in corpus]\n",
    "\n",
    "# Show the tokenized corpus to understand the data.\n",
    "print(\"Tokenized corpus:\")\n",
    "print(tokenized_corpus)\n",
    "\n",
    "# Count token frequencies using a plain dictionary.\n",
    "freqs = {}\n",
    "for sentence_tokens in tokenized_corpus:\n",
    "    for token in sentence_tokens:\n",
    "        freqs[token] = freqs.get(token, 0) + 1\n",
    "\n",
    "# Print the raw frequency dictionary for inspection.\n",
    "print(\"\\nToken frequencies:\")\n",
    "print(freqs)\n",
    "\n",
    "# Sort tokens by frequency then alphabetically for stability.\n",
    "sorted_tokens = sorted(\n",
    "    freqs.items(), key=lambda item: (-item[1], item[0])\n",
    ")\n",
    "\n",
    "# Define special tokens for padding and unknown words.\n",
    "PAD_TOKEN = \"<pad>\"\n",
    "UNK_TOKEN = \"<unk>\"\n",
    "\n",
    "# Choose a maximum vocabulary size including special tokens.\n",
    "max_vocab_size = 8\n",
    "\n",
    "# Start vocabulary with special tokens at fixed indices.\n",
    "vocab_tokens = [PAD_TOKEN, UNK_TOKEN]\n",
    "\n",
    "# Add most frequent tokens until reaching the size limit.\n",
    "for token, count in sorted_tokens:\n",
    "    if len(vocab_tokens) >= max_vocab_size:\n",
    "        break\n",
    "    vocab_tokens.append(token)\n",
    "\n",
    "# Build mapping from token strings to integer indices.\n",
    "token_to_idx = {token: idx for idx, token in enumerate(vocab_tokens)}\n",
    "\n",
    "# Also build reverse mapping from indices back to tokens.\n",
    "idx_to_token = {idx: token for token, idx in token_to_idx.items()}\n",
    "\n",
    "# Print the final vocabulary and its size.\n",
    "print(\"\\nFinal vocabulary tokens:\")\n",
    "print(vocab_tokens)\n",
    "\n",
    "# Show the mapping from tokens to integer indices.\n",
    "print(\"\\nToken to index mapping:\")\n",
    "print(token_to_idx)\n",
    "\n",
    "# Define a helper to convert tokens into index sequences.\n",
    "def tokens_to_indices(tokens, mapping, unk_token):\n",
    "    return [mapping.get(token, mapping[unk_token]) for token in tokens]\n",
    "\n",
    "# Convert each tokenized sentence into a list of indices.\n",
    "indexed_corpus = [\n",
    "    tokens_to_indices(sentence_tokens, token_to_idx, UNK_TOKEN)\n",
    "    for sentence_tokens in tokenized_corpus\n",
    "]\n",
    "\n",
    "# Print the indexed corpus to see integer representations.\n",
    "print(\"\\nIndexed corpus:\")\n",
    "print(indexed_corpus)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3455e8",
   "metadata": {},
   "source": [
    "### **2.2. Special UNK and PAD**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a85721",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_06/Lecture_A/image_02_02.jpg?v=1769753460\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* UNK token represents any unseen vocabulary item\n",
    ">* All unknown tokens share one reserved UNK index\n",
    "\n",
    ">* PAD lets batches share equal sequence length\n",
    ">* PAD index is ignored by model computations\n",
    "\n",
    ">* Give UNK and PAD fixed, known indices\n",
    ">* This enables masking, batching, and robust predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27820a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Special UNK and PAD\n",
    "\n",
    "# This script explains UNK and PAD tokens.\n",
    "# It builds a tiny vocabulary with special tokens.\n",
    "# It shows how text becomes indices for models.\n",
    "\n",
    "# !pip install tensorflow.\n",
    "\n",
    "# Define a tiny toy corpus of short sentences.\n",
    "corpus = [\n",
    "    \"i love pytorch\",\n",
    "    \"pytorch loves nlp\",\n",
    "    \"i enjoy deep learning\",\n",
    "]\n",
    "\n",
    "# Define special token strings for padding and unknown.\n",
    "PAD_TOKEN = \"<PAD>\"\n",
    "UNK_TOKEN = \"<UNK>\"\n",
    "\n",
    "# Decide fixed indices for PAD and UNK tokens.\n",
    "PAD_INDEX = 0\n",
    "UNK_INDEX = 1\n",
    "\n",
    "# Build a simple word frequency dictionary from corpus.\n",
    "word_freq = {}\n",
    "for sentence in corpus:\n",
    "    for word in sentence.split():\n",
    "        word_freq[word] = word_freq.get(word, 0) + 1\n",
    "\n",
    "# Choose a minimum frequency threshold for vocabulary.\n",
    "min_freq = 1\n",
    "\n",
    "# Start vocabulary with special tokens and fixed indices.\n",
    "vocab = {PAD_TOKEN: PAD_INDEX, UNK_TOKEN: UNK_INDEX}\n",
    "\n",
    "# Add remaining words to vocabulary with new indices.\n",
    "for word, freq in word_freq.items():\n",
    "    if freq >= min_freq and word not in vocab:\n",
    "        vocab[word] = len(vocab)\n",
    "\n",
    "# Create reverse mapping from indices back to tokens.\n",
    "index_to_token = {idx: tok for tok, idx in vocab.items()}\n",
    "\n",
    "# Define a function to convert tokens into index sequence.\n",
    "def tokens_to_indices(tokens, vocab_dict, unk_index):\n",
    "    indices = []\n",
    "    for tok in tokens:\n",
    "        indices.append(vocab_dict.get(tok, unk_index))\n",
    "    return indices\n",
    "\n",
    "# Define a function to pad sequences to a fixed length.\n",
    "def pad_sequence(indices, max_len, pad_index):\n",
    "    if len(indices) > max_len:\n",
    "        return indices[:max_len]\n",
    "    padding_needed = max_len - len(indices)\n",
    "    return indices + [pad_index] * padding_needed\n",
    "\n",
    "# Prepare some example sentences including unknown words.\n",
    "examples = [\n",
    "    \"i love pytorch\",\n",
    "    \"pytorch loves transformers\",\n",
    "    \"deep learning rocks\",\n",
    "]\n",
    "\n",
    "# Tokenize example sentences into word lists.\n",
    "example_tokens = [sent.split() for sent in examples]\n",
    "\n",
    "# Convert each token list into index sequence with UNK.\n",
    "example_indices = [\n",
    "    tokens_to_indices(tokens, vocab, UNK_INDEX)\n",
    "    for tokens in example_tokens\n",
    "]\n",
    "\n",
    "# Decide a maximum length for padded sequences.\n",
    "max_len = max(len(seq) for seq in example_indices)\n",
    "\n",
    "# Pad each index sequence with PAD tokens to max length.\n",
    "padded_indices = [\n",
    "    pad_sequence(seq, max_len, PAD_INDEX)\n",
    "    for seq in example_indices\n",
    "]\n",
    "\n",
    "# Print vocabulary to show special and normal tokens.\n",
    "print(\"Vocabulary with indices:\", vocab)\n",
    "\n",
    "# Print original tokens and their mapped index sequences.\n",
    "for tokens, indices in zip(example_tokens, example_indices):\n",
    "    print(\"Tokens:\", tokens, \"->\", indices)\n",
    "\n",
    "# Print final padded batch to show PAD positions.\n",
    "print(\"Padded batch indices:\", padded_indices)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31541ba",
   "metadata": {},
   "source": [
    "### **2.3. Indexing Token Sequences**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea55190",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_06/Lecture_A/image_02_03.jpg?v=1769753510\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Replace each token with its vocabulary index\n",
    ">* Keep indexing deterministic for consistent model behavior\n",
    "\n",
    ">* Map OOV tokens to a fixed UNK index\n",
    ">* Insert special tokens to mark sequence structure\n",
    "\n",
    ">* Keep tokens, indices, and labels perfectly aligned\n",
    ">* Alignment enables correct learning and accurate interpretation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db87e8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Indexing Token Sequences\n",
    "\n",
    "# This script demonstrates indexing token sequences.\n",
    "# It focuses on vocabulary mapping and special tokens.\n",
    "# Run each part to see clear printed outputs.\n",
    "\n",
    "# Required library installs for Colab if needed.\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Define a tiny example corpus of short sentences.\n",
    "corpus_sentences = [\n",
    "    \"great movie\",\n",
    "    \"really great acting\",\n",
    "    \"bad movie but great music\",\n",
    "]\n",
    "\n",
    "# Define special tokens for padding unknown and boundaries.\n",
    "PAD_TOKEN = \"<pad>\"\n",
    "UNK_TOKEN = \"<unk>\"\n",
    "BOS_TOKEN = \"<bos>\"\n",
    "EOS_TOKEN = \"<eos>\"\n",
    "\n",
    "# Build a simple vocabulary from the corpus tokens.\n",
    "all_tokens = []\n",
    "for sentence in corpus_sentences:\n",
    "    tokens = sentence.split()\n",
    "    all_tokens.extend(tokens)\n",
    "\n",
    "# Get unique tokens and sort for deterministic order.\n",
    "unique_tokens = sorted(set(all_tokens))\n",
    "\n",
    "# Start vocabulary with special tokens first.\n",
    "vocab_tokens = [PAD_TOKEN, UNK_TOKEN, BOS_TOKEN, EOS_TOKEN]\n",
    "\n",
    "# Extend vocabulary with regular tokens from corpus.\n",
    "vocab_tokens.extend(unique_tokens)\n",
    "\n",
    "# Create mapping from token string to integer index.\n",
    "token_to_index = {token: idx for idx, token in enumerate(vocab_tokens)}\n",
    "\n",
    "# Also create reverse mapping from index back to token.\n",
    "index_to_token = {idx: token for token, idx in token_to_index.items()}\n",
    "\n",
    "# Show vocabulary and indices in a compact way.\n",
    "print(\"Vocabulary tokens:\", vocab_tokens)\n",
    "print(\"Token to index mapping:\", token_to_index)\n",
    "\n",
    "# Helper function to convert tokens into index sequence.\n",
    "def tokens_to_indices(tokens, mapping, unk_token):\n",
    "    indices = []\n",
    "    for tok in tokens:\n",
    "        if tok in mapping:\n",
    "            indices.append(mapping[tok])\n",
    "        else:\n",
    "            indices.append(mapping[unk_token])\n",
    "    return indices\n",
    "\n",
    "# Helper function to add boundary tokens around sequence.\n",
    "def add_boundaries(tokens, bos_token, eos_token):\n",
    "    return [bos_token] + tokens + [eos_token]\n",
    "\n",
    "# Choose an example sentence from the corpus.\n",
    "example_sentence = corpus_sentences[1]\n",
    "example_tokens = example_sentence.split()\n",
    "\n",
    "# Add boundary tokens to the token list.\n",
    "example_with_bounds = add_boundaries(\n",
    "    example_tokens,\n",
    "    BOS_TOKEN,\n",
    "    EOS_TOKEN,\n",
    ")\n",
    "\n",
    "# Convert bounded tokens into index sequence.\n",
    "example_indices = tokens_to_indices(\n",
    "    example_with_bounds,\n",
    "    token_to_index,\n",
    "    UNK_TOKEN,\n",
    ")\n",
    "\n",
    "# Print original tokens and their index sequence.\n",
    "print(\"Original tokens:\", example_tokens)\n",
    "print(\"With boundaries:\", example_with_bounds)\n",
    "print(\"Indexed sequence:\", example_indices)\n",
    "\n",
    "# Demonstrate handling of an unknown token not in vocabulary.\n",
    "new_sentence = \"great soundtrack\"\n",
    "new_tokens = new_sentence.split()\n",
    "\n",
    "# Add boundaries to the new token list.\n",
    "new_with_bounds = add_boundaries(\n",
    "    new_tokens,\n",
    "    BOS_TOKEN,\n",
    "    EOS_TOKEN,\n",
    ")\n",
    "\n",
    "# Convert new tokens into indices using same mapping.\n",
    "new_indices = tokens_to_indices(\n",
    "    new_with_bounds,\n",
    "    token_to_index,\n",
    "    UNK_TOKEN,\n",
    ")\n",
    "\n",
    "# Print new tokens and show where unknown appears.\n",
    "print(\"New tokens:\", new_tokens)\n",
    "print(\"New with boundaries:\", new_with_bounds)\n",
    "print(\"New indexed sequence:\", new_indices)\n",
    "\n",
    "# Reconstruct tokens from indices to verify determinism.\n",
    "reconstructed_tokens = [index_to_token[i] for i in new_indices]\n",
    "print(\"Reconstructed tokens:\", reconstructed_tokens)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6a8a41",
   "metadata": {},
   "source": [
    "## **3. Padded Text Batching**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7909d418",
   "metadata": {},
   "source": [
    "### **3.1. Handling Variable Lengths**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1231eaf",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_06/Lecture_A/image_03_01.jpg?v=1769753563\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Text sequences vary, but tensors need uniformity\n",
    ">* Design pipelines to batch, pad, and manage lengths\n",
    "\n",
    ">* Different texts have very different sequence lengths\n",
    ">* Pipelines must pad sequences to uniform batch shapes\n",
    "\n",
    ">* Padding and truncation affect performance and memory\n",
    ">* Balance length limits to keep important context\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d56bee1",
   "metadata": {},
   "source": [
    "### **3.2. Padding and Attention Masks**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128a3d38",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_06/Lecture_A/image_03_02.jpg?v=1769753577\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Padding adds fake tokens for uniform batches\n",
    ">* Attention masks hide padding so models ignore it\n",
    "\n",
    ">* Attention mask tensor flags real versus padding tokens\n",
    ">* Model ignores masked padding when computing attention scores\n",
    "\n",
    ">* Masks stop models learning from padded tokens\n",
    ">* They improve many tasks and model stability\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d4de74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Padding and Attention Masks\n",
    "\n",
    "# This script shows padding and attention masks.\n",
    "# It uses tiny toy sentences for clarity.\n",
    "# Run all cells together inside Google Colab.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Import tensorflow and check version.\n",
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Set deterministic random seeds.\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# Define a tiny toy corpus.\n",
    "sentences = [\n",
    "    \"i love pytorch\",\n",
    "    \"pytorch loves nlp\",\n",
    "    \"i love nlp\",\n",
    "]\n",
    "\n",
    "# Build a simple word to index vocabulary.\n",
    "vocab = {\"<pad>\": 0, \"<unk>\": 1}\n",
    "\n",
    "# Populate vocabulary from sentences.\n",
    "for sent in sentences:\n",
    "    for word in sent.split():\n",
    "        if word not in vocab:\n",
    "            vocab[word] = len(vocab)\n",
    "\n",
    "# Show the vocabulary mapping.\n",
    "print(\"Vocabulary:\", vocab)\n",
    "\n",
    "# Encode sentences into index sequences.\n",
    "encoded = []\n",
    "for sent in sentences:\n",
    "    ids = [vocab.get(w, vocab[\"<unk>\"]) for w in sent.split()]\n",
    "    encoded.append(ids)\n",
    "\n",
    "# Print encoded sequences before padding.\n",
    "print(\"Encoded sequences:\", encoded)\n",
    "\n",
    "# Compute maximum sequence length.\n",
    "max_len = max(len(seq) for seq in encoded)\n",
    "print(\"Max length:\", max_len)\n",
    "\n",
    "# Pad sequences and build attention masks.\n",
    "padded = []\n",
    "masks = []\n",
    "for seq in encoded:\n",
    "    pad_length = max_len - len(seq)\n",
    "    padded_seq = seq + [vocab[\"<pad>\"]] * pad_length\n",
    "    mask_seq = [1] * len(seq) + [0] * pad_length\n",
    "    padded.append(padded_seq)\n",
    "    masks.append(mask_seq)\n",
    "\n",
    "# Convert lists to numpy arrays.\n",
    "padded = np.array(padded, dtype=np.int32)\n",
    "masks = np.array(masks, dtype=np.int32)\n",
    "\n",
    "# Validate shapes before creating tensors.\n",
    "assert padded.shape == masks.shape\n",
    "\n",
    "# Create tensorflow tensors from numpy arrays.\n",
    "inputs = tf.constant(padded, dtype=tf.int32)\n",
    "attn_mask = tf.constant(masks, dtype=tf.float32)\n",
    "\n",
    "# Print padded inputs and attention masks.\n",
    "print(\"Padded inputs:\\n\", inputs.numpy())\n",
    "print(\"Attention masks:\\n\", attn_mask.numpy())\n",
    "\n",
    "# Create a tiny embedding layer.\n",
    "embedding_dim = 4\n",
    "embed_layer = tf.keras.layers.Embedding(\n",
    "    input_dim=len(vocab), output_dim=embedding_dim, mask_zero=True\n",
    ")\n",
    "\n",
    "# Get embedded representations.\n",
    "embedded = embed_layer(inputs)\n",
    "\n",
    "# Compute a simple masked average representation.\n",
    "mask_expanded = tf.expand_dims(attn_mask, axis=-1)\n",
    "masked_embeddings = embedded * mask_expanded\n",
    "\n",
    "# Avoid division by zero using epsilon.\n",
    "valid_counts = tf.reduce_sum(attn_mask, axis=1, keepdims=True)\n",
    "valid_counts = tf.maximum(valid_counts, 1.0)\n",
    "\n",
    "# Compute sentence level representations.\n",
    "sentence_repr = tf.reduce_sum(masked_embeddings, axis=1) / valid_counts\n",
    "\n",
    "# Print final sentence representations.\n",
    "print(\"Sentence representations:\\n\", sentence_repr.numpy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129e80b6",
   "metadata": {},
   "source": [
    "### **3.3. Custom Collate Function**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bce88b",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_06/Lecture_A/image_03_03.jpg?v=1769753626\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Custom collate combines samples into uniform batches\n",
    ">* Handles padding, alignment, masks, and labels tensors\n",
    "\n",
    ">* Extract, pad, and stack sequences and labels\n",
    ">* Also build attention masks and length metadata\n",
    "\n",
    ">* Handles complex inputs, languages, and augmentations consistently\n",
    ">* Centralizes batch logic, simplifying debugging and future changes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fdb6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Custom Collate Function\n",
    "\n",
    "# This script shows a simple custom collate function.\n",
    "# We simulate padded text batching for small toy sentences.\n",
    "# Focus is on shapes and padding behavior only.\n",
    "\n",
    "# Required library is PyTorch for tensor operations.\n",
    "# !pip install torch torchvision torchaudio.\n",
    "\n",
    "# Import standard random and typing utilities.\n",
    "import random\n",
    "import typing as tp\n",
    "\n",
    "# Import torch for tensors and data utilities.\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Set deterministic random seeds for reproducibility.\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Define a tiny toy vocabulary mapping words to indices.\n",
    "VOCAB: dict[str, int] = {\n",
    "    \"<pad>\": 0,\n",
    "    \"<unk>\": 1,\n",
    "    \"i\": 2,\n",
    "    \"love\": 3,\n",
    "    \"pytorch\": 4,\n",
    "    \"nlp\": 5,\n",
    "    \"is\": 6,\n",
    "    \"fun\": 7,\n",
    "}\n",
    "\n",
    "# Define constants for padding and unknown tokens.\n",
    "PAD_IDX: int = VOCAB[\"<pad>\"]\n",
    "UNK_IDX: int = VOCAB[\"<unk>\"]\n",
    "\n",
    "# Simple tokenizer splitting on whitespace characters.\n",
    "def simple_tokenize(text: str) -> list[str]:\n",
    "    return text.lower().split()\n",
    "\n",
    "# Convert tokens to indices using vocabulary mapping.\n",
    "def tokens_to_indices(tokens: list[str]) -> list[int]:\n",
    "    return [VOCAB.get(tok, UNK_IDX) for tok in tokens]\n",
    "\n",
    "# Tiny in memory dataset of sentences and labels.\n",
    "RAW_SAMPLES: list[tuple[str, int]] = [\n",
    "    (\"I love PyTorch\", 1),\n",
    "    (\"PyTorch NLP is fun\", 1),\n",
    "    (\"I love NLP\", 1),\n",
    "    (\"NLP is fun\", 1),\n",
    "]\n",
    "\n",
    "# Custom dataset returning token index lists and labels.\n",
    "class ToyTextDataset(Dataset):\n",
    "    def __init__(self, samples: list[tuple[str, int]]):\n",
    "        self.samples = samples\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> dict[str, tp.Any]:\n",
    "        text, label = self.samples[idx]\n",
    "        tokens = simple_tokenize(text)\n",
    "        indices = tokens_to_indices(tokens)\n",
    "        return {\"input_ids\": indices, \"label\": int(label)}\n",
    "\n",
    "# Custom collate function to pad sequences in a batch.\n",
    "def collate_batch(batch: list[dict[str, tp.Any]]) -> dict[str, torch.Tensor]:\n",
    "    input_lists = [item[\"input_ids\"] for item in batch]\n",
    "    label_list = [item[\"label\"] for item in batch]\n",
    "\n",
    "    max_len = max(len(seq) for seq in input_lists)\n",
    "\n",
    "    padded_inputs: list[list[int]] = []\n",
    "    attention_masks: list[list[int]] = []\n",
    "\n",
    "    for seq in input_lists:\n",
    "        pad_length = max_len - len(seq)\n",
    "        padded_seq = seq + [PAD_IDX] * pad_length\n",
    "        mask_seq = [1] * len(seq) + [0] * pad_length\n",
    "        padded_inputs.append(padded_seq)\n",
    "        attention_masks.append(mask_seq)\n",
    "\n",
    "    inputs_tensor = torch.tensor(padded_inputs, dtype=torch.long)\n",
    "    labels_tensor = torch.tensor(label_list, dtype=torch.long)\n",
    "    masks_tensor = torch.tensor(attention_masks, dtype=torch.long)\n",
    "\n",
    "    assert inputs_tensor.shape[0] == labels_tensor.shape[0]\n",
    "    assert inputs_tensor.shape == masks_tensor.shape\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": inputs_tensor,\n",
    "        \"attention_mask\": masks_tensor,\n",
    "        \"labels\": labels_tensor,\n",
    "    }\n",
    "\n",
    "# Create dataset instance using the raw samples list.\n",
    "dataset = ToyTextDataset(RAW_SAMPLES)\n",
    "\n",
    "# Create data loader using the custom collate function.\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_batch,\n",
    ")\n",
    "\n",
    "# Print torch version in one short line.\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "\n",
    "# Take a single batch from the data loader.\n",
    "first_batch = next(iter(dataloader))\n",
    "\n",
    "# Print shapes of tensors returned by collate function.\n",
    "print(\"input_ids shape:\", first_batch[\"input_ids\"].shape)\n",
    "print(\"attention_mask shape:\", first_batch[\"attention_mask\"].shape)\n",
    "print(\"labels shape:\", first_batch[\"labels\"].shape)\n",
    "\n",
    "# Print the actual padded input ids tensor.\n",
    "print(\"input_ids tensor:\")\n",
    "print(first_batch[\"input_ids\"])\n",
    "\n",
    "# Print the corresponding attention mask tensor.\n",
    "print(\"attention_mask tensor:\")\n",
    "print(first_batch[\"attention_mask\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28e7a1b",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Text Pipelines**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9195be78",
   "metadata": {},
   "source": [
    "\n",
    "In this lecture, you learned to:\n",
    "- Tokenize raw text into sequences of tokens suitable for neural models. \n",
    "- Build vocabularies and map tokens to integer indices, handling unknown and padding tokens. \n",
    "- Create PyTorch datasets and dataloaders that yield padded text batches and labels. \n",
    "\n",
    "In the next Lecture (Lecture B), we will go over 'Text Models'"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
