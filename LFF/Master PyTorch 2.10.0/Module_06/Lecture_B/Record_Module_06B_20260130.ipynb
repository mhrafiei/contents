{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5cc3956",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Text Models**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cba2ab",
   "metadata": {},
   "source": [
    ">Last update: 20260130.\n",
    "    \n",
    "By the end of this Lecture, you will be able to:\n",
    "- Use nn.Embedding to convert token indices into dense vector representations. \n",
    "- Build a simple sequence model (e.g., RNN or CNN) for text classification in PyTorch. \n",
    "- Train and evaluate the text model on a labeled dataset, monitoring accuracy and loss. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89a5c0f",
   "metadata": {},
   "source": [
    "## **1. Working With Embeddings**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e379d5",
   "metadata": {},
   "source": [
    "### **1.1. Using Embedding Layers**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91973390",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_06/Lecture_B/image_01_01.jpg?v=1769756724\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Embedding layers turn token IDs into vectors\n",
    ">* Model uses these vectors to capture token similarities\n",
    "\n",
    ">* Embeddings start random and are trained via gradients\n",
    ">* Similar tokens move closer; different tokens separate\n",
    "\n",
    ">* Embeddings compress sparse one-hot vectors efficiently\n",
    ">* They capture similar meanings and enable better generalization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866abbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Using Embedding Layers\n",
    "\n",
    "# This script shows basic text embeddings.\n",
    "# It uses TensorFlow to mimic PyTorch embeddings.\n",
    "# Focus is on converting token indices to vectors.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Import TensorFlow and Keras layers.\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Set deterministic random seeds.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version briefly.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Define a tiny toy vocabulary.\n",
    "vocab = {\"<pad>\": 0, \"good\": 1, \"bad\": 2, \"movie\": 3}\n",
    "\n",
    "# Show the vocabulary mapping.\n",
    "print(\"Vocabulary mapping:\", vocab)\n",
    "\n",
    "# Prepare a small batch of token index sequences.\n",
    "sequences = np.array([[1, 3, 0], [2, 3, 0]], dtype=np.int32)\n",
    "\n",
    "# Print the raw index sequences.\n",
    "print(\"Token index sequences:\", sequences)\n",
    "\n",
    "# Define embedding hyperparameters.\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 4\n",
    "\n",
    "# Create an Embedding layer like nn.Embedding.\n",
    "embedding_layer = layers.Embedding(\n",
    "    input_dim=vocab_size,\n",
    "    output_dim=embedding_dim,\n",
    "    embeddings_initializer=\"uniform\",\n",
    ")\n",
    "\n",
    "# Pass the sequences through the embedding layer.\n",
    "embedded = embedding_layer(sequences)\n",
    "\n",
    "# Convert embeddings to a NumPy array.\n",
    "embedded_np = embedded.numpy()\n",
    "\n",
    "# Validate the resulting embedding shape.\n",
    "print(\"Embedding shape:\", embedded_np.shape)\n",
    "\n",
    "# Print embeddings for the first sequence only.\n",
    "print(\"Embeddings for first sequence:\")\n",
    "print(embedded_np[0])\n",
    "\n",
    "# Show that same token index shares same vector.\n",
    "print(\"Vector for 'movie' in both sequences:\")\n",
    "print(embedded_np[0, 1], embedded_np[1, 1])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f79877e",
   "metadata": {},
   "source": [
    "### **1.2. Handling Padding Tokens**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d893e102",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_06/Lecture_B/image_01_02.jpg?v=1769756764\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Padding tokens standardize sequence lengths for batching\n",
    ">* If learned, padding embeddings can mislead models\n",
    "\n",
    ">* Give padding a special index and treatment\n",
    ">* Neutralize padding embeddings so only real words matter\n",
    "\n",
    ">* Padding noise can corrupt sequence information aggregation\n",
    ">* Neutral padding improves generalization to varying lengths\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddb9d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Handling Padding Tokens\n",
    "\n",
    "# This script shows padding token embeddings clearly.\n",
    "# It uses TensorFlow to mimic PyTorch style embeddings.\n",
    "# Focus on padding index behavior inside embedding layers.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Import TensorFlow and Keras layers.\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Set deterministic random seeds for reproducibility.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Define a tiny toy vocabulary with a padding token.\n",
    "PAD_TOKEN = \"<pad>\"\n",
    "WORD_TO_INDEX = {PAD_TOKEN: 0, \"thanks\": 1, \"for\": 2}\n",
    "\n",
    "# Show the mapping so learners see the padding index.\n",
    "print(\"Vocabulary mapping:\", WORD_TO_INDEX)\n",
    "\n",
    "# Create two short example sentences as index sequences.\n",
    "sentence_a = [WORD_TO_INDEX[\"thanks\"], WORD_TO_INDEX[\"for\"]]\n",
    "sentence_b = [WORD_TO_INDEX[\"thanks\"]]\n",
    "\n",
    "# Pad the shorter sentence manually using the padding index.\n",
    "max_len = 2\n",
    "padded_a = sentence_a\n",
    "padded_b = sentence_b + [WORD_TO_INDEX[PAD_TOKEN]]\n",
    "\n",
    "# Stack sequences into a small batch tensor.\n",
    "batch_indices = np.array([padded_a, padded_b], dtype=np.int32)\n",
    "\n",
    "# Validate the batch shape before embedding lookup.\n",
    "print(\"Batch indices shape:\", batch_indices.shape)\n",
    "\n",
    "# Define embedding dimensions and vocabulary size.\n",
    "vocab_size = len(WORD_TO_INDEX)\n",
    "embedding_dim = 4\n",
    "\n",
    "# Create an embedding layer with a mask for padding index.\n",
    "embedding_layer = layers.Embedding(\n",
    "    input_dim=vocab_size,\n",
    "    output_dim=embedding_dim,\n",
    "    mask_zero=True,\n",
    "    name=\"toy_embedding\",\n",
    ")\n",
    "\n",
    "# Pass the batch through the embedding layer once.\n",
    "embedded_batch = embedding_layer(batch_indices)\n",
    "\n",
    "# Retrieve the boolean mask created for padding positions.\n",
    "mask = embedding_layer.compute_mask(batch_indices)\n",
    "\n",
    "# Convert tensors to numpy arrays for easy printing.\n",
    "embedded_np = embedded_batch.numpy()\n",
    "mask_np = mask.numpy()\n",
    "\n",
    "# Print the raw batch indices and corresponding mask.\n",
    "print(\"Batch indices:\\n\", batch_indices)\n",
    "print(\"Mask for non padding positions:\\n\", mask_np)\n",
    "\n",
    "# Show embeddings for the first sequence tokens.\n",
    "print(\"Embeddings for first sequence:\\n\", embedded_np[0])\n",
    "\n",
    "# Show embeddings for the second sequence including padding.\n",
    "print(\"Embeddings for second sequence:\\n\", embedded_np[1])\n",
    "\n",
    "# Demonstrate how the mask can ignore padding in a mean.\n",
    "masked_embeddings = embedded_batch * tf.cast(\n",
    "    tf.expand_dims(mask, axis=-1), tf.float32\n",
    ")\n",
    "\n",
    "# Compute a simple masked mean over sequence length.\n",
    "sum_embeddings = tf.reduce_sum(masked_embeddings, axis=1)\n",
    "valid_counts = tf.reduce_sum(tf.cast(mask, tf.float32), axis=1)\n",
    "mean_embeddings = sum_embeddings / tf.expand_dims(tf.maximum(valid_counts, 1.0), axis=-1)\n",
    "\n",
    "# Print the final masked sentence level representations.\n",
    "print(\"Masked mean sentence embeddings:\\n\", mean_embeddings.numpy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b175ab",
   "metadata": {},
   "source": [
    "### **1.3. Initializing embeddings**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f45b741",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_06/Lecture_B/image_01_03.jpg?v=1769756837\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Embeddings start as randomly initialized, meaningless vectors\n",
    ">* Training gradients reshape them so similar words cluster\n",
    "\n",
    ">* Use pre-trained embeddings for faster, better learning\n",
    ">* Match vocab, copy known vectors, randomly init unknowns\n",
    "\n",
    ">* Choose to freeze or fine-tune embeddings\n",
    ">* Initialization injects prior knowledge and guides learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668b0c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Initializing embeddings\n",
    "\n",
    "# This script shows how embeddings are initialized.\n",
    "# It uses TensorFlow to create simple embeddings.\n",
    "# Focus on shapes and values not heavy training.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Import TensorFlow and check version.\n",
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Set deterministic random seeds.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# Set TensorFlow random seed.\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Define a tiny toy vocabulary.\n",
    "vocab = [\"<pad>\", \"cat\", \"dog\", \"banana\", \"excellent\"]\n",
    "\n",
    "# Map tokens to integer indices.\n",
    "word_to_index = {w: i for i, w in enumerate(vocab)}\n",
    "\n",
    "# Show the vocabulary mapping.\n",
    "print(\"Vocabulary mapping:\", word_to_index)\n",
    "\n",
    "# Define embedding configuration.\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 4\n",
    "\n",
    "# Create a random initialized embedding layer.\n",
    "random_embedding_layer = tf.keras.layers.Embedding(\n",
    "    input_dim=vocab_size,\n",
    "    output_dim=embedding_dim,\n",
    "    embeddings_initializer=\"uniform\",\n",
    "    name=\"random_embedding\",\n",
    ")\n",
    "\n",
    "# Build the layer by calling it once.\n",
    "dummy_indices = tf.constant([[1, 2, 3]])\n",
    "random_vectors = random_embedding_layer(dummy_indices)\n",
    "\n",
    "# Confirm output shape is as expected.\n",
    "print(\"Random embedding output shape:\", random_vectors.shape)\n",
    "\n",
    "# Inspect the underlying random embedding matrix.\n",
    "random_matrix = random_embedding_layer.get_weights()[0]\n",
    "print(\"Random embedding matrix shape:\", random_matrix.shape)\n",
    "\n",
    "# Show embeddings for specific example tokens.\n",
    "cat_index = word_to_index[\"cat\"]\n",
    "dog_index = word_to_index[\"dog\"]\n",
    "\n",
    "# Gather their vectors from the matrix.\n",
    "cat_vector = random_matrix[cat_index]\n",
    "dog_vector = random_matrix[dog_index]\n",
    "\n",
    "# Print the random vectors for cat and dog.\n",
    "print(\"Random cat vector:\", np.round(cat_vector, 3))\n",
    "print(\"Random dog vector:\", np.round(dog_vector, 3))\n",
    "\n",
    "# Create fake pre trained embedding matrix.\n",
    "pretrained_matrix = np.zeros((vocab_size, embedding_dim), dtype=np.float32)\n",
    "\n",
    "# Manually define simple semantic patterns.\n",
    "pretrained_matrix[word_to_index[\"cat\"]] = np.array([1.0, 0.9, 0.0, 0.0])\n",
    "pretrained_matrix[word_to_index[\"dog\"]] = np.array([0.9, 1.0, 0.0, 0.0])\n",
    "\n",
    "# Put fruit in a different region.\n",
    "pretrained_matrix[word_to_index[\"banana\"]] = np.array([0.0, 0.0, 1.0, 0.8])\n",
    "\n",
    "# Put positive word near animals for illustration.\n",
    "pretrained_matrix[word_to_index[\"excellent\"]] = np.array([0.8, 0.7, 0.2, 0.1])\n",
    "\n",
    "# Keep padding token as zeros.\n",
    "pretrained_matrix[word_to_index[\"<pad>\"]] = np.zeros(embedding_dim)\n",
    "\n",
    "# Create an embedding layer using pretrained weights.\n",
    "pretrained_embedding_layer = tf.keras.layers.Embedding(\n",
    "    input_dim=vocab_size,\n",
    "    output_dim=embedding_dim,\n",
    "    embeddings_initializer=tf.keras.initializers.Constant(pretrained_matrix),\n",
    "    trainable=False,\n",
    "    name=\"pretrained_embedding\",\n",
    ")\n",
    "\n",
    "# Build the pretrained layer by calling it.\n",
    "pretrained_vectors = pretrained_embedding_layer(dummy_indices)\n",
    "\n",
    "# Confirm pretrained output shape.\n",
    "print(\"Pretrained embedding output shape:\", pretrained_vectors.shape)\n",
    "\n",
    "# Extract cat and dog vectors from pretrained matrix.\n",
    "pre_cat_vector = pretrained_matrix[cat_index]\n",
    "pre_dog_vector = pretrained_matrix[dog_index]\n",
    "\n",
    "# Print pretrained vectors for comparison.\n",
    "print(\"Pretrained cat vector:\", pre_cat_vector)\n",
    "print(\"Pretrained dog vector:\", pre_dog_vector)\n",
    "\n",
    "# Compute cosine similarity between cat and dog.\n",
    "def cosine_similarity(a, b):\n",
    "    a = a.astype(np.float32)\n",
    "    b = b.astype(np.float32)\n",
    "    num = np.dot(a, b)\n",
    "    den = np.linalg.norm(a) * np.linalg.norm(b)\n",
    "    return float(num / den) if den != 0 else 0.0\n",
    "\n",
    "# Compare similarity in pretrained space.\n",
    "cat_dog_sim = cosine_similarity(pre_cat_vector, pre_dog_vector)\n",
    "\n",
    "# Print final summary line about similarity.\n",
    "print(\"Cosine similarity between pretrained cat and dog:\", round(cat_dog_sim, 3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc5d5d6",
   "metadata": {},
   "source": [
    "## **2. Text Sequence Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21dc3476",
   "metadata": {},
   "source": [
    "### **2.1. Recurrent Layers Overview**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcabcce",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_06/Lecture_B/image_02_01.jpg?v=1769756911\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Recurrent layers read sequences step by step\n",
    ">* Hidden state summarizes past context and word meanings\n",
    "\n",
    ">* Recurrent layers turn variable text into vectors\n",
    ">* Shared parameters handle phrases, negations, and lengths\n",
    "\n",
    ">* RNNs, LSTMs, GRUs trade memory and cost\n",
    ">* Choosing a layer balances context needs and resources\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf15f44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Recurrent Layers Overview\n",
    "\n",
    "# This script shows simple recurrent layers usage.\n",
    "# We build a tiny RNN text classifier example.\n",
    "# Focus is understanding sequence processing mechanics.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Import tensorflow and keras submodules.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Set deterministic random seeds for reproducibility.\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Create a tiny toy sentiment dataset.\n",
    "texts = [\n",
    "    \"i love this movie\",\n",
    "    \"this film was great\",\n",
    "    \"what a fantastic story\",\n",
    "    \"i really enjoyed it\",\n",
    "    \"i hate this movie\",\n",
    "    \"this film was terrible\",\n",
    "    \"what a boring story\",\n",
    "    \"i really disliked it\",\n",
    "]\n",
    "\n",
    "# Create integer labels for sentiment classes.\n",
    "labels = np.array([1, 1, 1, 1, 0, 0, 0, 0], dtype=np.int32)\n",
    "\n",
    "# Build a simple word level vocabulary.\n",
    "vocab = {}\n",
    "for text in texts:\n",
    "    for token in text.split():\n",
    "        if token not in vocab:\n",
    "            vocab[token] = len(vocab) + 1\n",
    "\n",
    "# Show vocabulary mapping for clarity.\n",
    "print(\"Vocabulary:\", vocab)\n",
    "\n",
    "# Convert texts into lists of token indices.\n",
    "sequences = []\n",
    "for text in texts:\n",
    "    indices = [vocab[token] for token in text.split()]\n",
    "    sequences.append(indices)\n",
    "\n",
    "# Pad sequences to equal length for batching.\n",
    "max_len = max(len(seq) for seq in sequences)\n",
    "X = keras.preprocessing.sequence.pad_sequences(\n",
    "    sequences, maxlen=max_len, padding=\"post\", truncating=\"post\"\n",
    ")\n",
    "\n",
    "# Validate shapes before building the model.\n",
    "print(\"Input shape:\", X.shape, \"Labels shape:\", labels.shape)\n",
    "\n",
    "# Define basic hyperparameters for the model.\n",
    "vocab_size = len(vocab) + 1\n",
    "embedding_dim = 8\n",
    "rnn_units = 16\n",
    "\n",
    "# Build a simple RNN based text classifier.\n",
    "model = keras.Sequential([\n",
    "    layers.Embedding(vocab_size, embedding_dim, input_length=max_len),\n",
    "    layers.SimpleRNN(rnn_units),\n",
    "    layers.Dense(1, activation=\"sigmoid\"),\n",
    "])\n",
    "\n",
    "# Compile model with binary crossentropy loss.\n",
    "model.compile(\n",
    "    optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Train briefly with silent verbose setting.\n",
    "history = model.fit(\n",
    "    X, labels, epochs=20, batch_size=4, verbose=0, validation_split=0.25\n",
    ")\n",
    "\n",
    "# Evaluate model performance on training data.\n",
    "loss, acc = model.evaluate(X, labels, verbose=0)\n",
    "\n",
    "# Print final loss and accuracy in few lines.\n",
    "print(\"Final training loss:\", round(float(loss), 4))\n",
    "print(\"Final training accuracy:\", round(float(acc), 4))\n",
    "\n",
    "# Inspect model prediction for one sample.\n",
    "sample_text = \"i really enjoyed this movie\"\n",
    "\n",
    "# Tokenize sample text using existing vocabulary.\n",
    "sample_tokens = [\n",
    "    vocab.get(token, 0) for token in sample_text.split()\n",
    "]\n",
    "\n",
    "# Pad sample sequence to required length.\n",
    "sample_seq = keras.preprocessing.sequence.pad_sequences(\n",
    "    [sample_tokens], maxlen=max_len, padding=\"post\", truncating=\"post\"\n",
    ")\n",
    "\n",
    "# Run model prediction on the sample.\n",
    "prob = float(model.predict(sample_seq, verbose=0)[0][0])\n",
    "\n",
    "# Print predicted probability for positive sentiment.\n",
    "print(\"Sample text:\", sample_text)\n",
    "print(\"Predicted positive probability:\", round(prob, 4))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b344e71",
   "metadata": {},
   "source": [
    "### **2.2. Convolutional Text Patterns**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8d0f17",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_06/Lecture_B/image_02_02.jpg?v=1769756973\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* CNN filters scan local nâ€‘gram token windows\n",
    ">* Filters learn phrase patterns linked to labels\n",
    "\n",
    ">* Filters detect useful phrases anywhere in text\n",
    ">* Different filters learn varied local and stylistic patterns\n",
    "\n",
    ">* Filters act as feature detectors for phrases\n",
    ">* Stacked filters capture richer, increasingly abstract patterns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8798b75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Convolutional Text Patterns\n",
    "\n",
    "# This script shows convolutional text patterns.\n",
    "# We build a tiny CNN text classifier.\n",
    "# Focus is on local ngram style filters.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Import tensorflow and keras submodules.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Set deterministic random seeds everywhere.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print tensorflow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Prepare a tiny toy text dataset.\n",
    "texts = [\n",
    "    \"I absolutely love this product\",\n",
    "    \"This is not good at all\",\n",
    "    \"I am very disappointed today\",\n",
    "    \"I highly recommend this item\",\n",
    "    \"The quality is really bad\",\n",
    "    \"It works great and I am happy\",\n",
    "]\n",
    "\n",
    "# Create matching sentiment labels list.\n",
    "labels = [1, 0, 0, 1, 0, 1]\n",
    "\n",
    "# Build a simple Keras tokenizer.\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(num_words=50)\n",
    "\n",
    "# Fit tokenizer on the small texts.\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "# Convert texts into integer sequences.\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "# Choose a fixed maximum sequence length.\n",
    "max_len = 8\n",
    "\n",
    "# Pad sequences to the same length.\n",
    "X = keras.preprocessing.sequence.pad_sequences(\n",
    "    sequences, maxlen=max_len, padding=\"post\"\n",
    ")\n",
    "\n",
    "# Convert labels into numpy array.\n",
    "y = np.array(labels, dtype=\"int32\")\n",
    "\n",
    "# Validate shapes before modeling.\n",
    "print(\"Input shape:\", X.shape, \"Labels shape:\", y.shape)\n",
    "\n",
    "# Define vocabulary size from tokenizer.\n",
    "vocab_size = min(50, len(tokenizer.word_index) + 1)\n",
    "\n",
    "# Create a simple CNN text classification model.\n",
    "model = keras.Sequential([\n",
    "    layers.Embedding(vocab_size, 8, input_length=max_len),\n",
    "    layers.Conv1D(4, 3, activation=\"relu\"),\n",
    "    layers.GlobalMaxPooling1D(),\n",
    "    layers.Dense(1, activation=\"sigmoid\"),\n",
    "])\n",
    "\n",
    "# Build the model to ensure it has an input tensor.\n",
    "model.build(input_shape=(None, max_len))\n",
    "\n",
    "# Force-call the model once so it has a defined input tensor.\n",
    "_ = model(X[:1])\n",
    "\n",
    "# Compile model with binary crossentropy loss.\n",
    "model.compile(\n",
    "    optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Train model silently for few epochs.\n",
    "history = model.fit(\n",
    "    X, y, epochs=20, batch_size=2, verbose=0, validation_split=0.3\n",
    ")\n",
    "\n",
    "# Evaluate model performance on training data.\n",
    "loss, acc = model.evaluate(X, y, verbose=0)\n",
    "\n",
    "# Print final loss and accuracy values.\n",
    "print(\"Training loss:\", round(float(loss), 4))\n",
    "print(\"Training accuracy:\", round(float(acc), 4))\n",
    "\n",
    "# Select one example to inspect convolution.\n",
    "example_text = \"I am not happy with this\"\n",
    "\n",
    "# Convert example text to padded sequence.\n",
    "example_seq = tokenizer.texts_to_sequences([example_text])\n",
    "example_pad = keras.preprocessing.sequence.pad_sequences(\n",
    "    example_seq, maxlen=max_len, padding=\"post\"\n",
    ")\n",
    "\n",
    "# Build a submodel to see conv outputs.\n",
    "conv_layer = model.layers[1]\n",
    "conv_model = keras.Model(model.inputs, conv_layer.output)\n",
    "\n",
    "# Get convolution feature map for example.\n",
    "conv_output = conv_model.predict(example_pad, verbose=0)\n",
    "\n",
    "# Print shape of convolution feature map.\n",
    "print(\"Conv feature map shape:\", conv_output.shape)\n",
    "\n",
    "# Print feature map values for first filter.\n",
    "print(\"First filter activations:\", np.round(conv_output[0, :, 0], 3))\n",
    "\n",
    "# Show model prediction for the example.\n",
    "pred_prob = float(model.predict(example_pad, verbose=0)[0, 0])\n",
    "print(\"Predicted positive probability:\", round(pred_prob, 4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a965337",
   "metadata": {},
   "source": [
    "### **2.3. Temporal Pooling Basics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c22f83",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_06/Lecture_B/image_02_03.jpg?v=1769757089\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Temporal pooling compresses variable-length sequences into vectors\n",
    ">* Summarizes all token information for classification\n",
    "\n",
    ">* Average pooling captures overall context across tokens\n",
    ">* Max pooling highlights strongest, most informative time steps\n",
    "\n",
    ">* Combine pooling methods to capture varied sequence information\n",
    ">* Choose pooling carefully to summarize long, noisy texts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4ccda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Temporal Pooling Basics\n",
    "\n",
    "# This script shows temporal pooling basics clearly.\n",
    "# We use tiny sequences and simple PyTorch tensors.\n",
    "# Focus is on average and max temporal pooling.\n",
    "\n",
    "# import required PyTorch modules for tensors.\n",
    "import torch\n",
    "\n",
    "# set deterministic seed for reproducible tensors.\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# create a tiny batch of token index sequences.\n",
    "sequences = torch.tensor([[1, 2, 3, 0], [4, 5, 0, 0]])\n",
    "\n",
    "# print the raw token index sequences batch.\n",
    "print(\"Token index sequences:\", sequences)\n",
    "\n",
    "# define a small embedding layer for tokens.\n",
    "embedding_dim = 4\n",
    "vocab_size = 10\n",
    "embed = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "# get embedded representations for each token.\n",
    "embedded = embed(sequences)\n",
    "\n",
    "# confirm embedded tensor has expected shape.\n",
    "print(\"Embedded shape (batch, time, dim):\", embedded.shape)\n",
    "\n",
    "# create a mask to ignore padding tokens.\n",
    "pad_token = 0\n",
    "mask = (sequences != pad_token).unsqueeze(-1)\n",
    "\n",
    "# validate mask shape matches embedded shape.\n",
    "assert mask.shape == embedded.shape[:2] + (1,)\n",
    "\n",
    "# apply mask so padding embeddings become zero.\n",
    "masked_embedded = embedded * mask\n",
    "\n",
    "# compute lengths of each non padded sequence.\n",
    "lengths = mask.sum(dim=1).clamp(min=1)\n",
    "\n",
    "# perform average pooling over time dimension.\n",
    "avg_pooled = masked_embedded.sum(dim=1) / lengths\n",
    "\n",
    "# perform max pooling over time dimension safely.\n",
    "masked_for_max = masked_embedded.clone()\n",
    "\n",
    "# replace zero only positions with large negative.\n",
    "masked_for_max[~mask.expand_as(masked_for_max)] = -1e9\n",
    "\n",
    "# compute max pooled representation across time.\n",
    "max_pooled, _ = masked_for_max.max(dim=1)\n",
    "\n",
    "# print pooled vectors for interpretation.\n",
    "print(\"Average pooled vectors:\", avg_pooled)\n",
    "\n",
    "# show how max pooling keeps strongest features.\n",
    "print(\"Max pooled vectors:\", max_pooled)\n",
    "\n",
    "# final print summarizing temporal pooling usage.\n",
    "print(\"Temporal pooling gives fixed size sequence summaries.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbce93b",
   "metadata": {},
   "source": [
    "## **3. Training Text Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfd98b1",
   "metadata": {},
   "source": [
    "### **3.1. CrossEntropy Loss for Labels**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c91005",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_06/Lecture_B/image_03_01.jpg?v=1769757135\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Cross entropy measures how wrong class predictions are\n",
    ">* Lower loss means higher probability on correct label\n",
    "\n",
    ">* Penalizes wrong, overconfident predictions more strongly\n",
    ">* Training reduces loss, boosting confidence in correct labels\n",
    "\n",
    ">* Batch loss averages per-example cross entropy values\n",
    ">* Optimizer updates weights to boost correct label probabilities\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756694da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - CrossEntropy Loss for Labels\n",
    "\n",
    "# This script shows cross entropy loss usage.\n",
    "# We build a tiny text classifier example.\n",
    "# Focus is on labels and loss behavior.\n",
    "\n",
    "# Optional install for TensorFlow in some environments.\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Import TensorFlow and Keras utilities.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Set deterministic random seeds for reproducibility.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Define a tiny toy text dataset.\n",
    "texts = [\n",
    "    \"i love this movie\",\n",
    "    \"this film was great\",\n",
    "    \"what a fantastic story\",\n",
    "    \"i hate this movie\",\n",
    "    \"this film was terrible\",\n",
    "    \"what a boring story\",\n",
    "]\n",
    "\n",
    "# Define integer labels for sentiment classes.\n",
    "labels = [1, 1, 1, 0, 0, 0]\n",
    "\n",
    "# Create a simple word index mapping.\n",
    "vocab = {}\n",
    "for sentence in texts:\n",
    "    for word in sentence.split():\n",
    "        if word not in vocab:\n",
    "            vocab[word] = len(vocab) + 1\n",
    "\n",
    "# Convert sentences to sequences of token indices.\n",
    "sequences = []\n",
    "for sentence in texts:\n",
    "    seq = [vocab[word] for word in sentence.split()]\n",
    "    sequences.append(seq)\n",
    "\n",
    "# Pad sequences to equal length for batching.\n",
    "max_len = max(len(seq) for seq in sequences)\n",
    "X = keras.preprocessing.sequence.pad_sequences(\n",
    "    sequences, maxlen=max_len, padding=\"post\"\n",
    ")\n",
    "\n",
    "# Convert labels to a NumPy array of integers.\n",
    "y = np.array(labels, dtype=\"int32\")\n",
    "\n",
    "# Validate shapes before building the model.\n",
    "print(\"Input shape:\", X.shape, \"Labels shape:\", y.shape)\n",
    "\n",
    "# Define vocabulary size and embedding dimension.\n",
    "vocab_size = len(vocab) + 1\n",
    "embedding_dim = 8\n",
    "\n",
    "# Build a simple text classification model.\n",
    "inputs = keras.Input(shape=(max_len,))\n",
    "embedding = layers.Embedding(\n",
    "    input_dim=vocab_size, output_dim=embedding_dim\n",
    ")(inputs)\n",
    "\n",
    "# Pool over time to get a fixed size vector.\n",
    "pooled = layers.GlobalAveragePooling1D()(embedding)\n",
    "\n",
    "# Add a small dense layer for representation.\n",
    "hidden = layers.Dense(8, activation=\"relu\")(pooled)\n",
    "\n",
    "# Final layer outputs logits for two classes.\n",
    "logits = layers.Dense(2, activation=None)(hidden)\n",
    "\n",
    "# Create the Keras model object.\n",
    "model = keras.Model(inputs=inputs, outputs=logits)\n",
    "\n",
    "# Compile model with sparse categorical crossentropy.\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Train the model silently for a few epochs.\n",
    "history = model.fit(\n",
    "    X,\n",
    "    y,\n",
    "    epochs=20,\n",
    "    batch_size=2,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Evaluate model on the same tiny dataset.\n",
    "loss_value, acc_value = model.evaluate(\n",
    "    X,\n",
    "    y,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Print final loss and accuracy for inspection.\n",
    "print(\"Final loss (cross entropy):\", round(loss_value, 4))\n",
    "print(\"Final accuracy:\", round(acc_value, 4))\n",
    "\n",
    "# Get model logits and probabilities for first example.\n",
    "logits_example = model.predict(X[:1], verbose=0)\n",
    "probs_example = tf.nn.softmax(logits_example, axis=-1)\n",
    "\n",
    "# Print predicted probabilities and true label.\n",
    "print(\"Predicted probabilities:\", probs_example.numpy()[0])\n",
    "print(\"True label:\", int(y[0]))\n",
    "\n",
    "# Manually compute cross entropy for first example.\n",
    "true_label_index = int(y[0])\n",
    "true_prob = probs_example.numpy()[0][true_label_index]\n",
    "manual_loss = -np.log(true_prob + 1e-7)\n",
    "\n",
    "# Show manual loss compared with model loss.\n",
    "print(\"Manual cross entropy example:\", round(float(manual_loss), 4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d64de0",
   "metadata": {},
   "source": [
    "### **3.2. Ignoring Padding Tokens**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4026ba7",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_06/Lecture_B/image_03_02.jpg?v=1769757204\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Padding makes variable-length sequences share batch size\n",
    ">* Padding tokens are meaningless and must not affect loss\n",
    "\n",
    ">* Use a mask to ignore padded positions\n",
    ">* Focus loss and gradients on real tokens\n",
    "\n",
    ">* Padding noise worsens with very different lengths\n",
    ">* Masking padding gives cleaner gradients, better generalization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e8c65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Ignoring Padding Tokens\n",
    "\n",
    "# This script shows how padding masks work.\n",
    "# We build a tiny text classifier with padding.\n",
    "# We monitor loss while ignoring padding tokens.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Import TensorFlow and Keras layers.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Set deterministic random seeds.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version briefly.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Define a small toy text dataset.\n",
    "texts = [\n",
    "    \"good movie\",\n",
    "    \"great film\",\n",
    "    \"excellent acting\",\n",
    "    \"bad movie\",\n",
    "    \"terrible film\",\n",
    "    \"poor acting\",\n",
    "]\n",
    "\n",
    "# Define binary sentiment labels.\n",
    "labels = np.array([1, 1, 1, 0, 0, 0], dtype=np.int32)\n",
    "\n",
    "# Create a simple word index mapping.\n",
    "word_index = {\"<pad>\": 0, \"good\": 1, \"movie\": 2, \"great\": 3}\n",
    "word_index.update({\"film\": 4, \"excellent\": 5, \"acting\": 6})\n",
    "word_index.update({\"bad\": 7, \"terrible\": 8, \"poor\": 9})\n",
    "\n",
    "# Define maximum sequence length.\n",
    "max_len = 4\n",
    "\n",
    "# Encode texts into index sequences.\n",
    "encoded_sequences = []\n",
    "for text in texts:\n",
    "    tokens = text.split()\n",
    "    seq = [word_index.get(t, 0) for t in tokens]\n",
    "    encoded_sequences.append(seq)\n",
    "\n",
    "# Pad sequences with the pad index.\n",
    "padded_sequences = keras.preprocessing.sequence.pad_sequences(\n",
    "    encoded_sequences, maxlen=max_len, padding=\"post\", value=0\n",
    ")\n",
    "\n",
    "# Convert padded sequences to int32 array.\n",
    "inputs = np.array(padded_sequences, dtype=np.int32)\n",
    "\n",
    "# Validate shapes before building model.\n",
    "assert inputs.shape[0] == labels.shape[0]\n",
    "assert inputs.shape[1] == max_len\n",
    "\n",
    "# Define vocabulary size and embedding dimension.\n",
    "vocab_size = len(word_index)\n",
    "embedding_dim = 8\n",
    "\n",
    "# Build a simple text classification model.\n",
    "inputs_layer = keras.Input(shape=(max_len,), dtype=\"int32\")\n",
    "embedding_layer = layers.Embedding(\n",
    "    input_dim=vocab_size, output_dim=embedding_dim, mask_zero=True\n",
    ")(inputs_layer)\n",
    "\n",
    "# Use a masked global average pooling layer.\n",
    "pooled = layers.GlobalAveragePooling1D()(embedding_layer)\n",
    "\n",
    "# Add a small dense classification head.\n",
    "outputs_layer = layers.Dense(1, activation=\"sigmoid\")(pooled)\n",
    "model = keras.Model(inputs=inputs_layer, outputs=outputs_layer)\n",
    "\n",
    "# Compile model with binary crossentropy loss.\n",
    "model.compile(\n",
    "    optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Train model silently for a few epochs.\n",
    "history = model.fit(\n",
    "    inputs,\n",
    "    labels,\n",
    "    epochs=10,\n",
    "    batch_size=2,\n",
    "    verbose=0,\n",
    "    validation_split=0.33,\n",
    ")\n",
    "\n",
    "# Evaluate model performance on training data.\n",
    "loss, acc = model.evaluate(inputs, labels, verbose=0)\n",
    "\n",
    "# Print final loss and accuracy values.\n",
    "print(\"Final training loss (masked):\", round(float(loss), 4))\n",
    "print(\"Final training accuracy (masked):\", round(float(acc), 4))\n",
    "\n",
    "# Show padded inputs and model predictions.\n",
    "predictions = model.predict(inputs, verbose=0)\n",
    "print(\"Padded input sequences:\")\n",
    "print(inputs)\n",
    "print(\"Predicted probabilities:\")\n",
    "print(np.round(predictions.squeeze(), 4))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5666ec2",
   "metadata": {},
   "source": [
    "### **3.3. Measuring Model Accuracy**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a99e02",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_06/Lecture_B/image_03_03.jpg?v=1769757264\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Accuracy counts how many predictions are correct\n",
    ">* Correct predictions divided by total gives performance percentage\n",
    "\n",
    ">* Track accuracy on training and validation sets\n",
    ">* Diverging accuracies signal overfitting and poor generalization\n",
    "\n",
    ">* Accuracy works best on balanced classification tasks\n",
    ">* Use extra metrics and track loss, accuracy trends\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2a29e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Measuring Model Accuracy\n",
    "\n",
    "# This script shows how to measure accuracy.\n",
    "# We use a tiny text classification example.\n",
    "# Focus is on computing accuracy clearly.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Import tensorflow and keras utilities.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Set deterministic random seeds.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print tensorflow version briefly.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Prepare a tiny toy text dataset.\n",
    "texts = [\n",
    "    \"good movie\", \"great film\", \"nice story\", \"bad movie\",\n",
    "    \"terrible film\", \"awful story\", \"loved it\", \"hated it\",\n",
    "]\n",
    "\n",
    "# Create binary sentiment labels.\n",
    "labels = np.array([1, 1, 1, 0, 0, 0, 1, 0], dtype=np.int32)\n",
    "\n",
    "# Use a simple text vectorization layer.\n",
    "vectorizer = layers.TextVectorization(\n",
    "    max_tokens=20, output_mode=\"int\", output_sequence_length=4\n",
    ")\n",
    "\n",
    "# Adapt vectorizer on the small corpus.\n",
    "vectorizer.adapt(np.array(texts))\n",
    "\n",
    "# Vectorize all texts into integer sequences.\n",
    "text_ds = tf.data.Dataset.from_tensor_slices(texts)\n",
    "int_sequences = text_ds.map(vectorizer).batch(len(texts))\n",
    "\n",
    "# Materialize sequences as a numpy array.\n",
    "X_all = next(iter(int_sequences)).numpy()\n",
    "\n",
    "# Validate shapes before modeling.\n",
    "print(\"Input shape:\", X_all.shape, \"Labels shape:\", labels.shape)\n",
    "\n",
    "# Split into small train and validation sets.\n",
    "X_train, X_val = X_all[:6], X_all[6:]\n",
    "y_train, y_val = labels[:6], labels[6:]\n",
    "\n",
    "# Build a simple embedding based model.\n",
    "model = keras.Sequential([\n",
    "    layers.Embedding(input_dim=20, output_dim=8, input_length=4),\n",
    "    layers.GlobalAveragePooling1D(),\n",
    "    layers.Dense(1, activation=\"sigmoid\"),\n",
    "])\n",
    "\n",
    "# Compile model with accuracy metric.\n",
    "model.compile(\n",
    "    optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Train briefly with silent verbose setting.\n",
    "history = model.fit(\n",
    "    X_train, y_train, epochs=10, batch_size=2, verbose=0,\n",
    "    validation_data=(X_val, y_val)\n",
    ")\n",
    "\n",
    "# Get final training accuracy from history.\n",
    "train_acc = history.history[\"accuracy\"][-1]\n",
    "val_acc = history.history[\"val_accuracy\"][-1]\n",
    "\n",
    "# Print accuracy values in a friendly way.\n",
    "print(\"Final training accuracy:\", round(float(train_acc), 3))\n",
    "print(\"Final validation accuracy:\", round(float(val_acc), 3))\n",
    "\n",
    "# Compute predictions on validation set.\n",
    "val_probs = model.predict(X_val, verbose=0)\n",
    "val_preds = (val_probs.flatten() >= 0.5).astype(\"int32\")\n",
    "\n",
    "# Manually compute accuracy step by step.\n",
    "correct = np.sum(val_preds == y_val)\n",
    "manual_acc = correct / y_val.shape[0]\n",
    "\n",
    "# Show manual accuracy to match metric.\n",
    "print(\"Manual validation accuracy:\", round(float(manual_acc), 3))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8ca2fd",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Text Models**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccb333b",
   "metadata": {},
   "source": [
    "\n",
    "In this lecture, you learned to:\n",
    "- Use nn.Embedding to convert token indices into dense vector representations. \n",
    "- Build a simple sequence model (e.g., RNN or CNN) for text classification in PyTorch. \n",
    "- Train and evaluate the text model on a labeled dataset, monitoring accuracy and loss. \n",
    "\n",
    "In the next Module (Module 7), we will go over 'torch.compile and Speed'"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
