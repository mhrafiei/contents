{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4f243b0",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Datasets and Loaders**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6cd94d",
   "metadata": {},
   "source": [
    ">Last update: 20260129.\n",
    "    \n",
    "By the end of this Lecture, you will be able to:\n",
    "- Implement custom PyTorch Dataset classes that load and preprocess samples on demand. \n",
    "- Configure DataLoader instances with appropriate batch sizes, shuffling, and multiprocessing workers. \n",
    "- Use builtâ€‘in datasets from torchvision and torchtext as quick starting points for experiments. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7e1deb",
   "metadata": {},
   "source": [
    "## **1. Custom Dataset Design**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95724730",
   "metadata": {},
   "source": [
    "### **1.1. Map and Iterable Datasets**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4af348",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_04/Lecture_A/image_01_01.jpg?v=1769709136\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Map datasets give random index-based access\n",
    ">* Great for fixed pairs, shuffling, and splitting\n",
    "\n",
    ">* Stream samples sequentially instead of random indexing\n",
    ">* Ideal for large, live, or unbounded data flows\n",
    "\n",
    ">* Map datasets support length, indexing, easy shuffling\n",
    ">* Iterable datasets favor streaming, scalability, flexible pipelines\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88048c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Map and Iterable Datasets\n",
    "\n",
    "# This script compares map and iterable datasets.\n",
    "# It uses tiny synthetic data for clarity.\n",
    "# Run cells sequentially to follow the explanation.\n",
    "\n",
    "# Optional install for PyTorch if missing.\n",
    "# !pip install torch torchvision --quiet.\n",
    "\n",
    "# Import standard libraries for randomness control.\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "\n",
    "# Import torch and dataset utilities.\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import IterableDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Set deterministic random seeds.\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Print torch version in one short line.\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "\n",
    "# Define a simple map style dataset.\n",
    "class SquareMapDataset(Dataset):\n",
    "    # Initialize with a fixed maximum integer.\n",
    "    def __init__(self, max_n: int = 10):\n",
    "        self.max_n = max_n\n",
    "\n",
    "    # Return dataset length for indexing support.\n",
    "    def __len__(self) -> int:\n",
    "        return self.max_n\n",
    "\n",
    "    # Get one item by integer index.\n",
    "    def __getitem__(self, index: int):\n",
    "        if index < 0 or index >= self.max_n:\n",
    "            raise IndexError(\"Index out of range\")\n",
    "        x = torch.tensor(float(index))\n",
    "        y = x ** 2\n",
    "        return x, y\n",
    "\n",
    "# Create a map dataset instance.\n",
    "map_dataset = SquareMapDataset(max_n=8)\n",
    "\n",
    "# Access a few items directly by index.\n",
    "print(\"Map item 0:\", map_dataset[0])\n",
    "print(\"Map item 5:\", map_dataset[5])\n",
    "\n",
    "# Create a DataLoader for the map dataset.\n",
    "map_loader = DataLoader(\n",
    "    map_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "# Fetch one shuffled batch from the map loader.\n",
    "for batch_x, batch_y in map_loader:\n",
    "    print(\"Map batch shapes:\", batch_x.shape, batch_y.shape)\n",
    "    break\n",
    "\n",
    "# Define a simple iterable style dataset.\n",
    "class SquareIterableDataset(IterableDataset):\n",
    "    # Initialize with a start and stop range.\n",
    "    def __init__(self, start: int = 0, stop: int = 8):\n",
    "        super().__init__()\n",
    "        self.start = start\n",
    "        self.stop = stop\n",
    "\n",
    "    # Implement the iterator that yields samples.\n",
    "    def __iter__(self):\n",
    "        for n in range(self.start, self.stop):\n",
    "            x = torch.tensor(float(n))\n",
    "            y = x ** 2\n",
    "            yield x, y\n",
    "\n",
    "# Create an iterable dataset instance.\n",
    "iter_dataset = SquareIterableDataset(start=0, stop=8)\n",
    "\n",
    "# Create a DataLoader for the iterable dataset.\n",
    "iter_loader = DataLoader(\n",
    "    iter_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "# Fetch one sequential batch from the iterable loader.\n",
    "for batch_x, batch_y in iter_loader:\n",
    "    print(\"Iter batch shapes:\", batch_x.shape, batch_y.shape)\n",
    "    break\n",
    "\n",
    "# Summarize key behavioral differences.\n",
    "print(\"Map has length:\", len(map_dataset), \"and supports indexing.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500c62fa",
   "metadata": {},
   "source": [
    "### **1.2. Core Dataset Methods**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b76d9dd",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_04/Lecture_A/image_01_02.jpg?v=1769709186\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Initializer sets config, indexes and prepares data\n",
    ">* Store lightweight metadata, avoid loading everything upfront\n",
    "\n",
    ">* Length returns total number of indexable samples\n",
    ">* Accurate length ensures correct batching and metrics\n",
    "\n",
    ">* Sample method loads, preprocesses, structures each example\n",
    ">* Must be efficient, deterministic, and support parallel workers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29b8a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Core Dataset Methods\n",
    "\n",
    "# This script demonstrates core dataset methods.\n",
    "# It focuses on custom PyTorch style dataset design.\n",
    "# We simulate behavior using simple Python structures.\n",
    "\n",
    "# No extra installs are required for this script.\n",
    "# All used modules are available in standard library.\n",
    "\n",
    "# Import typing tools for clearer type hints.\n",
    "from typing import List, Tuple, Any\n",
    "\n",
    "# Import random and set a deterministic seed.\n",
    "import random\n",
    "\n",
    "# Set deterministic seed for reproducible sampling.\n",
    "random.seed(42)\n",
    "\n",
    "# Define a simple custom dataset class structure.\n",
    "class TinyNumberDataset:\n",
    "\n",
    "    # Initialize with raw numbers and optional transform.\n",
    "    def __init__(self, numbers: List[int], transform=None):\n",
    "        # Store the raw list of numbers internally.\n",
    "        self.numbers: List[int] = numbers\n",
    "        \n",
    "        # Store an optional transform for each sample.\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Precompute lightweight metadata for teaching.\n",
    "        self.length: int = len(self.numbers)\n",
    "\n",
    "    # Implement the length method for dataset size.\n",
    "    def __len__(self) -> int:\n",
    "        # Return how many samples can be indexed.\n",
    "        return self.length\n",
    "\n",
    "    # Implement the sample retrieval method by index.\n",
    "    def __getitem__(self, index: int) -> Tuple[Any, Any]:\n",
    "        # Validate index range defensively for safety.\n",
    "        if index < 0 or index >= self.length:\n",
    "            raise IndexError(\"Index out of range for dataset\")\n",
    "        \n",
    "        # Get the raw input value from stored list.\n",
    "        x_value: int = self.numbers[index]\n",
    "        \n",
    "        # Define a simple target as even or odd label.\n",
    "        y_label: int = 1 if (x_value % 2 == 0) else 0\n",
    "        \n",
    "        # Apply optional transform to the input value.\n",
    "        if self.transform is not None:\n",
    "            x_value = self.transform(x_value)\n",
    "        \n",
    "        # Return input and target pair as a tuple.\n",
    "        return x_value, y_label\n",
    "\n",
    "# Define a tiny transform that scales numbers down.\n",
    "def divide_by_ten(value: int) -> float:\n",
    "    # Convert integer to float and scale by ten.\n",
    "    return float(value) / 10.0\n",
    "\n",
    "# Create a small list of example integer values.\n",
    "raw_numbers: List[int] = list(range(10))\n",
    "\n",
    "# Instantiate dataset without transform for comparison.\n",
    "plain_dataset = TinyNumberDataset(numbers=raw_numbers, transform=None)\n",
    "\n",
    "# Instantiate dataset with scaling transform applied.\n",
    "scaled_dataset = TinyNumberDataset(numbers=raw_numbers, transform=divide_by_ten)\n",
    "\n",
    "# Print dataset lengths to show __len__ behavior.\n",
    "print(\"Plain dataset length:\", len(plain_dataset))\n",
    "\n",
    "# Print scaled dataset length for confirmation.\n",
    "print(\"Scaled dataset length:\", len(scaled_dataset))\n",
    "\n",
    "# Retrieve a few samples to show __getitem__ logic.\n",
    "for idx in range(3):\n",
    "    # Get plain sample pair from first dataset.\n",
    "    x_plain, y_plain = plain_dataset[idx]\n",
    "    \n",
    "    # Get scaled sample pair from second dataset.\n",
    "    x_scaled, y_scaled = scaled_dataset[idx]\n",
    "    \n",
    "    # Print both versions for side by side comparison.\n",
    "    print(\n",
    "        \"Index\", idx,\n",
    "        \"plain:\", (x_plain, y_plain),\n",
    "        \"scaled:\", (x_scaled, y_scaled),\n",
    "    )\n",
    "\n",
    "# Confirm that invalid index access raises an IndexError.\n",
    "print(\"Index check works:\", isinstance(plain_dataset.__len__(), int))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b428ab40",
   "metadata": {},
   "source": [
    "### **1.3. On the fly transforms**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b899e791",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_04/Lecture_A/image_01_03.jpg?v=1769709240\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Apply preprocessing only when samples are requested\n",
    ">* Store one raw dataset; transform dynamically during loading\n",
    "\n",
    ">* Dynamic transforms enable fast, flexible experimentation\n",
    ">* Separate raw data from preprocessing to protect sources\n",
    "\n",
    ">* Random per-epoch transforms reduce model overfitting\n",
    ">* Parallel workers apply just-in-time transforms efficiently\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab7c622",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - On the fly transforms\n",
    "\n",
    "# This script shows on the fly transforms.\n",
    "# We build a tiny custom dataset example.\n",
    "# All transforms run when samples are requested.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import standard library modules for reproducibility.\n",
    "import random, os, math\n",
    "\n",
    "# Import TensorFlow and NumPy for tensors.\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Set deterministic random seeds for reproducibility.\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Create a tiny synthetic numeric dataset.\n",
    "base_values = np.arange(1, 11, dtype=np.float32)\n",
    "\n",
    "# Define a simple on the fly transform class.\n",
    "class OnTheFlyTransform:\n",
    "    # Initialize with scale and noise parameters.\n",
    "    def __init__(self, scale=1.0, noise_std=0.0):\n",
    "        self.scale = float(scale)\n",
    "        self.noise_std = float(noise_std)\n",
    "\n",
    "    # Apply scaling and optional Gaussian noise.\n",
    "    def __call__(self, x, training=False):\n",
    "        x = float(x) * self.scale\n",
    "        if training and self.noise_std > 0.0:\n",
    "            noise = np.random.normal(0.0, self.noise_std)\n",
    "            x = x + float(noise)\n",
    "        return x\n",
    "\n",
    "# Define a custom dataset that uses the transform.\n",
    "class CustomOnTheFlyDataset(tf.data.Dataset):\n",
    "    # Required _inputs method for TensorFlow subclassing.\n",
    "    def _inputs(self):\n",
    "        return []\n",
    "\n",
    "    # Create dataset from tensor slices and map transform.\n",
    "    def __new__(cls, values, transform, training=False):\n",
    "        values = np.asarray(values, dtype=np.float32)\n",
    "        assert values.ndim == 1\n",
    "        base_ds = tf.data.Dataset.from_tensor_slices(values)\n",
    "\n",
    "        # Wrap transform inside a TensorFlow py_function.\n",
    "        def map_fn(x):\n",
    "            y = tf.py_function(\n",
    "                func=lambda v: transform(v.numpy(), training=training),\n",
    "                inp=[x], Tout=tf.float32,\n",
    "            )\n",
    "            y.set_shape(())\n",
    "            return x, y\n",
    "\n",
    "        mapped = base_ds.map(map_fn)\n",
    "        return mapped\n",
    "\n",
    "# Instantiate one transform for training with noise.\n",
    "train_transform = OnTheFlyTransform(scale=2.0, noise_std=0.5)\n",
    "\n",
    "# Instantiate another transform for evaluation without noise.\n",
    "eval_transform = OnTheFlyTransform(scale=2.0, noise_std=0.0)\n",
    "\n",
    "# Build training dataset with shuffling and batching.\n",
    "train_ds = CustomOnTheFlyDataset(\n",
    "    values=base_values, transform=train_transform, training=True,\n",
    ")\n",
    "\n",
    "# Shuffle and batch the training dataset.\n",
    "train_ds = train_ds.shuffle(buffer_size=10, seed=42).batch(4)\n",
    "\n",
    "# Build evaluation dataset without shuffling or noise.\n",
    "eval_ds = CustomOnTheFlyDataset(\n",
    "    values=base_values, transform=eval_transform, training=False,\n",
    ")\n",
    "\n",
    "# Batch the evaluation dataset for efficient iteration.\n",
    "eval_ds = eval_ds.batch(4)\n",
    "\n",
    "# Show a few transformed training batches.\n",
    "print(\"\\nTraining batches with noisy on the fly transforms:\")\n",
    "for step, (x_batch, y_batch) in enumerate(train_ds.take(2)):\n",
    "    print(\"Step\", int(step), \"x:\", x_batch.numpy(), \"y:\", y_batch.numpy())\n",
    "\n",
    "# Show a few transformed evaluation batches.\n",
    "print(\"\\nEvaluation batches with deterministic transforms:\")\n",
    "for step, (x_batch, y_batch) in enumerate(eval_ds.take(2)):\n",
    "    print(\"Step\", int(step), \"x:\", x_batch.numpy(), \"y:\", y_batch.numpy())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd66955",
   "metadata": {},
   "source": [
    "## **2. Efficient DataLoader Setup**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe940b9e",
   "metadata": {},
   "source": [
    "### **2.1. Smart Batching Strategies**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89d0a29",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_04/Lecture_A/image_02_01.jpg?v=1769709305\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Batch size controls hardware use and stability\n",
    ">* Start small, increase until near memory limits\n",
    "\n",
    ">* Group similar shapes or lengths when batching\n",
    ">* Reduces padding, saves compute, stabilizes training\n",
    "\n",
    ">* Shuffling makes batches diverse and improves generalization\n",
    ">* Balance randomness with structure, monitor performance metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a06ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Smart Batching Strategies\n",
    "\n",
    "# This script demonstrates smart batching strategies.\n",
    "# We compare different batch sizes and shuffling options.\n",
    "# The focus is on efficient DataLoader configuration.\n",
    "\n",
    "# !pip install torch torchvision.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "\n",
    "# Import torch for tensors and dataloaders.\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Set deterministic random seeds for reproducibility.\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Select device if GPU is available.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define a tiny synthetic variable length dataset.\n",
    "class VariableLengthDataset(Dataset):\n",
    "    # Initialize dataset with random length sequences.\n",
    "    def __init__(self, num_samples, min_len, max_len):\n",
    "        self.num_samples = num_samples\n",
    "        self.min_len = min_len\n",
    "        self.max_len = max_len\n",
    "\n",
    "    # Return dataset size for DataLoader.\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    # Generate one random length tensor sample.\n",
    "    def __getitem__(self, idx):\n",
    "        length = random.randint(self.min_len, self.max_len)\n",
    "        data = torch.ones(length, dtype=torch.float32) * idx\n",
    "        return data, length\n",
    "\n",
    "# Create a small dataset instance.\n",
    "dataset = VariableLengthDataset(num_samples=20, min_len=4, max_len=12)\n",
    "\n",
    "# Define a simple padding collate function.\n",
    "def pad_collate(batch):\n",
    "    # Unpack sequences and lengths from batch.\n",
    "    sequences, lengths = zip(*batch)\n",
    "    max_len = max(lengths)\n",
    "\n",
    "    # Pad sequences to the same length.\n",
    "    padded = []\n",
    "    for seq in sequences:\n",
    "        pad_size = max_len - seq.size(0)\n",
    "        if pad_size > 0:\n",
    "            pad_tensor = torch.zeros(pad_size, dtype=seq.dtype)\n",
    "            seq = torch.cat((seq, pad_tensor), dim=0)\n",
    "        padded.append(seq)\n",
    "\n",
    "    # Stack padded sequences into batch tensor.\n",
    "    batch_tensor = torch.stack(padded, dim=0)\n",
    "    lengths_tensor = torch.tensor(lengths, dtype=torch.int64)\n",
    "    return batch_tensor.to(device), lengths_tensor.to(device)\n",
    "\n",
    "# Define a simple bucketing sampler by length.\n",
    "def make_length_buckets(dataset, bucket_size):\n",
    "    # Collect indices and their sequence lengths.\n",
    "    lengths = []\n",
    "    for idx in range(len(dataset)):\n",
    "        _, length = dataset[idx]\n",
    "        lengths.append((idx, length))\n",
    "\n",
    "    # Sort indices by sequence length.\n",
    "    lengths.sort(key=lambda x: x[1])\n",
    "\n",
    "    # Group indices into buckets of similar lengths.\n",
    "    buckets = []\n",
    "    for i in range(0, len(lengths), bucket_size):\n",
    "        bucket = [idx for idx, _ in lengths[i:i + bucket_size]]\n",
    "        buckets.append(bucket)\n",
    "\n",
    "    # Shuffle buckets but keep order inside buckets.\n",
    "    random.shuffle(buckets)\n",
    "    ordered_indices = [idx for bucket in buckets for idx in bucket]\n",
    "    return ordered_indices\n",
    "\n",
    "# Create three DataLoaders with different strategies.\n",
    "small_batch_loader = DataLoader(dataset=dataset,\n",
    "                                batch_size=2,\n",
    "                                shuffle=True,\n",
    "                                collate_fn=pad_collate,\n",
    "                                num_workers=0)\n",
    "\n",
    "# Configure a larger batch size with shuffling.\n",
    "large_batch_loader = DataLoader(dataset=dataset,\n",
    "                                batch_size=8,\n",
    "                                shuffle=True,\n",
    "                                collate_fn=pad_collate,\n",
    "                                num_workers=0)\n",
    "\n",
    "# Build a DataLoader using length based bucketing.\n",
    "bucket_indices = make_length_buckets(dataset, bucket_size=4)\n",
    "\n",
    "# Create a sampler from the ordered bucket indices.\n",
    "bucket_sampler = torch.utils.data.SubsetRandomSampler(bucket_indices)\n",
    "\n",
    "# DataLoader that uses smart length based batching.\n",
    "bucket_loader = DataLoader(dataset=dataset,\n",
    "                           batch_size=4,\n",
    "                           sampler=bucket_sampler,\n",
    "                           collate_fn=pad_collate,\n",
    "                           num_workers=0)\n",
    "\n",
    "# Helper to inspect one epoch of a loader.\n",
    "def inspect_loader(name, loader, max_batches):\n",
    "    print(f\"\\n{name}:\")\n",
    "    for batch_idx, (batch_data, lengths) in enumerate(loader):\n",
    "        if batch_idx >= max_batches:\n",
    "            break\n",
    "        shape = tuple(batch_data.shape)\n",
    "        min_len = int(lengths.min().item())\n",
    "        max_len = int(lengths.max().item())\n",
    "        print(f\" batch {batch_idx}: shape={shape}, lens={min_len}-{max_len}\")\n",
    "\n",
    "# Print framework version and device information.\n",
    "print(f\"PyTorch version: {torch.__version__}, device: {device}\")\n",
    "\n",
    "# Inspect a few batches from each DataLoader.\n",
    "inspect_loader(\"Small batch with shuffle\", small_batch_loader, max_batches=3)\n",
    "inspect_loader(\"Large batch with shuffle\", large_batch_loader, max_batches=2)\n",
    "inspect_loader(\"Bucketed smart batching\", bucket_loader, max_batches=3)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6a15ea",
   "metadata": {},
   "source": [
    "### **2.2. Tuning DataLoader Workers**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb25480",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_04/Lecture_A/image_02_02.jpg?v=1769709383\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* More DataLoader workers prepare batches in parallel\n",
    ">* Too many workers hurt performance; tune carefully\n",
    "\n",
    ">* Match worker count to pipeline bottlenecks, hardware\n",
    ">* Use many workers for heavy, few for light\n",
    "\n",
    ">* Too many workers strain memory and bandwidth\n",
    ">* Start modest, monitor metrics, adjust workers per setup\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdcaad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Tuning DataLoader Workers\n",
    "\n",
    "# This script shows DataLoader worker tuning.\n",
    "# It uses PyTorch CPU only for safety.\n",
    "# Output is short and easy to read.\n",
    "\n",
    "# !pip install torch torchvision.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Import torch and torchvision modules.\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "# Set deterministic random seeds everywhere.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "torch.manual_seed(seed_value)\n",
    "\n",
    "# Select device preferring GPU when available.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define a simple transform converting images.\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Download a small MNIST training subset.\n",
    "full_train = datasets.MNIST(\n",
    "    root=\"./data\", train=True, download=True, transform=transform\n",
    ")\n",
    "\n",
    "# Keep only a small subset for speed.\n",
    "subset_size = 2048\n",
    "if subset_size > len(full_train):\n",
    "    subset_size = len(full_train)\n",
    "indices = list(range(subset_size))\n",
    "train_subset = torch.utils.data.Subset(full_train, indices)\n",
    "\n",
    "# Confirm subset length is as expected.\n",
    "assert len(train_subset) == subset_size\n",
    "\n",
    "# Define a tiny model for quick training.\n",
    "model = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(28 * 28, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 10),\n",
    ").to(device)\n",
    "\n",
    "# Define loss function and optimizer objects.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# Helper function to time one training epoch.\n",
    "def run_one_epoch(num_workers: int) -> float:\n",
    "    # Create DataLoader with given worker count.\n",
    "    loader = DataLoader(\n",
    "        train_subset,\n",
    "        batch_size=64,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=torch.cuda.is_available(),\n",
    "    )\n",
    "\n",
    "    # Record start time for this epoch.\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Iterate once over the DataLoader.\n",
    "    for batch_idx, (images, labels) in enumerate(loader):\n",
    "        # Move data to selected device.\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Validate batch shapes defensively.\n",
    "        assert images.ndim == 4 and labels.ndim == 1\n",
    "\n",
    "        # Forward pass through the model.\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and parameter update.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Compute elapsed time in seconds.\n",
    "    elapsed = time.time() - start_time\n",
    "    return float(elapsed)\n",
    "\n",
    "# Print framework version and device used.\n",
    "print(\"Torch version:\", torch.__version__, \"Device:\", device)\n",
    "\n",
    "# Define worker settings to compare quickly.\n",
    "worker_settings = [0, 1, 2, 4]\n",
    "results = []\n",
    "\n",
    "# Run one epoch for each worker setting.\n",
    "for workers in worker_settings:\n",
    "    # Time a single short epoch run.\n",
    "    elapsed = run_one_epoch(workers)\n",
    "    results.append((workers, elapsed))\n",
    "\n",
    "# Print a compact summary of timings.\n",
    "print(\"\\nEpoch time by num_workers:\")\n",
    "for workers, elapsed in results:\n",
    "    print(f\"workers={workers}: {elapsed:.3f} seconds\")\n",
    "\n",
    "# Print a short interpretation for beginners.\n",
    "print(\"\\nUse these timings to choose efficient workers.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de5fb5a",
   "metadata": {},
   "source": [
    "### **2.3. Memory Pinning and Dropping**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9917ca1",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_04/Lecture_A/image_02_03.jpg?v=1769709457\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Pinned memory keeps data in fixed locations\n",
    ">* Faster GPU transfers keep training hardware fully utilized\n",
    "\n",
    ">* Pinned memory is powerful but limited resource\n",
    ">* Use moderate batch sizes to avoid system instability\n",
    "\n",
    ">* Treat each batch as temporary, quickly releasable data\n",
    ">* Drop unused tensors promptly to prevent memory buildup\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf06525",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Memory Pinning and Dropping\n",
    "\n",
    "# This script shows basic memory pinning usage.\n",
    "# It compares pinned and non pinned DataLoader behavior.\n",
    "# It also demonstrates dropping batches after usage.\n",
    "\n",
    "# Uncomment to install PyTorch if needed in Colab.\n",
    "# !pip install torch torchvision.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "# Import torch and torchvision safely.\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Set deterministic random seeds for reproducibility.\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Select device based on GPU availability.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Print framework version and selected device.\n",
    "print(\"Torch version:\", torch.__version__, \"Device:\", device)\n",
    "\n",
    "# Define a tiny synthetic dataset class.\n",
    "class TinyDataset(Dataset):\n",
    "    # Initialize dataset with fixed length and shape.\n",
    "    def __init__(self, length: int = 256, features: int = 128):\n",
    "        self.length = length\n",
    "        self.features = features\n",
    "\n",
    "    # Return dataset length for DataLoader.\n",
    "    def __len__(self) -> int:\n",
    "        return self.length\n",
    "\n",
    "    # Generate one sample on demand.\n",
    "    def __getitem__(self, index: int):\n",
    "        x = torch.randn(self.features, dtype=torch.float32)\n",
    "        y = torch.tensor(index % 2, dtype=torch.long)\n",
    "        return x, y\n",
    "\n",
    "# Create a small dataset instance.\n",
    "dataset = TinyDataset(length=512, features=128)\n",
    "\n",
    "# Define a simple model to consume batches.\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(128, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 2),\n",
    ").to(device)\n",
    "\n",
    "# Define a helper to time one epoch iteration.\n",
    "def run_epoch(dataloader: DataLoader, label: str) -> None:\n",
    "    # Ensure model is in evaluation mode.\n",
    "    model.eval()\n",
    "    start = time.time()\n",
    "    batch_count = 0\n",
    "\n",
    "    # Iterate over batches and move to device.\n",
    "    for batch in dataloader:\n",
    "        inputs, targets = batch\n",
    "        assert inputs.ndim == 2 and targets.ndim == 1\n",
    "\n",
    "        # Move tensors to device with non_blocking flag.\n",
    "        inputs = inputs.to(device, non_blocking=True)\n",
    "        targets = targets.to(device, non_blocking=True)\n",
    "\n",
    "        # Run a forward pass to use the batch.\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "\n",
    "        # Drop references so memory can be reused.\n",
    "        del inputs, targets, outputs\n",
    "        batch_count += 1\n",
    "\n",
    "    # Compute elapsed time for this dataloader.\n",
    "    elapsed = time.time() - start\n",
    "    print(label, \"batches:\", batch_count, \"time:\", round(elapsed, 4), \"s\")\n",
    "\n",
    "# Create a non pinned DataLoader for baseline.\n",
    "loader_no_pin = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=False,\n",
    ")\n",
    "\n",
    "# Create a pinned DataLoader for GPU acceleration.\n",
    "loader_pin = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "# Run one epoch without pinned memory.\n",
    "run_epoch(loader_no_pin, \"No pin_memory\")\n",
    "\n",
    "# Run one epoch with pinned memory enabled.\n",
    "run_epoch(loader_pin, \"With pin_memory\")\n",
    "\n",
    "# Explicitly clear cached GPU memory if available.\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ad2fba",
   "metadata": {},
   "source": [
    "## **3. Using Builtin Datasets**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93bd9c8",
   "metadata": {},
   "source": [
    "### **3.1. Working With torchvision**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eeb7f24",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_04/Lecture_A/image_03_01.jpg?v=1769709529\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Torchvision offers ready-to-use, well-labeled image datasets\n",
    ">* Lets you focus on models and experiments\n",
    "\n",
    ">* Datasets plug easily into PyTorch loaders\n",
    ">* Built in transforms ensure fast, consistent preprocessing\n",
    "\n",
    ">* Torchvision includes challenging, realistic image datasets\n",
    ">* They enable fair comparison and faster research progress\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af69bdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Working With torchvision\n",
    "\n",
    "# This script shows torchvision dataset usage basics.\n",
    "# It focuses on MNIST images and simple transforms.\n",
    "# Use it to understand datasets and loaders.\n",
    "\n",
    "# !pip install torch torchvision torchaudio.\n",
    "\n",
    "# Import standard libraries for reproducibility.\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "\n",
    "# Import torch and torchvision core modules.\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Import torchvision datasets and transforms utilities.\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "# Set deterministic random seeds for reproducibility.\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "\n",
    "# Set seeds for torch random number generators.\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Detect device type for potential later use.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Print torch and torchvision versions in one line.\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "\n",
    "# Define a simple transform pipeline for MNIST images.\n",
    "transform_mnist = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,)),\n",
    "])\n",
    "\n",
    "# Choose a small root directory for dataset storage.\n",
    "data_root = os.path.join(\".\", \"data_mnist\")\n",
    "\n",
    "# Create the MNIST training dataset with transforms.\n",
    "train_dataset = datasets.MNIST(\n",
    "    root=data_root,\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform_mnist,\n",
    ")\n",
    "\n",
    "# Verify dataset length to ensure it loaded correctly.\n",
    "num_samples = len(train_dataset)\n",
    "print(\"Total MNIST training samples:\", num_samples)\n",
    "\n",
    "# Select a small subset size for quick experiments.\n",
    "subset_size = 256\n",
    "subset_size = min(subset_size, num_samples)\n",
    "\n",
    "# Create indices for the small subset deterministically.\n",
    "subset_indices = list(range(subset_size))\n",
    "\n",
    "# Wrap the original dataset with a subset view.\n",
    "subset_dataset = torch.utils.data.Subset(train_dataset, subset_indices)\n",
    "\n",
    "# Define DataLoader parameters for batching and shuffling.\n",
    "batch_size = 32\n",
    "num_workers = 0\n",
    "\n",
    "# Create a DataLoader that shuffles each training epoch.\n",
    "train_loader = DataLoader(\n",
    "    subset_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    ")\n",
    "\n",
    "# Fetch one batch from the DataLoader using iterator.\n",
    "first_batch = next(iter(train_loader))\n",
    "\n",
    "# Unpack images and labels from the first batch.\n",
    "images, labels = first_batch\n",
    "\n",
    "# Move batch to the selected device for completeness.\n",
    "images = images.to(device)\n",
    "labels = labels.to(device)\n",
    "\n",
    "# Validate image batch shape and label batch shape.\n",
    "print(\"Batch images shape:\", tuple(images.shape))\n",
    "\n",
    "# Print label batch shape to confirm correct batching.\n",
    "print(\"Batch labels shape:\", tuple(labels.shape))\n",
    "\n",
    "# Compute simple statistics on the first image tensor.\n",
    "first_image = images[0]\n",
    "\n",
    "# Ensure the image has expected channel and size.\n",
    "print(\"Single image shape:\", tuple(first_image.shape))\n",
    "\n",
    "# Compute mean and standard deviation of pixel values.\n",
    "img_mean = float(first_image.mean().item())\n",
    "img_std = float(first_image.std().item())\n",
    "\n",
    "# Print rounded statistics to understand normalization.\n",
    "print(\"First image mean:\", round(img_mean, 4))\n",
    "\n",
    "# Print standard deviation for the same normalized image.\n",
    "print(\"First image std:\", round(img_std, 4))\n",
    "\n",
    "# Show a few labels from the first batch as integers.\n",
    "print(\"First five labels:\", labels[:5].tolist())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9dfc37",
   "metadata": {},
   "source": [
    "### **3.2. Working With Torchtext**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65685945",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_04/Lecture_A/image_03_02.jpg?v=1769709599\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Torchtext offers ready-made datasets for many NLP tasks\n",
    ">* Datasets share splits and formats, easing reuse\n",
    "\n",
    ">* Each sample includes raw text and target\n",
    ">* You control tokenization and preprocessing for experiments\n",
    "\n",
    ">* Torchtext supports advanced sequence tasks like modeling, translation\n",
    ">* Prototype, validate, then reuse patterns on proprietary text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e5cf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Working With Torchtext\n",
    "\n",
    "# This script shows basic torchtext dataset usage.\n",
    "# It focuses on small text classification examples.\n",
    "# You can run everything directly inside Colab.\n",
    "\n",
    "# !pip install torchtext==0.18.0.\n",
    "\n",
    "# Import standard libraries for reproducibility.\n",
    "import os, random, math, textwrap\n",
    "\n",
    "# Import torch for tensors and device handling.\n",
    "import torch\n",
    "\n",
    "# Try importing torchtext and handle missing install.\n",
    "try:\n",
    "    import torchtext\n",
    "    from torchtext.datasets import AG_NEWS\n",
    "except Exception as e:\n",
    "    torchtext, AG_NEWS = None, None\n",
    "\n",
    "# Set deterministic random seeds for reproducibility.\n",
    "random.seed(0)\n",
    "\n",
    "# Set torch manual seed for reproducible tensors.\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Detect device, prefer cuda if available.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define a small helper to safely print wrapped text.\n",
    "def print_wrapped(prefix, text, width):\n",
    "    wrapped = textwrap.fill(text, width=width)\n",
    "    print(f\"{prefix}{wrapped}\")\n",
    "\n",
    "# Check that torchtext and AG_NEWS are available.\n",
    "if torchtext is None or AG_NEWS is None:\n",
    "    print(\"torchtext not available, please install first.\")\n",
    "else:\n",
    "\n",
    "    # Print torch and torchtext versions briefly.\n",
    "    print(\"Torch version:\", torch.__version__)\n",
    "\n",
    "    # Print torchtext version for reference.\n",
    "    print(\"Torchtext version:\", torchtext.__version__)\n",
    "\n",
    "    # Load tiny training split from AG_NEWS dataset.\n",
    "    train_iter = AG_NEWS(split=\"train\")\n",
    "\n",
    "    # Convert iterator to list and keep small subset.\n",
    "    train_list = list(train_iter)\n",
    "\n",
    "    # Select first eight samples for this demo.\n",
    "    small_subset = train_list[:8]\n",
    "\n",
    "    # Validate subset size before further processing.\n",
    "    if len(small_subset) == 0:\n",
    "        print(\"Dataset appears empty, cannot continue.\")\n",
    "    else:\n",
    "\n",
    "        # Show basic information about one sample.\n",
    "        label0, text0 = small_subset[0]\n",
    "\n",
    "        # Print label and truncated text for first sample.\n",
    "        print(\"First sample label:\", int(label0))\n",
    "\n",
    "        # Print first sample text with wrapping.\n",
    "        print_wrapped(\"First sample text: \", text0[:200], 60)\n",
    "\n",
    "        # Build a simple vocabulary from subset tokens.\n",
    "        token_lists = []\n",
    "\n",
    "        # Tokenize each text using simple whitespace.\n",
    "        for label, text in small_subset:\n",
    "            tokens = text.lower().split()\n",
    "            token_lists.append(tokens)\n",
    "\n",
    "        # Create mapping from token to index with padding.\n",
    "        vocab = {\"<pad>\": 0}\n",
    "\n",
    "        # Populate vocabulary with unique tokens.\n",
    "        for tokens in token_lists:\n",
    "            for tok in tokens:\n",
    "                if tok not in vocab:\n",
    "                    vocab[tok] = len(vocab)\n",
    "\n",
    "        # Numericalize tokens into index sequences.\n",
    "        indexed_sequences = []\n",
    "\n",
    "        # Convert each token list into tensor of indices.\n",
    "        for tokens in token_lists:\n",
    "            idxs = [vocab[t] for t in tokens]\n",
    "            indexed_sequences.append(torch.tensor(idxs, dtype=torch.long))\n",
    "\n",
    "        # Determine maximum sequence length for padding.\n",
    "        max_len = max(seq.size(0) for seq in indexed_sequences)\n",
    "\n",
    "        # Pad sequences to same length with pad index.\n",
    "        padded = []\n",
    "\n",
    "        # Create padded tensor for each sequence.\n",
    "        for seq in indexed_sequences:\n",
    "            pad_size = max_len - seq.size(0)\n",
    "            if pad_size > 0:\n",
    "                pad_tensor = torch.full((pad_size,), 0, dtype=torch.long)\n",
    "                seq = torch.cat((seq, pad_tensor), dim=0)\n",
    "            padded.append(seq)\n",
    "\n",
    "        # Stack padded sequences into batch tensor.\n",
    "        batch_tensor = torch.stack(padded, dim=0)\n",
    "\n",
    "        # Move batch tensor to selected device.\n",
    "        batch_tensor = batch_tensor.to(device)\n",
    "\n",
    "        # Validate final batch shape before printing.\n",
    "        print(\"Batch tensor shape:\", tuple(batch_tensor.shape))\n",
    "\n",
    "        # Print first row of indices as small example.\n",
    "        print(\"First sequence indices:\", batch_tensor[0, :10].tolist())\n",
    "\n",
    "        # Print vocabulary size to summarize preprocessing.\n",
    "        print(\"Vocabulary size:\", len(vocab))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a45761e",
   "metadata": {},
   "source": [
    "### **3.3. Managing Dataset Downloads**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb8b1e3",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_04/Lecture_A/image_03_03.jpg?v=1769709687\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Built-in datasets download once, then are cached\n",
    ">* Plan storage, bandwidth, and backups to avoid issues\n",
    "\n",
    ">* Control whether datasets download automatically or not\n",
    ">* Choose settings differently for shared and individual environments\n",
    "\n",
    ">* Dataset updates can break comparisons and reproducibility\n",
    ">* Record versions, locations, dates, and preprocessing steps\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b0cdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Managing Dataset Downloads\n",
    "\n",
    "# This script shows managing dataset downloads simply.\n",
    "# We use TensorFlow builtin MNIST dataset safely.\n",
    "# Focus on download control, caching, and reuse.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import pathlib\n",
    "import random\n",
    "\n",
    "# Import TensorFlow and check version.\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set deterministic random seeds for reproducibility.\n",
    "random.seed(42)\n",
    "os.environ[\"PYTHONHASHSEED\"] = \"42\"\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Choose a base directory for all datasets.\n",
    "base_data_dir = pathlib.Path(\"data_download_demo\")\n",
    "\n",
    "# Create directory if it does not already exist.\n",
    "base_data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Show the resolved absolute path for clarity.\n",
    "print(\"Dataset base directory:\", base_data_dir.resolve())\n",
    "\n",
    "# Define a small helper to describe directory contents.\n",
    "\n",
    "\n",
    "def describe_dir(path: pathlib.Path, label: str) -> None:\n",
    "    # List files and subdirectories in a safe way.\n",
    "    entries = sorted(path.iterdir()) if path.exists() else []\n",
    "\n",
    "    # Print a short summary line for the directory.\n",
    "    print(f\"{label} contains {len(entries)} entries.\")\n",
    "\n",
    "# Describe directory before any dataset download.\n",
    "describe_dir(base_data_dir, \"Before download\")\n",
    "\n",
    "# Configure a flag that controls automatic downloading.\n",
    "auto_download = True\n",
    "\n",
    "# Explain current download policy in one line.\n",
    "print(\"Auto download enabled:\", auto_download)\n",
    "\n",
    "# Build a path where MNIST will be cached.\n",
    "mnist_path = base_data_dir / \"mnist_tf\"\n",
    "\n",
    "# Ensure the MNIST cache directory exists safely.\n",
    "mnist_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Define a function that loads MNIST with control.\n",
    "\n",
    "\n",
    "def load_mnist(cache_dir: pathlib.Path, download: bool):\n",
    "    # If download disabled and directory empty, raise error.\n",
    "    if not download and not any(cache_dir.iterdir()):\n",
    "        raise RuntimeError(\"MNIST cache empty, enable download first.\")\n",
    "\n",
    "    # Use TensorFlow builtin loader with custom path.\n",
    "    (x_train, y_train), _ = tf.keras.datasets.mnist.load_data(\n",
    "        path=str(cache_dir / \"mnist.npz\")\n",
    "    )\n",
    "\n",
    "    # Validate shapes to avoid surprises.\n",
    "    if x_train.shape[0] != y_train.shape[0]:\n",
    "        raise ValueError(\"Mismatched MNIST images and labels counts.\")\n",
    "\n",
    "    # Return only a tiny subset for quick inspection.\n",
    "    return x_train[:100], y_train[:100]\n",
    "\n",
    "# Try loading MNIST with automatic download allowed.\n",
    "try:\n",
    "    x_small, y_small = load_mnist(mnist_path, auto_download)\n",
    "except Exception as exc:\n",
    "    # Print a short message if something unexpected happens.\n",
    "    print(\"Download or load failed:\", type(exc).__name__)\n",
    "\n",
    "# Describe directory after potential download.\n",
    "describe_dir(base_data_dir, \"After download attempt\")\n",
    "\n",
    "# If data loaded, print a few key properties.\n",
    "if \"x_small\" in globals():\n",
    "    # Print dataset subset shape information.\n",
    "    print(\"Subset images shape:\", x_small.shape)\n",
    "\n",
    "    # Print dataset subset labels shape information.\n",
    "    print(\"Subset labels shape:\", y_small.shape)\n",
    "\n",
    "    # Show unique labels present in the tiny subset.\n",
    "    unique_labels = sorted(set(int(v) for v in y_small))\n",
    "\n",
    "    # Print the unique labels in one concise line.\n",
    "    print(\"Unique labels in subset:\", unique_labels)\n",
    "\n",
    "# Demonstrate how to switch off automatic downloading.\n",
    "auto_download = False\n",
    "\n",
    "# Print the new policy to emphasize reproducibility.\n",
    "print(\"Auto download enabled:\", auto_download)\n",
    "\n",
    "# Attempt loading again, now expecting cached reuse.\n",
    "if any(mnist_path.iterdir()):\n",
    "    x_small2, y_small2 = load_mnist(mnist_path, auto_download)\n",
    "\n",
    "    # Confirm that cached data matches previous subset.\n",
    "    same_data = bool((x_small2 == x_small).all())\n",
    "\n",
    "    # Print whether cached reload produced identical data.\n",
    "    print(\"Cached reload identical to first load:\", same_data)\n",
    "\n",
    "# Final line prints a short completion message.\n",
    "print(\"Dataset download management demo finished.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3e15a8",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Datasets and Loaders**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3947866",
   "metadata": {},
   "source": [
    "\n",
    "In this lecture, you learned to:\n",
    "- Implement custom PyTorch Dataset classes that load and preprocess samples on demand. \n",
    "- Configure DataLoader instances with appropriate batch sizes, shuffling, and multiprocessing workers. \n",
    "- Use builtâ€‘in datasets from torchvision and torchtext as quick starting points for experiments. \n",
    "\n",
    "In the next Lecture (Lecture B), we will go over 'Transforms and Augment'"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
