{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7944a4e",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Profiling and Tuning**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94afd5c",
   "metadata": {},
   "source": [
    ">Last update: 20260130.\n",
    "    \n",
    "By the end of this Lecture, you will be able to:\n",
    "- Profile PyTorch models using torch.profiler to identify time‑consuming operations. \n",
    "- Apply basic performance optimizations such as adjusting batch size, using pin_memory, and enabling mixed precision. \n",
    "- Evaluate the trade‑offs between speed, memory usage, and numerical stability when tuning models. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb23a4cd",
   "metadata": {},
   "source": [
    "## **1. PyTorch Profiling Tools**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f147aaf2",
   "metadata": {},
   "source": [
    "### **1.1. torch profiler essentials**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e76482d",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_07/Lecture_B/image_01_01.jpg?v=1769830953\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* PyTorch profiler records detailed model runtime behavior\n",
    ">* Turns vague slowness into measurable, comparable data\n",
    "\n",
    ">* Profiler wraps training steps to record events\n",
    ">* Generates a timeline revealing bottlenecks and utilization\n",
    "\n",
    ">* Profile only a few typical training iterations\n",
    ">* Reuse focused profiles to compare optimization changes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0ec8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - torch profiler essentials\n",
    "\n",
    "# This script shows basic torch profiler usage.\n",
    "# It profiles a tiny model training step.\n",
    "# Focus on essentials without overwhelming output.\n",
    "\n",
    "# !pip install torch torchvision.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "# Import torch and related utilities.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Set deterministic random seeds everywhere.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "\n",
    "# Set torch manual seed for reproducibility.\n",
    "torch.manual_seed(seed_value)\n",
    "\n",
    "# Select device based on GPU availability.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Print torch version and selected device.\n",
    "print(\"Torch version and device:\", torch.__version__, device)\n",
    "\n",
    "# Define a tiny feedforward model class.\n",
    "class TinyNet(nn.Module):\n",
    "    # Initialize layers inside the constructor.\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define first linear layer.\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "\n",
    "        # Define second linear layer.\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    # Define forward pass computation.\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "\n",
    "        # Apply second linear layer.\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        # Return final output tensor.\n",
    "        return x\n",
    "\n",
    "# Create a small random dataset tensor.\n",
    "input_dim, hidden_dim, output_dim = 32, 64, 10\n",
    "\n",
    "# Define batch size for synthetic data.\n",
    "batch_size = 64\n",
    "\n",
    "# Create random input features tensor.\n",
    "inputs = torch.randn(batch_size, input_dim)\n",
    "\n",
    "# Create random integer labels tensor.\n",
    "labels = torch.randint(0, output_dim, (batch_size,))\n",
    "\n",
    "# Validate shapes before moving to device.\n",
    "assert inputs.shape == (batch_size, input_dim)\n",
    "\n",
    "# Move tensors to selected device.\n",
    "inputs = inputs.to(device)\n",
    "labels = labels.to(device)\n",
    "\n",
    "# Initialize model and move to device.\n",
    "model = TinyNet(input_dim, hidden_dim, output_dim).to(device)\n",
    "\n",
    "# Define loss function and optimizer.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# Warm up model with one forward backward pass.\n",
    "model.train()\n",
    "\n",
    "# Perform warmup forward pass.\n",
    "outputs = model(inputs)\n",
    "\n",
    "# Compute warmup loss value.\n",
    "loss = criterion(outputs, labels)\n",
    "\n",
    "# Backpropagate warmup gradients.\n",
    "loss.backward()\n",
    "\n",
    "# Optimizer step for warmup.\n",
    "optimizer.step()\n",
    "\n",
    "# Zero gradients after warmup step.\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# Import torch profiler utilities.\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "# Define a helper function for one training step.\n",
    "def train_step(data_inputs, data_labels):\n",
    "    # Ensure model is in training mode.\n",
    "    model.train()\n",
    "\n",
    "    # Forward pass through the model.\n",
    "    logits = model(data_inputs)\n",
    "\n",
    "    # Compute loss for this batch.\n",
    "    loss_value = criterion(logits, data_labels)\n",
    "\n",
    "    # Backward pass for gradients.\n",
    "    loss_value.backward()\n",
    "\n",
    "    # Optimizer update step.\n",
    "    optimizer.step()\n",
    "\n",
    "    # Reset gradients to zero.\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Return scalar loss value.\n",
    "    return loss_value.item()\n",
    "\n",
    "# Configure profiler activities and schedule.\n",
    "activities = [ProfilerActivity.CPU]\n",
    "\n",
    "# Add CUDA activity if GPU is available.\n",
    "if device.type == \"cuda\":\n",
    "    activities.append(ProfilerActivity.CUDA)\n",
    "\n",
    "# Define profiler schedule with warmup and active steps.\n",
    "prof_schedule = torch.profiler.schedule(\n",
    "    wait=1,\n",
    "    warmup=1,\n",
    "    active=2,\n",
    "    repeat=1,\n",
    ")\n",
    "\n",
    "# Create profiler context manager instance.\n",
    "profiler = profile(\n",
    "    activities=activities,\n",
    "    schedule=prof_schedule,\n",
    "    on_trace_ready=torch.profiler.tensorboard_trace_handler(\n",
    "        \"./profiler_logs\"\n",
    "    ),\n",
    "    record_shapes=True,\n",
    "    profile_memory=True,\n",
    "    with_stack=False,\n",
    ")\n",
    "\n",
    "# Run a few profiled training iterations.\n",
    "num_steps = 4\n",
    "\n",
    "# Start profiler context block.\n",
    "with profiler as prof:\n",
    "    for step in range(num_steps):\n",
    "        # Use record_function to label region.\n",
    "        with record_function(\"train_step_region\"):\n",
    "            loss_value = train_step(inputs, labels)\n",
    "\n",
    "        # Advance profiler internal step.\n",
    "        prof.step()\n",
    "\n",
    "# Print a short summary of top operations.\n",
    "print(\"Profiling finished, showing top operations by time.\")\n",
    "\n",
    "# Sort profiler events by self cpu time total.\n",
    "key = torch.profiler.ProfilerActivity.CPU\n",
    "\n",
    "# Use key to avoid long default tables.\n",
    "summary = profiler.key_averages().table(\n",
    "    sort_by=\"self_cpu_time_total\",\n",
    "    row_limit=5,\n",
    ")\n",
    "\n",
    "# Print the small profiler summary table.\n",
    "print(summary)\n",
    "\n",
    "# Print final loss value for reference.\n",
    "print(\"Final training step loss value:\", float(loss_value))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35320c39",
   "metadata": {},
   "source": [
    "### **1.2. Chrome Trace Viewer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c86e05e",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_07/Lecture_B/image_01_02.jpg?v=1769831014\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Shows timeline of threads and GPU streams\n",
    ">* Helps visually spot bottlenecks and idle gaps\n",
    "\n",
    ">* Shows how CPU, GPU, data transfers interact\n",
    ">* Helps distinguish slow layers from data bottlenecks\n",
    "\n",
    ">* Find subtle issues like launch overhead, synchronization\n",
    ">* Iteratively profile traces to refine performance intuition\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e25b212",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Chrome Trace Viewer\n",
    "\n",
    "# This script demonstrates Chrome Trace Viewer usage simply.\n",
    "# It creates a tiny TensorFlow model and profile trace.\n",
    "# Then it explains how to open the trace visually.\n",
    "\n",
    "# !pip install tensorflow.\n",
    "\n",
    "# Import required standard and TensorFlow modules.\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "\n",
    "# Import numpy and tensorflow with clear aliases.\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set deterministic random seeds for reproducibility.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version in a single concise line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Check and print whether a GPU device is available.\n",
    "physical_gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "print(\"GPU available:\", bool(physical_gpus))\n",
    "\n",
    "# Create a tiny synthetic dataset for quick profiling.\n",
    "num_samples = 256\n",
    "input_dim = 32\n",
    "num_classes = 10\n",
    "\n",
    "# Generate random input features and integer labels.\n",
    "features = np.random.randn(num_samples, input_dim).astype(np.float32)\n",
    "labels = np.random.randint(num_classes, size=(num_samples,)).astype(np.int32)\n",
    "\n",
    "# Validate shapes before building the model.\n",
    "assert features.shape == (num_samples, input_dim)\n",
    "assert labels.shape == (num_samples,)\n",
    "\n",
    "# Build a very small dense neural network model.\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(input_dim,)),\n",
    "    tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(num_classes, activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "# Compile the model with simple optimizer and loss.\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Create a tf.data dataset with small batch size.\n",
    "dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "dataset = dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Define a short training step function for profiling.\n",
    "@tf.function\n",
    "\n",
    "def train_step(batch_features, batch_labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(batch_features, training=True)\n",
    "        loss_value = model.compiled_loss(batch_labels, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss_value, model.trainable_variables)\n",
    "    model.optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    return loss_value\n",
    "\n",
    "\n",
    "# Prepare a directory for TensorFlow profiler trace files.\n",
    "logdir = \"tf_profile_logs\"\n",
    "os.makedirs(logdir, exist_ok=True)\n",
    "\n",
    "# Choose a small number of steps to keep runtime short.\n",
    "max_steps = 5\n",
    "step_counter = 0\n",
    "\n",
    "# Start TensorFlow profiler context to capture Chrome trace.\n",
    "options = tf.profiler.experimental.ProfilerOptions(\n",
    "    host_tracer_level=2,\n",
    "    python_tracer_level=1,\n",
    "    device_tracer_level=1,\n",
    ")\n",
    "\n",
    "# Use profiler context manager around a short training loop.\n",
    "tf.profiler.experimental.start(logdir, options=options)\n",
    "for batch_features, batch_labels in dataset:\n",
    "    loss_value = train_step(batch_features, batch_labels)\n",
    "    step_counter += 1\n",
    "\n",
    "    if step_counter >= max_steps:\n",
    "        break\n",
    "\n",
    "# Stop the profiler after the small training loop.\n",
    "tf.profiler.experimental.stop()\n",
    "\n",
    "# Print a short summary explaining where traces were saved.\n",
    "print(\"Profiler trace directory:\", os.path.abspath(logdir))\n",
    "\n",
    "# Explain how to open the trace using TensorBoard profile plugin.\n",
    "print(\"To view Chrome trace, run: tensorboard --logdir\", logdir)\n",
    "\n",
    "# Provide final hint about Chrome Trace Viewer timeline interpretation.\n",
    "print(\"Then open the profile tab and inspect CPU and GPU timelines.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffb6446",
   "metadata": {},
   "source": [
    "### **1.3. Reading Operator Metrics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045af046",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_07/Lecture_B/image_01_03.jpg?v=1769831064\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Operator view shows each operation and timings\n",
    ">* Find operators dominating runtime to target optimization\n",
    "\n",
    ">* Compare self time and total time meanings\n",
    ">* Sort by each to reveal true bottleneck operators\n",
    "\n",
    ">* Use counts and device breakdowns to spot bottlenecks\n",
    ">* Differentiate compute versus input limits and optimize\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceccf5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Reading Operator Metrics\n",
    "\n",
    "# This script demonstrates reading simple operator metrics.\n",
    "# We simulate profiling style metrics using plain Python structures.\n",
    "# Focus on understanding which operations dominate total runtime.\n",
    "\n",
    "# No extra installations are required for this simple demonstration.\n",
    "# Uncomment and adapt pip commands here if additional packages needed.\n",
    "# This script is designed for quick execution in Google Colab.\n",
    "\n",
    "# Define a small list of fake operator metric dictionaries.\n",
    "operator_metrics = [\n",
    "    {\"name\": \"conv2d\", \"calls\": 120, \"self_ms\": 2.5, \"total_ms\": 320.0},\n",
    "    {\"name\": \"relu\", \"calls\": 120, \"self_ms\": 0.3, \"total_ms\": 36.0},\n",
    "    {\"name\": \"matmul\", \"calls\": 40, \"self_ms\": 4.0, \"total_ms\": 160.0},\n",
    "    {\"name\": \"host_to_device\", \"calls\": 80, \"self_ms\": 1.0, \"total_ms\": 90.0},\n",
    "]\n",
    "\n",
    "# Compute total runtime across all operators for percentage calculations.\n",
    "\n",
    "total_runtime_ms = sum(op[\"total_ms\"] for op in operator_metrics)\n",
    "\n",
    "# Sort operators by total time descending to see dominant contributors.\n",
    "\n",
    "sorted_by_total = sorted(\n",
    "    operator_metrics,\n",
    "    key=lambda op: op[\"total_ms\"],\n",
    "    reverse=True,\n",
    ")\n",
    "\n",
    "# Print a compact header explaining the displayed metrics.\n",
    "\n",
    "print(\"Name  Calls  Self_ms  Total_ms  Total_percent\")\n",
    "\n",
    "# Loop through sorted operators and print key metrics per operator.\n",
    "\n",
    "for op in sorted_by_total:\n",
    "    percent = (op[\"total_ms\"] / total_runtime_ms) * 100.0\n",
    "    print(\n",
    "        f\"{op['name']:10s} {op['calls']:5d} {op['self_ms']:7.2f} \"\n",
    "        f\"{op['total_ms']:8.2f} {percent:13.1f}\"\n",
    "    )\n",
    "\n",
    "# Identify operators where self time is close to total time.\n",
    "\n",
    "heavy_kernels = [\n",
    "    op for op in operator_metrics\n",
    "    if op[\"total_ms\"] > 0 and (op[\"self_ms\"] / op[\"total_ms\"]) > 0.7\n",
    "]\n",
    "\n",
    "# Print a short summary highlighting intrinsically expensive kernels.\n",
    "\n",
    "print(\"\\nOperators where self time dominates total time:\")\n",
    "for op in heavy_kernels:\n",
    "    print(f\"Kernel {op['name']} is doing most of its own work.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a786c05d",
   "metadata": {},
   "source": [
    "## **2. Boosting Data Throughput**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91387d85",
   "metadata": {},
   "source": [
    "### **2.1. Optimal Batch Sizing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f109f9c6",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_07/Lecture_B/image_02_01.jpg?v=1769831100\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Batch size must keep the GPU busy\n",
    ">* Too big risks memory limits and slowdowns\n",
    "\n",
    ">* Batch size sweet spot depends on setup\n",
    ">* Increase batch gradually, watch speed and memory\n",
    "\n",
    ">* Batch size affects speed, memory, and convergence\n",
    ">* Use mixed precision and not-maximal batches for stability\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1440ad6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Optimal Batch Sizing\n",
    "\n",
    "# This script explores optimal batch sizing.\n",
    "# We compare throughput for different batch sizes.\n",
    "# Use this to understand speed memory tradeoffs.\n",
    "# !pip install torch torchvision.\n",
    "\n",
    "# Import required standard and torch modules.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "\n",
    "# Set deterministic random seed for reproducibility.\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Select device preferring GPU when available.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define a tiny convolutional network model.\n",
    "class TinyCNN(nn.Module):\n",
    "    # Initialize convolutional and linear layers.\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(1, 8, kernel_size=3)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Linear(5408, 10)\n",
    "\n",
    "    # Define forward pass through layers.\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Create a small random dataset for demonstration.\n",
    "class RandomImages(data.Dataset):\n",
    "    # Initialize dataset with fixed size.\n",
    "    def __init__(self, length):\n",
    "        self.length = length\n",
    "        self.images = torch.randn(length, 1, 28, 28)\n",
    "        self.labels = torch.randint(0, 10, (length,))\n",
    "\n",
    "    # Return dataset length when requested.\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    # Get one sample image and label pair.\n",
    "    def __getitem__(self, idx):\n",
    "        return self.images[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "# Instantiate dataset with modest number of samples.\n",
    "dataset = RandomImages(length=512)\n",
    "\n",
    "# Validate one sample shape and label type.\n",
    "sample_x, sample_y = dataset[0]\n",
    "assert sample_x.shape == (1, 28, 28)\n",
    "assert sample_y.dtype == torch.int64\n",
    "\n",
    "\n",
    "# Create model and move it to selected device.\n",
    "model = TinyCNN().to(device)\n",
    "\n",
    "# Define loss function and optimizer objects.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "# Function to measure throughput for one batch size.\n",
    "def measure_throughput(batch_size):\n",
    "    # Create data loader with given batch size.\n",
    "    loader = data.DataLoader(dataset, batch_size=batch_size,\n",
    "                             shuffle=False, pin_memory=True)\n",
    "\n",
    "    # Warmup single batch to stabilize timings.\n",
    "    model.train()\n",
    "    for images, labels in loader:\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.zero_grad()\n",
    "        break\n",
    "\n",
    "    # Synchronize device before timing loop.\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    # Measure time for one full pass.\n",
    "    start = torch.cuda.Event(enable_timing=True) if device.type == \"cuda\" else None\n",
    "    end = torch.cuda.Event(enable_timing=True) if device.type == \"cuda\" else None\n",
    "\n",
    "    # Record start event when using cuda device.\n",
    "    if device.type == \"cuda\":\n",
    "        start.record()\n",
    "\n",
    "    # Run one epoch like pass over loader.\n",
    "    total_samples = 0\n",
    "    for images, labels in loader:\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.zero_grad()\n",
    "        total_samples += images.size(0)\n",
    "\n",
    "    # Record end event and synchronize.\n",
    "    if device.type == \"cuda\":\n",
    "        end.record()\n",
    "        torch.cuda.synchronize()\n",
    "        elapsed_ms = start.elapsed_time(end)\n",
    "        elapsed_seconds = elapsed_ms / 1000.0\n",
    "    else:\n",
    "        # Fallback simple cpu timing measurement.\n",
    "        import time\n",
    "        start_time = time.time()\n",
    "        elapsed_seconds = time.time() - start_time\n",
    "\n",
    "    # Avoid division by zero in throughput.\n",
    "    elapsed_seconds = max(elapsed_seconds, 1e-6)\n",
    "\n",
    "    # Compute samples processed per second.\n",
    "    throughput = total_samples / elapsed_seconds\n",
    "    return throughput\n",
    "\n",
    "\n",
    "# Define several candidate batch sizes to compare.\n",
    "batch_sizes = [8, 32, 64, 128]\n",
    "\n",
    "# Print framework version and device information.\n",
    "print(\"PyTorch version:\", torch.__version__, \"Device:\", device)\n",
    "\n",
    "# Measure throughput for each candidate batch size.\n",
    "results = []\n",
    "for bs in batch_sizes:\n",
    "    throughput = measure_throughput(bs)\n",
    "    results.append((bs, throughput))\n",
    "\n",
    "# Sort results by batch size for readability.\n",
    "results.sort(key=lambda x: x[0])\n",
    "\n",
    "# Display concise throughput comparison table.\n",
    "print(\"Batch size and approximate samples per second:\")\n",
    "for bs, thr in results:\n",
    "    print(\"Batch:\", bs, \"Throughput:\", round(thr, 2))\n",
    "\n",
    "# Suggest best batch size based on throughput.\n",
    "best_bs, best_thr = max(results, key=lambda x: x[1])\n",
    "print(\"Best batch size here is:\", best_bs, \"with throughput:\", round(best_thr, 2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791da566",
   "metadata": {},
   "source": [
    "### **2.2. Efficient DataLoader Configuration**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3cf5a4",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_07/Lecture_B/image_02_02.jpg?v=1769831178\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Configure DataLoader so GPU never waits\n",
    ">* Tune workers, prefetching, and I/O for hardware\n",
    "\n",
    ">* Choose worker count to avoid GPU stalling\n",
    ">* Tune workers empirically; depends on hardware, dataset\n",
    "\n",
    ">* Shuffle and batch data to reduce bottlenecks\n",
    ">* Tune collation and prefetching to balance memory\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2920642e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Efficient DataLoader Configuration\n",
    "\n",
    "# This script compares DataLoader configurations throughput.\n",
    "# It shows workers and pin memory effects.\n",
    "# It uses a tiny synthetic dataset.\n",
    "\n",
    "# !pip install torch torchvision.\n",
    "\n",
    "# Import required standard and torch modules.\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Import torch core and utilities.\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# Import DataLoader and TensorDataset helpers.\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "# Set deterministic random seeds for reproducibility.\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Detect device preferring cuda then mps then cpu.\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# Print torch version and selected device name.\n",
    "print(\"torch version:\", torch.__version__, \"device:\", device.type)\n",
    "\n",
    "# Define a tiny synthetic dataset for demonstration.\n",
    "num_samples = 4096\n",
    "num_features = 128\n",
    "\n",
    "# Create random input features tensor.\n",
    "features = torch.randn(num_samples, num_features)\n",
    "\n",
    "# Create random integer labels tensor.\n",
    "labels = torch.randint(low=0, high=10, size=(num_samples,))\n",
    "\n",
    "# Wrap tensors inside a TensorDataset object.\n",
    "dataset = TensorDataset(features, labels)\n",
    "\n",
    "# Define a simple linear model for timing.\n",
    "model = nn.Linear(num_features, 10).to(device)\n",
    "\n",
    "# Put model in evaluation mode for consistent behavior.\n",
    "model.eval()\n",
    "\n",
    "# Define a helper function to measure one epoch time.\n",
    "def measure_epoch_time(loader, description):\n",
    "    start_time = time.time()\n",
    "    total_batches = 0\n",
    "\n",
    "    # Disable gradients for pure inference timing.\n",
    "    with torch.no_grad():\n",
    "        for batch_features, batch_labels in loader:\n",
    "            total_batches += 1\n",
    "\n",
    "            # Move batch to device respecting pin memory.\n",
    "            batch_features = batch_features.to(device, non_blocking=True)\n",
    "\n",
    "            # Run a forward pass through the model.\n",
    "            outputs = model(batch_features)\n",
    "\n",
    "    # Compute elapsed time and average per batch.\n",
    "    elapsed = time.time() - start_time\n",
    "    avg_per_batch = elapsed / max(total_batches, 1)\n",
    "\n",
    "    # Print a short summary line for this configuration.\n",
    "    print(description, \"batches:\", total_batches, \"sec_per_batch:\", round(avg_per_batch, 5))\n",
    "\n",
    "# Create a baseline DataLoader with one worker.\n",
    "batch_size = 64\n",
    "baseline_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=False,\n",
    "    persistent_workers=False,\n",
    ")\n",
    "\n",
    "# Create a DataLoader using more workers without pin memory.\n",
    "workers_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=False,\n",
    "    persistent_workers=False,\n",
    ")\n",
    "\n",
    "# Create a DataLoader using workers and pin memory.\n",
    "pin_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=(device.type == \"cuda\"),\n",
    "    persistent_workers=False,\n",
    ")\n",
    "\n",
    "# Warm up model and device with a quick pass.\n",
    "for _ in range(2):\n",
    "    for batch_features, batch_labels in baseline_loader:\n",
    "        batch_features = batch_features.to(device, non_blocking=True)\n",
    "        _ = model(batch_features)\n",
    "        break\n",
    "\n",
    "# Measure and compare epoch times for each configuration.\n",
    "measure_epoch_time(baseline_loader, \"baseline_workers_0_pin_False\")\n",
    "\n",
    "# Measure configuration with more workers only.\n",
    "measure_epoch_time(workers_loader, \"workers_2_pin_False\")\n",
    "\n",
    "# Measure configuration with workers and pin memory.\n",
    "measure_epoch_time(pin_loader, \"workers_2_pin_auto\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf675c3",
   "metadata": {},
   "source": [
    "### **2.3. Pinned Memory Prefetching**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1745a60",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_07/Lecture_B/image_02_03.jpg?v=1769831231\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Pinned memory speeds CPU‑to‑GPU data transfers\n",
    ">* Overlapping transfer and compute keeps GPU busy\n",
    "\n",
    ">* Slow CPU‑GPU transfers can bottleneck fast models\n",
    ">* Pinned prefetching smooths GPU usage and shortens epochs\n",
    "\n",
    ">* Pinned memory helps but can hurt systems\n",
    ">* Start moderate, monitor usage, tune batches carefully\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d95654",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Pinned Memory Prefetching\n",
    "\n",
    "# This script shows pinned memory prefetching basics.\n",
    "# We compare DataLoader settings for GPU data throughput.\n",
    "# Run on CPU safely when no GPU exists.\n",
    "\n",
    "# !pip install torch torchvision.\n",
    "\n",
    "# Import required standard and torch modules.\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "# Import torch and torchvision utilities.\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# Set deterministic random seeds for reproducibility.\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Detect GPU availability for this runtime.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Print torch version and device once.\n",
    "print(\"Torch version and device:\", torch.__version__, device)\n",
    "\n",
    "# Define a simple transform converting images to tensors.\n",
    "transform = T.Compose([T.ToTensor()])\n",
    "\n",
    "# Download a tiny MNIST training subset.\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "# Keep only a small subset for faster demonstration.\n",
    "subset_size = 512\n",
    "indices = list(range(subset_size))\n",
    "train_subset = torch.utils.data.Subset(train_dataset, indices)\n",
    "\n",
    "\n",
    "# Helper function building a DataLoader with given options.\n",
    "def make_loader(pin_memory, num_workers, batch_size):\n",
    "    loader = torch.utils.data.DataLoader(\n",
    "        train_subset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "    )\n",
    "\n",
    "    return loader\n",
    "\n",
    "\n",
    "# Define a tiny model to create some GPU work.\n",
    "class TinyNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.fc = torch.nn.Linear(28 * 28, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Instantiate model and move to selected device.\n",
    "model = TinyNet().to(device)\n",
    "\n",
    "# Define a simple loss function and optimizer.\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "\n",
    "# Training loop helper measuring one epoch time.\n",
    "def run_epoch(loader, description):\n",
    "    model.train()\n",
    "    start = time.time()\n",
    "    total_batches = 0\n",
    "\n",
    "    for images, labels in loader:\n",
    "        if device.type == \"cuda\":\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "        else:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_batches += 1\n",
    "\n",
    "    end = time.time()\n",
    "    elapsed = end - start\n",
    "    print(description, \"batches:\", total_batches, \"seconds:\", round(elapsed, 3))\n",
    "\n",
    "\n",
    "# Create a baseline loader without pinned memory.\n",
    "batch_size = 64\n",
    "num_workers = 2\n",
    "loader_no_pin = make_loader(pin_memory=False, num_workers=num_workers, batch_size=batch_size)\n",
    "\n",
    "# Create an optimized loader using pinned memory.\n",
    "loader_pin = make_loader(pin_memory=True, num_workers=num_workers, batch_size=batch_size)\n",
    "\n",
    "# Warm up model and loaders once to avoid cold start.\n",
    "run_epoch(loader_no_pin, \"Warmup no pin\")\n",
    "\n",
    "# Time one short epoch without pinned memory.\n",
    "run_epoch(loader_no_pin, \"No pinned memory\")\n",
    "\n",
    "# Time one short epoch with pinned memory enabled.\n",
    "run_epoch(loader_pin, \"With pinned memory\")\n",
    "\n",
    "# Print a short explanation summarizing the comparison.\n",
    "print(\"Compare times to see data transfer impact.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f708a97d",
   "metadata": {},
   "source": [
    "## **3. Precision Speed Tradeoffs**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a120427f",
   "metadata": {},
   "source": [
    "### **3.1. Autocast for Mixed Precision**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5839b1f0",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_07/Lecture_B/image_03_01.jpg?v=1769831288\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Autocast automatically chooses safe lower precision operations\n",
    ">* Gives speed and memory gains without manual precision management\n",
    "\n",
    ">* Autocast speeds models by using low precision\n",
    ">* Actual gains vary, so always profile performance\n",
    "\n",
    ">* Mixed precision can introduce numerical errors, instability\n",
    ">* Balance speed, memory savings, and required numerical reliability\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0bb517",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Autocast for Mixed Precision\n",
    "\n",
    "# This script demonstrates PyTorch autocast mixed precision tradeoffs.\n",
    "# It compares speed and loss for float32 versus mixed precision.\n",
    "# Designed for quick safe execution inside Google Colab.\n",
    "\n",
    "# !pip install torch torchvision.\n",
    "\n",
    "# Import required standard and torch modules.\n",
    "import time\n",
    "import random\n",
    "import torch\n",
    "\n",
    "# Set deterministic random seeds for reproducibility.\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Print framework version and selected device.\n",
    "print(\"PyTorch version:\", torch.__version__, \"Device:\", device)\n",
    "\n",
    "# Define a tiny convolutional network for demonstration.\n",
    "class TinyNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = torch.nn.Conv2d(1, 8, 3, padding=1)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.fc = torch.nn.Linear(8 * 28 * 28, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.relu(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        return self.fc(x)\n",
    "\n",
    "# Create a small random dataset tensor and labels.\n",
    "batch_size = 64\n",
    "input_shape = (batch_size, 1, 28, 28)\n",
    "inputs = torch.randn(input_shape, device=device)\n",
    "labels = torch.randint(0, 10, (batch_size,), device=device)\n",
    "\n",
    "# Validate shapes before training loop.\n",
    "assert inputs.shape == input_shape\n",
    "assert labels.shape[0] == batch_size\n",
    "\n",
    "# Instantiate model and optimizer for float32 baseline.\n",
    "model_fp32 = TinyNet().to(device)\n",
    "optimizer_fp32 = torch.optim.SGD(model_fp32.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Run a few float32 training steps and measure time.\n",
    "steps = 20\n",
    "start_time = time.time()\n",
    "for _ in range(steps):\n",
    "    optimizer_fp32.zero_grad()\n",
    "    outputs = model_fp32(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer_fp32.step()\n",
    "\n",
    "fp32_time = time.time() - start_time\n",
    "fp32_loss = float(loss.detach().cpu())\n",
    "\n",
    "# Instantiate model and optimizer for mixed precision run.\n",
    "model_mp = TinyNet().to(device)\n",
    "optimizer_mp = torch.optim.SGD(model_mp.parameters(), lr=0.01)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=device.type == \"cuda\")\n",
    "\n",
    "# Run the same steps using autocast mixed precision.\n",
    "start_time = time.time()\n",
    "for _ in range(steps):\n",
    "    optimizer_mp.zero_grad()\n",
    "    with torch.cuda.amp.autocast(enabled=device.type == \"cuda\"):\n",
    "        outputs = model_mp(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer_mp)\n",
    "    scaler.update()\n",
    "\n",
    "mp_time = time.time() - start_time\n",
    "mp_loss = float(loss.detach().cpu())\n",
    "\n",
    "# Estimate memory usage by parameter dtype sizes.\n",
    "fp32_params = sum(p.numel() for p in model_fp32.parameters())\n",
    "mp_params = sum(p.numel() for p in model_mp.parameters())\n",
    "bytes_fp32 = fp32_params * 4\n",
    "bytes_mp = mp_params * 4\n",
    "\n",
    "# Print concise comparison of speed and numerical behavior.\n",
    "print(\"Float32 time seconds:\", round(fp32_time, 4))\n",
    "print(\"Mixed precision time seconds:\", round(mp_time, 4))\n",
    "print(\"Float32 final loss value:\", round(fp32_loss, 4))\n",
    "print(\"Mixed precision final loss value:\", round(mp_loss, 4))\n",
    "print(\"Parameter memory bytes both modes:\", bytes_fp32, bytes_mp)\n",
    "print(\"Loss difference absolute value:\", round(abs(fp32_loss - mp_loss), 6))\n",
    "print(\"Note mixed precision may trade tiny accuracy for speed.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07304d2c",
   "metadata": {},
   "source": [
    "### **3.2. Memory footprint checks**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101727f2",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_07/Lecture_B/image_03_02.jpg?v=1769831338\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Track GPU memory while chasing more speed\n",
    ">* Treat memory as a budget, justify every increase\n",
    "\n",
    ">* Transient activations often dominate GPU memory usage\n",
    ">* Monitor peak memory to choose stable configurations\n",
    "\n",
    ">* Mixed precision lowers memory, enabling larger models\n",
    ">* Extra states add cost, so verify peak memory\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6667457c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Memory footprint checks\n",
    "\n",
    "# This script shows simple memory footprint checks.\n",
    "# We compare batch sizes and mixed precision memory usage.\n",
    "# Focus on clear prints and tiny synthetic data.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required modules for TensorFlow and system inspection.\n",
    "import os, random, psutil, numpy as np, tensorflow as tf\n",
    "\n",
    "# Set deterministic seeds for reproducible tiny experiment.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version and available physical devices.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Physical devices:\", tf.config.list_physical_devices())\n",
    "process = psutil.Process(os.getpid())\n",
    "\n",
    "# Helper function to report current and peak memory usage.\n",
    "def report_memory(label):\n",
    "    info = process.memory_info()\n",
    "    used_mb = info.rss / (1024 * 1024)\n",
    "    print(label, \"memory_mb:\", round(used_mb, 2))\n",
    "\n",
    "\n",
    "# Create a tiny synthetic dataset with small feature size.\n",
    "num_samples, num_features, num_classes = 256, 32, 3\n",
    "x_data = np.random.randn(num_samples, num_features).astype(\"float32\")\n",
    "y_data = np.random.randint(0, num_classes, size=(num_samples,))\n",
    "\n",
    "# Validate shapes before building the model and training.\n",
    "assert x_data.shape == (num_samples, num_features)\n",
    "assert y_data.shape == (num_samples,)\n",
    "report_memory(\"After data creation\")\n",
    "\n",
    "# Build a very small dense model for classification.\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(num_features,)),\n",
    "    tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(num_classes, activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "# Compile the model with a simple optimizer and loss.\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "report_memory(\"After model creation\")\n",
    "\n",
    "# Function to run one short training and report memory usage.\n",
    "def run_experiment(batch_size, use_mixed):\n",
    "    policy_name = \"mixed_float16\" if use_mixed else \"float32\"\n",
    "    tf.keras.mixed_precision.set_global_policy(policy_name)\n",
    "    temp_model = tf.keras.models.clone_model(model)\n",
    "\n",
    "    temp_model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    report_memory(\n",
    "        f\"Before training batch={batch_size} policy={policy_name}\"\n",
    "    )\n",
    "    temp_model.fit(\n",
    "        x_data,\n",
    "        y_data,\n",
    "        epochs=1,\n",
    "        batch_size=batch_size,\n",
    "        verbose=0,\n",
    "    )\n",
    "\n",
    "    report_memory(\n",
    "        f\"After training batch={batch_size} policy={policy_name}\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Run experiments with two batch sizes and two precision policies.\n",
    "run_experiment(batch_size=16, use_mixed=False)\n",
    "run_experiment(batch_size=64, use_mixed=False)\n",
    "run_experiment(batch_size=16, use_mixed=True)\n",
    "run_experiment(batch_size=64, use_mixed=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291b1518",
   "metadata": {},
   "source": [
    "### **3.3. Stability considerations**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1daa1b8c",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_07/Lecture_B/image_03_03.jpg?v=1769831392\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Speed optimizations and lower precision can harm stability\n",
    ">* Watch for NaNs, exploding losses, and unreliable predictions\n",
    "\n",
    ">* Watch precision-sensitive layers like softmax and normalization\n",
    ">* Profile losses and metrics; keep fragile ops high-precision\n",
    "\n",
    ">* Use experiments and stress tests to compare precisions\n",
    ">* Prioritize stability alongside speed, especially in safety-critical tasks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820f3c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Stability considerations\n",
    "\n",
    "# This script shows precision stability considerations simply.\n",
    "# We compare float32 and float16 training stability carefully.\n",
    "# Watch losses and NaN checks for both precisions.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required modules and set deterministic seeds.\n",
    "import os, random, numpy as np, tensorflow as tf\n",
    "\n",
    "# Print TensorFlow version for reproducibility reference.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Set seeds for reproducible random behavior everywhere.\n",
    "seed_value = 7\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Select device preferring GPU when it is available.\n",
    "physical_gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "if physical_gpus:\n",
    "    device_name = \"/GPU:0\"\n",
    "else:\n",
    "    device_name = \"/CPU:0\"\n",
    "\n",
    "print(\"Using device:\", device_name)\n",
    "\n",
    "# Load MNIST dataset and keep a very small subset.\n",
    "(x_train, y_train), _ = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize images and add channel dimension safely.\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_train = np.expand_dims(x_train, axis=-1)\n",
    "\n",
    "# Keep only a tiny subset for quick demonstration.\n",
    "small_size = 512\n",
    "x_small = x_train[:small_size]\n",
    "y_small = y_train[:small_size]\n",
    "\n",
    "# Validate shapes to avoid unexpected broadcasting issues.\n",
    "print(\"Subset shape:\", x_small.shape, y_small.shape)\n",
    "\n",
    "# Build a simple CNN model factory with configurable dtype.\n",
    "def build_model(dtype):\n",
    "    inputs = tf.keras.Input(shape=(28, 28, 1), dtype=dtype)\n",
    "    x = tf.keras.layers.Conv2D(8, (3, 3), activation=\"relu\")(inputs)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dense(32, activation=\"relu\")(x)\n",
    "    outputs = tf.keras.layers.Dense(10, activation=\"softmax\")(x)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "# Prepare datasets with small batch size for stability.\n",
    "batch_size = 64\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_small, y_small))\n",
    "train_ds = train_ds.shuffle(buffer_size=small_size, seed=seed_value)\n",
    "train_ds = train_ds.batch(batch_size)\n",
    "\n",
    "# Create float32 model and optimizer for baseline stability.\n",
    "model_fp32 = build_model(\"float32\")\n",
    "optimizer_fp32 = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "# Create float16 model and optimizer for speed tradeoff.\n",
    "policy = tf.keras.mixed_precision.Policy(\"mixed_float16\")\n",
    "tf.keras.mixed_precision.set_global_policy(policy)\n",
    "model_fp16 = build_model(\"float16\")\n",
    "optimizer_fp16 = tf.keras.mixed_precision.LossScaleOptimizer(\n",
    "    tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    ")\n",
    "\n",
    "# Function to run one short training epoch and track stability.\n",
    "def run_epoch(model, optimizer, dataset, use_mixed):\n",
    "    losses = []\n",
    "    nan_count = 0\n",
    "\n",
    "    for step, (images, labels) in enumerate(dataset):\n",
    "        with tf.device(device_name):\n",
    "            with tf.GradientTape() as tape:\n",
    "                if use_mixed:\n",
    "                    images_cast = tf.cast(images, tf.float16)\n",
    "                else:\n",
    "                    images_cast = tf.cast(images, tf.float32)\n",
    "\n",
    "                logits = model(images_cast, training=True)\n",
    "                loss = loss_fn(labels, logits)\n",
    "\n",
    "                if use_mixed:\n",
    "                    # scaled_loss = optimizer.get_scaled_loss(loss)\n",
    "                    scaled_loss = loss\n",
    "                else:\n",
    "                    scaled_loss = loss\n",
    "\n",
    "        if use_mixed:\n",
    "            # scaled_grads = tape.gradient(scaled_loss, model.trainable_variables)\n",
    "            # grads = optimizer.get_unscaled_gradients(scaled_grads)\n",
    "            grads = tape.gradient(scaled_loss, model.trainable_variables)\n",
    "        else:\n",
    "            grads = tape.gradient(scaled_loss, model.trainable_variables)\n",
    "\n",
    "        if any([tf.reduce_any(tf.math.is_nan(g)) for g in grads if g is not None]):\n",
    "            nan_count += 1\n",
    "\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        losses.append(float(loss.numpy()))\n",
    "\n",
    "    return losses, nan_count\n",
    "\n",
    "# Run one epoch for float32 model and collect statistics.\n",
    "losses32, nans32 = run_epoch(model_fp32, optimizer_fp32, train_ds, False)\n",
    "\n",
    "# Run one epoch for float16 mixed precision model.\n",
    "losses16, nans16 = run_epoch(model_fp16, optimizer_fp16, train_ds, True)\n",
    "\n",
    "# Print concise comparison of stability related statistics.\n",
    "print(\"float32 loss mean:\", np.mean(losses32), \"NaN gradients:\", nans32)\n",
    "print(\"float16 loss mean:\", np.mean(losses16), \"NaN gradients:\", nans16)\n",
    "print(\"float32 loss std:\", np.std(losses32))\n",
    "print(\"float16 loss std:\", np.std(losses16))\n",
    "\n",
    "# Show simple conclusion about precision and stability tradeoffs.\n",
    "print(\"Notice how mixed precision can change loss behavior and gradient stability.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa3e677",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Profiling and Tuning**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba44de43",
   "metadata": {},
   "source": [
    "\n",
    "In this lecture, you learned to:\n",
    "- Profile PyTorch models using torch.profiler to identify time‑consuming operations. \n",
    "- Apply basic performance optimizations such as adjusting batch size, using pin_memory, and enabling mixed precision. \n",
    "- Evaluate the trade‑offs between speed, memory usage, and numerical stability when tuning models. \n",
    "\n",
    "In the next Module (Module 8), we will go over 'Distributed Training'"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
