{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97df6f1b",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Profiling and Tuning**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da5f42e",
   "metadata": {},
   "source": [
    ">Last update: 20260130.\n",
    "    \n",
    "By the end of this Lecture, you will be able to:\n",
    "- Profile PyTorch models using torch.profiler to identify time‑consuming operations. \n",
    "- Apply basic performance optimizations such as adjusting batch size, using pin_memory, and enabling mixed precision. \n",
    "- Evaluate the trade‑offs between speed, memory usage, and numerical stability when tuning models. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fd777e",
   "metadata": {},
   "source": [
    "## **1. PyTorch Profiling Tools**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d939ba1",
   "metadata": {},
   "source": [
    "### **1.1. torch profiler essentials**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568ad8db",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_07/Lecture_B/image_01_01.jpg?v=1769763494\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Profiler logs detailed timing for every operation\n",
    ">* Helps pinpoint hidden bottlenecks and slow layers\n",
    "\n",
    ">* Profile a representative loop with warmup and steady-state\n",
    ">* Inspect timelines to spot idle GPUs and bottlenecks\n",
    "\n",
    ">* Profiler unifies CPU and GPU activity timelines\n",
    ">* Helps diagnose bottlenecks and meet performance constraints\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf868b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - torch profiler essentials\n",
    "\n",
    "# This script shows basic PyTorch profiling essentials.\n",
    "# It compares unoptimized and optimized training steps.\n",
    "# Focus on simple timing and profiler table output.\n",
    "\n",
    "# !pip install torch torchvision.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "# Import torch and torchvision modules.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Import torchvision datasets and transforms.\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Import the PyTorch profiler utilities.\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "# Set deterministic random seeds for reproducibility.\n",
    "random.seed(0)\n",
    "\n",
    "# Set numpy like seed using torch manual seed.\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Select device based on GPU availability.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Print the PyTorch version and selected device.\n",
    "print(\"PyTorch version:\", torch.__version__, \"Device:\", device)\n",
    "\n",
    "# Define a simple convolutional neural network model.\n",
    "class SmallCNN(nn.Module):\n",
    "    # Initialize layers inside the constructor.\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(1, 8, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(8, 10)\n",
    "\n",
    "    # Define the forward computation graph.\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Create a small MNIST training subset transform.\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# Download MNIST dataset with training split only.\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root=\"./data\", train=True, download=True, transform=transform\n",
    ")\n",
    "\n",
    "# Use only a small subset of the dataset.\n",
    "subset_size = 256\n",
    "\n",
    "# Create indices and subset for faster execution.\n",
    "indices = list(range(subset_size))\n",
    "train_subset = torch.utils.data.Subset(train_dataset, indices)\n",
    "\n",
    "# Create a DataLoader without optimizations first.\n",
    "loader_basic = torch.utils.data.DataLoader(\n",
    "    train_subset, batch_size=64, shuffle=True, num_workers=0\n",
    ")\n",
    "\n",
    "# Create a DataLoader with pin_memory optimization.\n",
    "loader_optimized = torch.utils.data.DataLoader(\n",
    "    train_subset,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "# Helper function to run one training step.\n",
    "def train_step(model, data_iter, optimizer, use_amp=False):\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "    try:\n",
    "        images, labels = next(data_iter)\n",
    "    except StopIteration:\n",
    "        return False\n",
    "    if images.ndim != 4 or labels.ndim != 1:\n",
    "        raise ValueError(\"Unexpected batch shapes in train_step.\")\n",
    "    images = images.to(device, non_blocking=True)\n",
    "    labels = labels.to(device, non_blocking=True)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    with torch.cuda.amp.autocast(enabled=use_amp):\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    return True\n",
    "\n",
    "# Function to profile a single training step.\n",
    "def profile_one_step(data_loader, use_amp=False, label=\"basic\"):\n",
    "    model = SmallCNN().to(device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "    data_iter = iter(data_loader)\n",
    "    for _ in range(2):\n",
    "        ok = train_step(model, data_iter, optimizer, use_amp)\n",
    "        if not ok:\n",
    "            return\n",
    "    activities = [ProfilerActivity.CPU]\n",
    "    if torch.cuda.is_available():\n",
    "        activities.append(ProfilerActivity.CUDA)\n",
    "    with profile(activities=activities, record_shapes=True) as prof:\n",
    "        with record_function(f\"train_step_{label}\"):\n",
    "            train_step(model, data_iter, optimizer, use_amp)\n",
    "    key_averages = prof.key_averages().table(\n",
    "        sort_by=\"self_cpu_time_total\", row_limit=8\n",
    "    )\n",
    "    return key_averages\n",
    "\n",
    "# Profile the basic DataLoader without mixed precision.\n",
    "start_basic = time.time()\n",
    "prof_basic = profile_one_step(loader_basic, use_amp=False, label=\"basic\")\n",
    "end_basic = time.time()\n",
    "\n",
    "# Profile the optimized DataLoader with mixed precision.\n",
    "start_opt = time.time()\n",
    "prof_opt = profile_one_step(loader_optimized, use_amp=True, label=\"optimized\")\n",
    "end_opt = time.time()\n",
    "\n",
    "# Print short timing comparison for both configurations.\n",
    "print(\"Basic step wall time seconds:\", round(end_basic - start_basic, 4))\n",
    "\n",
    "# Print optimized configuration timing for comparison.\n",
    "print(\"Optimized step wall time seconds:\", round(end_opt - start_opt, 4))\n",
    "\n",
    "# Print a small header for profiler summaries.\n",
    "print(\"\\nTop operations from basic configuration:\")\n",
    "\n",
    "# Print the profiler table for the basic configuration.\n",
    "print(prof_basic)\n",
    "\n",
    "# Print a small header for optimized configuration.\n",
    "print(\"\\nTop operations from optimized configuration:\")\n",
    "\n",
    "# Print the profiler table for the optimized configuration.\n",
    "print(prof_opt)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4eb016",
   "metadata": {},
   "source": [
    "### **1.2. Chrome Trace Viewer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a85fcf7",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_07/Lecture_B/image_01_02.jpg?v=1769763655\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Visual timeline shows operations across CPU and GPU\n",
    ">* Helps spot bottlenecks and overlaps in training\n",
    "\n",
    ">* Tracks show threads, GPU streams, and events\n",
    ">* Helps spot runtime bottlenecks like slow data\n",
    "\n",
    ">* Compare traces to see optimization effects clearly\n",
    ">* Build intuition, locate bottlenecks, validate performance changes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec3e7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Chrome Trace Viewer\n",
    "\n",
    "# This script shows a tiny profiling example.\n",
    "# We simulate a trace style workflow for beginners.\n",
    "# Focus is on Chrome Trace style profiler output.\n",
    "\n",
    "# Install PyTorch if not already available in Colab.\n",
    "# !pip install torch torchvision torchaudio --quiet.\n",
    "\n",
    "# Import standard libraries for system checks.\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# Try importing torch and handle missing installation.\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "    from torch.profiler import profile\n",
    "    from torch.profiler import ProfilerActivity\n",
    "except Exception as e:\n",
    "    print(\"PyTorch import failed, please install first.\")\n",
    "    raise e\n",
    "\n",
    "# Set a deterministic random seed for reproducibility.\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Select device, prefer cuda if available and supported.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Print a short line with PyTorch version and device.\n",
    "print(\"Torch version:\", torch.__version__, \"Device:\", device)\n",
    "\n",
    "# Define a tiny convolutional network for demonstration.\n",
    "class TinyConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 8, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=3)\n",
    "        self.fc1 = nn.Linear(16 * 24 * 24, 32)\n",
    "        self.fc2 = nn.Linear(32, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Create a small random input batch similar to MNIST.\n",
    "batch_size = 8\n",
    "input_tensor = torch.randn(batch_size, 1, 28, 28)\n",
    "\n",
    "# Validate the input shape before moving to device.\n",
    "assert input_tensor.shape == (8, 1, 28, 28)\n",
    "\n",
    "# Move model and data to the selected device.\n",
    "model = TinyConvNet().to(device)\n",
    "input_tensor = input_tensor.to(device)\n",
    "\n",
    "# Warm up the model once to avoid cold start noise.\n",
    "with torch.no_grad():\n",
    "    _ = model(input_tensor)\n",
    "\n",
    "# Define a simple training like step for profiling.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Create tiny target labels for the fake batch.\n",
    "target = torch.randint(0, 10, (batch_size,), device=device)\n",
    "\n",
    "# Ensure target shape matches batch dimension exactly.\n",
    "assert target.shape[0] == input_tensor.shape[0]\n",
    "\n",
    "# Set up profiler to record CPU and CUDA activities.\n",
    "activities = [ProfilerActivity.CPU]\n",
    "if device.type == \"cuda\":\n",
    "    activities.append(ProfilerActivity.CUDA)\n",
    "\n",
    "# Define a small helper function to run one step.\n",
    "def train_step():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(input_tensor)\n",
    "    loss = criterion(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Use torch.profiler to record a few training steps.\n",
    "with profile(activities=activities, record_shapes=True) as prof:\n",
    "    for _ in range(3):\n",
    "        train_step()\n",
    "\n",
    "# Choose a directory for saving the Chrome trace file.\n",
    "trace_dir = \"./profiler_traces\"\n",
    "\n",
    "# Create the directory if it does not already exist.\n",
    "os.makedirs(trace_dir, exist_ok=True)\n",
    "\n",
    "# Export the trace as a Chrome Trace JSON file.\n",
    "trace_path = os.path.join(trace_dir, \"tiny_trace.json\")\n",
    "prof.export_chrome_trace(trace_path)\n",
    "\n",
    "# Print short instructions for opening the trace file.\n",
    "print(\"Trace saved to:\", trace_path)\n",
    "print(\"To view, open chrome://tracing in Chrome.\")\n",
    "print(\"Then load tiny_trace.json to explore the timeline.\")\n",
    "print(\"Look for CPU and CUDA tracks and kernel durations.\")\n",
    "print(\"Use zoom and pan to inspect slow operations visually.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d65863b",
   "metadata": {},
   "source": [
    "### **1.3. Reading Operator Metrics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83918867",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_07/Lecture_B/image_01_03.jpg?v=1769763977\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Profiler rows show each operation and timings\n",
    ">* Find operators dominating runtime to target optimization\n",
    "\n",
    ">* Compare self time and total time meanings\n",
    ">* Use them to locate true performance bottlenecks\n",
    "\n",
    ">* Watch CPU, GPU time and data transfers\n",
    ">* Use patterns to choose effective performance optimizations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4602213",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Reading Operator Metrics\n",
    "\n",
    "# This script shows basic operator profiling.\n",
    "# It uses TensorFlow to mimic profiling ideas.\n",
    "# Focus on reading timing metrics for operations.\n",
    "\n",
    "# !pip install tensorflow-io-gcs-filesystem.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Import TensorFlow and check version.\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "\n",
    "# Set TensorFlow random seed for determinism.\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Select device string based on availability.\n",
    "physical_gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "\n",
    "# Choose GPU if available otherwise use CPU.\n",
    "if physical_gpus:\n",
    "    device_name = \"/GPU:0\"\n",
    "else:\n",
    "    device_name = \"/CPU:0\"\n",
    "\n",
    "# Print which device will run the operations.\n",
    "print(\"Using device:\", device_name)\n",
    "\n",
    "# Define a helper to time a TensorFlow function.\n",
    "def time_tf_function(fn, *args, **kwargs):\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    # Run the function once to get result.\n",
    "    result = fn(*args, **kwargs)\n",
    "\n",
    "    # Ensure all pending ops are finished.\n",
    "    if hasattr(tf.experimental, \"sync_devices\"):\n",
    "        tf.experimental.sync_devices()\n",
    "\n",
    "    # Compute elapsed time in milliseconds.\n",
    "    elapsed_ms = (time.perf_counter() - start_time) * 1000.0\n",
    "\n",
    "    # Return both result and elapsed milliseconds.\n",
    "    return result, elapsed_ms\n",
    "\n",
    "# Create a simple dense layer model for profiling.\n",
    "inputs = tf.keras.Input(shape=(512,), name=\"features\")\n",
    "\n",
    "# Add two dense layers to create some work.\n",
    "x = tf.keras.layers.Dense(256, activation=\"relu\")(inputs)\n",
    "\n",
    "# Add another dense layer with relu activation.\n",
    "x = tf.keras.layers.Dense(256, activation=\"relu\")(x)\n",
    "\n",
    "# Final output layer with ten units softmax.\n",
    "outputs = tf.keras.layers.Dense(10, activation=\"softmax\")(x)\n",
    "\n",
    "# Build the Keras model object.\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Create a small random batch of input data.\n",
    "batch_size = 64\n",
    "\n",
    "# Generate random input tensor with correct shape.\n",
    "input_data = tf.random.normal(shape=(batch_size, 512))\n",
    "\n",
    "# Validate the input tensor shape explicitly.\n",
    "assert input_data.shape == (batch_size, 512)\n",
    "\n",
    "# Warm up the model once to build graphs.\n",
    "_ = model(input_data, training=False)\n",
    "\n",
    "# Define a function that runs the full forward pass.\n",
    "@tf.function\n",
    "def run_full_model(x):\n",
    "    return model(x, training=False)\n",
    "\n",
    "# Define a function that runs only first dense layer.\n",
    "@tf.function\n",
    "def run_first_layer(x):\n",
    "    return model.layers[1](x)\n",
    "\n",
    "# Define a function that runs only second dense layer.\n",
    "@tf.function\n",
    "def run_second_layer(x):\n",
    "    first = model.layers[1](x)\n",
    "    return model.layers[2](first)\n",
    "\n",
    "# Time the full model forward pass once.\n",
    "_, full_time_ms = time_tf_function(run_full_model, input_data)\n",
    "\n",
    "# Time the first dense layer forward pass.\n",
    "_, first_time_ms = time_tf_function(run_first_layer, input_data)\n",
    "\n",
    "# Time the second dense layer forward pass.\n",
    "_, second_time_ms = time_tf_function(run_second_layer, input_data)\n",
    "\n",
    "# Print a short header for timing results.\n",
    "print(\"\\nOperator like timing results (milliseconds):\")\n",
    "\n",
    "# Print timing for the full model call.\n",
    "print(\"Full model total time:\", round(full_time_ms, 3))\n",
    "\n",
    "# Print timing for first dense layer only.\n",
    "print(\"First dense self like time:\", round(first_time_ms, 3))\n",
    "\n",
    "# Print timing for second dense layer only.\n",
    "print(\"Second dense self like time:\", round(second_time_ms, 3))\n",
    "\n",
    "# Compute approximate fraction of time per layer.\n",
    "first_fraction = first_time_ms / full_time_ms\n",
    "\n",
    "# Compute second layer fraction of total time.\n",
    "second_fraction = second_time_ms / full_time_ms\n",
    "\n",
    "# Print fractions to mimic operator contribution.\n",
    "print(\"First layer fraction of total:\", round(first_fraction, 3))\n",
    "\n",
    "# Print second layer fraction of total time.\n",
    "print(\"Second layer fraction of total:\", round(second_fraction, 3))\n",
    "\n",
    "# Show which layer appears more time consuming.\n",
    "print(\"Heavier layer index:\", 1 if first_time_ms > second_time_ms else 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150b7e5f",
   "metadata": {},
   "source": [
    "## **2. Boosting Data Throughput**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb7559b",
   "metadata": {},
   "source": [
    "### **2.1. Optimal Batch Sizing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feeccd07",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_07/Lecture_B/image_02_01.jpg?v=1769764222\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Balance GPU usage without overloading its memory\n",
    ">* Tune batch size to maximize speed safely\n",
    "\n",
    ">* Increase batch size slowly while tracking throughput\n",
    ">* Stop increasing when gains plateau or memory fails\n",
    "\n",
    ">* Batch size interacts with optimization and stability\n",
    ">* Retune batch size whenever settings or hardware change\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5b8bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Optimal Batch Sizing\n",
    "\n",
    "# This script explores optimal batch sizing simply.\n",
    "# We use TensorFlow to simulate training speed.\n",
    "# Focus on throughput changes with different batch sizes.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Import TensorFlow and NumPy.\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Set deterministic random seeds.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "\n",
    "# Set NumPy and TensorFlow seeds.\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version briefly.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Select device string based on availability.\n",
    "physical_gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "if physical_gpus:\n",
    "    device_name = \"/GPU:0\"\n",
    "else:\n",
    "    device_name = \"/CPU:0\"\n",
    "\n",
    "# Inform which device is used.\n",
    "print(\"Using device:\", device_name)\n",
    "\n",
    "# Define small synthetic dataset size.\n",
    "num_samples = 4096\n",
    "input_dim = 128\n",
    "\n",
    "# Create random input features.\n",
    "features = np.random.randn(num_samples, input_dim).astype(\"float32\")\n",
    "\n",
    "# Create random binary labels.\n",
    "labels = np.random.randint(0, 2, size=(num_samples, 1)).astype(\"float32\")\n",
    "\n",
    "# Validate shapes before building dataset.\n",
    "assert features.shape[0] == labels.shape[0]\n",
    "\n",
    "# Build a simple dense model.\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(input_dim,)),\n",
    "    tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "])\n",
    "\n",
    "# Compile model with basic optimizer.\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Prepare list of batch sizes to test.\n",
    "batch_sizes = [16, 64, 256, 512]\n",
    "\n",
    "# Dictionary to store throughput results.\n",
    "throughput_results = {}\n",
    "\n",
    "# Use device scope for potential GPU.\n",
    "with tf.device(device_name):\n",
    "\n",
    "    # Loop over candidate batch sizes.\n",
    "    for batch_size in batch_sizes:\n",
    "\n",
    "        # Build tf.data dataset with given batch size.\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "        dataset = dataset.shuffle(buffer_size=num_samples, seed=seed_value)\n",
    "\n",
    "        # Batch and prefetch for better throughput.\n",
    "        dataset = dataset.batch(batch_size)\n",
    "        dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "        # Warm up one small training step.\n",
    "        _ = model.fit(\n",
    "            dataset.take(1),\n",
    "            epochs=1,\n",
    "            verbose=0,\n",
    "        )\n",
    "\n",
    "        # Time one short epoch over dataset.\n",
    "        start_time = time.time()\n",
    "        history = model.fit(\n",
    "            dataset,\n",
    "            epochs=1,\n",
    "            verbose=0,\n",
    "        )\n",
    "        end_time = time.time()\n",
    "\n",
    "        # Compute elapsed time and throughput.\n",
    "        elapsed = end_time - start_time\n",
    "        samples_per_second = num_samples / max(elapsed, 1e-6)\n",
    "\n",
    "        # Store throughput for this batch size.\n",
    "        throughput_results[batch_size] = samples_per_second\n",
    "\n",
    "# Print concise summary header.\n",
    "print(\"\\nBatch size vs samples per second:\")\n",
    "\n",
    "# Print results for each tested batch size.\n",
    "for bs in batch_sizes:\n",
    "    value = throughput_results[bs]\n",
    "    print(f\"Batch {bs:4d}: {value:8.1f} samples/sec\")\n",
    "\n",
    "# Print simple guidance based on best throughput.\n",
    "best_bs = max(throughput_results, key=throughput_results.get)\n",
    "print(\"\\nBest throughput batch size in this demo:\", best_bs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca486d8",
   "metadata": {},
   "source": [
    "### **2.2. Efficient DataLoader Configuration**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1eff564",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_07/Lecture_B/image_02_02.jpg?v=1769764492\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Slow data loaders starve GPUs and hurt throughput\n",
    ">* Treat data loader configuration as key optimization lever\n",
    "\n",
    ">* More DataLoader workers prepare batches in parallel\n",
    ">* Tune worker count to balance speed and resources\n",
    "\n",
    ">* Shuffle and batch data to balance randomness, speed\n",
    ">* Use smart prefetching to keep accelerators busy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc79ce2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Efficient DataLoader Configuration\n",
    "\n",
    "# This script shows efficient DataLoader configuration.\n",
    "# We compare slow and fast data input pipelines.\n",
    "# Focus is on batch size and prefetch settings.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Import tensorflow and check version.\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set deterministic random seeds.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "\n",
    "# Set numpy random seed for reproducibility.\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# Set tensorflow random seed for reproducibility.\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Detect available device type for information.\n",
    "physical_gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "\n",
    "# Print framework version and device information.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Print whether a GPU is available or not.\n",
    "print(\"GPU available:\", bool(physical_gpus))\n",
    "\n",
    "# Create a small synthetic dataset in memory.\n",
    "num_samples = 2000\n",
    "\n",
    "# Define feature dimension for synthetic data.\n",
    "feature_dim = 32\n",
    "\n",
    "# Generate random input features as float32.\n",
    "features = np.random.randn(num_samples, feature_dim).astype(\"float32\")\n",
    "\n",
    "# Generate simple binary labels from features.\n",
    "labels = (np.sum(features, axis=1) > 0).astype(\"int32\")\n",
    "\n",
    "# Validate shapes before building datasets.\n",
    "assert features.shape[0] == labels.shape[0]\n",
    "\n",
    "# Create a base tf.data.Dataset from tensors.\n",
    "base_ds = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "\n",
    "# Shuffle the dataset with a small buffer.\n",
    "base_ds = base_ds.shuffle(buffer_size=512, seed=seed_value)\n",
    "\n",
    "# Define a simple function to build a dataset.\n",
    "def make_dataset(batch_size, prefetch_size, num_parallel_calls):\n",
    "    # Batch the dataset with given batch size.\n",
    "    ds = base_ds.batch(batch_size)\n",
    "\n",
    "    # Map a light preprocessing step.\n",
    "    ds = ds.map(lambda x, y: (tf.math.l2_normalize(x, axis=1), y),\n",
    "                num_parallel_calls=num_parallel_calls)\n",
    "\n",
    "    # Prefetch batches to overlap compute and input.\n",
    "    ds = ds.prefetch(prefetch_size)\n",
    "\n",
    "    # Return the configured dataset.\n",
    "    return ds\n",
    "\n",
    "# Build a deliberately slow input pipeline.\n",
    "slow_batch_size = 16\n",
    "\n",
    "# Use no parallel calls and minimal prefetch.\n",
    "slow_ds = make_dataset(slow_batch_size, prefetch_size=1,\n",
    "                       num_parallel_calls=None)\n",
    "\n",
    "# Build a more efficient input pipeline.\n",
    "fast_batch_size = 64\n",
    "\n",
    "# Use autotune for parallel calls and prefetch.\n",
    "fast_ds = make_dataset(fast_batch_size,\n",
    "                       prefetch_size=tf.data.AUTOTUNE,\n",
    "                       num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "# Define a simple dense model for demonstration.\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(feature_dim,)),\n",
    "    tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "])\n",
    "\n",
    "# Compile the model with binary crossentropy loss.\n",
    "model.compile(optimizer=\"adam\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# Train briefly with the slow input pipeline.\n",
    "print(\"Training with slow DataLoader configuration...\")\n",
    "\n",
    "# Use small epochs and silent verbose setting.\n",
    "history_slow = model.fit(slow_ds,\n",
    "                         epochs=2,\n",
    "                         verbose=0)\n",
    "\n",
    "# Train briefly with the fast input pipeline.\n",
    "print(\"Training with efficient DataLoader configuration...\")\n",
    "\n",
    "# Reuse same model weights for fair comparison.\n",
    "history_fast = model.fit(fast_ds,\n",
    "                         epochs=2,\n",
    "                         verbose=0)\n",
    "\n",
    "# Extract final losses and accuracies for comparison.\n",
    "slow_loss = history_slow.history[\"loss\"][-1]\n",
    "\n",
    "# Get final accuracy from slow configuration.\n",
    "slow_acc = history_slow.history[\"accuracy\"][-1]\n",
    "\n",
    "# Extract final metrics from fast configuration.\n",
    "fast_loss = history_fast.history[\"loss\"][-1]\n",
    "\n",
    "# Get final accuracy from fast configuration.\n",
    "fast_acc = history_fast.history[\"accuracy\"][-1]\n",
    "\n",
    "# Print a short comparison summary for learners.\n",
    "print(\"Slow config - batch:\", slow_batch_size,\n",
    "      \"loss:\", round(float(slow_loss), 4),\n",
    "      \"acc:\", round(float(slow_acc), 4))\n",
    "\n",
    "# Print metrics for the efficient configuration.\n",
    "print(\"Fast config - batch:\", fast_batch_size,\n",
    "      \"loss:\", round(float(fast_loss), 4),\n",
    "      \"acc:\", round(float(fast_acc), 4))\n",
    "\n",
    "# Explain that both configs learn but differ in throughput.\n",
    "print(\"Both runs learn similarly, but fast config feeds faster.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3701848",
   "metadata": {},
   "source": [
    "### **2.3. Pinned Memory Prefetching**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa0ed1d",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_07/Lecture_B/image_02_03.jpg?v=1769764641\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Pinned memory cuts CPU‑to‑GPU transfer delays\n",
    ">* Prefetching keeps GPU busy and speeds training\n",
    "\n",
    ">* Pinned memory overlaps data transfer with computation\n",
    ">* This prevents GPU idle time and improves throughput\n",
    "\n",
    ">* Pinned memory improves speed but risks host pressure\n",
    ">* Tune prefetch, batch size, workers using monitoring\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0694308e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Pinned Memory Prefetching\n",
    "\n",
    "# This script shows pinned memory prefetching basics.\n",
    "# We compare DataLoader settings for GPU data throughput.\n",
    "# Focus on batch size pin_memory and non_blocking transfers.\n",
    "\n",
    "# !pip install torch torchvision.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "# Import torch and torchvision utilities.\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# Set deterministic random seeds.\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Detect device preferring GPU when available.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Print torch version and selected device.\n",
    "print(\"torch\", torch.__version__, \"device\", device)\n",
    "\n",
    "# Define a simple transform converting images to tensors.\n",
    "transform = T.Compose([T.ToTensor()])\n",
    "\n",
    "# Download a tiny MNIST training subset.\n",
    "full_dataset = torchvision.datasets.MNIST(\n",
    "    root=\"./data\", train=True, download=True, transform=transform\n",
    ")\n",
    "\n",
    "# Select a small subset for quick profiling.\n",
    "subset_size = 2048\n",
    "indices = list(range(subset_size))\n",
    "small_dataset = torch.utils.data.Subset(full_dataset, indices)\n",
    "\n",
    "# Define a simple convolutional network.\n",
    "class SmallCNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = torch.nn.Conv2d(1, 8, kernel_size=3, padding=1)\n",
    "        self.pool = torch.nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = torch.nn.Linear(8, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "# Instantiate model and move to selected device.\n",
    "model = SmallCNN().to(device)\n",
    "\n",
    "# Define loss function and optimizer.\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# Helper function to run one short training epoch.\n",
    "def run_epoch(dataloader, use_non_blocking):\n",
    "    model.train()\n",
    "    start = time.time()\n",
    "    total_loss = 0.0\n",
    "    total_batches = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
    "        if batch_idx >= 20:\n",
    "            break\n",
    "        assert inputs.ndim == 4 and targets.ndim == 1\n",
    "        inputs = inputs.to(device, non_blocking=use_non_blocking)\n",
    "        targets = targets.to(device, non_blocking=use_non_blocking)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        total_batches += 1\n",
    "    end = time.time()\n",
    "    avg_loss = total_loss / max(total_batches, 1)\n",
    "    return end - start, avg_loss\n",
    "\n",
    "# Create baseline DataLoader without pinned memory.\n",
    "batch_size = 128\n",
    "loader_baseline = torch.utils.data.DataLoader(\n",
    "    small_dataset, batch_size=batch_size, shuffle=True,\n",
    "    num_workers=0, pin_memory=False\n",
    ")\n",
    "\n",
    "# Create optimized DataLoader with pinned memory enabled.\n",
    "loader_pinned = torch.utils.data.DataLoader(\n",
    "    small_dataset, batch_size=batch_size, shuffle=True,\n",
    "    num_workers=2, pin_memory=True\n",
    ")\n",
    "\n",
    "# Warm up model and CUDA context if available.\n",
    "_ = next(iter(loader_baseline))\n",
    "if device.type == \"cuda\":\n",
    "    dummy = torch.randn(1, 1, 28, 28, device=device)\n",
    "    _ = model(dummy)\n",
    "\n",
    "# Time baseline epoch without non_blocking transfers.\n",
    "baseline_time, baseline_loss = run_epoch(loader_baseline, False)\n",
    "\n",
    "# Time pinned memory epoch with non_blocking transfers.\n",
    "pinned_time, pinned_loss = run_epoch(loader_pinned, True)\n",
    "\n",
    "# Print concise comparison of timings and losses.\n",
    "print(\"Baseline loader seconds\", round(baseline_time, 3))\n",
    "print(\"Pinned loader seconds\", round(pinned_time, 3))\n",
    "print(\"Baseline loss\", round(baseline_loss, 4))\n",
    "print(\"Pinned loss\", round(pinned_loss, 4))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47efbd6",
   "metadata": {},
   "source": [
    "## **3. Precision Speed Tradeoffs**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697658c9",
   "metadata": {},
   "source": [
    "### **3.1. Autocast for Mixed Precision**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1674259",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_07/Lecture_B/image_03_01.jpg?v=1769764767\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Autocast automates mixed precision for speed, memory\n",
    ">* Chooses low or full precision per operation safely\n",
    "\n",
    ">* Autocast speeds training and lowers memory use\n",
    ">* Must test for rounding errors and stability risks\n",
    "\n",
    ">* Autocast often keeps accuracy while boosting speed\n",
    ">* Monitor metrics and tune usage for sensitive tasks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8481f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Autocast for Mixed Precision\n",
    "\n",
    "# This script demonstrates mixed precision autocast tradeoffs.\n",
    "# It compares speed and loss with and without autocast.\n",
    "# Designed for small quick runs in Colab.\n",
    "\n",
    "# !pip install torch torchvision.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Import torch and check availability.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Set deterministic random seeds.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "\n",
    "# Set numpy and torch seeds.\n",
    "np.random.seed(seed_value)\n",
    "torch.manual_seed(seed_value)\n",
    "\n",
    "# Select device based on availability.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Print framework version and device.\n",
    "print(\"Torch version:\", torch.__version__, \"Device:\", device)\n",
    "\n",
    "# Define a tiny feedforward model.\n",
    "class TinyNet(nn.Module):\n",
    "    # Initialize linear layers and activation.\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.act = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    # Define forward computation.\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Create small synthetic regression dataset.\n",
    "num_samples, input_dim = 512, 64\n",
    "\n",
    "# Generate random inputs and targets.\n",
    "X = torch.randn(num_samples, input_dim)\n",
    "y = torch.randn(num_samples, 1)\n",
    "\n",
    "# Move data to selected device.\n",
    "X = X.to(device)\n",
    "y = y.to(device)\n",
    "\n",
    "# Validate shapes before training.\n",
    "assert X.shape == (num_samples, input_dim)\n",
    "assert y.shape == (num_samples, 1)\n",
    "\n",
    "# Helper function to run one training pass.\n",
    "def run_epoch(model, optimizer, use_autocast):\n",
    "    # Set model to training mode.\n",
    "    model.train()\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Choose batch size for loop.\n",
    "    batch_size = 64\n",
    "    num_batches = num_samples // batch_size\n",
    "\n",
    "    # Track cumulative loss value.\n",
    "    total_loss = 0.0\n",
    "\n",
    "    # Loop over mini batches.\n",
    "    for i in range(num_batches):\n",
    "        # Slice batch from tensors.\n",
    "        xb = X[i * batch_size:(i + 1) * batch_size]\n",
    "        yb = y[i * batch_size:(i + 1) * batch_size]\n",
    "\n",
    "        # Zero gradients before backward.\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # Use autocast context when requested.\n",
    "        if use_autocast and device.type == \"cuda\":\n",
    "            with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "                preds = model(xb)\n",
    "                loss = criterion(preds, yb)\n",
    "        else:\n",
    "            preds = model(xb)\n",
    "            loss = criterion(preds, yb)\n",
    "\n",
    "        # Backpropagate gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        # Optimizer step updates weights.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate detached loss value.\n",
    "        total_loss += loss.detach().item()\n",
    "\n",
    "    # Return average loss for epoch.\n",
    "    return total_loss / max(num_batches, 1)\n",
    "\n",
    "# Function to benchmark one configuration.\n",
    "def benchmark_run(use_autocast):\n",
    "    # Create fresh model instance.\n",
    "    model = TinyNet(input_dim, 128, 1).to(device)\n",
    "\n",
    "    # Use simple Adam optimizer.\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    # Warmup run without timing.\n",
    "    _ = run_epoch(model, optimizer, use_autocast)\n",
    "\n",
    "    # Start timing for one epoch.\n",
    "    start_time = time.perf_counter()\n",
    "    avg_loss = run_epoch(model, optimizer, use_autocast)\n",
    "    elapsed = time.perf_counter() - start_time\n",
    "\n",
    "    # Estimate memory usage if cuda.\n",
    "    if device.type == \"cuda\":\n",
    "        mem_bytes = torch.cuda.max_memory_allocated(device)\n",
    "        mem_mb = mem_bytes / (1024 ** 2)\n",
    "    else:\n",
    "        mem_mb = 0.0\n",
    "\n",
    "    # Return metrics dictionary.\n",
    "    return avg_loss, elapsed, mem_mb\n",
    "\n",
    "# Run benchmark without autocast first.\n",
    "loss_fp32, time_fp32, mem_fp32 = benchmark_run(use_autocast=False)\n",
    "\n",
    "# Run benchmark with autocast if possible.\n",
    "loss_amp, time_amp, mem_amp = benchmark_run(use_autocast=True)\n",
    "\n",
    "# Print concise comparison header.\n",
    "print(\"\\nMixed precision autocast comparison:\")\n",
    "\n",
    "# Print full precision metrics.\n",
    "print(\"FP32 -> loss:\", round(loss_fp32, 4), \"time:\", round(time_fp32, 4))\n",
    "\n",
    "# Print autocast metrics.\n",
    "print(\"AMP  -> loss:\", round(loss_amp, 4), \"time:\", round(time_amp, 4))\n",
    "\n",
    "# Print relative speedup information.\n",
    "speedup = time_fp32 / time_amp if time_amp > 0 else 1.0\n",
    "print(\"Speedup factor (FP32/AMP):\", round(speedup, 3))\n",
    "\n",
    "# Print simple memory comparison when available.\n",
    "if device.type == \"cuda\":\n",
    "    print(\"FP32 max memory MB:\", round(mem_fp32, 2))\n",
    "    print(\"AMP  max memory MB:\", round(mem_amp, 2))\n",
    "\n",
    "# Final line prints brief tradeoff summary.\n",
    "print(\"Autocast trades tiny loss changes for speed and memory gains.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faaf2b3b",
   "metadata": {},
   "source": [
    "### **3.2. Memory Usage Monitoring**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e739e8",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_07/Lecture_B/image_03_02.jpg?v=1769764883\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Track GPU memory, not just latency, always\n",
    ">* Watch peak usage as settings change for stability\n",
    "\n",
    ">* Faster settings often sharply increase GPU memory use\n",
    ">* Test memory on real data to avoid crashes\n",
    "\n",
    ">* Memory-saving tricks affect speed and accuracy\n",
    ">* Monitor memory to keep models stable and reliable\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c937e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Memory Usage Monitoring\n",
    "\n",
    "# This script shows basic memory usage monitoring.\n",
    "# It uses TensorFlow to simulate model memory behavior.\n",
    "# Focus on speed and memory tradeoffs with batches.\n",
    "\n",
    "# !pip install tensorflow-2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Import TensorFlow and check version.\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set deterministic random seeds.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "\n",
    "# Set numpy random seed for reproducibility.\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# Set TensorFlow random seed for reproducibility.\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Detect GPU availability for potential memory monitoring.\n",
    "physical_gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "\n",
    "# Choose device string based on GPU presence.\n",
    "if physical_gpus:\n",
    "    device_name = \"/GPU:0\"\n",
    "else:\n",
    "    device_name = \"/CPU:0\"\n",
    "\n",
    "# Print which device will be used.\n",
    "print(\"Using device:\", device_name)\n",
    "\n",
    "# Define a small utility to get memory info safely.\n",
    "def get_memory_info():\n",
    "    # Handle GPU memory query if GPU exists.\n",
    "    if physical_gpus:\n",
    "        try:\n",
    "            details = tf.config.experimental.get_memory_info(\"GPU:0\")\n",
    "        except Exception:\n",
    "            return None\n",
    "        return details\n",
    "    # Return None when only CPU is available.\n",
    "    return None\n",
    "\n",
    "# Build a tiny dense model for demonstration.\n",
    "def build_model():\n",
    "    # Create a simple sequential dense network.\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(128,)),\n",
    "        tf.keras.layers.Dense(256, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(256, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(10, activation=\"softmax\"),\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Create synthetic data with configurable batch size.\n",
    "def make_data(batch_size):\n",
    "    # Create random features with fixed dimension.\n",
    "    x = tf.random.normal((batch_size, 128))\n",
    "    # Create random integer labels for classes.\n",
    "    y = tf.random.uniform((batch_size,), 0, 10, dtype=tf.int32)\n",
    "    return x, y\n",
    "\n",
    "# Run one training step and measure memory usage.\n",
    "def run_step(model, optimizer, loss_fn, batch_size, mixed):\n",
    "    # Prepare input batch and labels.\n",
    "    x, y = make_data(batch_size)\n",
    "    # Optionally use mixed precision autocast.\n",
    "    if mixed:\n",
    "        policy = tf.keras.mixed_precision.Policy(\"mixed_float16\")\n",
    "        tf.keras.mixed_precision.set_global_policy(policy)\n",
    "    else:\n",
    "        policy = tf.keras.mixed_precision.Policy(\"float32\")\n",
    "        tf.keras.mixed_precision.set_global_policy(policy)\n",
    "\n",
    "    # Record memory before step if possible.\n",
    "    before = get_memory_info()\n",
    "\n",
    "    # Use GradientTape for one training step.\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(x, training=True)\n",
    "        loss = loss_fn(y, logits)\n",
    "    # Compute gradients and apply update.\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    # Record memory after step if possible.\n",
    "    after = get_memory_info()\n",
    "\n",
    "    # Compute simple metrics for reporting.\n",
    "    loss_value = float(loss.numpy())\n",
    "    return before, after, loss_value\n",
    "\n",
    "# Nicely format memory information in megabytes.\n",
    "def format_memory(info):\n",
    "    # Return message when memory info is unavailable.\n",
    "    if info is None:\n",
    "        return \"Memory info not available on this device.\"\n",
    "    # Convert bytes to megabytes for readability.\n",
    "    current_mb = info[\"current\"] / (1024 * 1024)\n",
    "    peak_mb = info[\"peak\"] / (1024 * 1024)\n",
    "    return f\"current={current_mb:.1f}MB, peak={peak_mb:.1f}MB\"\n",
    "\n",
    "# Main demonstration comparing two configurations.\n",
    "def main():\n",
    "    # Build model and optimizer once for fairness.\n",
    "    model = build_model()\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "    # Define two configurations to compare.\n",
    "    configs = [\n",
    "        {\"name\": \"small_batch_fp32\", \"batch\": 32, \"mixed\": False},\n",
    "        {\"name\": \"large_batch_mixed\", \"batch\": 128, \"mixed\": True},\n",
    "    ]\n",
    "\n",
    "    # Run each configuration and print summary.\n",
    "    for cfg in configs:\n",
    "        before, after, loss_value = run_step(\n",
    "            model, optimizer, loss_fn, cfg[\"batch\"], cfg[\"mixed\"]\n",
    "        )\n",
    "        print(\"\\nConfiguration:\", cfg[\"name\"])\n",
    "        print(\"Batch size:\", cfg[\"batch\"], \"Mixed precision:\", cfg[\"mixed\"])\n",
    "        print(\"Loss value:\", round(loss_value, 4))\n",
    "        print(\"Before step memory:\", format_memory(before))\n",
    "        print(\"After step memory:\", format_memory(after))\n",
    "\n",
    "    # Provide short guidance on interpreting results.\n",
    "    print(\"\\nObserve how batch size and precision affect memory.\")\n",
    "\n",
    "# Execute main demonstration function.\n",
    "main()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d716e24",
   "metadata": {},
   "source": [
    "### **3.3. Numerical Stability Tradeoffs**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43e9de9",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_07/Lecture_B/image_03_03.jpg?v=1769765017\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Lower precision boosts speed but reduces accuracy\n",
    ">* Can cause instability, especially in sensitive tasks\n",
    "\n",
    ">* Watch for NaNs, exploding gradients, unstable metrics\n",
    ">* Balance speed gains against reliability and reproducibility\n",
    "\n",
    ">* Iterate tuning with validation, not speed alone\n",
    ">* Choose precision per component using evidence-based tradeoffs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a8989f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Numerical Stability Tradeoffs\n",
    "\n",
    "# This script shows precision and stability tradeoffs.\n",
    "# We compare float32 and float16 on a tiny example.\n",
    "# Focus on speed, memory, and numerical behavior.\n",
    "\n",
    "# Optional install for TensorFlow if missing.\n",
    "# !pip install tensorflow==2.20.0 --quiet.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Import numpy for numeric helpers.\n",
    "import numpy as np\n",
    "\n",
    "# Import tensorflow and check version.\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set deterministic random seeds.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version once.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Select device, prefer GPU when available.\n",
    "physical_gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "if physical_gpus:\n",
    "    device_name = \"/GPU:0\"\n",
    "else:\n",
    "    device_name = \"/CPU:0\"\n",
    "\n",
    "# Create a simple numeric stability test function.\n",
    "def build_tiny_model(dtype):\n",
    "    inputs = tf.keras.Input(shape=(10,), dtype=dtype)\n",
    "    x = tf.keras.layers.Dense(\n",
    "        32,\n",
    "        activation=\"relu\",\n",
    "        dtype=dtype,\n",
    "    )(inputs)\n",
    "    outputs = tf.keras.layers.Dense(\n",
    "        1,\n",
    "        activation=\"linear\",\n",
    "        dtype=dtype,\n",
    "    )(x)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.SGD(0.01),\n",
    "        loss=\"mse\",\n",
    "        run_eagerly=False,\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Generate a tiny synthetic regression dataset.\n",
    "num_samples = 512\n",
    "x_data = np.random.randn(num_samples, 10).astype(\"float32\")\n",
    "true_w = np.linspace(0.1, 1.0, 10).astype(\"float32\")\n",
    "y_data = x_data @ true_w + 0.5\n",
    "\n",
    "# Validate shapes before training.\n",
    "assert x_data.shape == (num_samples, 10)\n",
    "assert y_data.shape == (num_samples,)\n",
    "\n",
    "# Prepare datasets for float32 and float16 models.\n",
    "train_ds_f32 = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_data.astype(\"float32\"), y_data.astype(\"float32\"))\n",
    ").batch(64)\n",
    "\n",
    "train_ds_f16 = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_data.astype(\"float16\"), y_data.astype(\"float16\"))\n",
    ").batch(64)\n",
    "\n",
    "# Build models with different numeric precision.\n",
    "model_f32 = build_tiny_model(\"float32\")\n",
    "model_f16 = build_tiny_model(\"float16\")\n",
    "\n",
    "# Train float32 model and measure time.\n",
    "with tf.device(device_name):\n",
    "    start_f32 = time.time()\n",
    "    history_f32 = model_f32.fit(\n",
    "        train_ds_f32,\n",
    "        epochs=5,\n",
    "        verbose=0,\n",
    "    )\n",
    "    end_f32 = time.time()\n",
    "\n",
    "# Train float16 model and measure time.\n",
    "with tf.device(device_name):\n",
    "    start_f16 = time.time()\n",
    "    history_f16 = model_f16.fit(\n",
    "        train_ds_f16,\n",
    "        epochs=5,\n",
    "        verbose=0,\n",
    "    )\n",
    "    end_f16 = time.time()\n",
    "\n",
    "# Collect final losses for comparison.\n",
    "final_loss_f32 = float(history_f32.history[\"loss\"][-1])\n",
    "final_loss_f16 = float(history_f16.history[\"loss\"][-1])\n",
    "\n",
    "# Check for obvious numerical issues.\n",
    "has_nan_f32 = np.isnan(final_loss_f32)\n",
    "has_nan_f16 = np.isnan(final_loss_f16)\n",
    "\n",
    "# Print concise comparison summary.\n",
    "print(\"Device used:\", device_name)\n",
    "print(\"float32 final loss:\", round(final_loss_f32, 6))\n",
    "print(\"float16 final loss:\", round(final_loss_f16, 6))\n",
    "print(\"float32 time seconds:\", round(end_f32 - start_f32, 4))\n",
    "print(\"float16 time seconds:\", round(end_f16 - start_f16, 4))\n",
    "print(\"float32 loss is NaN:\", has_nan_f32)\n",
    "print(\"float16 loss is NaN:\", has_nan_f16)\n",
    "print(\"Loss difference:\", round(final_loss_f16 - final_loss_f32, 6))\n",
    "print(\"Remember: faster precision may change loss behavior.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488370b3",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Profiling and Tuning**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501fe47d",
   "metadata": {},
   "source": [
    "\n",
    "In this lecture, you learned to:\n",
    "- Profile PyTorch models using torch.profiler to identify time‑consuming operations. \n",
    "- Apply basic performance optimizations such as adjusting batch size, using pin_memory, and enabling mixed precision. \n",
    "- Evaluate the trade‑offs between speed, memory usage, and numerical stability when tuning models. \n",
    "\n",
    "In the next Module (Module 8), we will go over 'Distributed Training'"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
