{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b41cf1a",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Using torch.compile**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b829da4",
   "metadata": {},
   "source": [
    ">Last update: 20260130.\n",
    "    \n",
    "By the end of this Lecture, you will be able to:\n",
    "- Describe the purpose and high‑level behavior of torch.compile in PyTorch 2.10.0. \n",
    "- Wrap existing nn.Module models with torch.compile and configure basic compilation options. \n",
    "- Measure and interpret performance changes after compilation, including warmup effects. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed457a9b",
   "metadata": {},
   "source": [
    "## **1. Core Compile Concepts**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e353b9b",
   "metadata": {},
   "source": [
    "### **1.1. Eager and Compiled Modes**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db3511d",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_07/Lecture_A/image_01_01.jpg?v=1769827107\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Eager mode runs each tensor operation immediately\n",
    ">* Python overhead can slow large, repeated workloads\n",
    "\n",
    ">* Compiler records model runs, builds optimized graph\n",
    ">* Optimized graph skips Python overhead, fuses operations\n",
    "\n",
    ">* Eager favors flexibility, debugging, and quick feedback\n",
    ">* Compiled favors speed after warmup for stable workloads\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9254618b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Eager and Compiled Modes\n",
    "\n",
    "# This script compares eager and compiled execution modes.\n",
    "# It uses TensorFlow to mimic PyTorch compile ideas.\n",
    "# Focus on timing simple model runs in two execution styles.\n",
    "\n",
    "# !pip install tensorflow-2.20.0.\n",
    "\n",
    "# Import required standard and TensorFlow modules.\n",
    "import os, random, time, numpy as np, tensorflow as tf\n",
    "\n",
    "# Set deterministic seeds for reproducible behavior.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version as framework information.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Select device string based on GPU availability.\n",
    "physical_gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "use_gpu = bool(physical_gpus)\n",
    "device_name = \"/GPU:0\" if use_gpu else \"/CPU:0\"\n",
    "\n",
    "# Create a tiny dense model for demonstration.\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(32,)),\n",
    "    tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "# Build model by running one dummy forward pass.\n",
    "dummy_input = tf.zeros((1, 32), dtype=tf.float32)\n",
    "_ = model(dummy_input)\n",
    "\n",
    "# Validate model output shape for safety.\n",
    "output_shape = model(dummy_input).shape.as_list()\n",
    "assert output_shape == [1, 10], \"Unexpected model output shape detected.\"\n",
    "\n",
    "# Create random input batch for timing tests.\n",
    "batch_size = 256\n",
    "x_batch = tf.random.normal((batch_size, 32), dtype=tf.float32)\n",
    "\n",
    "# Define eager forward function without decoration.\n",
    "def eager_forward(x_batch_input):\n",
    "    return model(x_batch_input)\n",
    "\n",
    "\n",
    "# Define compiled forward function using tf.function.\n",
    "@tf.function\n",
    "def compiled_forward(x_batch_input):\n",
    "    return model(x_batch_input)\n",
    "\n",
    "\n",
    "# Helper function to time multiple runs of a callable.\n",
    "def time_runs(fn, x_batch_input, warmup_runs, timed_runs):\n",
    "    for _ in range(warmup_runs):\n",
    "        _ = fn(x_batch_input)\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "    for _ in range(timed_runs):\n",
    "        _ = fn(x_batch_input)\n",
    "\n",
    "    end_time = time.perf_counter()\n",
    "    return (end_time - start_time) / float(timed_runs)\n",
    "\n",
    "\n",
    "# Configure warmup and timed run counts.\n",
    "warmup_runs = 3\n",
    "timed_runs = 10\n",
    "\n",
    "# Run timing for eager execution mode.\n",
    "with tf.device(device_name):\n",
    "    eager_time = time_runs(eager_forward, x_batch, warmup_runs, timed_runs)\n",
    "\n",
    "# Run timing for compiled execution mode.\n",
    "with tf.device(device_name):\n",
    "    compiled_time = time_runs(compiled_forward, x_batch, warmup_runs, timed_runs)\n",
    "\n",
    "# Compute relative speedup factor safely.\n",
    "speedup = eager_time / compiled_time if compiled_time > 0.0 else 1.0\n",
    "\n",
    "# Print concise summary comparing both execution modes.\n",
    "print(\"Device used for runs:\", device_name)\n",
    "print(\"Average eager forward time (seconds):\", round(eager_time, 6))\n",
    "print(\"Average compiled forward time (seconds):\", round(compiled_time, 6))\n",
    "print(\"Approximate compiled speedup factor:\", round(speedup, 2))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915486d4",
   "metadata": {},
   "source": [
    "### **1.2. Tracing and Graphs**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2ce479",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_07/Lecture_A/image_01_02.jpg?v=1769827158\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Tracing records tensor ops into a graph\n",
    ">* Static graphs let compilers analyze and optimize\n",
    "\n",
    ">* Tracing records tensor ops into a static graph\n",
    ">* Compiler optimizes this graph for faster execution\n",
    "\n",
    ">* Traced graphs capture one specific run’s behavior\n",
    ">* Stable, predictable models benefit most; dynamic ones retrace\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee7972d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Tracing and Graphs\n",
    "\n",
    "# This script illustrates tracing and graphs conceptually.\n",
    "# We simulate a tiny tensor computation graph manually.\n",
    "# Focus on understanding nodes edges and execution order.\n",
    "\n",
    "# No extra installs are required for this simple example.\n",
    "# Uncomment and adapt if additional packages become necessary.\n",
    "# Example placeholder pip install line shown here.\n",
    "# !pip install some_required_package_if_needed.\n",
    "\n",
    "# Define a simple Node class representing one graph operation.\n",
    "class Node:\n",
    "    def __init__(self, name, op, inputs):\n",
    "        self.name = name\n",
    "        self.op = op\n",
    "        self.inputs = inputs\n",
    "\n",
    "\n",
    "# Define a tiny tensor like container using Python lists.\n",
    "class TinyTensor:\n",
    "    def __init__(self, values):\n",
    "        self.values = list(values)\n",
    "\n",
    "\n",
    "# Define a function that adds two TinyTensor objects safely.\n",
    "def add_tensors(a, b):\n",
    "    assert len(a.values) == len(b.values)\n",
    "\n",
    "    return TinyTensor([x + y for x, y in zip(a.values, b.values)])\n",
    "\n",
    "\n",
    "# Define a function that multiplies two TinyTensor objects safely.\n",
    "def mul_tensors(a, b):\n",
    "    assert len(a.values) == len(b.values)\n",
    "\n",
    "    return TinyTensor([x * y for x, y in zip(a.values, b.values)])\n",
    "\n",
    "\n",
    "# Define a simple tracer that records operations as nodes.\n",
    "class Tracer:\n",
    "    def __init__(self):\n",
    "        self.nodes = []\n",
    "\n",
    "    # Method to record an operation node inside the tracer.\n",
    "    def record(self, name, op, inputs):\n",
    "        node = Node(name=name, op=op, inputs=inputs)\n",
    "        self.nodes.append(node)\n",
    "\n",
    "        return node\n",
    "\n",
    "    # Method to execute nodes in recorded order and show results.\n",
    "    def run(self):\n",
    "        env = {}\n",
    "        for node in self.nodes:\n",
    "            input_tensors = [env[i] for i in node.inputs]\n",
    "            result = node.op(*input_tensors)\n",
    "            env[node.name] = result\n",
    "\n",
    "        return env\n",
    "\n",
    "\n",
    "# Create example TinyTensor inputs representing traced model inputs.\n",
    "input_a = TinyTensor([1.0, 2.0, 3.0])\n",
    "input_b = TinyTensor([0.5, 0.5, 0.5])\n",
    "\n",
    "\n",
    "# Initialize tracer and seed environment with input tensors.\n",
    "tracer = Tracer()\n",
    "initial_env = {\"a\": input_a, \"b\": input_b}\n",
    "\n",
    "\n",
    "# Record an addition node representing a first layer operation.\n",
    "node_add = tracer.record(\n",
    "    name=\"c\",\n",
    "    op=lambda x, y: add_tensors(x, y),\n",
    "    inputs=[\"a\", \"b\"],\n",
    ")\n",
    "\n",
    "\n",
    "# Record a multiplication node representing another layer operation.\n",
    "node_mul = tracer.record(\n",
    "    name=\"d\",\n",
    "    op=lambda x, y: mul_tensors(x, y),\n",
    "    inputs=[\"c\", \"b\"],\n",
    ")\n",
    "\n",
    "\n",
    "# Execute the recorded graph using the initial environment mapping.\n",
    "env = dict(initial_env)\n",
    "for node in tracer.nodes:\n",
    "    input_tensors = [env[i] for i in node.inputs]\n",
    "    env[node.name] = node.op(*input_tensors)\n",
    "\n",
    "\n",
    "# Print a short summary showing traced nodes and final outputs.\n",
    "print(\"Recorded nodes in tiny computation graph:\")\n",
    "for node in tracer.nodes:\n",
    "    print(\"Node\", node.name, \"inputs\", node.inputs)\n",
    "\n",
    "\n",
    "# Print final tensor values to illustrate graph execution result.\n",
    "print(\"Final tensor c values:\", env[\"c\"].values)\n",
    "print(\"Final tensor d values:\", env[\"d\"].values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba5595f",
   "metadata": {},
   "source": [
    "### **1.3. Supported operations overview**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de146499",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_07/Lecture_A/image_01_03.jpg?v=1769827204\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Compiler fuses supported tensor ops into graphs\n",
    ">* Common layers compile into faster, larger kernels\n",
    "\n",
    ">* Compiler supports many ops, falls back when needed\n",
    ">* Models can mix compiled regions with eager code\n",
    "\n",
    ">* Compilation is flexible; unsupported parts safely fallback\n",
    ">* Speedups grow with long stretches of supported ops\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5806c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Supported operations overview\n",
    "\n",
    "# This script explains supported operations conceptually.\n",
    "# It uses TensorFlow to simulate tensor operations.\n",
    "# Focus is on compile friendly versus unfriendly patterns.\n",
    "\n",
    "# !pip install tensorflow-io-gcs-filesystem.\n",
    "\n",
    "# Import required standard and TensorFlow modules.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "seed_value = 7\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version as our tensor framework.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Create a small random tensor representing model activations.\n",
    "activations = tf.random.normal(shape=(2, 4), dtype=tf.float32)\n",
    "print(\"Input activations shape:\", activations.shape)\n",
    "\n",
    "# Define a compile friendly block using dense and activation.\n",
    "friendly_layer = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(units=4, activation=\"relu\"),\n",
    "])\n",
    "\n",
    "# Run the friendly block once to build internal variables.\n",
    "friendly_output = friendly_layer(activations)\n",
    "print(\"Friendly block output shape:\", friendly_output.shape)\n",
    "\n",
    "# Define a Python heavy function simulating unsupported operations.\n",
    "def python_heavy_transform(tensor):\n",
    "    values = tensor.numpy().flatten().tolist()\n",
    "    squared = [v * v for v in values]\n",
    "\n",
    "    return tf.convert_to_tensor(squared, dtype=tensor.dtype)\n",
    "\n",
    "\n",
    "# Apply friendly block then Python heavy transform sequentially.\n",
    "intermediate = friendly_layer(activations)\n",
    "heavy_output = python_heavy_transform(intermediate)\n",
    "print(\"Heavy transform output shape:\", heavy_output.shape)\n",
    "\n",
    "# Reshape heavy output back to matrix form safely.\n",
    "if heavy_output.shape[0] == activations.shape[0] * activations.shape[1]:\n",
    "    reshaped_output = tf.reshape(\n",
    "        heavy_output,\n",
    "        shape=(activations.shape[0], activations.shape[1]),\n",
    "    )\n",
    "else:\n",
    "    reshaped_output = activations\n",
    "\n",
    "# Show final tensor shape after mixed style processing.\n",
    "print(\"Final mixed pipeline shape:\", reshaped_output.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7a1014",
   "metadata": {},
   "source": [
    "## **2. torch compile options**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b8d1a9",
   "metadata": {},
   "source": [
    "### **2.1. torch compile basics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7df8f2",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_07/Lecture_A/image_02_01.jpg?v=1769827248\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Think of torch.compile as wrapping existing models\n",
    ">* Compiled model keeps same interface, improves performance\n",
    "\n",
    ">* Compiled model is a seamless drop-in replacement\n",
    ">* Initial runs are slower; later runs faster\n",
    "\n",
    ">* Compile wrapper fits into existing workflows easily\n",
    ">* Treat compilation as an optional performance layer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905ea946",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - torch compile basics\n",
    "\n",
    "# This script demonstrates basic torch compile wrapping.\n",
    "# It compares eager and compiled model forward speeds.\n",
    "# It keeps the model and training loop unchanged.\n",
    "\n",
    "# !pip install torch torchvision.\n",
    "\n",
    "# Import required standard and torch modules.\n",
    "import time\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Select device based on GPU availability.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define a simple convolutional neural network model.\n",
    "class SmallConvNet(nn.Module):\n",
    "    # Initialize layers inside the constructor.\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(8, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Linear(16, 10)\n",
    "\n",
    "    # Define the forward computation for inputs.\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        return self.classifier(x)\n",
    "\n",
    "\n",
    "# Create an instance of the model and move to device.\n",
    "model_eager = SmallConvNet().to(device)\n",
    "\n",
    "# Create a compiled version using torch compile wrapper.\n",
    "model_compiled = torch.compile(model_eager)\n",
    "\n",
    "# Create a small random input batch for timing tests.\n",
    "batch_size = 32\n",
    "input_shape = (batch_size, 1, 28, 28)\n",
    "\n",
    "# Build input tensor and move to selected device.\n",
    "example_inputs = torch.randn(input_shape, device=device)\n",
    "\n",
    "# Validate that model outputs have matching shapes.\n",
    "with torch.no_grad():\n",
    "    out_eager = model_eager(example_inputs)\n",
    "    out_compiled = model_compiled(example_inputs)\n",
    "\n",
    "# Assert shapes are equal to ensure drop in replacement.\n",
    "assert out_eager.shape == out_compiled.shape\n",
    "\n",
    "\n",
    "# Define a helper function to time multiple forward passes.\n",
    "def time_forwards(model, inputs, warmup, iters):\n",
    "    # Ensure gradients are disabled during timing.\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Optional warmup iterations to trigger compilation.\n",
    "        for _ in range(warmup):\n",
    "            _ = model(inputs)\n",
    "\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "        start = time.perf_counter()\n",
    "        for _ in range(iters):\n",
    "            _ = model(inputs)\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.synchronize()\n",
    "        end = time.perf_counter()\n",
    "\n",
    "    # Return average time per iteration.\n",
    "    return (end - start) / iters\n",
    "\n",
    "\n",
    "# Configure warmup and iteration counts for quick demo.\n",
    "warmup_iters = 1\n",
    "measure_iters = 5\n",
    "\n",
    "# Time the eager model without compilation overhead.\n",
    "avg_eager = time_forwards(model_eager, example_inputs, 0, measure_iters)\n",
    "\n",
    "# Time the compiled model including warmup behavior.\n",
    "avg_compiled = time_forwards(model_compiled, example_inputs, warmup_iters, measure_iters)\n",
    "\n",
    "# Print framework version and device information.\n",
    "print(\"PyTorch version:\", torch.__version__, \"Device:\", device)\n",
    "\n",
    "# Print average forward times for both model variants.\n",
    "print(\"Average eager forward seconds:\", round(avg_eager, 6))\n",
    "print(\"Average compiled forward seconds:\", round(avg_compiled, 6))\n",
    "\n",
    "# Show that both models produce numerically close outputs.\n",
    "max_diff = (out_eager - out_compiled).abs().max().item()\n",
    "print(\"Maximum absolute difference between outputs:\", float(max_diff))\n",
    "\n",
    "# Indicate which model was faster in this short experiment.\n",
    "print(\"Compiled faster than eager:\", bool(avg_compiled < avg_eager))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20086c37",
   "metadata": {},
   "source": [
    "### **2.2. Backends and modes**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba26181b",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_07/Lecture_A/image_02_02.jpg?v=1769827306\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Backends optimize the model graph for hardware\n",
    ">* Modes control optimization aggressiveness and compilation robustness\n",
    "\n",
    ">* Different backends specialize for different hardware workloads\n",
    ">* Choose aggressive or conservative backends based on priorities\n",
    "\n",
    ">* Modes trade off debuggability versus raw performance\n",
    ">* Start conservative, then switch to aggressive for production\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2543cde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Backends and modes\n",
    "\n",
    "# This script compares simple backend and mode choices.\n",
    "# It uses TensorFlow to simulate backend style behavior.\n",
    "# Focus on configuration ideas not exact PyTorch details.\n",
    "# !pip install tensorflow-io-gcs-filesystem.\n",
    "\n",
    "# Import required TensorFlow and system modules.\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Set deterministic seeds for reproducible behavior.\n",
    "tf.random.set_seed(7)\n",
    "os.environ[\"PYTHONHASHSEED\"] = \"7\"\n",
    "\n",
    "# Print TensorFlow version as framework information.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Create a tiny synthetic dataset for demonstration.\n",
    "features = tf.random.normal((256, 16))\n",
    "labels = tf.random.normal((256, 1))\n",
    "\n",
    "# Validate dataset shapes before building models.\n",
    "assert features.shape[1] == 16 and labels.shape[1] == 1\n",
    "\n",
    "# Define a simple dense regression model function.\n",
    "\n",
    "def build_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(16,)),\n",
    "        tf.keras.layers.Dense(32, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(1),\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.SGD(learning_rate=0.01),\n",
    "        loss=\"mse\",\n",
    "        run_eagerly=True,\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Build a baseline model using eager execution mode.\n",
    "baseline_model = build_model()\n",
    "\n",
    "# Train baseline model briefly with silent logging.\n",
    "start_eager = time.time()\n",
    "baseline_model.fit(features, labels, epochs=3, batch_size=32, verbose=0)\n",
    "end_eager = time.time()\n",
    "\n",
    "# Measure eager execution training time.\n",
    "eager_time = end_eager - start_eager\n",
    "\n",
    "# Rebuild model configured for graph mode backend style.\n",
    "graph_model = build_model()\n",
    "\n",
    "# Switch model to graph mode by disabling eager run.\n",
    "\n",
    "graph_model.compile(\n",
    "    optimizer=tf.keras.optimizers.SGD(learning_rate=0.01),\n",
    "    loss=\"mse\",\n",
    "    run_eagerly=False,\n",
    ")\n",
    "\n",
    "# Run a warmup epoch to trigger tracing and compilation.\n",
    "_ = graph_model.fit(features, labels, epochs=1, batch_size=32, verbose=0)\n",
    "\n",
    "# Measure compiled style training time after warmup.\n",
    "start_graph = time.time()\n",
    "_ = graph_model.fit(features, labels, epochs=2, batch_size=32, verbose=0)\n",
    "end_graph = time.time()\n",
    "\n",
    "# Compute total graph mode time excluding warmup epoch.\n",
    "graph_time = end_graph - start_graph\n",
    "\n",
    "# Print concise timing comparison for both modes.\n",
    "print(\"Eager backend style time seconds:\", round(eager_time, 4))\n",
    "print(\"Graph backend style time seconds:\", round(graph_time, 4))\n",
    "\n",
    "# Show simple interpretation of backend and mode behavior.\n",
    "print(\"Eager mode prioritizes simplicity and easier debugging.\")\n",
    "print(\"Graph mode acts like optimized backend after warmup.\")\n",
    "print(\"Warmup tracing cost appears before measured graph timing.\")\n",
    "print(\"Choose modes based on development or performance priorities.\")\n",
    "print(\"This mirrors torch.compile backend and mode tradeoffs.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f67c28",
   "metadata": {},
   "source": [
    "### **2.3. Handling Compiler Fallbacks**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340ece2c",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_07/Lecture_A/image_02_03.jpg?v=1769827354\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Compiler may only optimize parts of models\n",
    ">* Fallback to eager mode preserves correctness and reliability\n",
    "\n",
    ">* Watch logs to spot modules running in eager\n",
    ">* Refactor or replace unsupported ops to reduce fallbacks\n",
    "\n",
    ">* Balance fallbacks differently for research and production\n",
    ">* Trade off performance, flexibility, correctness, and maintainability\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d207a694",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Handling Compiler Fallbacks\n",
    "\n",
    "# This script shows torch compile fallbacks simply.\n",
    "# We simulate unsupported operations using a tiny model.\n",
    "# Focus on understanding messages and simple refactors.\n",
    "\n",
    "# !pip install torch torchvision.\n",
    "\n",
    "# Import required modules for this small example.\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "\n",
    "# Set deterministic seed for reproducible random tensors.\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Select device preferring cuda then cpu safely.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define a tiny model with a problematic python branch.\n",
    "class FallbackNet(nn.Module):\n",
    "    # Initialize layers and store threshold attribute.\n",
    "    def __init__(self, threshold: float = 0.0) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Create a single linear layer for demonstration.\n",
    "        self.fc = nn.Linear(4, 4)\n",
    "\n",
    "        # Store threshold used inside python control flow.\n",
    "        self.threshold = threshold\n",
    "\n",
    "    # Define forward with python side control flow.\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Validate input shape to avoid silent broadcasting.\n",
    "        assert x.shape[-1] == 4, \"Input last dimension must equal four\"\n",
    "\n",
    "        # Use a torch-based compare instead of a Python data-dependent branch.\n",
    "        cond = torch.mean(x) > self.threshold\n",
    "        # Replace data-dependent Python branch with tensor-friendly control flow.\n",
    "        x_lin = F.relu(self.fc(x))\n",
    "        return torch.where(cond, x_lin, x * 0.5)\n",
    "\n",
    "\n",
    "# Create eager model instance and move to device.\n",
    "model_eager = FallbackNet(threshold=0.0).to(device)\n",
    "\n",
    "# Create compiled model using default backend settings.\n",
    "model_compiled = torch.compile(model_eager)\n",
    "\n",
    "# Create a small input tensor for testing behavior.\n",
    "example_input = torch.randn(2, 4, device=device)\n",
    "\n",
    "# Run eager model once to get reference output.\n",
    "with torch.no_grad():\n",
    "    out_eager = model_eager(example_input)\n",
    "\n",
    "# Run compiled model once to trigger compilation.\n",
    "with torch.no_grad():\n",
    "    out_compiled = model_compiled(example_input)\n",
    "\n",
    "# Check numerical closeness between eager and compiled outputs.\n",
    "max_diff = (out_eager - out_compiled).abs().max().item()\n",
    "\n",
    "# Print framework version and basic comparison summary.\n",
    "print(\"torch version:\", torch.__version__)\n",
    "\n",
    "# Print maximum difference to confirm correctness preservation.\n",
    "print(\"max difference between eager and compiled:\", float(max_diff))\n",
    "\n",
    "# Recompile with fullgraph true to encourage fewer fallbacks.\n",
    "model_compiled_full = torch.compile(model_eager, fullgraph=True)\n",
    "\n",
    "# Run compiled fullgraph model once to warm up compilation.\n",
    "with torch.no_grad():\n",
    "    _ = model_compiled_full(example_input)\n",
    "\n",
    "# Time several runs for eager and compiled models respectively.\n",
    "import time\n",
    "\n",
    "# Define helper function to measure average runtime per iteration.\n",
    "def measure_runtime(fn, x, iters: int = 20) -> float:\n",
    "    # Warmup a few iterations before timing loop.\n",
    "    for _ in range(5):\n",
    "        fn(x)\n",
    "\n",
    "    # Synchronize cuda if available before timing.\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    # Record start time using time module.\n",
    "    start = time.time()\n",
    "\n",
    "    # Run function repeatedly without gradient tracking.\n",
    "    with torch.no_grad():\n",
    "        for _ in range(iters):\n",
    "            fn(x)\n",
    "\n",
    "    # Synchronize again for accurate measurement.\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    # Compute average milliseconds per iteration.\n",
    "    elapsed = (time.time() - start) * 1000.0 / iters\n",
    "\n",
    "    # Return elapsed milliseconds as float value.\n",
    "    return float(elapsed)\n",
    "\n",
    "\n",
    "# Measure runtimes for eager and compiled fullgraph models.\n",
    "ms_eager = measure_runtime(model_eager, example_input)\n",
    "\n",
    "# Measure compiled runtime which may include fewer fallbacks.\n",
    "ms_compiled = measure_runtime(model_compiled_full, example_input)\n",
    "\n",
    "# Print simple timing comparison to interpret speed impact.\n",
    "print(\"eager average milliseconds per run:\", round(ms_eager, 4))\n",
    "\n",
    "# Print compiled timing and note that fallbacks may reduce gains.\n",
    "print(\"compiled average milliseconds per run:\", round(ms_compiled, 4))\n",
    "\n",
    "# Show whether compiled model appears faster despite possible fallbacks.\n",
    "print(\"compiled faster than eager:\", bool(ms_compiled < ms_eager))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c502e3",
   "metadata": {},
   "source": [
    "## **3. Benchmarking Compiled Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89eadb5",
   "metadata": {},
   "source": [
    "### **3.1. Warmup Iterations Explained**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d44713",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_07/Lecture_A/image_03_01.jpg?v=1769827491\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Warmup iterations include heavy one-time setup work\n",
    ">* Ignoring warmup can hide true compiled performance\n",
    "\n",
    ">* Warmup is like warming a cold car\n",
    ">* Initial runs build optimizations that speed later runs\n",
    "\n",
    ">* Separate warmup and steady-state when timing\n",
    ">* Ignore warmup runs; measure only stable performance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cae2a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Warmup Iterations Explained\n",
    "\n",
    "# This script explains warmup iterations conceptually using timing examples.\n",
    "# We simulate a compiled model warmup using simple Python functions.\n",
    "# Focus on how early iterations differ from later steady iterations.\n",
    "\n",
    "# import required standard libraries for timing and randomness.\n",
    "import time\n",
    "import random\n",
    "import statistics\n",
    "\n",
    "# set deterministic random seed for reproducible behavior.\n",
    "random.seed(42)\n",
    "\n",
    "# define a fake compiled step with slower first call behavior.\n",
    "compiled_cache = {\"initialized\": False}\n",
    "\n",
    "# define a function that simulates compiled model work.\n",
    "def fake_compiled_step(batch_size, work_scale):\n",
    "    if not compiled_cache[\"initialized\"]:\n",
    "        time.sleep(0.08)\n",
    "        compiled_cache[\"initialized\"] = True\n",
    "\n",
    "    work_units = batch_size * work_scale\n",
    "    total = 0\n",
    "\n",
    "    for _ in range(work_units):\n",
    "        total += random.random() * 0.0001\n",
    "\n",
    "    return total\n",
    "\n",
    "\n",
    "# define a simple baseline step without compilation warmup.\n",
    "def baseline_step(batch_size, work_scale):\n",
    "    work_units = batch_size * work_scale\n",
    "    total = 0\n",
    "\n",
    "    for _ in range(work_units):\n",
    "        total += random.random() * 0.0001\n",
    "\n",
    "    return total\n",
    "\n",
    "\n",
    "# define a helper function to time several iterations.\n",
    "def run_timed_steps(step_fn, iterations, batch_size, work_scale):\n",
    "    times = []\n",
    "\n",
    "    for _ in range(iterations):\n",
    "        start = time.perf_counter()\n",
    "        _ = step_fn(batch_size, work_scale)\n",
    "        end = time.perf_counter()\n",
    "        times.append(end - start)\n",
    "\n",
    "    return times\n",
    "\n",
    "\n",
    "# choose small parameters to keep runtime short and safe.\n",
    "batch_size = 32\n",
    "work_scale = 120\n",
    "warmup_iterations = 2\n",
    "measured_iterations = 5\n",
    "\n",
    "# run baseline timings without any warmup behavior.\n",
    "baseline_times = run_timed_steps(\n",
    "    baseline_step,\n",
    "    warmup_iterations + measured_iterations,\n",
    "    batch_size,\n",
    "    work_scale,\n",
    ")\n",
    "\n",
    "# reset compiled cache before compiled timings.\n",
    "compiled_cache[\"initialized\"] = False\n",
    "\n",
    "# run compiled timings including warmup and steady iterations.\n",
    "compiled_times = run_timed_steps(\n",
    "    fake_compiled_step,\n",
    "    warmup_iterations + measured_iterations,\n",
    "    batch_size,\n",
    "    work_scale,\n",
    ")\n",
    "\n",
    "# separate warmup and steady state segments for compiled timings.\n",
    "compiled_warmup = compiled_times[:warmup_iterations]\n",
    "compiled_steady = compiled_times[warmup_iterations:]\n",
    "\n",
    "# compute simple averages for clear comparison.\n",
    "avg_baseline = statistics.mean(baseline_times)\n",
    "avg_compiled_all = statistics.mean(compiled_times)\n",
    "avg_compiled_steady = statistics.mean(compiled_steady)\n",
    "\n",
    "# print a short header describing the experiment purpose.\n",
    "print(\"Comparing baseline and simulated compiled warmup behavior.\")\n",
    "\n",
    "# print first few iteration times to highlight warmup effect.\n",
    "print(\"Baseline first three iteration times seconds:\", baseline_times[:3])\n",
    "print(\"Compiled first three iteration times seconds:\", compiled_times[:3])\n",
    "\n",
    "# print average times including and excluding warmup iterations.\n",
    "print(\"Average baseline time all iterations seconds:\", round(avg_baseline, 5))\n",
    "print(\"Average compiled time all iterations seconds:\", round(avg_compiled_all, 5))\n",
    "print(\n",
    "    \"Average compiled time steady iterations seconds:\",\n",
    "    round(avg_compiled_steady, 5),\n",
    ")\n",
    "\n",
    "# print a short explanation connecting warmup to measurement practice.\n",
    "print(\n",
    "    \"Notice compiled first iterations are slower, so we exclude them\",\n",
    ")\n",
    "print(\n",
    "    \"Steady compiled average better reflects real repeated deployment performance\",\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4047d5d",
   "metadata": {},
   "source": [
    "### **3.2. Reliable Timing Techniques**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d73093",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_07/Lecture_A/image_03_02.jpg?v=1769827531\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Keep hardware, inputs, and preprocessing exactly consistent\n",
    ">* Time only forward and backward passes, exclude overheads\n",
    "\n",
    ">* Run many timed iterations and summarize results\n",
    ">* Discard warmup, ignore outliers, handle noisy systems\n",
    "\n",
    ">* Sync GPU operations before stopping performance timers\n",
    ">* Keep GPU environment stable to ensure trustworthy benchmarks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b75ab89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Reliable Timing Techniques\n",
    "\n",
    "# This script demonstrates reliable timing techniques.\n",
    "# It uses TensorFlow to simulate model execution.\n",
    "# Focus on warmup, repetition, and synchronization ideas.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard and TensorFlow modules.\n",
    "import os, time, random, numpy as np, tensorflow as tf\n",
    "\n",
    "# Set deterministic seeds for reproducible random behavior.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version and device information.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"GPU available:\", bool(tf.config.list_physical_devices(\"GPU\")))\n",
    "\n",
    "# Create a tiny dense model to benchmark execution.\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(128,)),\n",
    "    tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "# Build model by running one dummy forward pass.\n",
    "dummy_input = tf.random.uniform(shape=(32, 128))\n",
    "output = model(dummy_input)\n",
    "assert output.shape == (32, 10)\n",
    "\n",
    "# Create a compiled function to simulate torch.compile.\n",
    "@tf.function(jit_compile=True)\n",
    "def compiled_forward(x):\n",
    "    return model(x)\n",
    "\n",
    "# Define a helper that measures average step time.\n",
    "def measure_step_time(step_fn, inputs, warmup, repeats, label):\n",
    "    times = []\n",
    "\n",
    "    for i in range(warmup + repeats):\n",
    "        start = time.perf_counter()\n",
    "        _ = step_fn(inputs)\n",
    "        if tf.config.list_physical_devices(\"GPU\"):\n",
    "            tf.experimental.sync_devices()\n",
    "        end = time.perf_counter()\n",
    "        if i >= warmup:\n",
    "            times.append(end - start)\n",
    "\n",
    "    times = np.array(times, dtype=np.float64)\n",
    "    mean_time = float(times.mean())\n",
    "    std_time = float(times.std())\n",
    "    print(label, \"mean:\", round(mean_time * 1000, 3), \"ms\",\n",
    "          \"std:\", round(std_time * 1000, 3), \"ms\")\n",
    "\n",
    "# Prepare fixed random input batch for fair comparison.\n",
    "inputs = tf.random.uniform(shape=(32, 128))\n",
    "assert inputs.shape[0] == 32 and inputs.shape[1] == 128\n",
    "\n",
    "# Warm up both eager and compiled paths before measuring.\n",
    "_ = model(inputs)\n",
    "_ = compiled_forward(inputs)\n",
    "\n",
    "# Measure eager execution time with warmup and repeats.\n",
    "measure_step_time(model, inputs, warmup=3, repeats=10,\n",
    "                  label=\"Eager forward time\")\n",
    "\n",
    "# Measure compiled execution time with same settings.\n",
    "measure_step_time(compiled_forward, inputs, warmup=3, repeats=10,\n",
    "                  label=\"Compiled forward time\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745f46fd",
   "metadata": {},
   "source": [
    "### **3.3. Interpreting Speedup Results**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879726de",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_07/Lecture_A/image_03_03.jpg?v=1769827574\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Compute speedup as eager time divided compiled\n",
    ">* Always link speedup claims to specific experimental setup\n",
    "\n",
    ">* Check variability and consistency, not just averages\n",
    ">* Use percentiles to judge best and worst cases\n",
    "\n",
    ">* Balance runtime gains against compile overhead costs\n",
    ">* Consider job length and real-world operational constraints\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17567321",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Using torch.compile**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98020d2f",
   "metadata": {},
   "source": [
    "\n",
    "In this lecture, you learned to:\n",
    "- Describe the purpose and high‑level behavior of torch.compile in PyTorch 2.10.0. \n",
    "- Wrap existing nn.Module models with torch.compile and configure basic compilation options. \n",
    "- Measure and interpret performance changes after compilation, including warmup effects. \n",
    "\n",
    "In the next Lecture (Lecture B), we will go over 'Profiling and Tuning'"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
