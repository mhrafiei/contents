{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d78d09d",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Using torch.compile**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26aa317",
   "metadata": {},
   "source": [
    ">Last update: 20260130.\n",
    "    \n",
    "By the end of this Lecture, you will be able to:\n",
    "- Describe the purpose and high‑level behavior of torch.compile in PyTorch 2.10.0. \n",
    "- Wrap existing nn.Module models with torch.compile and configure basic compilation options. \n",
    "- Measure and interpret performance changes after compilation, including warmup effects. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a609636",
   "metadata": {},
   "source": [
    "## **1. Core Compile Concepts**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f1161e",
   "metadata": {},
   "source": [
    "### **1.1. Eager and Compiled Execution**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6820b0f8",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_07/Lecture_A/image_01_01.jpg?v=1769761473\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Eager mode runs operations immediately, stepwise in Python\n",
    ">* Python overhead limits optimization, hurting large repeated models\n",
    "\n",
    ">* Compiler records runs and builds a computation graph\n",
    ">* Optimized backends run the graph faster with less overhead\n",
    "\n",
    ">* Compiled mode has a slower warmup phase\n",
    ">* After warmup, repeated runs can be much faster\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72199c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Eager and Compiled Execution\n",
    "\n",
    "# This script compares eager and compiled execution.\n",
    "# It uses PyTorch to run a tiny model.\n",
    "# Focus is on timing and warmup behavior.\n",
    "\n",
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Import torch and check availability.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Set deterministic random seeds.\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Select device based on availability.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Print framework version and device.\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Define a tiny feedforward model.\n",
    "class TinyNet(nn.Module):\n",
    "    # Initialize layers with small sizes.\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(128, 64)\n",
    "        self.act = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    "\n",
    "    # Define forward computation.\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Create model instance and move to device.\n",
    "model_eager = TinyNet().to(device)\n",
    "\n",
    "# Create a compiled version of the same model.\n",
    "model_compiled = torch.compile(model_eager)\n",
    "\n",
    "# Create a small dummy input batch.\n",
    "batch_size = 32\n",
    "input_dim = 128\n",
    "\n",
    "# Validate dimensions before creating tensor.\n",
    "assert batch_size > 0 and input_dim > 0\n",
    "\n",
    "# Create deterministic input tensor.\n",
    "example_input = torch.randn(batch_size, input_dim, device=device)\n",
    "\n",
    "# Function to time multiple forward passes.\n",
    "def time_forwards(model, x, repeats, label):\n",
    "    # Ensure model is in eval mode.\n",
    "    model.eval()\n",
    "    # Warmup for fair GPU timing.\n",
    "    with torch.no_grad():\n",
    "        _ = model(x)\n",
    "    # Synchronize if using GPU.\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    # Measure repeated forward passes.\n",
    "    start = time.perf_counter()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(repeats):\n",
    "            _ = model(x)\n",
    "    # Synchronize again for accurate timing.\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    end = time.perf_counter()\n",
    "    # Compute average time per iteration.\n",
    "    avg_ms = (end - start) * 1000.0 / repeats\n",
    "    print(f\"{label} average time: {avg_ms:.4f} ms\")\n",
    "\n",
    "# Time eager model for several iterations.\n",
    "repeats = 20\n",
    "time_forwards(model_eager, example_input, repeats, \"Eager\")\n",
    "\n",
    "# Time compiled model including warmup behavior.\n",
    "time_forwards(model_compiled, example_input, repeats, \"Compiled\")\n",
    "\n",
    "# Show a single forward output shape for sanity.\n",
    "with torch.no_grad():\n",
    "    out = model_compiled(example_input)\n",
    "\n",
    "# Validate output shape matches expectation.\n",
    "assert out.shape == (batch_size, 10)\n",
    "\n",
    "# Print final confirmation line.\n",
    "print(\"Output shape from compiled model:\", tuple(out.shape))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c976f947",
   "metadata": {},
   "source": [
    "### **1.2. Graph Tracing Basics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794c3dc6",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_07/Lecture_A/image_01_02.jpg?v=1769761537\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Compilation traces model operations into a computation graph\n",
    ">* Whole-graph view enables powerful performance optimizations\n",
    "\n",
    ">* First run records operations and tensor details\n",
    ">* Captured graph is optimized and reused later\n",
    "\n",
    ">* Works best when computation pattern stays consistent\n",
    ">* Dynamic, branch-heavy logic may need retracing, underperforms\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879dfff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Graph Tracing Basics\n",
    "\n",
    "# This script illustrates simple graph like tracing concepts.\n",
    "# We simulate recording operations during a model forward pass.\n",
    "# Focus is on understanding computation graphs conceptually.\n",
    "\n",
    "# Required installs for extra libraries if needed.\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import standard modules for numerical work and typing.\n",
    "import math\n",
    "import random\n",
    "import textwrap\n",
    "\n",
    "# Set deterministic seeds for reproducible behavior.\n",
    "random.seed(0)\n",
    "\n",
    "# Define a tiny tensor class to hold values and name.\n",
    "class TinyTensor:\n",
    "    def __init__(self, value, name):\n",
    "        self.value = float(value)\n",
    "        self.name = str(name)\n",
    "\n",
    "# Define a node class representing one operation in graph.\n",
    "class OpNode:\n",
    "    def __init__(self, op_type, inputs, output):\n",
    "        self.op_type = op_type\n",
    "        self.inputs = inputs\n",
    "        self.output = output\n",
    "\n",
    "# Define a tracer that records operations during execution.\n",
    "class Tracer:\n",
    "    def __init__(self):\n",
    "        self.nodes = []\n",
    "\n",
    "    # Record a new operation node into internal list.\n",
    "    def record(self, op_type, inputs, output):\n",
    "        node = OpNode(op_type, inputs, output)\n",
    "        self.nodes.append(node)\n",
    "\n",
    "# Create a global tracer instance used by operations.\n",
    "GLOBAL_TRACER = Tracer()\n",
    "\n",
    "# Define traced addition that logs into the tracer.\n",
    "def traced_add(a, b, name):\n",
    "    out = TinyTensor(a.value + b.value, name)\n",
    "    GLOBAL_TRACER.record(\"add\", [a, b], out)\n",
    "    return out\n",
    "\n",
    "# Define traced matrix multiply for scalar like values.\n",
    "def traced_matmul(a, b, name):\n",
    "    out = TinyTensor(a.value * b.value, name)\n",
    "    GLOBAL_TRACER.record(\"matmul\", [a, b], out)\n",
    "    return out\n",
    "\n",
    "# Define traced activation using tanh as simple nonlinearity.\n",
    "def traced_activation(a, name):\n",
    "    out = TinyTensor(math.tanh(a.value), name)\n",
    "    GLOBAL_TRACER.record(\"tanh\", [a], out)\n",
    "    return out\n",
    "\n",
    "# Define a tiny model that uses our traced operations.\n",
    "class TinyModel:\n",
    "    def __init__(self, w1, w2, bias):\n",
    "        self.w1 = TinyTensor(w1, \"w1\")\n",
    "        self.w2 = TinyTensor(w2, \"w2\")\n",
    "        self.bias = TinyTensor(bias, \"bias\")\n",
    "\n",
    "    # Forward pass that will be traced into a computation graph.\n",
    "    def forward(self, x_tensor):\n",
    "        h = traced_matmul(x_tensor, self.w1, \"h_linear\")\n",
    "        h_act = traced_activation(h, \"h_tanh\")\n",
    "        y_lin = traced_matmul(h_act, self.w2, \"y_linear\")\n",
    "        y_out = traced_add(y_lin, self.bias, \"y_out\")\n",
    "        return y_out\n",
    "\n",
    "# Helper function to pretty print the traced computation graph.\n",
    "def print_graph(tracer):\n",
    "    print(\"Recorded computation graph nodes:\")\n",
    "    for idx, node in enumerate(tracer.nodes):\n",
    "        in_names = \",\".join(t.name for t in node.inputs)\n",
    "        line = f\"{idx}: {node.op_type}({in_names}) -> {node.output.name}\"\n",
    "        print(line)\n",
    "\n",
    "# Create model instance with simple deterministic parameters.\n",
    "model = TinyModel(w1=1.5, w2=0.5, bias=0.1)\n",
    "\n",
    "# Create an input tensor representing example model input.\n",
    "input_value = TinyTensor(2.0, \"x_input\")\n",
    "\n",
    "# Run a first forward pass to simulate tracing warmup.\n",
    "output_one = model.forward(input_value)\n",
    "\n",
    "# Print the numeric result of the first forward pass.\n",
    "print(\"First run output value:\", round(output_one.value, 4))\n",
    "\n",
    "# Show the recorded computation graph after first run.\n",
    "print_graph(GLOBAL_TRACER)\n",
    "\n",
    "# Clear tracer nodes to simulate cached optimized graph usage.\n",
    "GLOBAL_TRACER.nodes = []\n",
    "\n",
    "# Run a second forward pass representing compiled fast path.\n",
    "output_two = model.forward(input_value)\n",
    "\n",
    "# Print the numeric result and node count for second run.\n",
    "print(\"Second run output value:\", round(output_two.value, 4))\n",
    "print(\"Second run recorded nodes:\", len(GLOBAL_TRACER.nodes))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec2e89a",
   "metadata": {},
   "source": [
    "### **1.3. Supported operations**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad83f2b",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_07/Lecture_A/image_01_03.jpg?v=1769761612\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Compiler targets safe, graph-friendly model operations\n",
    ">* Most common tensor layers compile automatically for speed\n",
    "\n",
    ">* Compiler prefers regular PyTorch tensor-style operations\n",
    ">* Unsupported Python logic runs eagerly and reduces speedup\n",
    "\n",
    ">* Dynamic shapes may trigger multiple graphs or fallback\n",
    ">* Stable tensor shapes and libraries give best speedups\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8fe6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Supported operations\n",
    "\n",
    "# This script shows supported compiled operations simply.\n",
    "# We compare compiled and eager tensor operations briefly.\n",
    "# Focus is on torch.compile friendly tensor behavior.\n",
    "\n",
    "# !pip install torch torchvision torchaudio.\n",
    "\n",
    "# Import required standard libraries safely.\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Import torch and check availability defensively.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Detect device preferring cuda then cpu.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Print torch version and chosen device once.\n",
    "print(\"Torch version:\", torch.__version__, \"Device:\", device)\n",
    "\n",
    "# Define a simple model using supported operations.\n",
    "class SmallSupportedNet(nn.Module):\n",
    "    # Initialize layers with common building blocks.\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(28 * 28, 64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    "\n",
    "    # Define forward using standard tensor operations.\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Create model instance and move to device.\n",
    "model_eager = SmallSupportedNet().to(device)\n",
    "\n",
    "# Create a compiled version using torch.compile.\n",
    "model_compiled = torch.compile(model_eager)\n",
    "\n",
    "# Create a small batch of fake image data.\n",
    "batch_size = 32\n",
    "input_shape = (batch_size, 1, 28, 28)\n",
    "\n",
    "# Validate shape before creating random input.\n",
    "if len(input_shape) != 4:\n",
    "    raise ValueError(\"Input shape must have four dimensions\")\n",
    "\n",
    "# Create deterministic random input tensor.\n",
    "example_input = torch.randn(input_shape, device=device)\n",
    "\n",
    "# Function to time a single forward pass.\n",
    "def time_forward(model, x, label):\n",
    "    # Warm up cuda if available and synchronize.\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        out = model(x)\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    end = time.time()\n",
    "    # Validate output shape for safety.\n",
    "    if out.shape != (batch_size, 10):\n",
    "        raise ValueError(\"Unexpected output shape from model\")\n",
    "    elapsed = (end - start) * 1000.0\n",
    "    print(label, \"time ms:\", round(elapsed, 3))\n",
    "\n",
    "# Show that operations are supported in eager mode.\n",
    "print(\"Running eager model with supported operations\")\n",
    "\n",
    "# Time eager forward pass once.\n",
    "time_forward(model_eager, example_input, \"Eager first\")\n",
    "\n",
    "# First compiled call includes graph capture overhead.\n",
    "print(\"Running compiled model first time for warmup\")\n",
    "\n",
    "# Time compiled forward pass first warmup.\n",
    "time_forward(model_compiled, example_input, \"Compiled first\")\n",
    "\n",
    "# Second compiled call usually runs optimized graph.\n",
    "print(\"Running compiled model second time optimized\")\n",
    "\n",
    "# Time compiled forward pass second run.\n",
    "time_forward(model_compiled, example_input, \"Compiled second\")\n",
    "\n",
    "# Demonstrate unsupported style using heavy python branching.\n",
    "def unsupported_style(x):\n",
    "    # Use python side branching not tensor friendly.\n",
    "    values = []\n",
    "    for i in range(x.shape[0]):\n",
    "        if i % 2 == 0:\n",
    "            values.append(x[i].sum().item())\n",
    "        else:\n",
    "            values.append(x[i].mean().item())\n",
    "    return values\n",
    "\n",
    "# Call unsupported style once to show it still works.\n",
    "result_values = unsupported_style(example_input.cpu())\n",
    "\n",
    "# Print short message about unsupported style behavior.\n",
    "print(\"Unsupported style ran in python, length:\", len(result_values))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903c7556",
   "metadata": {},
   "source": [
    "## **2. torch compile Options**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca123ae",
   "metadata": {},
   "source": [
    "### **2.1. Basic compile syntax**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9994b7d8",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_07/Lecture_A/image_02_01.jpg?v=1769761691\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Wrap your existing model with an optimizer\n",
    ">* Compiled model keeps same interface and training code\n",
    "\n",
    ">* Compiled model is used like original model\n",
    ">* Training code stays same except compile line\n",
    "\n",
    ">* Use the compiled model consistently in workflows\n",
    ">* Compile main performance-critical models; mix with uncompiled\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f9ca6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Basic compile syntax\n",
    "\n",
    "# This script demonstrates basic torch compile syntax.\n",
    "# It compares original and compiled PyTorch models simply.\n",
    "# Focus on wrapping nn Module with torch compile.\n",
    "\n",
    "# !pip install torch torchvision.\n",
    "\n",
    "# Import standard libraries for reproducibility.\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "# Import torch and neural network modules.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Set deterministic random seeds for reproducibility.\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Detect device preferring cuda when available.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Print the PyTorch version and chosen device.\n",
    "print(\"PyTorch version:\", torch.__version__, \"Device:\", device)\n",
    "\n",
    "# Define a simple feedforward neural network model.\n",
    "class SimpleNet(nn.Module):\n",
    "    # Initialize layers with small hidden dimension.\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    # Define the forward computation for the model.\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Create an instance of the original model.\n",
    "input_dim, hidden_dim, output_dim = 128, 64, 10\n",
    "model = SimpleNet(input_dim, hidden_dim, output_dim).to(device)\n",
    "\n",
    "# Create a small random input batch for testing.\n",
    "batch_size = 32\n",
    "example_input = torch.randn(batch_size, input_dim, device=device)\n",
    "\n",
    "# Validate the input shape before running model.\n",
    "assert example_input.shape == (batch_size, input_dim)\n",
    "\n",
    "# Run a single forward pass with original model.\n",
    "with torch.no_grad():\n",
    "    original_output = model(example_input)\n",
    "\n",
    "# Validate output shape matches expected dimensions.\n",
    "assert original_output.shape == (batch_size, output_dim)\n",
    "\n",
    "# Wrap the existing model using torch compile function.\n",
    "compiled_model = torch.compile(model, mode=\"default\", fullgraph=False)\n",
    "\n",
    "# Run a warmup pass to trigger compilation.\n",
    "with torch.no_grad():\n",
    "    _ = compiled_model(example_input)\n",
    "\n",
    "# Time several runs of the original model.\n",
    "def time_model(run_model: nn.Module, inputs: torch.Tensor, runs: int) -> float:\n",
    "    # Ensure no gradients are tracked during timing.\n",
    "    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "    start_time = time.perf_counter()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(runs):\n",
    "            _ = run_model(inputs)\n",
    "    end_time = time.perf_counter()\n",
    "    return (end_time - start_time) / runs\n",
    "\n",
    "# Choose a small number of runs for quick timing.\n",
    "num_runs = 20\n",
    "\n",
    "# Measure average time for original model.\n",
    "avg_time_original = time_model(model, example_input, num_runs)\n",
    "\n",
    "# Measure average time for compiled model.\n",
    "avg_time_compiled = time_model(compiled_model, example_input, num_runs)\n",
    "\n",
    "# Compute simple speedup ratio safely.\n",
    "speedup = avg_time_original / avg_time_compiled if avg_time_compiled > 0 else 1.0\n",
    "\n",
    "# Print concise timing comparison results.\n",
    "print(\"Average original time (ms):\", round(avg_time_original * 1000, 3))\n",
    "print(\"Average compiled time (ms):\", round(avg_time_compiled * 1000, 3))\n",
    "print(\"Speedup factor (original divided by compiled):\", round(speedup, 3))\n",
    "\n",
    "# Demonstrate that compiled model is drop in replacement.\n",
    "with torch.no_grad():\n",
    "    another_input = torch.randn(batch_size, input_dim, device=device)\n",
    "\n",
    "# Validate new input shape before using compiled model.\n",
    "assert another_input.shape == (batch_size, input_dim)\n",
    "\n",
    "# Run compiled model on new input and check shape.\n",
    "with torch.no_grad():\n",
    "    compiled_output = compiled_model(another_input)\n",
    "\n",
    "# Final assertion confirms interface compatibility.\n",
    "assert compiled_output.shape == (batch_size, output_dim)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63e76eb",
   "metadata": {},
   "source": [
    "### **2.2. Backends and modes**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb5d116",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_07/Lecture_A/image_02_02.jpg?v=1769761870\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Backend chooses how models run on hardware\n",
    ">* Backend and mode define speed, stability, debuggability\n",
    "\n",
    ">* Start with default backend and general mode\n",
    ">* Later tune backends and modes for workloads\n",
    "\n",
    ">* Production needs stable, predictable backends and modes\n",
    ">* Match backend and mode to workload constraints\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d7efca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Backends and modes\n",
    "\n",
    "# This script demonstrates torch compile backends.\n",
    "# It focuses on backends and modes usage.\n",
    "# Run cells sequentially inside Google Colab.\n",
    "\n",
    "# !pip install torch torchvision.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "# Import torch and check availability.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Set deterministic random seeds.\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Select device based on availability.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Print basic environment information.\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Define a tiny convolutional network.\n",
    "class TinyConvNet(nn.Module):\n",
    "    # Initialize layers for the tiny network.\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(1, 4, kernel_size=3)\n",
    "        self.fc = nn.Linear(4 * 26 * 26, 10)\n",
    "\n",
    "    # Define forward pass with simple operations.\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = F.relu(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Create model instance and move to device.\n",
    "model = TinyConvNet().to(device)\n",
    "\n",
    "# Create a small random input batch.\n",
    "batch_size = 16\n",
    "input_shape = (batch_size, 1, 28, 28)\n",
    "\n",
    "# Validate input shape before using it.\n",
    "assert len(input_shape) == 4\n",
    "x = torch.randn(input_shape, device=device)\n",
    "\n",
    "# Define a helper function for timing.\n",
    "def run_timed(model_fn, x, steps, label):\n",
    "    # Warm up model before timing.\n",
    "    with torch.no_grad():\n",
    "        for _ in range(2):\n",
    "            _ = model_fn(x)\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(steps):\n",
    "            _ = model_fn(x)\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    end = time.time()\n",
    "    avg = (end - start) / steps\n",
    "    print(label, \"average seconds:\", round(avg, 6))\n",
    "\n",
    "# Define an eager baseline function.\n",
    "baseline_fn = model\n",
    "\n",
    "# Time the eager baseline model.\n",
    "run_timed(baseline_fn, x, steps=10, label=\"Eager baseline\")\n",
    "\n",
    "# Choose a backend and mode for compilation.\n",
    "backend_choice = \"inductor\"\n",
    "mode_choice = \"default\"\n",
    "\n",
    "# Compile the model with chosen options.\n",
    "compiled_model = torch.compile(\n",
    "    model,\n",
    "    backend=backend_choice,\n",
    "    mode=mode_choice,\n",
    ")\n",
    "\n",
    "# Time the compiled model with warmup.\n",
    "run_timed(compiled_model, x, steps=10, label=\"Compiled default mode\")\n",
    "\n",
    "# Try a different mode if available.\n",
    "alt_mode_choice = \"reduce-overhead\"\n",
    "\n",
    "# Compile another model variant with new mode.\n",
    "compiled_fast_start = torch.compile(\n",
    "    model,\n",
    "    backend=backend_choice,\n",
    "    mode=alt_mode_choice,\n",
    ")\n",
    "\n",
    "# Time the alternative mode compiled model.\n",
    "run_timed(\n",
    "    compiled_fast_start,\n",
    "    x,\n",
    "    steps=10,\n",
    "    label=\"Compiled reduce-overhead mode\",\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeac7200",
   "metadata": {},
   "source": [
    "### **2.3. Handling Compiler Fallbacks**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d51fae",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_07/Lecture_A/image_02_03.jpg?v=1769761935\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Compiled and eager execution can run together\n",
    ">* Fallbacks are normal and don’t mean failure\n",
    "\n",
    ">* Watch compiler warnings to spot eager fallbacks\n",
    ">* Use fallbacks to guide refactoring performance hotspots\n",
    "\n",
    ">* Accept some fallbacks if they’re not bottlenecks\n",
    ">* Profile, optimize hot paths, let others fallback\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb631e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Handling Compiler Fallbacks\n",
    "\n",
    "# This script shows torch compile fallbacks simply.\n",
    "# We simulate compile behavior using plain Python logic.\n",
    "# Focus on understanding mixed fast and slow paths.\n",
    "\n",
    "# Required PyTorch is unavailable so avoid installing.\n",
    "# In real use you would install torch separately.\n",
    "\n",
    "# Import standard modules for timing and arrays.\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "\n",
    "# Set deterministic random seed for reproducibility.\n",
    "random.seed(0)\n",
    "\n",
    "# Define a simple fast path using pure math.\n",
    "def fast_path(x_list):\n",
    "    # Use list comprehension to simulate compiled operations.\n",
    "    return [math.tanh(v) for v in x_list]\n",
    "\n",
    "# Define a slower path with dynamic Python branching.\n",
    "def slow_path(x_list):\n",
    "    # Use explicit loop and branches to mimic fallback.\n",
    "    out = []\n",
    "    for v in x_list:\n",
    "        if v > 0.5:\n",
    "            out.append(math.sin(v))\n",
    "        else:\n",
    "            out.append(math.cos(v))\n",
    "    return out\n",
    "\n",
    "# Define a model that mixes fast and slow paths.\n",
    "def mixed_model(x_list, use_slow):\n",
    "    # Always apply fast path first for all elements.\n",
    "    mid = fast_path(x_list)\n",
    "\n",
    "    # Optionally apply slow path to simulate fallback.\n",
    "    if use_slow:\n",
    "        return slow_path(mid)\n",
    "    return mid\n",
    "\n",
    "# Create small deterministic input data list.\n",
    "input_data = [i / 50.0 for i in range(50)]\n",
    "\n",
    "# Validate input size before timing operations.\n",
    "if len(input_data) != 50:\n",
    "    raise ValueError(\"Unexpected input size for demo\")\n",
    "\n",
    "# Warm up both modes to simulate compiler warmup.\n",
    "_ = mixed_model(input_data, use_slow=False)\n",
    "_ = mixed_model(input_data, use_slow=True)\n",
    "\n",
    "# Time the mostly compiled style without fallback.\n",
    "start_fast = time.perf_counter()\n",
    "for _ in range(2000):\n",
    "    _ = mixed_model(input_data, use_slow=False)\n",
    "fast_duration = time.perf_counter() - start_fast\n",
    "\n",
    "# Time the mixed execution with fallback enabled.\n",
    "start_slow = time.perf_counter()\n",
    "for _ in range(2000):\n",
    "    _ = mixed_model(input_data, use_slow=True)\n",
    "slow_duration = time.perf_counter() - start_slow\n",
    "\n",
    "# Print short summary explaining the timing results.\n",
    "print(\"Fast path only duration:\", round(fast_duration, 4))\n",
    "print(\"Mixed path with fallback duration:\", round(slow_duration, 4))\n",
    "print(\"Fallback overhead factor:\", round(slow_duration / fast_duration, 2))\n",
    "print(\"Note: Here Python simulates compile fallbacks conceptually.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad976820",
   "metadata": {},
   "source": [
    "## **3. Benchmarking Model Speed**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cde164",
   "metadata": {},
   "source": [
    "### **3.1. Warmup Iterations Explained**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69dd3310",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_07/Lecture_A/image_03_01.jpg?v=1769761996\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Early compiled runs are slow warmup iterations\n",
    ">* Ignore warmup to judge true steady performance\n",
    "\n",
    ">* Early runs include compilation and hardware setup\n",
    ">* Ignore these noisy iterations; measure only stabilized performance\n",
    "\n",
    ">* Warmup is like warming a cold car\n",
    ">* Ignore warmup timings; measure only steady performance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52c1547",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Warmup Iterations Explained\n",
    "\n",
    "# This script explains warmup iterations clearly.\n",
    "# It uses TensorFlow to simulate model timing.\n",
    "# Focus is on warmup versus steady state.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Import TensorFlow and numpy libraries.\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Set deterministic random seeds for reproducibility.\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Select device string based on GPU availability.\n",
    "physical_gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "if physical_gpus:\n",
    "    device_name = \"/GPU:0\"\n",
    "else:\n",
    "    device_name = \"/CPU:0\"\n",
    "\n",
    "# Define a small dense model for timing.\n",
    "model = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Input(shape=(128,)),\n",
    "        tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(10, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Build model once by calling on dummy data.\n",
    "dummy_input = tf.zeros((1, 128), dtype=tf.float32)\n",
    "_ = model(dummy_input)\n",
    "\n",
    "\n",
    "# Create a tf.function to mimic compiled graph.\n",
    "@tf.function\n",
    "def compiled_forward(x):\n",
    "    return model(x)\n",
    "\n",
    "# Create a small batch of random input data.\n",
    "batch_size = 64\n",
    "input_data = tf.random.normal((batch_size, 128))\n",
    "\n",
    "# Validate input shape before timing.\n",
    "assert input_data.shape == (batch_size, 128)\n",
    "\n",
    "# Helper function to time several iterations.\n",
    "def time_function(fn, warmup, measured):\n",
    "    times = []\n",
    "    for i in range(warmup + measured):\n",
    "        start = time.perf_counter()\n",
    "        _ = fn(input_data)\n",
    "        end = time.perf_counter()\n",
    "        if i >= warmup:\n",
    "            times.append(end - start)\n",
    "    return times\n",
    "\n",
    "# Choose warmup and measured iteration counts.\n",
    "warmup_iters = 3\n",
    "measured_iters = 5\n",
    "\n",
    "# Time eager model without warmup separation.\n",
    "raw_times = time_function(model, warmup=0, measured=measured_iters)\n",
    "\n",
    "# Time compiled model with explicit warmup.\n",
    "compiled_times = time_function(\n",
    "    compiled_forward,\n",
    "    warmup=warmup_iters,\n",
    "    measured=measured_iters,\n",
    ")\n",
    "\n",
    "# Compute simple averages for both timing lists.\n",
    "raw_avg = float(np.mean(raw_times))\n",
    "compiled_avg = float(np.mean(compiled_times))\n",
    "\n",
    "# Print explanation of warmup configuration.\n",
    "print(\"Device used:\", device_name)\n",
    "print(\"Warmup iterations for compiled:\", warmup_iters)\n",
    "print(\"Measured iterations per run:\", measured_iters)\n",
    "\n",
    "# Show per iteration times for compiled model.\n",
    "print(\"Compiled measured times (seconds):\")\n",
    "for t in compiled_times:\n",
    "    print(round(t, 6))\n",
    "\n",
    "# Print average times to compare steady performance.\n",
    "print(\"Average eager time (seconds):\", round(raw_avg, 6))\n",
    "print(\"Average compiled time (seconds):\", round(compiled_avg, 6))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abdb604",
   "metadata": {},
   "source": [
    "### **3.2. Reliable Timing Methods**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965dc32f",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_07/Lecture_A/image_03_02.jpg?v=1769762065\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Use consistent, repeatable timing with fixed settings\n",
    ">* Standardize conditions to isolate true compilation speedup\n",
    "\n",
    ">* Separate one-time setup from steady-state work\n",
    ">* Time steady-state iterations repeatedly for meaningful averages\n",
    "\n",
    ">* Control randomness and system noise during benchmarking\n",
    ">* Repeat tests, stabilize environment, analyze result distributions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b8f04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Reliable Timing Methods\n",
    "\n",
    "# This script shows reliable timing methods.\n",
    "# We compare simple eager and compiled functions.\n",
    "# Focus is on warmup and steady state timing.\n",
    "\n",
    "# !pip install torch.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Import torch and check availability.\n",
    "import torch\n",
    "\n",
    "# Set deterministic random seeds.\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Select device based on availability.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Print framework version and device.\n",
    "print(\"PyTorch version:\", torch.__version__, \"Device:\", device)\n",
    "\n",
    "# Define a tiny model for timing.\n",
    "class TinyNet(torch.nn.Module):\n",
    "    # Initialize linear layers and activation.\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = torch.nn.Linear(128, 128)\n",
    "        self.act = torch.nn.ReLU()\n",
    "        self.fc2 = torch.nn.Linear(128, 10)\n",
    "\n",
    "    # Define forward computation path.\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Create model instance and move to device.\n",
    "model_eager = TinyNet().to(device)\n",
    "\n",
    "# Create compiled version if available.\n",
    "if hasattr(torch, \"compile\"):\n",
    "    model_compiled = torch.compile(model_eager)\n",
    "else:\n",
    "    model_compiled = model_eager\n",
    "\n",
    "# Put models in evaluation mode.\n",
    "model_eager.eval()\n",
    "model_compiled.eval()\n",
    "\n",
    "# Create fixed input batch for timing.\n",
    "batch_size = 64\n",
    "input_shape = (batch_size, 128)\n",
    "\n",
    "# Build input tensor and move to device.\n",
    "inputs = torch.randn(input_shape, device=device)\n",
    "\n",
    "# Validate input shape before timing.\n",
    "assert inputs.shape == torch.Size(input_shape)\n",
    "\n",
    "# Helper to synchronize device safely.\n",
    "def sync_device():\n",
    "    # Synchronize only when cuda is available.\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "# Helper to time multiple iterations.\n",
    "def time_model(model, n_warmup, n_iters):\n",
    "    # Warmup iterations not included in timing.\n",
    "    with torch.no_grad():\n",
    "        for _ in range(n_warmup):\n",
    "            _ = model(inputs)\n",
    "        sync_device()\n",
    "        start = time.perf_counter()\n",
    "        for _ in range(n_iters):\n",
    "            _ = model(inputs)\n",
    "        sync_device()\n",
    "        end = time.perf_counter()\n",
    "    # Compute average time per iteration.\n",
    "    avg = (end - start) / float(n_iters)\n",
    "    return avg\n",
    "\n",
    "# Define warmup and measured iteration counts.\n",
    "warmup_iters = 5\n",
    "measured_iters = 20\n",
    "\n",
    "# Time eager model multiple runs.\n",
    "eager_times = []\n",
    "for _ in range(3):\n",
    "    t = time_model(model_eager, warmup_iters, measured_iters)\n",
    "    eager_times.append(t)\n",
    "\n",
    "# Time compiled model multiple runs.\n",
    "compiled_times = []\n",
    "for _ in range(3):\n",
    "    t = time_model(model_compiled, warmup_iters, measured_iters)\n",
    "    compiled_times.append(t)\n",
    "\n",
    "# Compute simple statistics helper.\n",
    "def summarize(times):\n",
    "    # Return min, max, and mean values.\n",
    "    mn = min(times)\n",
    "    mx = max(times)\n",
    "    avg = sum(times) / float(len(times))\n",
    "    return mn, mx, avg\n",
    "\n",
    "# Summarize eager and compiled timings.\n",
    "e_min, e_max, e_avg = summarize(eager_times)\n",
    "c_min, c_max, c_avg = summarize(compiled_times)\n",
    "\n",
    "# Print timing summary with few lines.\n",
    "print(\"Eager avg per iter (s):\", round(e_avg, 6))\n",
    "print(\"Eager range (s):\", round(e_min, 6), \"to\", round(e_max, 6))\n",
    "print(\"Compiled avg per iter (s):\", round(c_avg, 6))\n",
    "print(\"Compiled range (s):\", round(c_min, 6), \"to\", round(c_max, 6))\n",
    "\n",
    "# Print relative speedup if compiled is faster.\n",
    "speedup = e_avg / c_avg if c_avg > 0 else 1.0\n",
    "print(\"Estimated speedup factor:\", round(speedup, 3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b81d2ae",
   "metadata": {},
   "source": [
    "### **3.3. Interpreting Speedup Results**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72d8bbc",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_07/Lecture_A/image_03_03.jpg?v=1769762146\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Compare pre‑ and post‑compile times as speedup\n",
    ">* Use steady‑state, post‑warmup timings for decisions\n",
    "\n",
    ">* Raw speedup may not improve full pipeline\n",
    ">* Interpret gains by workload type and resource costs\n",
    "\n",
    ">* Look for patterns, batch effects, and variability\n",
    ">* Decide when compilation helps, hurts, or needs tuning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3064965",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Using torch.compile**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db12ed3d",
   "metadata": {},
   "source": [
    "\n",
    "In this lecture, you learned to:\n",
    "- Describe the purpose and high‑level behavior of torch.compile in PyTorch 2.10.0. \n",
    "- Wrap existing nn.Module models with torch.compile and configure basic compilation options. \n",
    "- Measure and interpret performance changes after compilation, including warmup effects. \n",
    "\n",
    "In the next Lecture (Lecture B), we will go over 'Profiling and Tuning'"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
