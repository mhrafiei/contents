{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20bb18ff",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Execution and Review**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f775b3",
   "metadata": {},
   "source": [
    ">Last update: 20260130.\n",
    "    \n",
    "By the end of this Lecture, you will be able to:\n",
    "- Implement the full training and evaluation pipeline for the capstone project using PyTorch 2.10.0. \n",
    "- Apply at least one performance or scalability technique, such as torch.compile, AMP, or DDP, to the project. \n",
    "- Analyze project outcomes, document lessons learned, and identify areas for future improvement. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93fd004",
   "metadata": {},
   "source": [
    "## **1. Running PyTorch Experiments**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3409fa4e",
   "metadata": {},
   "source": [
    "### **1.1. Running the Training Loop**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4027eac",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_10/Lecture_B/image_01_01.jpg?v=1769784542\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Training loop batches data, predicts, computes loss, backpropagates\n",
    ">* Consistent structure, modes, and randomness ensure reliable learning\n",
    "\n",
    ">* Carefully manage training state, mode, and gradients\n",
    ">* Handle devices, graphs, and errors for stability\n",
    "\n",
    ">* Add advanced controls like clipping, gradient accumulation, logging\n",
    ">* Use a modular loop for reliable, scalable experiments\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5203b2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Running the Training Loop\n",
    "\n",
    "# This script shows a simple PyTorch training loop.\n",
    "# It focuses on running epochs and batches clearly.\n",
    "# Use it to understand each training step precisely.\n",
    "\n",
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "\n",
    "# Import torch and related modules.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Import torchvision for a tiny MNIST subset.\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "# Set deterministic random seeds.\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Detect device for training loop.\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Print PyTorch version and device.\n",
    "print(\"PyTorch\", torch.__version__, \"Device\", DEVICE)\n",
    "\n",
    "# Define a simple transform for MNIST.\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,)),\n",
    "])\n",
    "\n",
    "# Download MNIST training data.\n",
    "train_dataset_full = datasets.MNIST(\n",
    "    root=\"./data\", train=True, download=True, transform=transform\n",
    ")\n",
    "\n",
    "# Use a small subset for quick training.\n",
    "subset_size = 512\n",
    "indices = list(range(subset_size))\n",
    "train_dataset = torch.utils.data.Subset(train_dataset_full, indices)\n",
    "\n",
    "# Create a data loader with small batch size.\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=64, shuffle=True\n",
    ")\n",
    "\n",
    "# Define a simple fully connected classifier.\n",
    "class SimpleMLP(nn.Module):\n",
    "    # Initialize layers for the network.\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(28 * 28, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    # Define forward pass for inputs.\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Create model and move to device.\n",
    "model = SimpleMLP().to(DEVICE)\n",
    "\n",
    "# Define loss function and optimizer.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# Optionally compile model for performance.\n",
    "if hasattr(torch, \"compile\"):\n",
    "    model = torch.compile(model)\n",
    "\n",
    "# Define a single training epoch function.\n",
    "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
    "    # Set model to training mode.\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    total_batches = 0\n",
    "\n",
    "    # Iterate over data batches.\n",
    "    for batch_idx, (inputs, targets) in enumerate(loader):\n",
    "        # Move data to the selected device.\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # Validate input and target shapes.\n",
    "        assert inputs.ndim == 4\n",
    "        assert targets.ndim == 1\n",
    "\n",
    "        # Zero gradients from previous step.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass to get predictions.\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Check output shape matches targets.\n",
    "        assert outputs.shape[0] == targets.shape[0]\n",
    "\n",
    "        # Compute loss for this batch.\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backward pass to compute gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        # Update model parameters.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate loss for reporting.\n",
    "        running_loss += loss.item()\n",
    "        total_batches += 1\n",
    "\n",
    "    # Compute average loss for the epoch.\n",
    "    avg_loss = running_loss / max(total_batches, 1)\n",
    "    return avg_loss\n",
    "\n",
    "# Run a small number of epochs.\n",
    "num_epochs = 3\n",
    "\n",
    "# Training loop over epochs.\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    # Train for one full epoch.\n",
    "    avg_loss = train_one_epoch(\n",
    "        model, train_loader, optimizer, criterion, DEVICE\n",
    "    )\n",
    "\n",
    "    # Print concise epoch summary.\n",
    "    print(f\"Epoch {epoch} average training loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Confirm script finished successfully.\n",
    "print(\"Training loop completed without errors.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cb2b66",
   "metadata": {},
   "source": [
    "### **1.2. Experiment Metrics Logging**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a470c42",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_10/Lecture_B/image_01_02.jpg?v=1769784632\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Log training and validation metrics every epoch\n",
    ">* Use consistent logging to track learning and reproducibility\n",
    "\n",
    ">* Combine live feedback, saved logs, and visualizations\n",
    ">* Use plots to compare settings and performance tradeoffs\n",
    "\n",
    ">* Log task-specific metrics that match real goals\n",
    ">* Store rich metadata to enable reproducible experiments\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f2ac37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Experiment Metrics Logging\n",
    "\n",
    "# This script shows simple experiment metrics logging.\n",
    "# It uses TensorFlow to simulate a training experiment.\n",
    "# Focus on recording metrics and visualizing learning progress.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import standard libraries for math and plotting.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import TensorFlow and check version.\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set deterministic random seeds for reproducibility.\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# Detect device type for information only.\n",
    "physical_gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "DEVICE = \"GPU\" if physical_gpus else \"CPU\"\n",
    "\n",
    "# Print framework version and device summary.\n",
    "print(\"TensorFlow version:\", tf.__version__, \"Device:\", DEVICE)\n",
    "\n",
    "# Load a small subset of MNIST digits dataset.\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Reduce dataset size to keep runtime very small.\n",
    "train_samples = 2000\n",
    "test_samples = 500\n",
    "x_train = x_train[:train_samples]\n",
    "y_train = y_train[:train_samples]\n",
    "\n",
    "# Slice test data for quick evaluation.\n",
    "x_test = x_test[:test_samples]\n",
    "y_test = y_test[:test_samples]\n",
    "\n",
    "# Normalize images to range zero to one.\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_test = x_test.astype(\"float32\") / 255.0\n",
    "\n",
    "# Add channel dimension for convolutional layers.\n",
    "x_train = np.expand_dims(x_train, axis=-1)\n",
    "x_test = np.expand_dims(x_test, axis=-1)\n",
    "\n",
    "# Validate shapes before building the model.\n",
    "assert x_train.shape[0] == train_samples\n",
    "assert x_test.shape[0] == test_samples\n",
    "\n",
    "# Build a tiny convolutional classification model.\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(\n",
    "        8,\n",
    "        (3, 3),\n",
    "        activation=\"relu\",\n",
    "        input_shape=(28, 28, 1),\n",
    "    ),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(32, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "# Compile model with optimizer, loss, and accuracy metric.\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Prepare a simple dictionary for metrics logging.\n",
    "metrics_log = {\n",
    "    \"epoch\": [],\n",
    "    \"train_loss\": [],\n",
    "    \"train_accuracy\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"val_accuracy\": [],\n",
    "}\n",
    "\n",
    "\n",
    "# Define a custom callback to capture metrics each epoch.\n",
    "class SimpleMetricsLogger(tf.keras.callbacks.Callback):\n",
    "    # Initialize callback with external log dictionary.\n",
    "    def __init__(self, log_dict):\n",
    "        super().__init__()\n",
    "        self.log_dict = log_dict\n",
    "\n",
    "    # At epoch end, store metrics into the dictionary.\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        self.log_dict[\"epoch\"].append(epoch + 1)\n",
    "        self.log_dict[\"train_loss\"].append(float(logs.get(\"loss\", 0.0)))\n",
    "        self.log_dict[\"train_accuracy\"].append(\n",
    "            float(logs.get(\"accuracy\", 0.0))\n",
    "        )\n",
    "        self.log_dict[\"val_loss\"].append(float(logs.get(\"val_loss\", 0.0)))\n",
    "        self.log_dict[\"val_accuracy\"].append(\n",
    "            float(logs.get(\"val_accuracy\", 0.0))\n",
    "        )\n",
    "\n",
    "\n",
    "# Create callback instance for training.\n",
    "logger_callback = SimpleMetricsLogger(metrics_log)\n",
    "\n",
    "# Train the model quietly while logging metrics.\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=5,\n",
    "    batch_size=64,\n",
    "    validation_data=(x_test, y_test),\n",
    "    verbose=0,\n",
    "    callbacks=[logger_callback],\n",
    ")\n",
    "\n",
    "# Convert metrics log to a small structured array.\n",
    "logged_epochs = np.array(metrics_log[\"epoch\"], dtype=int)\n",
    "train_loss = np.array(metrics_log[\"train_loss\"], dtype=float)\n",
    "\n",
    "# Print a compact summary of logged metrics.\n",
    "print(\"Logged epochs:\", logged_epochs.tolist())\n",
    "print(\"Train loss per epoch:\", np.round(train_loss, 3).tolist())\n",
    "\n",
    "# Plot training and validation accuracy curves.\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(metrics_log[\"epoch\"], metrics_log[\"train_accuracy\"], label=\"train_acc\")\n",
    "plt.plot(metrics_log[\"epoch\"], metrics_log[\"val_accuracy\"], label=\"val_acc\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Experiment metrics logging example\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75da2e48",
   "metadata": {},
   "source": [
    "### **1.3. Managing checkpoints**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d0f0bd",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_10/Lecture_B/image_01_03.jpg?v=1769784719\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Checkpoints save model and training state snapshots\n",
    ">* They prevent progress loss and enable flexible experimentation\n",
    "\n",
    ">* Save weights, optimizer, and best model regularly\n",
    ">* Include metadata and clear filenames for reproducibility\n",
    "\n",
    ">* Plan retention, pruning, and safe checkpoint saving\n",
    ">* Regularly test restoring checkpoints to ensure reliability\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5aa278b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Managing checkpoints\n",
    "\n",
    "# This script shows simple PyTorch checkpointing.\n",
    "# It focuses on saving and loading safely.\n",
    "# Use it as a template for experiments.\n",
    "\n",
    "# !pip install torch torchvision.\n",
    "\n",
    "# Import standard libraries for paths and randomness.\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "\n",
    "# Import torch and basic neural network tools.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Set a deterministic random seed value.\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "\n",
    "# Set seeds for torch and CUDA if available.\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Select device based on GPU availability.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Print the torch version and selected device.\n",
    "print(\"torch version:\", torch.__version__, \"device:\", device)\n",
    "\n",
    "# Define a tiny feedforward model for demonstration.\n",
    "class TinyNet(nn.Module):\n",
    "    # Initialize layers with small hidden size.\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, num_classes),\n",
    "        )\n",
    "\n",
    "    # Define forward pass through the network.\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n",
    "\n",
    "# Create a tiny synthetic classification dataset.\n",
    "def make_tiny_dataset(num_samples: int, input_dim: int, num_classes: int):\n",
    "    # Create random features with normal distribution.\n",
    "    x = torch.randn(num_samples, input_dim)\n",
    "    # Create random integer labels for classes.\n",
    "    y = torch.randint(0, num_classes, (num_samples,))\n",
    "    return x, y\n",
    "\n",
    "# Simple training step for one small epoch.\n",
    "def train_one_epoch(model, optimizer, criterion, x, y):\n",
    "    # Set model to training mode for updates.\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    # Forward pass and loss computation.\n",
    "    logits = model(x)\n",
    "    loss = criterion(logits, y)\n",
    "    # Backward pass and parameter update.\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "# Simple evaluation step without gradient tracking.\n",
    "def evaluate(model, criterion, x, y):\n",
    "    # Set model to evaluation mode for metrics.\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        # Compute accuracy over tiny dataset.\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        correct = (preds == y).sum().item()\n",
    "        acc = correct / max(1, y.numel())\n",
    "    return loss.item(), acc\n",
    "\n",
    "# Helper to build a checkpoint dictionary.\n",
    "def build_checkpoint(epoch, model, optimizer, best_val_loss, config):\n",
    "    # Store model and optimizer state dictionaries.\n",
    "    return {\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"optimizer_state\": optimizer.state_dict(),\n",
    "        \"best_val_loss\": best_val_loss,\n",
    "        \"config\": config,\n",
    "    }\n",
    "\n",
    "# Helper to save checkpoint safely to disk.\n",
    "def save_checkpoint(state, is_best, checkpoint_dir):\n",
    "    # Ensure checkpoint directory exists on disk.\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    # Define main checkpoint file path.\n",
    "    ckpt_path = os.path.join(checkpoint_dir, \"last_checkpoint.pt\")\n",
    "    # Save last checkpoint with torch save.\n",
    "    torch.save(state, ckpt_path)\n",
    "    # Optionally save separate best checkpoint file.\n",
    "    if is_best:\n",
    "        best_path = os.path.join(checkpoint_dir, \"best_checkpoint.pt\")\n",
    "        torch.save(state, best_path)\n",
    "\n",
    "# Helper to load checkpoint if it exists.\n",
    "def load_checkpoint_if_available(model, optimizer, checkpoint_dir):\n",
    "    # Define checkpoint path for last checkpoint.\n",
    "    ckpt_path = os.path.join(checkpoint_dir, \"last_checkpoint.pt\")\n",
    "    if not os.path.exists(ckpt_path):\n",
    "        # Return defaults when no checkpoint exists.\n",
    "        return 0, math.inf\n",
    "    # Load checkpoint to CPU for safety.\n",
    "    checkpoint = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "    # Restore model and optimizer states.\n",
    "    model.load_state_dict(checkpoint[\"model_state\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n",
    "    # Return starting epoch and best validation loss.\n",
    "    return int(checkpoint[\"epoch\"]) + 1, float(checkpoint[\"best_val_loss\"])\n",
    "\n",
    "# Main function to run tiny experiment.\n",
    "def main():\n",
    "    # Define simple configuration dictionary.\n",
    "    config = {\n",
    "        \"input_dim\": 8,\n",
    "        \"hidden_dim\": 16,\n",
    "        \"num_classes\": 3,\n",
    "        \"num_epochs\": 4,\n",
    "        \"lr\": 0.01,\n",
    "        \"checkpoint_dir\": \"checkpoints_demo\",\n",
    "    }\n",
    "\n",
    "    # Create tiny dataset and move to device.\n",
    "    x_train, y_train = make_tiny_dataset(64, config[\"input_dim\"], config[\"num_classes\"])\n",
    "    x_val, y_val = make_tiny_dataset(32, config[\"input_dim\"], config[\"num_classes\"])\n",
    "\n",
    "    # Validate shapes before training loop.\n",
    "    assert x_train.shape[1] == config[\"input_dim\"]\n",
    "    assert x_val.shape[1] == config[\"input_dim\"]\n",
    "\n",
    "    # Move tensors to selected device.\n",
    "    x_train_device = x_train.to(device)\n",
    "    y_train_device = y_train.to(device)\n",
    "    x_val_device = x_val.to(device)\n",
    "    y_val_device = y_val.to(device)\n",
    "\n",
    "    # Initialize model, loss function, and optimizer.\n",
    "    model = TinyNet(config[\"input_dim\"], config[\"hidden_dim\"], config[\"num_classes\"]).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=config[\"lr\"])\n",
    "\n",
    "    # Try to resume from existing checkpoint.\n",
    "    start_epoch, best_val_loss = load_checkpoint_if_available(\n",
    "        model, optimizer, config[\"checkpoint_dir\"]\n",
    "    )\n",
    "\n",
    "    # Print basic resume information for clarity.\n",
    "    print(\"Starting epoch:\", start_epoch, \"best_val_loss:\", round(best_val_loss, 4))\n",
    "\n",
    "    # Run a short training loop with checkpointing.\n",
    "    for epoch in range(start_epoch, config[\"num_epochs\"]):\n",
    "        train_loss = train_one_epoch(\n",
    "            model, optimizer, criterion, x_train_device, y_train_device\n",
    "        )\n",
    "        val_loss, val_acc = evaluate(\n",
    "            model, criterion, x_val_device, y_val_device\n",
    "        )\n",
    "        # Decide if this is the best validation loss.\n",
    "        is_best = val_loss < best_val_loss\n",
    "        if is_best:\n",
    "            best_val_loss = val_loss\n",
    "        # Build checkpoint state dictionary.\n",
    "        state = build_checkpoint(epoch, model, optimizer, best_val_loss, config)\n",
    "        # Save last and maybe best checkpoint.\n",
    "        save_checkpoint(state, is_best, config[\"checkpoint_dir\"])\n",
    "        # Print compact progress information.\n",
    "        print(\n",
    "            f\"Epoch {epoch} train_loss={train_loss:.3f} val_loss={val_loss:.3f} val_acc={val_acc:.2f}\"\n",
    "        )\n",
    "\n",
    "    # Load best checkpoint to evaluate final performance.\n",
    "    best_path = os.path.join(config[\"checkpoint_dir\"], \"best_checkpoint.pt\")\n",
    "    if os.path.exists(best_path):\n",
    "        best_state = torch.load(best_path, map_location=device)\n",
    "        model.load_state_dict(best_state[\"model_state\"])\n",
    "        # Evaluate best model on validation data.\n",
    "        best_loss, best_acc = evaluate(\n",
    "            model, criterion, x_val_device, y_val_device\n",
    "        )\n",
    "        print(\"Best checkpoint val_loss=\", round(best_loss, 3), \"val_acc=\", round(best_acc, 2))\n",
    "    else:\n",
    "        # Inform user when no best checkpoint exists.\n",
    "        print(\"No best checkpoint found, used last model only.\")\n",
    "\n",
    "# Run the main function when script executes.\n",
    "main()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d770fb",
   "metadata": {},
   "source": [
    "## **2. Performance Optimizations in PyTorch**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cff62f2",
   "metadata": {},
   "source": [
    "### **2.1. Adding torch compile**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0441fd3d",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_10/Lecture_B/image_02_01.jpg?v=1769784840\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Compilation analyzes your model and optimizes execution\n",
    ">* Training runs faster without changing most code\n",
    "\n",
    ">* Wrap the model with compile, change little\n",
    ">* Start safe, then try faster compile settings\n",
    "\n",
    ">* Compilation has warm-up cost but speeds training\n",
    ">* Refactor dynamic models so compiler optimizes effectively\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7740a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Adding torch compile\n",
    "\n",
    "# This script shows how torch compile speeds training.\n",
    "# We compare eager and compiled models on tiny data.\n",
    "# Focus on clear structure and minimal printed output.\n",
    "\n",
    "# Install PyTorch if needed for local environments.\n",
    "# !pip install torch torchvision torchaudio --quiet.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "# Import torch and related modules.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "\n",
    "# Set torch manual seed for cpu and cuda.\n",
    "torch.manual_seed(seed_value)\n",
    "\n",
    "# Detect device preferring cuda when available.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Print torch version and selected device.\n",
    "print(\"Torch version:\", torch.__version__, \"Device:\", device)\n",
    "\n",
    "# Define a tiny feedforward model for classification.\n",
    "class TinyNet(nn.Module):\n",
    "    # Initialize layers with small hidden size.\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, num_classes),\n",
    "        )\n",
    "\n",
    "    # Define forward pass using sequential network.\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n",
    "\n",
    "# Create a tiny synthetic classification dataset.\n",
    "def make_tiny_dataset(num_samples: int, input_dim: int, num_classes: int):\n",
    "    # Create random features with normal distribution.\n",
    "    x = torch.randn(num_samples, input_dim)\n",
    "    # Create random integer labels for classes.\n",
    "    y = torch.randint(0, num_classes, (num_samples,))\n",
    "    # Validate shapes before returning tensors.\n",
    "    assert x.shape == (num_samples, input_dim)\n",
    "    assert y.shape == (num_samples,)\n",
    "    return x, y\n",
    "\n",
    "# Simple training loop returning final loss value.\n",
    "def train_one_model(model: nn.Module, data, labels, epochs: int = 3):\n",
    "    # Move model and data to selected device.\n",
    "    model = model.to(device)\n",
    "    data = data.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    # Define optimizer and loss function.\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Track loss for final reporting.\n",
    "    final_loss = None\n",
    "\n",
    "    # Loop over epochs with small number.\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        final_loss = loss.item()\n",
    "\n",
    "    # Return last epoch loss value.\n",
    "    return final_loss\n",
    "\n",
    "# Helper to time training for a given model.\n",
    "def time_training(model: nn.Module, data, labels, label: str):\n",
    "    # Synchronize cuda before timing if available.\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    start = time.time()\n",
    "\n",
    "    # Train model and capture final loss.\n",
    "    final_loss = train_one_model(model, data, labels, epochs=5)\n",
    "\n",
    "    # Synchronize cuda again after training.\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    duration = time.time() - start\n",
    "\n",
    "    # Print short summary line for this run.\n",
    "    print(label, \"time: {:.4f}s loss: {:.4f}\".format(duration, final_loss))\n",
    "\n",
    "# Main execution block for eager and compiled runs.\n",
    "def main():\n",
    "    # Define tiny problem dimensions.\n",
    "    input_dim = 32\n",
    "    hidden_dim = 64\n",
    "    num_classes = 4\n",
    "\n",
    "    # Build small synthetic dataset.\n",
    "    x, y = make_tiny_dataset(num_samples=512, input_dim=input_dim, num_classes=num_classes)\n",
    "\n",
    "    # Create eager model instance.\n",
    "    eager_model = TinyNet(input_dim=input_dim, hidden_dim=hidden_dim, num_classes=num_classes)\n",
    "\n",
    "    # Time training for eager model.\n",
    "    time_training(eager_model, x, y, label=\"Eager model\")\n",
    "\n",
    "    # Create fresh model for compiled run.\n",
    "    compiled_model = TinyNet(input_dim=input_dim, hidden_dim=hidden_dim, num_classes=num_classes)\n",
    "\n",
    "    # Wrap model with torch compile using default mode.\n",
    "    compiled_model = torch.compile(compiled_model)\n",
    "\n",
    "    # Time training for compiled model.\n",
    "    time_training(compiled_model, x, y, label=\"Compiled model\")\n",
    "\n",
    "# Run main when script is executed.\n",
    "main()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe93c266",
   "metadata": {},
   "source": [
    "### **2.2. Using AMP or DDP**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317b41be",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_10/Lecture_B/image_02_02.jpg?v=1769784921\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* AMP uses lower precision to boost GPU efficiency\n",
    ">* Train faster, larger models without extra hardware\n",
    "\n",
    ">* DDP trains across multiple GPUs in parallel\n",
    ">* Faster runs enable more experiments within deadlines\n",
    "\n",
    ">* Pick AMP for memory or single-GPU limits\n",
    ">* Use DDP or both after profiling bottlenecks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4826ea7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Using AMP or DDP\n",
    "\n",
    "# This script shows basic PyTorch AMP usage.\n",
    "# It trains a tiny model on random data.\n",
    "# Focus is on clear beginner friendly structure.\n",
    "\n",
    "# !pip install torch torchvision torchaudio.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "\n",
    "# Import torch and related modules.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Set deterministic random seeds.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "\n",
    "# Set seeds for torch randomness.\n",
    "torch.manual_seed(seed_value)\n",
    "\n",
    "# Select device based on availability.\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "# Print PyTorch version and device.\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "\n",
    "# Define a tiny feedforward network.\n",
    "class TinyNet(nn.Module):\n",
    "    # Initialize layers for the network.\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "        )\n",
    "\n",
    "    # Define forward computation step.\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Set simple problem dimensions.\n",
    "input_dim = 20\n",
    "hidden_dim = 32\n",
    "\n",
    "# Set number of output classes.\n",
    "output_dim = 3\n",
    "\n",
    "# Create random training data tensor.\n",
    "num_samples = 256\n",
    "\n",
    "# Generate random features and labels.\n",
    "features = torch.randn(num_samples, input_dim)\n",
    "\n",
    "# Create random integer class labels.\n",
    "labels = torch.randint(0, output_dim, (num_samples,))\n",
    "\n",
    "# Move data to selected device.\n",
    "features = features.to(device)\n",
    "labels = labels.to(device)\n",
    "\n",
    "# Validate shapes before training.\n",
    "assert features.shape == (num_samples, input_dim)\n",
    "\n",
    "# Validate labels shape and type.\n",
    "assert labels.shape == (num_samples,)\n",
    "\n",
    "# Create model and move to device.\n",
    "model = TinyNet(input_dim, hidden_dim, output_dim).to(device)\n",
    "\n",
    "# Define loss function and optimizer.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Create gradient scaler for AMP.\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=use_cuda)\n",
    "\n",
    "# Set training hyperparameters.\n",
    "batch_size = 32\n",
    "epochs = 3\n",
    "\n",
    "# Compute number of batches per epoch.\n",
    "num_batches = math.ceil(num_samples / batch_size)\n",
    "\n",
    "# Training loop with AMP enabled.\n",
    "for epoch in range(epochs):\n",
    "    # Set model to training mode.\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    # Iterate over mini batches.\n",
    "    for batch_idx in range(num_batches):\n",
    "        start = batch_idx * batch_size\n",
    "        end = start + batch_size\n",
    "\n",
    "        # Slice current batch safely.\n",
    "        batch_x = features[start:end]\n",
    "        batch_y = labels[start:end]\n",
    "\n",
    "        # Ensure batch is non empty.\n",
    "        if batch_x.size(0) == 0:\n",
    "            continue\n",
    "\n",
    "        # Zero gradients before backward.\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # Forward pass under autocast.\n",
    "        with torch.cuda.amp.autocast(enabled=use_cuda):\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "\n",
    "        # Scale loss and backpropagate.\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # Step optimizer through scaler.\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # Accumulate loss value.\n",
    "        epoch_loss += loss.item() * batch_x.size(0)\n",
    "\n",
    "    # Compute average epoch loss.\n",
    "    avg_loss = epoch_loss / num_samples\n",
    "\n",
    "    # Print compact training progress.\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, loss {avg_loss:.4f}\")\n",
    "\n",
    "# Switch model to evaluation mode.\n",
    "model.eval()\n",
    "\n",
    "# Disable gradients for evaluation.\n",
    "with torch.no_grad():\n",
    "    # Use autocast also during evaluation.\n",
    "    with torch.cuda.amp.autocast(enabled=use_cuda):\n",
    "        logits = model(features)\n",
    "\n",
    "# Compute predicted classes.\n",
    "preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "# Calculate accuracy on training data.\n",
    "correct = (preds == labels).sum().item()\n",
    "\n",
    "# Compute accuracy fraction safely.\n",
    "accuracy = correct / num_samples\n",
    "\n",
    "# Print final accuracy summary.\n",
    "print(f\"Final training accuracy with AMP: {accuracy:.2%}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a069c814",
   "metadata": {},
   "source": [
    "### **2.3. Evaluating Optimization Results**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9969899d",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_10/Lecture_B/image_02_03.jpg?v=1769784994\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Define what performance improvement means for you\n",
    ">* Measure baseline metrics to compare future optimizations\n",
    "\n",
    ">* Compare runs with identical settings and metrics\n",
    ">* Weigh speed gains against accuracy and usefulness\n",
    "\n",
    ">* Check stability, consistency, and numerical issues carefully\n",
    ">* Balance speed gains with deployment complexity and workflow\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b87c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Evaluating Optimization Results\n",
    "\n",
    "# This script compares baseline and optimized training performance.\n",
    "# It uses a tiny synthetic dataset for speed and clarity.\n",
    "# We focus on timing and accuracy for simple evaluation.\n",
    "\n",
    "# !pip install torch torchvision.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Import torch and related modules.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "torch.manual_seed(seed_value)\n",
    "\n",
    "# Detect device and print PyTorch version.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "\n",
    "# Create a tiny synthetic classification dataset.\n",
    "num_samples = 512\n",
    "num_features = 20\n",
    "num_classes = 3\n",
    "\n",
    "# Generate random features and integer labels.\n",
    "X = torch.randn(num_samples, num_features)\n",
    "y = torch.randint(0, num_classes, (num_samples,))\n",
    "\n",
    "# Split into train and validation subsets.\n",
    "train_size = int(0.8 * num_samples)\n",
    "val_size = num_samples - train_size\n",
    "\n",
    "# Use torch.utils.data random split for subsets.\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "    list(zip(X, y)), [train_size, val_size]\n",
    ")\n",
    "\n",
    "# Define a simple feedforward neural network.\n",
    "class TinyNet(nn.Module):\n",
    "\n",
    "    # Initialize layers with small hidden size.\n",
    "    def __init__(self, in_features, hidden_size, num_classes):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_features, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, num_classes),\n",
    "        )\n",
    "\n",
    "    # Forward pass through the network.\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Create data loaders with small batch size.\n",
    "batch_size = 64\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=batch_size, shuffle=False\n",
    ")\n",
    "\n",
    "# Utility function to run one training epoch.\n",
    "def train_one_epoch(model, loader, optimizer, criterion, scaler=None):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for inputs, targets in loader:\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        if scaler is None:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        else:\n",
    "            with torch.autocast(device_type=device.type, dtype=torch.float16):\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        total_loss += loss.item() * inputs.size(0)\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "# Utility function to evaluate accuracy on validation set.\n",
    "@torch.no_grad()\n",
    "def evaluate_accuracy(model, loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, targets in loader:\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        correct += (preds == targets).sum().item()\n",
    "        total += targets.size(0)\n",
    "    if total == 0:\n",
    "        return 0.0\n",
    "    return correct / total\n",
    "\n",
    "# Helper to run a short experiment and collect metrics.\n",
    "def run_experiment(use_amp=False, use_compile=False):\n",
    "    model = TinyNet(num_features, 32, num_classes).to(device)\n",
    "    if use_compile and hasattr(torch, \"compile\"):\n",
    "        model = torch.compile(model)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "    scaler = torch.cuda.amp.GradScaler() if use_amp and device.type == \"cuda\" else None\n",
    "    epochs = 3\n",
    "    start_time = time.time()\n",
    "    for _ in range(epochs):\n",
    "        train_one_epoch(model, train_loader, optimizer, criterion, scaler)\n",
    "    total_time = time.time() - start_time\n",
    "    val_acc = evaluate_accuracy(model, val_loader)\n",
    "    return total_time, val_acc\n",
    "\n",
    "# Run baseline experiment without optimizations.\n",
    "baseline_time, baseline_acc = run_experiment(\n",
    "    use_amp=False, use_compile=False\n",
    ")\n",
    "\n",
    "# Run optimized experiment with AMP if possible.\n",
    "use_amp_flag = device.type == \"cuda\"\n",
    "opt_time, opt_acc = run_experiment(\n",
    "    use_amp=use_amp_flag, use_compile=False\n",
    ")\n",
    "\n",
    "# Compute relative speedup and accuracy change.\n",
    "speedup = baseline_time / opt_time if opt_time > 0 else 1.0\n",
    "acc_diff = opt_acc - baseline_acc\n",
    "\n",
    "# Print concise summary of both runs.\n",
    "print(\"Device used:\", device)\n",
    "print(\"Baseline time (s):\", round(baseline_time, 4))\n",
    "print(\"Baseline accuracy:\", round(baseline_acc, 4))\n",
    "print(\"Optimized time (s):\", round(opt_time, 4))\n",
    "print(\"Optimized accuracy:\", round(opt_acc, 4))\n",
    "print(\"Speedup factor:\", round(speedup, 3))\n",
    "print(\"Accuracy difference:\", round(acc_diff, 4))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817a41a8",
   "metadata": {},
   "source": [
    "## **3. Reflect Review Improve**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d3467a",
   "metadata": {},
   "source": [
    "### **3.1. Capstone Results Summary**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120d8003",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_10/Lecture_B/image_03_01.jpg?v=1769785081\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Describe model goal, audience, setup, and metrics\n",
    ">* Tell a concise story of performance and limitations\n",
    "\n",
    ">* Compare performance across data slices and subgroups\n",
    ">* Note fairness issues, surprises, and inconsistent behaviors\n",
    "\n",
    ">* Link metrics to real-world use and constraints\n",
    ">* Summarize strengths, limits, and best future improvements\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002f3a9d",
   "metadata": {},
   "source": [
    "### **3.2. Common PyTorch Pitfalls**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926451fd",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_10/Lecture_B/image_03_02.jpg?v=1769785097\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Switch train/eval modes to avoid distorted metrics\n",
    ">* Manage gradient tracking carefully to preserve computation graphs\n",
    "\n",
    ">* Unoptimized data pipelines cause bottlenecks and instability\n",
    ">* Poor device placement and memory choices hurt performance\n",
    "\n",
    ">* Avoid data leakage, overused validation, uncontrolled randomness\n",
    ">* Ensure clean splits, documentation, and reproducible experiments\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6132e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Common PyTorch Pitfalls\n",
    "\n",
    "# This script shows common PyTorch beginner pitfalls.\n",
    "# We compare wrong and correct training evaluation patterns.\n",
    "# Focus on modes devices gradients and evaluation mistakes.\n",
    "\n",
    "# !pip install torch torchvision.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "\n",
    "# Import torch and torchvision if available.\n",
    "try:\n",
    "    import torch\n",
    "    from torch import nn\n",
    "    from torch.utils.data import DataLoader\n",
    "    from torchvision import datasets\n",
    "    from torchvision import transforms\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"PyTorch and torchvision are required here\") from e\n",
    "\n",
    "# Set deterministic random seeds for reproducibility.\n",
    "random.seed(0)\n",
    "\n",
    "# Set torch manual seed for reproducibility.\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Select device safely with fallback to cpu.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Print framework version and selected device.\n",
    "print(\"Torch\", torch.__version__, \"Device\", device)\n",
    "\n",
    "# Define a tiny neural network with dropout layer.\n",
    "class TinyNet(nn.Module):\n",
    "    # Initialize layers including dropout and linear.\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(28 * 28, 64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.drop = nn.Dropout(p=0.5)\n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    "\n",
    "    # Define forward pass using dropout and relu.\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Prepare a simple transform to tensor only.\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# Download a tiny MNIST training subset.\n",
    "train_dataset = datasets.MNIST(\n",
    "    root=\"./data\", train=True, download=True, transform=transform\n",
    ")\n",
    "\n",
    "# Download a tiny MNIST test subset.\n",
    "test_dataset = datasets.MNIST(\n",
    "    root=\"./data\", train=False, download=True, transform=transform\n",
    ")\n",
    "\n",
    "# Use only a small subset for speed.\n",
    "small_train_size = 256\n",
    "\n",
    "# Use only a small subset for evaluation.\n",
    "small_test_size = 256\n",
    "\n",
    "# Create subset indices safely within dataset length.\n",
    "train_indices = list(range(min(small_train_size, len(train_dataset))))\n",
    "\n",
    "# Create subset indices for test dataset.\n",
    "test_indices = list(range(min(small_test_size, len(test_dataset))))\n",
    "\n",
    "# Wrap subsets with torch subset utility.\n",
    "train_subset = torch.utils.data.Subset(train_dataset, train_indices)\n",
    "\n",
    "# Wrap test subset similarly for evaluation.\n",
    "test_subset = torch.utils.data.Subset(test_dataset, test_indices)\n",
    "\n",
    "# Create data loaders with small batch size.\n",
    "train_loader = DataLoader(train_subset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Create test loader without shuffle for evaluation.\n",
    "test_loader = DataLoader(test_subset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Helper function to compute accuracy on loader.\n",
    "def evaluate_model(model, loader, use_eval_mode, use_no_grad):\n",
    "    # Optionally set evaluation or training mode.\n",
    "    if use_eval_mode:\n",
    "        model.eval()\n",
    "    else:\n",
    "        model.train()\n",
    "\n",
    "    # Optionally disable gradient tracking.\n",
    "    context = torch.no_grad() if use_no_grad else torch.enable_grad()\n",
    "\n",
    "    # Initialize counters for accuracy computation.\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # Use chosen context manager for evaluation.\n",
    "    with context:\n",
    "        for images, labels in loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    # Avoid division by zero with safe check.\n",
    "    if total == 0:\n",
    "        return 0.0\n",
    "\n",
    "    # Return accuracy as float percentage.\n",
    "    return 100.0 * correct / float(total)\n",
    "\n",
    "# Create model instance and move to device.\n",
    "model = TinyNet().to(device)\n",
    "\n",
    "# Define loss function and optimizer.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Use simple stochastic gradient descent optimizer.\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# Train for one short epoch to get non random weights.\n",
    "model.train()\n",
    "\n",
    "# Iterate over one epoch only for speed.\n",
    "for images, labels in train_loader:\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(images)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Demonstrate bad practice evaluation with train mode.\n",
    "bad_acc = evaluate_model(\n",
    "    model=model, loader=test_loader, use_eval_mode=False, use_no_grad=False\n",
    ")\n",
    "\n",
    "# Demonstrate good practice evaluation with eval mode.\n",
    "good_acc = evaluate_model(\n",
    "    model=model, loader=test_loader, use_eval_mode=True, use_no_grad=True\n",
    ")\n",
    "\n",
    "# Show both accuracies to highlight dropout effect.\n",
    "print(\"Bad evaluation accuracy with train mode:\", round(bad_acc, 2))\n",
    "\n",
    "# Show correct evaluation accuracy with eval and no_grad.\n",
    "print(\"Good evaluation accuracy with eval mode:\", round(good_acc, 2))\n",
    "\n",
    "# Intentionally create device mismatch then fix it.\n",
    "cpu_tensor = torch.ones(2, 2)\n",
    "\n",
    "# Move another tensor to selected device.\n",
    "device_tensor = torch.ones(2, 2, device=device)\n",
    "\n",
    "# Safely check devices before risky operation.\n",
    "if cpu_tensor.device != device_tensor.device:\n",
    "    print(\"Warning device mismatch detected and avoided.\")\n",
    "\n",
    "# Move cpu tensor to correct device before addition.\n",
    "fixed_tensor = cpu_tensor.to(device) + device_tensor\n",
    "\n",
    "# Print small tensor to confirm successful safe addition.\n",
    "print(\"Safe addition result shape:\", tuple(fixed_tensor.shape))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5036311",
   "metadata": {},
   "source": [
    "### **3.3. Next Steps with PyTorch**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b21fd5",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_10/Lecture_B/image_03_03.jpg?v=1769785188\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Turn your capstone into an evolving project\n",
    ">* Plan concrete growth directions tied to real use\n",
    "\n",
    ">* Explore new deployment options and advanced training methods\n",
    ">* Run focused experiments to understand model behavior deeply\n",
    "\n",
    ">* Embed projects in real-world, collaborative contexts\n",
    ">* Use new constraints to iteratively refine systems\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f6bc94",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Execution and Review**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1df075",
   "metadata": {},
   "source": [
    "\n",
    "In this lecture, you learned to:\n",
    "- Implement the full training and evaluation pipeline for the capstone project using PyTorch 2.10.0. \n",
    "- Apply at least one performance or scalability technique, such as torch.compile, AMP, or DDP, to the project. \n",
    "- Analyze project outcomes, document lessons learned, and identify areas for future improvement. \n",
    "\n",
    "<font color='yellow'>Congratulations on completing this course!</font>"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
