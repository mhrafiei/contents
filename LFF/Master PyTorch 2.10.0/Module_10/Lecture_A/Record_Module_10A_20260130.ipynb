{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f42a700",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Project Design**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee9534b",
   "metadata": {},
   "source": [
    ">Last update: 20260130.\n",
    "    \n",
    "By the end of this Lecture, you will be able to:\n",
    "- Define a concrete project goal, dataset choice, and evaluation metrics for a PyTorch capstone. \n",
    "- Select an appropriate model architecture and training strategy aligned with project constraints. \n",
    "- Plan an experiment schedule that includes baselines, ablations, and potential optimizations. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8244fd4",
   "metadata": {},
   "source": [
    "## **1. Scoping PyTorch Projects**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aae2b75",
   "metadata": {},
   "source": [
    "### **1.1. Selecting a Project Domain**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef3e16b",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_10/Lecture_A/image_01_01.jpg?v=1769783154\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Domain choice shapes data, models, evaluation\n",
    ">* Consider users, their decisions, and potential risks\n",
    "\n",
    ">* Start from your interests, then check feasibility\n",
    ">* Choose a focused domain with enough usable data\n",
    "\n",
    ">* Consider ethical, social, and data sensitivity issues\n",
    ">* Balance ambition with fairness, risk, and practicality\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8255aaae",
   "metadata": {},
   "source": [
    "### **1.2. Checking Data Sources**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0a0a40",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_10/Lecture_A/image_01_02.jpg?v=1769783170\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Check if data truly matches project goal\n",
    ">* Ensure data is recent, representative, and aligned\n",
    "\n",
    ">* Check dataset quality, balance, and missing information\n",
    ">* Data issues shape goals, metrics, and preprocessing\n",
    "\n",
    ">* Check licenses, privacy rules, and data access\n",
    ">* Ensure projects stay ethical, compliant, and shareable\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38120b38",
   "metadata": {},
   "source": [
    "### **1.3. Success Criteria Design**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ceaa6a2",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_10/Lecture_A/image_01_03.jpg?v=1769783189\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Define measurable, realistic metrics before training models\n",
    ">* Align metrics with real-world costs and priorities\n",
    "\n",
    ">* Combine performance metrics with real-world constraints\n",
    ">* Set explicit targets for accuracy, speed, and robustness\n",
    "\n",
    ">* Compare models against simple, well-chosen baselines\n",
    ">* Define clear improvement margins and stopping conditions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6384553f",
   "metadata": {},
   "source": [
    "## **2. Choosing Model Architectures**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0000a0f",
   "metadata": {},
   "source": [
    "### **2.1. Task Aligned Architectures**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0ec3bd",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_10/Lecture_A/image_02_01.jpg?v=1769783215\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Match architecture to the task’s prediction type\n",
    ">* Use data structure to guide model family choice\n",
    "\n",
    ">* Match architectures to locality, hierarchy, connectivity patterns\n",
    ">* Use domain structures to learn meaningful representations\n",
    "\n",
    ">* Match architectures to outputs and project constraints\n",
    ">* Balance accuracy, calibration, latency, robustness, and updates\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9564c414",
   "metadata": {},
   "source": [
    "### **2.2. Reuse or Build Anew**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f9f807",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_10/Lecture_A/image_02_02.jpg?v=1769783239\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Choose reuse or new model by constraints\n",
    ">* Reuse for small data; build for unique, large\n",
    "\n",
    ">* Match pretrained models to your data domain\n",
    ">* Choose model family, size, and adaptation strategy\n",
    "\n",
    ">* Design new models when reuse conflicts constraints\n",
    ">* Weigh flexibility, resources, deployment, and performance tradeoffs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a989d58",
   "metadata": {},
   "source": [
    "### **2.3. Scaling to Hardware Limits**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f861adfe",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_10/Lecture_A/image_02_03.jpg?v=1769783262\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Balance model size with hardware and time\n",
    ">* Prefer smaller models that allow rapid experimentation\n",
    "\n",
    ">* Start small, then gradually scale model complexity\n",
    ">* Increase size until hardware or time constraints appear\n",
    "\n",
    ">* Use configs that ease memory or time limits\n",
    ">* Align model and training plan with resources\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08b306c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Scaling to Hardware Limits\n",
    "\n",
    "# This script shows scaling models to hardware limits.\n",
    "# We simulate small and larger models with tiny data.\n",
    "# Focus on batch size memory and training time tradeoffs.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Import numpy for numeric operations.\n",
    "import numpy as np\n",
    "\n",
    "# Import tensorflow and check version.\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print tensorflow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Detect available device type for information.\n",
    "physical_gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "if physical_gpus:\n",
    "    device_type = \"GPU\"\n",
    "else:\n",
    "    device_type = \"CPU\"\n",
    "\n",
    "# Print detected device type briefly.\n",
    "print(\"Running on device type:\", device_type)\n",
    "\n",
    "# Create a tiny synthetic image classification dataset.\n",
    "num_samples = 512\n",
    "img_height = 28\n",
    "img_width = 28\n",
    "num_classes = 10\n",
    "\n",
    "# Generate random images with small values.\n",
    "images = np.random.rand(num_samples, img_height, img_width, 1).astype(\"float32\")\n",
    "\n",
    "# Generate random integer labels for classes.\n",
    "labels = np.random.randint(0, num_classes, size=(num_samples,))\n",
    "\n",
    "# Convert labels to categorical one hot encoding.\n",
    "labels_categorical = tf.keras.utils.to_categorical(labels, num_classes)\n",
    "\n",
    "# Validate shapes before building datasets.\n",
    "assert images.shape[0] == labels_categorical.shape[0]\n",
    "\n",
    "# Define a helper function to build datasets.\n",
    "def make_dataset(batch_size):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((images, labels_categorical))\n",
    "    ds = ds.shuffle(buffer_size=num_samples, seed=seed_value)\n",
    "    ds = ds.batch(batch_size)\n",
    "    return ds\n",
    "\n",
    "# Define a small baseline convolutional model.\n",
    "def build_small_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(8, (3, 3), activation=\"relu\", input_shape=(img_height, img_width, 1)),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(16, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Define a larger model that stresses memory more.\n",
    "def build_large_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\", input_shape=(img_height, img_width, 1)),\n",
    "        tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\"),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Prepare two datasets with different batch sizes.\n",
    "small_batch_size = 32\n",
    "large_batch_size = 128\n",
    "train_ds_small = make_dataset(small_batch_size)\n",
    "train_ds_large = make_dataset(large_batch_size)\n",
    "\n",
    "# Function to train a model and measure time.\n",
    "def timed_train(model, dataset, epochs, description):\n",
    "    start = time.time()\n",
    "    history = model.fit(dataset, epochs=epochs, verbose=0)\n",
    "    end = time.time()\n",
    "    duration = end - start\n",
    "    final_acc = history.history[\"accuracy\"][-1]\n",
    "    print(description, \"time_sec=\", round(duration, 3), \"acc=\", round(float(final_acc), 3))\n",
    "\n",
    "# Train small model with comfortable batch size.\n",
    "small_model = build_small_model()\n",
    "timed_train(small_model, train_ds_small, epochs=3, description=\"Small model, batch 32:\")\n",
    "\n",
    "# Train large model with same batch size.\n",
    "large_model = build_large_model()\n",
    "timed_train(large_model, train_ds_small, epochs=3, description=\"Large model, batch 32:\")\n",
    "\n",
    "# Train large model with larger batch size.\n",
    "large_model_big_batch = build_large_model()\n",
    "timed_train(large_model_big_batch, train_ds_large, epochs=3, description=\"Large model, batch 128:\")\n",
    "\n",
    "# Show how many batches each dataset uses.\n",
    "num_batches_small = len(list(train_ds_small))\n",
    "num_batches_large = len(list(train_ds_large))\n",
    "print(\"Batches with batch 32:\", num_batches_small)\n",
    "print(\"Batches with batch 128:\", num_batches_large)\n",
    "\n",
    "# Final line prints simple summary message.\n",
    "print(\"Example complete: compare speed versus model and batch choices.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cb0a09",
   "metadata": {},
   "source": [
    "## **3. Designing Experiment Plans**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a189fc7",
   "metadata": {},
   "source": [
    "### **3.1. Defining Strong Baselines**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2145fb",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_10/Lecture_A/image_03_01.jpg?v=1769783367\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Baselines are simple methods setting minimum performance\n",
    ">* They mirror realistic non-deep solutions for comparison\n",
    "\n",
    ">* Choose simple, realistic models as careful baselines\n",
    ">* Document preprocessing, training, and evaluation for fairness\n",
    "\n",
    ">* Use multiple baselines from simple to advanced\n",
    ">* Compare gains to justify complexity and deployment costs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe9fb32",
   "metadata": {},
   "source": [
    "### **3.2. Targeted Ablation Strategies**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262d5455",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_10/Lecture_A/image_03_02.jpg?v=1769783386\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Use controlled ablations to test component importance\n",
    ">* Plan focused ablations ahead to structure experiments\n",
    "\n",
    ">* Ablate uncertain, novel, or costly components first\n",
    ">* Compare variants to measure each design choice’s value\n",
    "\n",
    ">* Prioritize informative ablations within time, compute limits\n",
    ">* Predefine training, metrics, thresholds, and document rationale\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14174598",
   "metadata": {},
   "source": [
    "### **3.3. Optimization roadmap**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57d6a02",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_10/Lecture_A/image_03_03.jpg?v=1769783405\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Turn vague tuning into a staged, structured plan\n",
    ">* Start with cheap tweaks, then advanced techniques deliberately\n",
    "\n",
    ">* Plan phased experiments with clear themes, budgets\n",
    ">* Progress from stability to capacity, data, advanced techniques\n",
    "\n",
    ">* Set clear improvement thresholds and stopping rules\n",
    ">* Preplan reactions to outcomes for disciplined experimentation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833694ac",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Project Design**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13eb6d70",
   "metadata": {},
   "source": [
    "\n",
    "In this lecture, you learned to:\n",
    "- Define a concrete project goal, dataset choice, and evaluation metrics for a PyTorch capstone. \n",
    "- Select an appropriate model architecture and training strategy aligned with project constraints. \n",
    "- Plan an experiment schedule that includes baselines, ablations, and potential optimizations. \n",
    "\n",
    "<font color='yellow'>Congratulations on completing this course!</font>"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
