{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7028a4fc",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Project Design**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff5b253",
   "metadata": {},
   "source": [
    ">Last update: 20260131.\n",
    "    \n",
    "By the end of this Lecture, you will be able to:\n",
    "- Define a concrete project goal, dataset choice, and evaluation metrics for a PyTorch capstone. \n",
    "- Select an appropriate model architecture and training strategy aligned with project constraints. \n",
    "- Plan an experiment schedule that includes baselines, ablations, and potential optimizations. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909655f9",
   "metadata": {},
   "source": [
    "## **1. Scoping Your Project**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40262c34",
   "metadata": {},
   "source": [
    "### **1.1. Selecting a Project Domain**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df19bc4c",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_10/Lecture_A/image_01_01.jpg?v=1769863066\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Choose a domain matching interests, skills, constraints\n",
    ">* Focus on real users, problems, and clear goals\n",
    "\n",
    ">* Match domain with its natural data type\n",
    ">* Ensure tasks and evaluation fit project constraints\n",
    "\n",
    ">* Match project ambition to time, hardware, experience\n",
    ">* Right-size the domain to enable focused experimentation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc84e39",
   "metadata": {},
   "source": [
    "### **1.2. Checking Data Sources**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67ad62b",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_10/Lecture_A/image_01_02.jpg?v=1769863081\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Inspect datasets carefully for content and labels\n",
    ">* Ensure data truly supports your project goals\n",
    "\n",
    ">* Check dataset size, class balance, and coverage\n",
    ">* Ensure data represents target population; adjust or augment\n",
    "\n",
    ">* Check legal, technical, and resource data limits\n",
    ">* Consider privacy, consent, and potential harms\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16f0ba1",
   "metadata": {},
   "source": [
    "### **1.3. Defining success metrics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad59883",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_10/Lecture_A/image_01_03.jpg?v=1769863096\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Turn vague goals into measurable success metrics\n",
    ">* Pick primary and secondary metrics matching real impact\n",
    "\n",
    ">* Match metrics to task type and data\n",
    ">* Choose metrics reflecting real-world costs and benefits\n",
    "\n",
    ">* Set realistic metric targets using simple baselines\n",
    ">* Balance performance, speed, size, fairness, and stopping\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c91d4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Defining success metrics\n",
    "\n",
    "# This script shows simple success metrics selection.\n",
    "# We use a toy classification example with imbalanced labels.\n",
    "# Focus is on defining and comparing different evaluation metrics.\n",
    "\n",
    "# import required standard libraries for numerical work.\n",
    "import numpy as np\n",
    "\n",
    "# set deterministic random seed for reproducible results.\n",
    "np.random.seed(42)\n",
    "\n",
    "# create small synthetic dataset with binary labels.\n",
    "true_labels = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1])\n",
    "\n",
    "# create model A predicted probabilities for positive class.\n",
    "probs_a = np.array([0.10, 0.05, 0.20, 0.15, 0.30, 0.60, 0.70, 0.80, 0.55, 0.65])\n",
    "\n",
    "# create model B predicted probabilities for positive class.\n",
    "probs_b = np.array([0.05, 0.10, 0.15, 0.10, 0.25, 0.90, 0.85, 0.80, 0.70, 0.60])\n",
    "\n",
    "# validate shapes to avoid broadcasting mistakes.\n",
    "assert true_labels.shape == probs_a.shape == probs_b.shape\n",
    "\n",
    "# define helper to convert probabilities to class predictions.\n",
    "def probs_to_labels(probs, threshold):\n",
    "    # apply threshold to get binary predictions.\n",
    "    return (probs >= threshold).astype(int)\n",
    "\n",
    "\n",
    "# define helper to compute confusion matrix counts.\n",
    "def confusion_counts(y_true, y_pred):\n",
    "    # compute true positives, false positives, true negatives, false negatives.\n",
    "    tp = int(np.sum((y_true == 1) & (y_pred == 1)))\n",
    "    fp = int(np.sum((y_true == 0) & (y_pred == 1)))\n",
    "    tn = int(np.sum((y_true == 0) & (y_pred == 0)))\n",
    "    fn = int(np.sum((y_true == 1) & (y_pred == 0)))\n",
    "    return tp, fp, tn, fn\n",
    "\n",
    "\n",
    "# define helper to compute accuracy, precision, recall safely.\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    # get confusion matrix components for metrics.\n",
    "    tp, fp, tn, fn = confusion_counts(y_true, y_pred)\n",
    "    total = tp + fp + tn + fn\n",
    "\n",
    "    # compute accuracy with safe division.\n",
    "    accuracy = (tp + tn) / total if total > 0 else 0.0\n",
    "\n",
    "    # compute precision with safe division.\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "\n",
    "    # compute recall with safe division.\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    return accuracy, precision, recall\n",
    "\n",
    "\n",
    "# choose threshold representing business tolerance for false alarms.\n",
    "threshold = 0.5\n",
    "\n",
    "# get hard predictions for both models.\n",
    "preds_a = probs_to_labels(probs_a, threshold)\n",
    "\n",
    "# get hard predictions for both models.\n",
    "preds_b = probs_to_labels(probs_b, threshold)\n",
    "\n",
    "# compute metrics for model A.\n",
    "acc_a, prec_a, rec_a = compute_metrics(true_labels, preds_a)\n",
    "\n",
    "# compute metrics for model B.\n",
    "acc_b, prec_b, rec_b = compute_metrics(true_labels, preds_b)\n",
    "\n",
    "# print short explanation of project goal and metric focus.\n",
    "print(\"Goal: detect rare positives, prioritize recall over accuracy.\")\n",
    "\n",
    "# print metrics for model A in one concise line.\n",
    "print(\"Model A -> accuracy\", round(acc_a, 2), \"precision\", round(prec_a, 2), \"recall\", round(rec_a, 2))\n",
    "\n",
    "# print metrics for model B in one concise line.\n",
    "print(\"Model B -> accuracy\", round(acc_b, 2), \"precision\", round(prec_b, 2), \"recall\", round(rec_b, 2))\n",
    "\n",
    "# decide which model wins under recall focused success metric.\n",
    "better_model = \"A\" if rec_a >= rec_b else \"B\"\n",
    "\n",
    "# print final success metric decision for this project.\n",
    "print(\"Chosen primary metric: recall, selected model\", better_model)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6776b0e",
   "metadata": {},
   "source": [
    "## **2. Choosing Model Architectures**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048b11b2",
   "metadata": {},
   "source": [
    "### **2.1. Task Aligned Architectures**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29dbe176",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_10/Lecture_A/image_02_01.jpg?v=1769863158\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Match model type to task and data\n",
    ">* Use task structure to avoid misaligned architectures\n",
    "\n",
    ">* Match architecture to output type and granularity\n",
    ">* Use specialized structures for dense, ranked, paired outputs\n",
    "\n",
    ">* Match model complexity to task difficulty and data\n",
    ">* Consider latency, interpretability, robustness, and deployment limits\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b47f93c",
   "metadata": {},
   "source": [
    "### **2.2. Reuse or Build Anew**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05135c73",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_10/Lecture_A/image_02_02.jpg?v=1769863171\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Choose reuse or scratch based on problem constraints\n",
    ">* Pretrained suits limited data; scratch suits specialized tasks\n",
    "\n",
    ">* Reuse stable models to reduce project risk\n",
    ">* Build custom models for deeper learning and novelty\n",
    "\n",
    ">* Start from pretrained backbones, then customize key parts\n",
    ">* Match training plan to architecture choice and rationale\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894c09d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Reuse or Build Anew\n",
    "\n",
    "# This script compares reusing and building models simply.\n",
    "# We use TensorFlow to simulate PyTorch style choices.\n",
    "# Focus on architecture reuse versus building from scratch.\n",
    "\n",
    "# !pip install tensorflow.\n",
    "\n",
    "# Import required standard libraries safely.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Import TensorFlow and check version.\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# Configure TensorFlow random seed deterministically.\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version in one concise line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Create a tiny synthetic dataset for binary classification.\n",
    "num_samples = 200\n",
    "input_dim = 10\n",
    "\n",
    "# Generate random features with controlled distribution.\n",
    "X = np.random.randn(num_samples, input_dim).astype(\"float32\")\n",
    "\n",
    "# Generate simple labels based on a linear rule.\n",
    "linear_scores = X.sum(axis=1)\n",
    "y = (linear_scores > 0).astype(\"float32\")\n",
    "\n",
    "# Validate dataset shapes before training.\n",
    "assert X.shape == (num_samples, input_dim)\n",
    "assert y.shape == (num_samples,)\n",
    "\n",
    "# Split into small train and validation subsets.\n",
    "train_size = 160\n",
    "X_train, X_val = X[:train_size], X[train_size:]\n",
    "\n",
    "# Split labels into matching subsets.\n",
    "y_train, y_val = y[:train_size], y[train_size:]\n",
    "\n",
    "# Define a reusable base model representing pretrained features.\n",
    "base_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(input_dim,)),\n",
    "    tf.keras.layers.Dense(16, activation=\"relu\"),\n",
    "])\n",
    "\n",
    "# Freeze base model to simulate feature extraction reuse.\n",
    "base_model.trainable = False\n",
    "\n",
    "# Build a reuse strategy model using frozen base features.\n",
    "reuse_model = tf.keras.Sequential([\n",
    "    base_model,\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "])\n",
    "\n",
    "# Compile reuse model with conservative learning rate.\n",
    "reuse_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Train reuse model briefly with silent verbose setting.\n",
    "reuse_history = reuse_model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=10,\n",
    "    batch_size=16,\n",
    "    verbose=0,\n",
    "    validation_data=(X_val, y_val),\n",
    ")\n",
    "\n",
    "# Evaluate reuse model performance on validation data.\n",
    "reuse_loss, reuse_acc = reuse_model.evaluate(\n",
    "    X_val,\n",
    "    y_val,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Build a from scratch model with all layers trainable.\n",
    "scratch_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(input_dim,)),\n",
    "    tf.keras.layers.Dense(16, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "])\n",
    "\n",
    "# Compile scratch model with slightly larger learning rate.\n",
    "scratch_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.005),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Train scratch model briefly with silent verbose setting.\n",
    "scratch_history = scratch_model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=10,\n",
    "    batch_size=16,\n",
    "    verbose=0,\n",
    "    validation_data=(X_val, y_val),\n",
    ")\n",
    "\n",
    "# Evaluate scratch model performance on validation data.\n",
    "scratch_loss, scratch_acc = scratch_model.evaluate(\n",
    "    X_val,\n",
    "    y_val,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Print concise comparison of both strategies.\n",
    "print(\"Reuse model validation accuracy:\", float(reuse_acc))\n",
    "print(\"Scratch model validation accuracy:\", float(scratch_acc))\n",
    "print(\"Reuse model trainable parameters:\", reuse_model.count_params())\n",
    "print(\"Scratch model trainable parameters:\", scratch_model.count_params())\n",
    "print(\"Base model trainable flag:\", base_model.trainable)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8939c3c3",
   "metadata": {},
   "source": [
    "### **2.3. Computing Budget Tradeoffs**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a210aee",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_10/Lecture_A/image_02_03.jpg?v=1769863233\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Model size must match your compute budget\n",
    ">* Prefer smaller models to iterate, learn, refine\n",
    "\n",
    ">* Split compute budget into training, validation, experimentation\n",
    ">* Prefer smaller pretrained models to enable rich experimentation\n",
    "\n",
    ">* Stage training and use efficiency techniques thoughtfully\n",
    ">* Match model and training to hardware, latency needs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422e89c1",
   "metadata": {},
   "source": [
    "## **3. Designing Experiment Baselines**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e79082",
   "metadata": {},
   "source": [
    "### **3.1. Defining Strong Baselines**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fb36ae",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_10/Lecture_A/image_03_01.jpg?v=1769863254\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Strong baselines are simple, honest reference models\n",
    ">* They reveal if complex methods truly add value\n",
    "\n",
    ">* Start with simple, task-aware models, then refine\n",
    ">* Stabilize training, tune key hyperparameters, avoid unfair comparisons\n",
    "\n",
    ">* Match baselines to real project constraints and tradeoffs\n",
    ">* Document assumptions, metrics, and improvements over baselines\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34afe79",
   "metadata": {},
   "source": [
    "### **3.2. Designing Ablation Studies**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3304d85",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_10/Lecture_A/image_03_02.jpg?v=1769863268\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Use ablations to test each design choice\n",
    ">* Run targeted variants to find truly essential components\n",
    "\n",
    ">* Change one component per ablation, ask why\n",
    ">* Keep everything else fixed for fair comparisons\n",
    "\n",
    ">* Prioritize ablations on costly or uncertain components\n",
    ">* Use small, targeted runs to gain insights\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7891d782",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Designing Ablation Studies\n",
    "\n",
    "# This script demonstrates simple ablation studies.\n",
    "# We compare a baseline model and two ablations.\n",
    "# Focus on planning controlled PyTorch style experiments.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Import TensorFlow for quick experiments.\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set deterministic random seeds everywhere.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version briefly.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Load MNIST dataset from Keras utilities.\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Reduce dataset size for faster experiments.\n",
    "small_train_size = 6000\n",
    "small_test_size = 1000\n",
    "x_train = x_train[:small_train_size]\n",
    "y_train = y_train[:small_train_size]\n",
    "\n",
    "# Slice test data to keep experiments quick.\n",
    "x_test = x_test[:small_test_size]\n",
    "y_test = y_test[:small_test_size]\n",
    "\n",
    "# Normalize images to zero one range.\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_test = x_test.astype(\"float32\") / 255.0\n",
    "\n",
    "# Add channel dimension for convolutional layers.\n",
    "x_train = np.expand_dims(x_train, axis=-1)\n",
    "x_test = np.expand_dims(x_test, axis=-1)\n",
    "\n",
    "# Validate shapes before building models.\n",
    "print(\"Train shape:\", x_train.shape, \"Test shape:\", x_test.shape)\n",
    "\n",
    "\n",
    "# Define a function building the baseline model.\n",
    "def build_baseline_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(\n",
    "            16,\n",
    "            (3, 3),\n",
    "            activation=\"relu\",\n",
    "            input_shape=(28, 28, 1),\n",
    "        ),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(10, activation=\"softmax\"),\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Define a function building no dropout ablation.\n",
    "def build_no_dropout_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(\n",
    "            16,\n",
    "            (3, 3),\n",
    "            activation=\"relu\",\n",
    "            input_shape=(28, 28, 1),\n",
    "        ),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(10, activation=\"softmax\"),\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Define a function building small capacity ablation.\n",
    "def build_small_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(\n",
    "            8,\n",
    "            (3, 3),\n",
    "            activation=\"relu\",\n",
    "            input_shape=(28, 28, 1),\n",
    "        ),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(32, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(10, activation=\"softmax\"),\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Define a helper function training and evaluating.\n",
    "def train_and_evaluate(model_builder, label):\n",
    "    model = model_builder()\n",
    "    history = model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        epochs=2,\n",
    "        batch_size=64,\n",
    "        verbose=0,\n",
    "        validation_split=0.1,\n",
    "    )\n",
    "\n",
    "    test_loss, test_acc = model.evaluate(\n",
    "        x_test,\n",
    "        y_test,\n",
    "        verbose=0,\n",
    "    )\n",
    "\n",
    "    print(label, \"test accuracy:\", round(float(test_acc), 4))\n",
    "\n",
    "    return test_acc\n",
    "\n",
    "\n",
    "# Run baseline experiment first for comparison.\n",
    "baseline_acc = train_and_evaluate(build_baseline_model, \"Baseline model\")\n",
    "\n",
    "# Run ablation removing dropout regularization.\n",
    "no_dropout_acc = train_and_evaluate(build_no_dropout_model, \"No dropout ablation\")\n",
    "\n",
    "# Run ablation reducing model capacity.\n",
    "small_model_acc = train_and_evaluate(build_small_model, \"Small model ablation\")\n",
    "\n",
    "# Summarize results to support ablation planning.\n",
    "print(\"Baseline accuracy:\", round(float(baseline_acc), 4))\n",
    "print(\"No dropout accuracy:\", round(float(no_dropout_acc), 4))\n",
    "print(\"Small model accuracy:\", round(float(small_model_acc), 4))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd76fdb8",
   "metadata": {},
   "source": [
    "### **3.3. Optimization roadmap**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2f7250",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_10/Lecture_A/image_03_03.jpg?v=1769863349\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Plan staged, low-risk to complex improvements\n",
    ">* Each step has hypothesis, limited changes, criteria\n",
    "\n",
    ">* Start with cheap, simple changes for gains\n",
    ">* Delay expensive experiments until basics are optimized\n",
    "\n",
    ">* Use results to choose next optimization steps\n",
    ">* Keep a clear experiment log and rationale\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f18493",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Project Design**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9553901",
   "metadata": {},
   "source": [
    "\n",
    "In this lecture, you learned to:\n",
    "- Define a concrete project goal, dataset choice, and evaluation metrics for a PyTorch capstone. \n",
    "- Select an appropriate model architecture and training strategy aligned with project constraints. \n",
    "- Plan an experiment schedule that includes baselines, ablations, and potential optimizations. \n",
    "\n",
    "<font color='yellow'>Congratulations on completing this course!</font>"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
