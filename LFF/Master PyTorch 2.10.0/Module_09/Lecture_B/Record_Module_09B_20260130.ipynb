{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abf7d961",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Serving Models**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cbfd37",
   "metadata": {},
   "source": [
    ">Last update: 20260130.\n",
    "    \n",
    "By the end of this Lecture, you will be able to:\n",
    "- Wrap a PyTorch model in a simple inference function that handles preprocessing and postprocessing. \n",
    "- Integrate the inference function into a lightweight REST API or batch inference script. \n",
    "- Evaluate latency and throughput of the serving setup and identify simple optimizations. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc36909",
   "metadata": {},
   "source": [
    "## **1. Designing Inference Functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59aafc59",
   "metadata": {},
   "source": [
    "### **1.1. Robust Input Checks**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc82f78",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_09/Lecture_B/image_01_01.jpg?v=1769778782\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Inference functions must guard against bad inputs\n",
    ">* Validate types, structure, and fields; give clear errors\n",
    "\n",
    ">* Check input shapes, sizes, and channels carefully\n",
    ">* Align all validation rules with model training data\n",
    "\n",
    ">* Catch errors, give clear, contextual messages\n",
    ">* Log invalid inputs to monitor and improve\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a5acdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Robust Input Checks\n",
    "\n",
    "# This script shows robust input checks.\n",
    "# We wrap a tiny model with validation.\n",
    "# Focus on safe preprocessing before inference.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import standard libraries for typing.\n",
    "from typing import List, Tuple, Union\n",
    "\n",
    "# Import numpy for simple array handling.\n",
    "import numpy as np\n",
    "\n",
    "# Import tensorflow and set random seeds.\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "np.random.seed(42)\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Define expected image height and width.\n",
    "EXPECTED_HEIGHT: int = 28\n",
    "\n",
    "# Define expected image width and channels.\n",
    "EXPECTED_WIDTH: int = 28\n",
    "\n",
    "# Define expected number of channels.\n",
    "EXPECTED_CHANNELS: int = 1\n",
    "\n",
    "# Create a tiny dummy model for demonstration.\n",
    "model = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.InputLayer(\n",
    "            input_shape=(EXPECTED_HEIGHT, EXPECTED_WIDTH, EXPECTED_CHANNELS)\n",
    "        ),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(8, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(3, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Compile the model with simple settings.\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\")\n",
    "\n",
    "# Create a small dummy training batch.\n",
    "dummy_images = np.random.rand(4, EXPECTED_HEIGHT, EXPECTED_WIDTH, EXPECTED_CHANNELS)\n",
    "\n",
    "# Create small dummy labels for training.\n",
    "dummy_labels = np.random.randint(0, 3, size=(4,))\n",
    "\n",
    "# Train briefly with silent verbose setting.\n",
    "model.fit(dummy_images, dummy_labels, epochs=1, verbose=0)\n",
    "\n",
    "# Define a custom exception for input errors.\n",
    "class InputValidationError(Exception):\n",
    "    pass\n",
    "\n",
    "# Validate a single image like object carefully.\n",
    "def validate_single_image(image: np.ndarray) -> np.ndarray:\n",
    "    # Check that input is a numpy array.\n",
    "    if not isinstance(image, np.ndarray):\n",
    "        raise InputValidationError(\"Input must be a numpy array.\")\n",
    "\n",
    "    # Check that array has four dimensions.\n",
    "    if image.ndim != 3:\n",
    "        raise InputValidationError(\"Image must have three dimensions.\")\n",
    "\n",
    "    # Unpack height width and channels.\n",
    "    height, width, channels = image.shape\n",
    "\n",
    "    # Check that channels match expectation.\n",
    "    if channels != EXPECTED_CHANNELS:\n",
    "        raise InputValidationError(\"Image must have one channel only.\")\n",
    "\n",
    "    # Check that height and width are positive.\n",
    "    if height <= 0 or width <= 0:\n",
    "        raise InputValidationError(\"Image height and width must be positive.\")\n",
    "\n",
    "    # Resize if shape differs from expected.\n",
    "    if height != EXPECTED_HEIGHT or width != EXPECTED_WIDTH:\n",
    "        image = tf.image.resize(\n",
    "            image, size=(EXPECTED_HEIGHT, EXPECTED_WIDTH)\n",
    "        ).numpy()\n",
    "\n",
    "    # Normalize pixel values to zero one.\n",
    "    image = image.astype(\"float32\") / 255.0\n",
    "\n",
    "    # Return validated and normalized image.\n",
    "    return image\n",
    "\n",
    "# Inference function with robust input checks.\n",
    "def predict_single_image(image: Union[np.ndarray, None]) -> Tuple[int, float]:\n",
    "    # Check that something was actually provided.\n",
    "    if image is None:\n",
    "        raise InputValidationError(\"No image was provided to function.\")\n",
    "\n",
    "    # Validate and preprocess the image.\n",
    "    processed = validate_single_image(image)\n",
    "\n",
    "    # Add batch dimension for model call.\n",
    "    batch = np.expand_dims(processed, axis=0)\n",
    "\n",
    "    # Run model prediction inside try block.\n",
    "    try:\n",
    "        probs = model.predict(batch, verbose=0)\n",
    "    except Exception as exc:\n",
    "        raise RuntimeError(\"Model inference failed.\") from exc\n",
    "\n",
    "    # Extract predicted class and confidence.\n",
    "    class_index = int(np.argmax(probs[0]))\n",
    "\n",
    "    # Extract confidence as float value.\n",
    "    confidence = float(np.max(probs[0]))\n",
    "\n",
    "    # Return prediction and confidence tuple.\n",
    "    return class_index, confidence\n",
    "\n",
    "# Helper to run a scenario and print result.\n",
    "def run_scenario(name: str, image_candidate) -> None:\n",
    "    # Print scenario name for clarity.\n",
    "    print(\"Scenario:\", name)\n",
    "\n",
    "    # Try running prediction and catch errors.\n",
    "    try:\n",
    "        pred_class, conf = predict_single_image(image_candidate)\n",
    "        print(\" Prediction:\", pred_class, \"Confidence:\", round(conf, 3))\n",
    "    except InputValidationError as ive:\n",
    "        print(\" InputValidationError:\", str(ive))\n",
    "    except Exception as exc:\n",
    "        print(\" Other error:\", type(exc).__name__)\n",
    "\n",
    "# Create a valid dummy grayscale image.\n",
    "valid_image = np.random.randint(\n",
    "    0, 255, size=(28, 28, 1), dtype=\"uint8\"\n",
    ")\n",
    "\n",
    "# Create an image with wrong number of channels.\n",
    "wrong_channels_image = np.random.randint(\n",
    "    0, 255, size=(28, 28, 3), dtype=\"uint8\"\n",
    ")\n",
    "\n",
    "# Create an image with wrong spatial size.\n",
    "wrong_size_image = np.random.randint(\n",
    "    0, 255, size=(40, 20, 1), dtype=\"uint8\"\n",
    ")\n",
    "\n",
    "# Run scenario with valid image input.\n",
    "run_scenario(\"valid image\", valid_image)\n",
    "\n",
    "# Run scenario with wrong channels image.\n",
    "run_scenario(\"wrong channels\", wrong_channels_image)\n",
    "\n",
    "# Run scenario with None as invalid input.\n",
    "run_scenario(\"none input\", None)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9a09d4",
   "metadata": {},
   "source": [
    "### **1.2. Input to Tensor Conversion**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad93612",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_09/Lecture_B/image_01_02.jpg?v=1769778858\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Convert varied inputs into standardized model tensors\n",
    ">* Centralize conversion to avoid shape and type bugs\n",
    "\n",
    ">* Handle single items and batches consistently\n",
    ">* Standardize shapes, precision, and device for performance\n",
    "\n",
    ">* Reuse training-time preprocessing during tensor conversion\n",
    ">* Centralized transforms keep serving consistent and reliable\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44ecdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Input to Tensor Conversion\n",
    "\n",
    "# This script shows input tensor conversion.\n",
    "# We focus on simple image like arrays.\n",
    "# All steps are small and beginner friendly.\n",
    "\n",
    "# Required library is tensorflow for tensor handling.\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import standard libraries for arrays and typing.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Import tensorflow for tensor operations and models.\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set deterministic seeds for reproducible behavior.\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "# Print tensorflow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Define expected image height and width values.\n",
    "IMG_HEIGHT = 28\n",
    "IMG_WIDTH = 28\n",
    "IMG_CHANNELS = 1\n",
    "\n",
    "# Define expected tensor dtype and device placement.\n",
    "EXPECTED_DTYPE = tf.float32\n",
    "DEVICE = \"/CPU:0\"\n",
    "\n",
    "# Create a tiny dummy model for demonstration.\n",
    "with tf.device(DEVICE):\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            tf.keras.layers.InputLayer(\n",
    "                input_shape=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)\n",
    "            ),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(10, activation=\"softmax\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "# Compile model with simple configuration.\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Create a small random batch for quick training.\n",
    "train_images = np.random.randint(\n",
    "    0,\n",
    "    256,\n",
    "    size=(16, IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS),\n",
    "    dtype=np.uint8,\n",
    ")\n",
    "\n",
    "train_labels = np.random.randint(0, 10, size=(16,), dtype=np.int32)\n",
    "\n",
    "# Train briefly to make model usable.\n",
    "model.fit(train_images, train_labels, epochs=1, verbose=0)\n",
    "\n",
    "# Define a function to normalize uint8 image arrays.\n",
    "def normalize_image_array(image_array: np.ndarray) -> np.ndarray:\n",
    "    # Ensure array has correct number of dimensions.\n",
    "    if image_array.ndim not in (2, 3):\n",
    "        raise ValueError(\"Image array must be 2D or 3D.\")\n",
    "\n",
    "    # If grayscale without channel, add channel dimension.\n",
    "    if image_array.ndim == 2:\n",
    "        image_array = image_array[..., np.newaxis]\n",
    "\n",
    "    # Validate spatial dimensions before resizing.\n",
    "    h, w, c = image_array.shape\n",
    "    if c != IMG_CHANNELS:\n",
    "        raise ValueError(\"Unexpected channel count in image array.\")\n",
    "\n",
    "    # Convert to float32 and scale to zero one range.\n",
    "    image_array = image_array.astype(\"float32\") / 255.0\n",
    "    return image_array\n",
    "\n",
    "# Define function to convert various inputs into tensor.\n",
    "def to_model_tensor(input_data) -> tf.Tensor:\n",
    "    # Handle single numpy array input case.\n",
    "    if isinstance(input_data, np.ndarray):\n",
    "        image_array = normalize_image_array(input_data)\n",
    "        batch_array = np.expand_dims(image_array, axis=0)\n",
    "\n",
    "    # Handle list of numpy arrays as batch input.\n",
    "    elif isinstance(input_data, list):\n",
    "        if not input_data:\n",
    "            raise ValueError(\"Input list must not be empty.\")\n",
    "        normalized_list = []\n",
    "        for idx, item in enumerate(input_data):\n",
    "            if not isinstance(item, np.ndarray):\n",
    "                raise TypeError(\"All list items must be numpy arrays.\")\n",
    "            norm_item = normalize_image_array(item)\n",
    "            normalized_list.append(norm_item)\n",
    "        batch_array = np.stack(normalized_list, axis=0)\n",
    "\n",
    "    # Raise error for unsupported input types.\n",
    "    else:\n",
    "        raise TypeError(\"Input must be numpy array or list of arrays.\")\n",
    "\n",
    "    # Convert numpy batch into tensorflow tensor.\n",
    "    tensor = tf.convert_to_tensor(batch_array, dtype=EXPECTED_DTYPE)\n",
    "\n",
    "    # Validate final tensor shape before returning.\n",
    "    if tensor.shape.rank != 4:\n",
    "        raise ValueError(\"Tensor must be rank four for model.\")\n",
    "    if tensor.shape[1:] != (IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS):\n",
    "        raise ValueError(\"Tensor spatial shape does not match.\")\n",
    "\n",
    "    # Move tensor to desired device context.\n",
    "    with tf.device(DEVICE):\n",
    "        tensor_on_device = tf.identity(tensor)\n",
    "    return tensor_on_device\n",
    "\n",
    "# Define simple inference function using conversion helper.\n",
    "def run_inference(input_data):\n",
    "    # Convert raw input into clean model tensor.\n",
    "    input_tensor = to_model_tensor(input_data)\n",
    "\n",
    "    # Run model prediction inside device context.\n",
    "    with tf.device(DEVICE):\n",
    "        probs = model(input_tensor, training=False)\n",
    "\n",
    "    # Convert probabilities to predicted class indices.\n",
    "    predicted_classes = tf.argmax(probs, axis=1)\n",
    "    return probs.numpy(), predicted_classes.numpy()\n",
    "\n",
    "# Create a fake single image as height width array.\n",
    "single_image = np.random.randint(\n",
    "    0,\n",
    "    256,\n",
    "    size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    dtype=np.uint8,\n",
    ")\n",
    "\n",
    "# Create a small batch list of fake images.\n",
    "batch_images = [\n",
    "    np.random.randint(\n",
    "        0,\n",
    "        256,\n",
    "        size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "        dtype=np.uint8,\n",
    "    )\n",
    "    for _ in range(3)\n",
    "]\n",
    "\n",
    "# Run inference on single image input.\n",
    "probs_single, preds_single = run_inference(single_image)\n",
    "\n",
    "# Run inference on batch list input.\n",
    "probs_batch, preds_batch = run_inference(batch_images)\n",
    "\n",
    "# Print shapes to show consistent tensor conversion.\n",
    "print(\"Single input tensor shape:\", probs_single.shape)\n",
    "print(\"Single input predicted class:\", preds_single)\n",
    "print(\"Batch input tensor shape:\", probs_batch.shape)\n",
    "print(\"Batch input predicted classes:\", preds_batch)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de416f88",
   "metadata": {},
   "source": [
    "### **1.3. Interpreting Model Outputs**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7f27ae",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_09/Lecture_B/image_01_03.jpg?v=1769778930\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Inference function turns raw tensors into results\n",
    ">* Centralized logic ensures consistent, structured model outputs\n",
    "\n",
    ">* Convert numeric model scores into domain meanings\n",
    ">* Bridge tensors to labels, decisions, and UI outputs\n",
    "\n",
    ">* Return predictions plus helpful contextual details\n",
    ">* Use stable structured outputs for easy integration\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec649ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Interpreting Model Outputs\n",
    "\n",
    "# This script explains interpreting model outputs.\n",
    "# We simulate a tiny classifier prediction pipeline.\n",
    "# Focus is on postprocessing logits into labels.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "\n",
    "# Set deterministic random seeds for reproducibility.\n",
    "random.seed(42)\n",
    "\n",
    "# Define a tiny label vocabulary for our classifier.\n",
    "LABELS = [\"negative\", \"neutral\", \"positive\"]\n",
    "\n",
    "# Create a fake model function returning raw logits.\n",
    "def fake_model_logits(text_batch):\n",
    "    # Validate input type and basic structure.\n",
    "    if not isinstance(text_batch, list):\n",
    "        raise TypeError(\"text_batch must be list\")\n",
    "\n",
    "    # Build deterministic logits using text length features.\n",
    "    logits_batch = []\n",
    "    for text in text_batch:\n",
    "        # Ensure each element is a string input.\n",
    "        if not isinstance(text, str):\n",
    "            raise TypeError(\"each item must string\")\n",
    "\n",
    "        # Use simple handcrafted scoring heuristics.\n",
    "        length_score = len(text) / 20.0\n",
    "        exclam_score = text.count(\"!\") * 0.5\n",
    "        # Compose three class logits from features.\n",
    "        neg_logit = 0.5 - length_score\n",
    "        neu_logit = 0.1\n",
    "        pos_logit = length_score + exclam_score\n",
    "\n",
    "        # Collect logits for this single example.\n",
    "        logits_batch.append([neg_logit, neu_logit, pos_logit])\n",
    "\n",
    "    # Return list of lists representing logits.\n",
    "    return logits_batch\n",
    "\n",
    "# Convert logits into probabilities using softmax.\n",
    "def softmax(logits):\n",
    "    # Subtract max for numerical stability.\n",
    "    max_logit = max(logits)\n",
    "    shifted = [x - max_logit for x in logits]\n",
    "\n",
    "    # Exponentiate shifted logits safely.\n",
    "    exps = [math.exp(x) for x in shifted]\n",
    "    total = sum(exps)\n",
    "\n",
    "    # Avoid division by zero with fallback.\n",
    "    if total == 0.0:\n",
    "        return [1.0 / len(logits)] * len(logits)\n",
    "\n",
    "    # Normalize to obtain probabilities.\n",
    "    return [x / total for x in exps]\n",
    "\n",
    "# Interpret logits into structured prediction dictionaries.\n",
    "def interpret_logits(logits_batch, top_k=2):\n",
    "    # Validate batch shape and label alignment.\n",
    "    if len(LABELS) == 0:\n",
    "        raise ValueError(\"LABELS list empty\")\n",
    "\n",
    "    # Ensure each logits vector matches labels length.\n",
    "    for logits in logits_batch:\n",
    "        if len(logits) != len(LABELS):\n",
    "            raise ValueError(\"logits length mismatch\")\n",
    "\n",
    "    # Build structured outputs for each example.\n",
    "    results = []\n",
    "    for logits in logits_batch:\n",
    "        # Convert logits to probabilities via softmax.\n",
    "        probs = softmax(logits)\n",
    "\n",
    "        # Pair labels with probabilities together.\n",
    "        label_probs = list(zip(LABELS, probs))\n",
    "        # Sort by probability descending order.\n",
    "        label_probs.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # Select top_k predictions for transparency.\n",
    "        top = label_probs[:top_k]\n",
    "        primary_label, primary_prob = top[0]\n",
    "\n",
    "        # Package prediction and auxiliary information.\n",
    "        result = {\n",
    "            \"label\": primary_label,\n",
    "            \"confidence\": round(primary_prob, 3),\n",
    "            \"top_k\": [\n",
    "                {\"label\": lab, \"prob\": round(p, 3)}\n",
    "                for lab, p in top\n",
    "            ],\n",
    "            \"raw_logits\": [round(x, 3) for x in logits],\n",
    "        }\n",
    "\n",
    "        # Append structured result to batch list.\n",
    "        results.append(result)\n",
    "\n",
    "    # Return list of structured prediction dictionaries.\n",
    "    return results\n",
    "\n",
    "# High level inference function wrapping full pipeline.\n",
    "def run_inference(text_batch):\n",
    "    # Get raw logits from our fake model.\n",
    "    logits_batch = fake_model_logits(text_batch)\n",
    "\n",
    "    # Interpret logits into semantic outputs.\n",
    "    interpreted = interpret_logits(logits_batch, top_k=3)\n",
    "\n",
    "    # Return final structured predictions to caller.\n",
    "    return interpreted\n",
    "\n",
    "# Prepare a tiny batch of example input texts.\n",
    "example_texts = [\n",
    "    \"I love this course!\",\n",
    "    \"The video was okay.\",\n",
    "    \"This is really disappointing\",\n",
    "]\n",
    "\n",
    "# Run inference and obtain structured outputs.\n",
    "predictions = run_inference(example_texts)\n",
    "\n",
    "# Print a concise summary for each example.\n",
    "for text, pred in zip(example_texts, predictions):\n",
    "    print(\"Text:\", text)\n",
    "    print(\"Primary:\", pred[\"label\"], pred[\"confidence\"])\n",
    "    print(\"Top k:\", pred[\"top_k\"])\n",
    "    print(\"Raw logits:\", pred[\"raw_logits\"])\n",
    "    print(\"---\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4990f4ff",
   "metadata": {},
   "source": [
    "## **2. APIs and Batch Serving**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c619b5",
   "metadata": {},
   "source": [
    "### **2.1. Building a REST API**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d30fec",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_09/Lecture_B/image_02_01.jpg?v=1769778995\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Expose notebook models through a simple web service\n",
    ">* REST API wraps inference, handling requests and responses\n",
    "\n",
    ">* Define endpoint method, request body, and response\n",
    ">* Connect inference function and return predictions with metadata\n",
    "\n",
    ">* Optimize model loading, concurrency, and request validation\n",
    ">* Design API as reliable, production-ready prediction bridge\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c122b77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Building a REST API\n",
    "\n",
    "# This script shows a tiny REST style API example.\n",
    "# We simulate serving a model with a simple function.\n",
    "# Focus is on clear structure and beginner friendly code.\n",
    "\n",
    "# Required external libraries would be installed like this.\n",
    "# !pip install flask.\n",
    "\n",
    "# Import standard libraries for typing and randomness.\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "\n",
    "# Set a deterministic seed for reproducible behavior.\n",
    "random.seed(42)\n",
    "\n",
    "# Define a tiny pretend model using a simple function.\n",
    "def tiny_model_predict(x_values):\n",
    "    # Validate that input is a list of numbers.\n",
    "    if not isinstance(x_values, list):\n",
    "        raise TypeError(\"input must be list of numbers\")\n",
    "\n",
    "    # Check each element type for safety.\n",
    "    for value in x_values:\n",
    "        if not isinstance(value, (int, float)):\n",
    "            raise TypeError(\"all items must be numeric\")\n",
    "\n",
    "    # Apply a simple linear transformation as prediction.\n",
    "    predictions = [0.3 * v + 0.7 for v in x_values]\n",
    "    return predictions\n",
    "\n",
    "# Wrap preprocessing, model call, and postprocessing.\n",
    "def inference_pipeline(payload):\n",
    "    # Ensure payload is a dictionary like a JSON body.\n",
    "    if not isinstance(payload, dict):\n",
    "        raise TypeError(\"payload must be a dictionary\")\n",
    "\n",
    "    # Extract feature list with a safe default.\n",
    "    features = payload.get(\"features\", [])\n",
    "\n",
    "    # Validate that features list is not empty.\n",
    "    if len(features) == 0:\n",
    "        raise ValueError(\"features list must not be empty\")\n",
    "\n",
    "    # Record start time for simple latency measurement.\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Call the tiny model prediction function.\n",
    "    raw_predictions = tiny_model_predict(features)\n",
    "\n",
    "    # Convert raw predictions into rounded scores.\n",
    "    scores = [round(p, 3) for p in raw_predictions]\n",
    "\n",
    "    # Build a simple label based on a threshold.\n",
    "    labels = [\"high\" if s > 1.0 else \"low\" for s in scores]\n",
    "\n",
    "    # Compute elapsed time in milliseconds.\n",
    "    latency_ms = (time.time() - start_time) * 1000.0\n",
    "\n",
    "    # Build a structured response dictionary.\n",
    "    response = {\n",
    "        \"scores\": scores,\n",
    "        \"labels\": labels,\n",
    "        \"latency_ms\": round(latency_ms, 3),\n",
    "    }\n",
    "    return response\n",
    "\n",
    "# Simulate a minimal REST style handler function.\n",
    "def handle_predict_request(json_body):\n",
    "    # Parse JSON string into a Python dictionary.\n",
    "    try:\n",
    "        payload = json.loads(json_body)\n",
    "    except json.JSONDecodeError as exc:\n",
    "        return {\"error\": f\"invalid json: {exc}\"}\n",
    "\n",
    "    # Call inference pipeline and catch possible errors.\n",
    "    try:\n",
    "        result = inference_pipeline(payload)\n",
    "    except Exception as exc:\n",
    "        return {\"error\": str(exc)}\n",
    "\n",
    "    # Return successful result as a dictionary.\n",
    "    return {\"result\": result}\n",
    "\n",
    "# Build a small helper to measure throughput.\n",
    "def measure_throughput(example_body, num_requests):\n",
    "    # Ensure request count is a positive integer.\n",
    "    if not isinstance(num_requests, int) or num_requests <= 0:\n",
    "        raise ValueError(\"num_requests must be positive integer\")\n",
    "\n",
    "    # Record start time before the loop.\n",
    "    start = time.time()\n",
    "\n",
    "    # Send repeated requests through the handler.\n",
    "    for _ in range(num_requests):\n",
    "        _ = handle_predict_request(example_body)\n",
    "\n",
    "    # Compute total elapsed time in seconds.\n",
    "    total_time = time.time() - start\n",
    "\n",
    "    # Derive requests per second as throughput.\n",
    "    rps = num_requests / total_time if total_time > 0 else 0.0\n",
    "\n",
    "    # Return both total time and throughput.\n",
    "    return total_time, rps\n",
    "\n",
    "# Create a small example request body for testing.\n",
    "example_request = {\"features\": [0.0, 1.0, 2.0, 3.5]}\n",
    "\n",
    "# Convert example request to a JSON string.\n",
    "example_json = json.dumps(example_request)\n",
    "\n",
    "# Call the handler once to see a single response.\n",
    "single_response = handle_predict_request(example_json)\n",
    "\n",
    "# Measure throughput for a small batch of requests.\n",
    "num_test_requests = 50\n",
    "\n",
    "# Use the helper to compute timing statistics.\n",
    "elapsed, throughput = measure_throughput(example_json, num_test_requests)\n",
    "\n",
    "# Print a short summary of the simulated API behavior.\n",
    "print(\"Tiny REST style API simulation summary:\")\n",
    "print(\"Single response keys:\", list(single_response.keys()))\n",
    "print(\"Single result sample:\", single_response.get(\"result\"))\n",
    "print(\"Requests tested:\", num_test_requests)\n",
    "print(\"Total time seconds:\", round(elapsed, 4))\n",
    "print(\"Requests per second:\", round(throughput, 2))\n",
    "print(\"Average latency ms:\", round(1000.0 / throughput, 3))\n",
    "print(\"This demonstrates wrapping inference in an API handler.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b2784a",
   "metadata": {},
   "source": [
    "### **2.2. Batch Inference Workflows**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee80e39c",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_09/Lecture_B/image_02_02.jpg?v=1769779060\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Run models offline on large data batches\n",
    ">* Scripts handle I/O, preprocessing, and storage integration\n",
    "\n",
    ">* Load data, batch it, run model inference\n",
    ">* Store predictions, log progress, handle partial failures\n",
    "\n",
    ">* Schedule and scale batch jobs using parallel workers\n",
    ">* Ensure workflows are fault tolerant, traceable, auditable\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2727d577",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Batch Inference Workflows\n",
    "\n",
    "# This script shows simple batch inference workflows.\n",
    "# We simulate a PyTorch like model using TensorFlow.\n",
    "# Focus is on offline batch scoring from a file.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import csv\n",
    "import random\n",
    "\n",
    "# Import TensorFlow as lightweight model backend.\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Print TensorFlow version for environment clarity.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Define a tiny dense model for numeric features.\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(3,)),\n",
    "    tf.keras.layers.Dense(4, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "])\n",
    "\n",
    "# Compile model with simple optimizer and loss.\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")\n",
    "\n",
    "# Create a tiny synthetic training dataset.\n",
    "train_features = tf.constant([\n",
    "    [0.1, 0.2, 0.3],\n",
    "    [0.9, 0.8, 0.7],\n",
    "    [0.2, 0.1, 0.4],\n",
    "    [0.8, 0.9, 0.6],\n",
    "], dtype=tf.float32)\n",
    "\n",
    "# Create matching binary labels for training.\n",
    "train_labels = tf.constant([[0.0], [1.0], [0.0], [1.0]], dtype=tf.float32)\n",
    "\n",
    "# Train briefly with silent output for speed.\n",
    "model.fit(train_features, train_labels, epochs=20, verbose=0)\n",
    "\n",
    "# Define a simple preprocessing function for rows.\n",
    "def preprocess_row(row):\n",
    "    # Convert numeric strings to float features.\n",
    "    features = [float(row[\"f1\"]), float(row[\"f2\"]), float(row[\"f3\"])]\n",
    "    return tf.constant(features, dtype=tf.float32)\n",
    "\n",
    "\n",
    "# Define a postprocessing function for model outputs.\n",
    "def postprocess_scores(scores):\n",
    "    # Convert probabilities to labels with threshold.\n",
    "    labels = [1 if float(s) >= 0.5 else 0 for s in scores]\n",
    "    return labels\n",
    "\n",
    "\n",
    "# Define a batch inference function for many rows.\n",
    "def run_batch_inference(rows, batch_size=2):\n",
    "    # Collect preprocessed feature vectors.\n",
    "    features = [preprocess_row(r) for r in rows]\n",
    "    feature_tensor = tf.stack(features, axis=0)\n",
    "\n",
    "    # Validate feature tensor shape before prediction.\n",
    "    if feature_tensor.shape[1] != 3:\n",
    "        raise ValueError(\"Expected three features per row\")\n",
    "\n",
    "    # Run predictions in small batches for efficiency.\n",
    "    predictions = []\n",
    "    for start in range(0, len(rows), batch_size):\n",
    "        end = start + batch_size\n",
    "        batch = feature_tensor[start:end]\n",
    "        batch_scores = model.predict(batch, verbose=0)\n",
    "        predictions.extend(batch_scores[:, 0].tolist())\n",
    "\n",
    "    # Postprocess raw scores into integer labels.\n",
    "    labels = postprocess_scores(predictions)\n",
    "    return predictions, labels\n",
    "\n",
    "\n",
    "# Create a small CSV file representing offline data.\n",
    "input_path = \"batch_inputs.csv\"\n",
    "fieldnames = [\"id\", \"f1\", \"f2\", \"f3\"]\n",
    "\n",
    "# Write a few synthetic rows to the CSV file.\n",
    "with open(input_path, \"w\", newline=\"\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerow({\"id\": \"u1\", \"f1\": 0.1, \"f2\": 0.2, \"f3\": 0.3})\n",
    "    writer.writerow({\"id\": \"u2\", \"f1\": 0.9, \"f2\": 0.8, \"f3\": 0.7})\n",
    "    writer.writerow({\"id\": \"u3\", \"f1\": 0.4, \"f2\": 0.3, \"f3\": 0.2})\n",
    "    writer.writerow({\"id\": \"u4\", \"f1\": 0.7, \"f2\": 0.6, \"f3\": 0.9})\n",
    "\n",
    "\n",
    "# Read rows from the CSV file into memory.\n",
    "rows = []\n",
    "with open(input_path, newline=\"\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        rows.append(row)\n",
    "\n",
    "\n",
    "# Run batch inference on all loaded rows.\n",
    "raw_scores, labels = run_batch_inference(rows, batch_size=2)\n",
    "\n",
    "# Pair each input id with its prediction results.\n",
    "results = []\n",
    "for row, score, label in zip(rows, raw_scores, labels):\n",
    "    results.append({\n",
    "        \"id\": row[\"id\"],\n",
    "        \"score\": round(float(score), 4),\n",
    "        \"label\": int(label),\n",
    "    })\n",
    "\n",
    "\n",
    "# Print a short summary of batch inference results.\n",
    "print(\"Total rows processed:\", len(results))\n",
    "for r in results:\n",
    "    print(\"ID:\", r[\"id\"], \"Score:\", r[\"score\"], \"Label:\", r[\"label\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef78517",
   "metadata": {},
   "source": [
    "### **2.3. Robust Error Handling**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5333023",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_09/Lecture_B/image_02_03.jpg?v=1769779145\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* APIs need strong validation to prevent failures\n",
    ">* Return clear error messages instead of wrong predictions\n",
    "\n",
    ">* Separate client and server errors, respond appropriately\n",
    ">* Log server issues, isolate bad records in batches\n",
    "\n",
    ">* Log failures and recurring issues for observability\n",
    ">* Use clear errors, logging, and fallbacks for resilience\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0545713a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Robust Error Handling\n",
    "\n",
    "# This script shows robust error handling concepts.\n",
    "# We simulate a tiny model serving API example.\n",
    "# Focus is on validating inputs and handling failures.\n",
    "\n",
    "# Example uses only standard library modules.\n",
    "# No external installation is required here.\n",
    "# Uncomment installs if you extend this script.\n",
    "# pip install fastapi uvicorn.\n",
    "\n",
    "# Import required standard library modules.\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Set deterministic seed for reproducibility.\n",
    "random.seed(42)\n",
    "\n",
    "# Define a tiny dummy model inference function.\n",
    "def dummy_model_predict(features):\n",
    "    # Simulate simple numeric prediction logic.\n",
    "    score = sum(features) / max(len(features), 1)\n",
    "    return float(round(score, 3))\n",
    "\n",
    "# Validate that payload is a dictionary.\n",
    "def validate_is_dict(payload):\n",
    "    # Raise error if payload is not a dictionary.\n",
    "    if not isinstance(payload, dict):\n",
    "        raise TypeError(\"Payload must be a dictionary.\")\n",
    "\n",
    "# Validate required keys and types.\n",
    "def validate_required_fields(payload):\n",
    "    # Check that 'features' key exists and is a list.\n",
    "    if \"features\" not in payload:\n",
    "        raise KeyError(\"Missing required 'features' field.\")\n",
    "\n",
    "    # Ensure features is a list of numbers.\n",
    "    features = payload[\"features\"]\n",
    "    if not isinstance(features, list):\n",
    "        raise TypeError(\"'features' must be a list.\")\n",
    "\n",
    "    # Limit list length to avoid huge payloads.\n",
    "    if len(features) == 0 or len(features) > 1000:\n",
    "        raise ValueError(\"'features' length must be between one and 1000.\")\n",
    "\n",
    "    # Ensure each element is numeric.\n",
    "    for value in features:\n",
    "        if not isinstance(value, (int, float)):\n",
    "            raise TypeError(\"All feature values must be numeric.\")\n",
    "\n",
    "    return features\n",
    "\n",
    "# Simulate latency measurement for model call.\n",
    "def timed_model_call(features):\n",
    "    # Record start time before prediction.\n",
    "    start = time.perf_counter()\n",
    "    prediction = dummy_model_predict(features)\n",
    "    duration_ms = (time.perf_counter() - start) * 1000.0\n",
    "    return prediction, duration_ms\n",
    "\n",
    "# Build a robust inference handler function.\n",
    "def handle_inference_request(raw_body):\n",
    "    # Prepare base response structure.\n",
    "    response = {\"ok\": False, \"prediction\": None, \"error\": None}\n",
    "\n",
    "    try:\n",
    "        # Try to parse JSON body safely.\n",
    "        payload = json.loads(raw_body)\n",
    "\n",
    "        # Validate payload structure and content.\n",
    "        validate_is_dict(payload)\n",
    "        features = validate_required_fields(payload)\n",
    "\n",
    "        # Simulate occasional server side timeout.\n",
    "        if len(features) > 10 and random.random() < 0.1:\n",
    "            raise TimeoutError(\"Model backend timeout occurred.\")\n",
    "\n",
    "        # Run model prediction with timing.\n",
    "        prediction, latency_ms = timed_model_call(features)\n",
    "\n",
    "        # Build successful response body.\n",
    "        response[\"ok\"] = True\n",
    "        response[\"prediction\"] = prediction\n",
    "        response[\"latency_ms\"] = round(latency_ms, 3)\n",
    "\n",
    "    except json.JSONDecodeError as exc:\n",
    "        # Handle invalid JSON from client.\n",
    "        response[\"error\"] = {\n",
    "            \"type\": \"client_error\",\n",
    "            \"message\": \"Invalid JSON payload.\",\n",
    "        }\n",
    "\n",
    "    except (TypeError, KeyError, ValueError) as exc:\n",
    "        # Handle validation related client errors.\n",
    "        response[\"error\"] = {\n",
    "            \"type\": \"client_error\",\n",
    "            \"message\": str(exc),\n",
    "        }\n",
    "\n",
    "    except TimeoutError as exc:\n",
    "        # Handle simulated server side timeout.\n",
    "        response[\"error\"] = {\n",
    "            \"type\": \"server_error\",\n",
    "            \"message\": \"Temporary model timeout, retry later.\",\n",
    "        }\n",
    "\n",
    "    except Exception as exc:\n",
    "        # Catch unexpected server side failures.\n",
    "        response[\"error\"] = {\n",
    "            \"type\": \"server_error\",\n",
    "            \"message\": \"Unexpected internal error occurred.\",\n",
    "        }\n",
    "\n",
    "    # Return structured response dictionary.\n",
    "    return response\n",
    "\n",
    "# Prepare several example request bodies.\n",
    "example_requests = [\n",
    "    # Valid small request should succeed.\n",
    "    json.dumps({\"features\": [1.0, 2.0, 3.0]}),\n",
    "    # Invalid JSON string should trigger parse error.\n",
    "    \"{features: [1, 2, 3]}\",\n",
    "    # Missing features field should trigger key error.\n",
    "    json.dumps({\"values\": [1, 2, 3]}),\n",
    "    # Non numeric feature should trigger type error.\n",
    "    json.dumps({\"features\": [1, \"bad\", 3]}),\n",
    "]\n",
    "\n",
    "# Process each example request and print results.\n",
    "for idx, body in enumerate(example_requests, start=1):\n",
    "    # Call handler and capture structured response.\n",
    "    result = handle_inference_request(body)\n",
    "\n",
    "    # Print concise summary for each example.\n",
    "    print(f\"Example {idx} -> ok={result['ok']}, error={result['error']}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d141f695",
   "metadata": {},
   "source": [
    "## **3. Serving Performance Metrics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98dce398",
   "metadata": {},
   "source": [
    "### **3.1. Latency measurement**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb39897",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_09/Lecture_B/image_03_01.jpg?v=1769779224\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Latency is total request round-trip time\n",
    ">* Covers all pipeline stages where delays appear\n",
    "\n",
    ">* Define and log client and server latency\n",
    ">* Analyze averages and percentiles to spot slow requests\n",
    "\n",
    ">* Analyze latency patterns, variability, and external factors\n",
    ">* Use insights to apply simple performance optimizations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645bfc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Latency measurement\n",
    "\n",
    "# This script demonstrates simple latency measurement.\n",
    "# We simulate a tiny model and measure request times.\n",
    "# Focus on end to end timing for beginners.\n",
    "\n",
    "# Required install for TensorFlow in some environments.\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import standard timing and math utilities.\n",
    "import time\n",
    "import statistics\n",
    "import random\n",
    "\n",
    "# Import TensorFlow for a tiny example model.\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set deterministic random seeds for reproducibility.\n",
    "random.seed(0)\n",
    "\n",
    "# Print TensorFlow version in one concise line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Select CPU device for predictable performance here.\n",
    "tf.config.set_visible_devices([], \"GPU\")\n",
    "\n",
    "# Define a tiny dense model for demonstration.\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(4,)),\n",
    "    tf.keras.layers.Dense(4, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "])\n",
    "\n",
    "# Compile model with simple optimizer and loss.\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "\n",
    "# Create a tiny deterministic training dataset.\n",
    "train_x = tf.constant([[0.1, 0.2, 0.3, 0.4]], dtype=tf.float32)\n",
    "train_y = tf.constant([[0.0]], dtype=tf.float32)\n",
    "\n",
    "# Train briefly with silent output to initialize weights.\n",
    "model.fit(train_x, train_y, epochs=3, verbose=0)\n",
    "\n",
    "# Define a simple preprocessing function for inputs.\n",
    "def preprocess(raw_features):\n",
    "    # Convert list to tensor and normalize values.\n",
    "    tensor = tf.convert_to_tensor([raw_features], dtype=tf.float32)\n",
    "    return tensor / 1.0\n",
    "\n",
    "\n",
    "# Define a simple postprocessing function for outputs.\n",
    "def postprocess(raw_output):\n",
    "    # Convert tensor to float and round value.\n",
    "    value = float(raw_output.numpy()[0][0])\n",
    "    return round(value, 4)\n",
    "\n",
    "\n",
    "# Define an end to end inference function wrapper.\n",
    "def run_inference(raw_features):\n",
    "    # Validate input length for safety here.\n",
    "    if len(raw_features) != 4:\n",
    "        raise ValueError(\"Expected exactly four features.\")\n",
    "    # Apply preprocessing then model prediction.\n",
    "    inputs = preprocess(raw_features)\n",
    "    outputs = model(inputs, training=False)\n",
    "    # Apply postprocessing to raw model outputs.\n",
    "    return postprocess(outputs)\n",
    "\n",
    "\n",
    "# Measure latency for a single synthetic request.\n",
    "def measure_single_latency(features):\n",
    "    # Record start time just before sending request.\n",
    "    start = time.perf_counter()\n",
    "    _ = run_inference(features)\n",
    "    # Record end time right after receiving response.\n",
    "    end = time.perf_counter()\n",
    "    return (end - start) * 1000.0\n",
    "\n",
    "\n",
    "# Measure latency for many requests and collect stats.\n",
    "def measure_many_latencies(num_requests):\n",
    "    # Store individual latencies in a simple list.\n",
    "    latencies = []\n",
    "    for _ in range(num_requests):\n",
    "        # Use small random features for each request.\n",
    "        features = [random.random() for _ in range(4)]\n",
    "        latency_ms = measure_single_latency(features)\n",
    "        latencies.append(latency_ms)\n",
    "    # Validate we collected the expected number.\n",
    "    if len(latencies) != num_requests:\n",
    "        raise RuntimeError(\"Latency collection size mismatch.\")\n",
    "    return latencies\n",
    "\n",
    "\n",
    "# Compute basic latency statistics including percentiles.\n",
    "def summarize_latencies(latencies):\n",
    "    # Sort a copy to compute percentile values.\n",
    "    sorted_lat = sorted(latencies)\n",
    "    count = len(sorted_lat)\n",
    "    # Guard against empty input list here.\n",
    "    if count == 0:\n",
    "        raise ValueError(\"Latency list must not be empty.\")\n",
    "    # Helper to compute simple percentile index.\n",
    "    def percentile(p):\n",
    "        rank = int(round((p / 100.0) * (count - 1)))\n",
    "        return sorted_lat[rank]\n",
    "    # Build a dictionary of summary statistics.\n",
    "    summary = {\n",
    "        \"min_ms\": min(sorted_lat),\n",
    "        \"max_ms\": max(sorted_lat),\n",
    "        \"mean_ms\": statistics.mean(sorted_lat),\n",
    "        \"p50_ms\": percentile(50),\n",
    "        \"p95_ms\": percentile(95),\n",
    "        \"p99_ms\": percentile(99),\n",
    "    }\n",
    "    return summary\n",
    "\n",
    "\n",
    "# Run latency experiment with a small number of requests.\n",
    "num_requests = 30\n",
    "latencies = measure_many_latencies(num_requests)\n",
    "\n",
    "# Compute throughput as requests per second value.\n",
    "total_time_sec = sum(latencies) / 1000.0\n",
    "throughput_rps = num_requests / total_time_sec if total_time_sec > 0 else 0.0\n",
    "\n",
    "# Summarize latency distribution using helper function.\n",
    "summary = summarize_latencies(latencies)\n",
    "\n",
    "# Print concise latency statistics for interpretation.\n",
    "print(\"Requests:\", num_requests)\n",
    "print(\"Min latency ms:\", round(summary[\"min_ms\"], 4))\n",
    "print(\"P50 latency ms:\", round(summary[\"p50_ms\"], 4))\n",
    "print(\"P95 latency ms:\", round(summary[\"p95_ms\"], 4))\n",
    "print(\"P99 latency ms:\", round(summary[\"p99_ms\"], 4))\n",
    "print(\"Max latency ms:\", round(summary[\"max_ms\"], 4))\n",
    "print(\"Mean latency ms:\", round(summary[\"mean_ms\"], 4))\n",
    "print(\"Throughput req_per_sec:\", round(throughput_rps, 2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99744711",
   "metadata": {},
   "source": [
    "### **3.2. Throughput estimation basics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61cb792",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_09/Lecture_B/image_03_02.jpg?v=1769779304\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Throughput counts how many requests are handled\n",
    ">* Must consider full pipeline under concurrent load\n",
    "\n",
    ">* Throughput rises with concurrency by using hardware parallelism\n",
    ">* Beyond saturation, queues grow and latency explodes\n",
    "\n",
    ">* Run realistic load tests, measure requests per second\n",
    ">* Tune batch size, workers, resources for balanced throughput\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ddbb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Throughput estimation basics\n",
    "\n",
    "# This script explores basic throughput estimation concepts.\n",
    "# It simulates a tiny inference server handling fake requests.\n",
    "# We measure latency and throughput under different batch sizes.\n",
    "\n",
    "# Required install for TensorFlow if missing in environment.\n",
    "# !pip install tensorflow==2.20.0 --quiet.\n",
    "\n",
    "# Import standard libraries for timing and math.\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "\n",
    "# Import TensorFlow to simulate a lightweight model.\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set deterministic seeds for reproducible behavior.\n",
    "random.seed(0)\n",
    "\n",
    "# Print TensorFlow version in a single concise line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Choose CPU or GPU device based on availability.\n",
    "physical_gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "\n",
    "# Select device string for placing the fake model.\n",
    "device_name = \"/GPU:0\" if physical_gpus else \"/CPU:0\"\n",
    "\n",
    "# Create a tiny dense model to mimic inference cost.\n",
    "with tf.device(device_name):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(16,)),\n",
    "        tf.keras.layers.Dense(32, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(16, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(4, activation=\"softmax\"),\n",
    "    ])\n",
    "\n",
    "# Build model by running one dummy forward pass.\n",
    "dummy_input = tf.zeros((1, 16), dtype=tf.float32)\n",
    "\n",
    "# Run once to ensure any lazy initialization completes.\n",
    "_ = model(dummy_input)\n",
    "\n",
    "# Define a simple inference function with preprocessing.\n",
    "def run_inference(batch_inputs: tf.Tensor) -> tf.Tensor:\n",
    "    # Validate input rank and feature dimension.\n",
    "    if batch_inputs.ndim != 2 or batch_inputs.shape[1] != 16:\n",
    "        raise ValueError(\"Input must be [batch,16] tensor.\")\n",
    "\n",
    "    # Normalize inputs to mimic preprocessing step.\n",
    "    normalized = tf.clip_by_value(batch_inputs, 0.0, 1.0)\n",
    "\n",
    "    # Run model forward pass to obtain predictions.\n",
    "    outputs = model(normalized, training=False)\n",
    "\n",
    "    # Return probabilities as final postprocessed outputs.\n",
    "    return outputs\n",
    "\n",
    "# Helper function to measure latency and throughput.\n",
    "def benchmark_batch(batch_size: int, num_batches: int) -> dict:\n",
    "    # Create random inputs for the entire benchmark.\n",
    "    total_samples = batch_size * num_batches\n",
    "\n",
    "    # Build tensor of random floats in a small range.\n",
    "    inputs = tf.random.uniform((total_samples, 16), 0.0, 1.0)\n",
    "\n",
    "    # Warm up once to avoid cold start effects.\n",
    "    _ = run_inference(inputs[:batch_size])\n",
    "\n",
    "    # Start timing the full benchmark loop.\n",
    "    start = time.perf_counter()\n",
    "\n",
    "    # Process inputs in fixed size batches.\n",
    "    for i in range(num_batches):\n",
    "        batch = inputs[i * batch_size : (i + 1) * batch_size]\n",
    "        _ = run_inference(batch)\n",
    "\n",
    "    # Stop timing and compute elapsed seconds.\n",
    "    elapsed = time.perf_counter() - start\n",
    "\n",
    "    # Avoid division by zero in degenerate cases.\n",
    "    if elapsed <= 0.0:\n",
    "        elapsed = 1e-6\n",
    "\n",
    "    # Compute average latency per batch in milliseconds.\n",
    "    batch_latency_ms = (elapsed / num_batches) * 1000.0\n",
    "\n",
    "    # Compute average latency per sample in milliseconds.\n",
    "    sample_latency_ms = (elapsed / total_samples) * 1000.0\n",
    "\n",
    "    # Compute throughput as samples processed per second.\n",
    "    throughput_sps = total_samples / elapsed\n",
    "\n",
    "    # Return metrics in a small dictionary structure.\n",
    "    return {\n",
    "        \"batch_size\": batch_size,\n",
    "        \"num_batches\": num_batches,\n",
    "        \"batch_latency_ms\": batch_latency_ms,\n",
    "        \"sample_latency_ms\": sample_latency_ms,\n",
    "        \"throughput_sps\": throughput_sps,\n",
    "    }\n",
    "\n",
    "# Define different batch sizes to compare throughput behavior.\n",
    "batch_sizes = [1, 4, 16, 64]\n",
    "\n",
    "# Use a small number of batches to keep runtime short.\n",
    "num_batches = 40\n",
    "\n",
    "# Run benchmarks for each batch size and collect metrics.\n",
    "results = []\n",
    "for b in batch_sizes:\n",
    "    metrics = benchmark_batch(b, num_batches)\n",
    "    results.append(metrics)\n",
    "\n",
    "# Print a compact header explaining the upcoming metrics.\n",
    "print(\"Batch,AvgBatchMs,AvgSampleMs,ThroughputSamplesPerSec\")\n",
    "\n",
    "# Loop through results and print rounded metric values.\n",
    "for m in results:\n",
    "    line = (\n",
    "        f\"{m['batch_size']},\"\\\n",
    "        f\"{m['batch_latency_ms']:.2f},\"\\\n",
    "        f\"{m['sample_latency_ms']:.4f},\"\\\n",
    "        f\"{m['throughput_sps']:.1f}\"\n",
    "    )\n",
    "    print(line)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f3de0d",
   "metadata": {},
   "source": [
    "### **3.3. Caching for Faster Serving**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbf09c0",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_09/Lecture_B/image_03_03.jpg?v=1769779380\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Cache saves previous results to answer instantly\n",
    ">* Cold and warm cache responses affect latency\n",
    "\n",
    ">* Caching happens at application, infrastructure, server levels\n",
    ">* Stacked caches affect latency; measure best and worst\n",
    "\n",
    ">* Balance cache speed with memory, freshness, fairness\n",
    ">* Profile hit rates, latency, and tune policies\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ab0943",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Caching for Faster Serving\n",
    "\n",
    "# This script demonstrates simple caching concepts.\n",
    "# We simulate model serving with and without caching.\n",
    "# Focus on latency and throughput style measurements.\n",
    "\n",
    "# Required external installs would be placed here.\n",
    "# !pip install tensorflow.\n",
    "\n",
    "# Import standard timing and random modules.\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "\n",
    "# Set deterministic random seed for reproducibility.\n",
    "random.seed(42)\n",
    "\n",
    "# Define a fake expensive model inference function.\n",
    "def fake_model_inference(x_value: float) -> float:\n",
    "    # Simulate heavy computation using sleep.\n",
    "    time.sleep(0.01)\n",
    "    # Return a deterministic nonlinear transformation.\n",
    "    return math.tanh(x_value) * math.sin(x_value)\n",
    "\n",
    "# Define a simple cache dictionary for results.\n",
    "cache_store: dict[float, float] = {}\n",
    "\n",
    "# Define an inference function that uses caching.\n",
    "def cached_inference(x_value: float) -> float:\n",
    "    # Check if value already exists in cache.\n",
    "    if x_value in cache_store:\n",
    "        return cache_store[x_value]\n",
    "    # Compute result when cache miss occurs.\n",
    "    result_value = fake_model_inference(x_value)\n",
    "    # Store result in cache for future reuse.\n",
    "    cache_store[x_value] = result_value\n",
    "    return result_value\n",
    "\n",
    "# Generate a small workload with repeated values.\n",
    "workload_values: list[float] = []\n",
    "\n",
    "# Fill workload with many repeated and few unique values.\n",
    "for index_value in range(50):\n",
    "    # Use few popular values and some random ones.\n",
    "    if index_value % 5 == 0:\n",
    "        workload_values.append(0.5)\n",
    "    elif index_value % 5 == 1:\n",
    "        workload_values.append(1.0)\n",
    "    else:\n",
    "        workload_values.append(round(random.uniform(0.0, 2.0), 2))\n",
    "\n",
    "# Validate workload size before running experiments.\n",
    "if len(workload_values) != 50:\n",
    "    raise ValueError(\"Workload size must equal fifty entries\")\n",
    "\n",
    "# Measure latency without any caching enabled.\n",
    "start_no_cache = time.perf_counter()\n",
    "\n",
    "# Run fake model directly for each workload value.\n",
    "results_no_cache: list[float] = []\n",
    "for value_item in workload_values:\n",
    "    results_no_cache.append(fake_model_inference(value_item))\n",
    "\n",
    "# Compute total time for no cache scenario.\n",
    "end_no_cache = time.perf_counter()\n",
    "no_cache_seconds = end_no_cache - start_no_cache\n",
    "\n",
    "# Measure latency with caching enabled for same workload.\n",
    "start_with_cache = time.perf_counter()\n",
    "\n",
    "# Run cached inference for each workload value.\n",
    "results_with_cache: list[float] = []\n",
    "for value_item in workload_values:\n",
    "    results_with_cache.append(cached_inference(value_item))\n",
    "\n",
    "# Compute total time for cached scenario.\n",
    "end_with_cache = time.perf_counter()\n",
    "with_cache_seconds = end_with_cache - start_with_cache\n",
    "\n",
    "# Verify both result lists have identical values.\n",
    "if len(results_no_cache) != len(results_with_cache):\n",
    "    raise ValueError(\"Result lengths must match for comparison\")\n",
    "\n",
    "# Compute maximum absolute difference between results.\n",
    "max_difference = max(\n",
    "    abs(a_value - b_value)\n",
    "    for a_value, b_value in zip(results_no_cache, results_with_cache)\n",
    ")\n",
    "\n",
    "# Estimate simple throughput as requests per second.\n",
    "throughput_no_cache = len(workload_values) / no_cache_seconds\n",
    "throughput_with_cache = len(workload_values) / with_cache_seconds\n",
    "\n",
    "# Count how many cache entries were actually stored.\n",
    "cache_entries = len(cache_store)\n",
    "\n",
    "# Print concise summary of performance measurements.\n",
    "print(\"Requests processed:\", len(workload_values))\n",
    "print(\"Unique cached entries:\", cache_entries)\n",
    "print(\"Time without cache seconds:\", round(no_cache_seconds, 4))\n",
    "print(\"Time with cache seconds:\", round(with_cache_seconds, 4))\n",
    "print(\"Throughput without cache rps:\", round(throughput_no_cache, 1))\n",
    "print(\"Throughput with cache rps:\", round(throughput_with_cache, 1))\n",
    "print(\"Maximum result difference:\", max_difference)\n",
    "print(\"Cache speedup factor:\", round(no_cache_seconds / with_cache_seconds, 2))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86451efa",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Serving Models**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83cc184",
   "metadata": {},
   "source": [
    "\n",
    "In this lecture, you learned to:\n",
    "- Wrap a PyTorch model in a simple inference function that handles preprocessing and postprocessing. \n",
    "- Integrate the inference function into a lightweight REST API or batch inference script. \n",
    "- Evaluate latency and throughput of the serving setup and identify simple optimizations. \n",
    "\n",
    "In the next Module (Module 10), we will go over 'Capstone and Best Practices'"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
