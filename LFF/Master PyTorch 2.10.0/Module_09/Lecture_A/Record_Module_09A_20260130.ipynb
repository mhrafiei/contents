{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d82a9dcf",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Model Export**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad1583c",
   "metadata": {},
   "source": [
    ">Last update: 20260130.\n",
    "    \n",
    "By the end of this Lecture, you will be able to:\n",
    "- Describe the purpose and capabilities of torch.export in PyTorch 2.10.0. \n",
    "- Export a trained nn.Module to an exported program suitable for deployment or further compilation. \n",
    "- Inspect and validate exported models to ensure they produce consistent outputs with the original model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9006f635",
   "metadata": {},
   "source": [
    "## **1. Core Export Concepts**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44692f89",
   "metadata": {},
   "source": [
    "### **1.1. Eager vs Exported**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca702801",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_09/Lecture_A/image_01_01.jpg?v=1769774425\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Eager mode runs Python code dynamically each call\n",
    ">* Great for experimentation, but weak for stable deployment\n",
    "\n",
    ">* Exports eager models into static, Python-free graphs\n",
    ">* Creates stable contracts for deployment and tooling\n",
    "\n",
    ">* Eager mode favors flexible experimentation and debugging\n",
    ">* Exported models trade flexibility for stability and deployment\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09cae06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Eager vs Exported\n",
    "\n",
    "# This script compares eager and exported style behavior.\n",
    "# It uses simple functions to illustrate static computation.\n",
    "# Focus is on concepts not real torch.export internals.\n",
    "# Example import for numerical work if needed.\n",
    "import math\n",
    "\n",
    "# Define a simple eager style Python function.\n",
    "def eager_square_sum(x_list):\n",
    "    total = 0\n",
    "    for value in x_list:\n",
    "        total += value * value\n",
    "    return total\n",
    "\n",
    "# Define a tiny exported style representation.\n",
    "class ExportedSquareSum:\n",
    "    def __init__(self, length):\n",
    "        self.length = length\n",
    "\n",
    "    def run(self, x_list):\n",
    "        if len(x_list) != self.length:\n",
    "            raise ValueError(\"Unexpected input length for program\")\n",
    "        total = 0\n",
    "        for value in x_list:\n",
    "            total += value * value\n",
    "        return total\n",
    "\n",
    "# Helper function to print a short separator line.\n",
    "def print_separator(title):\n",
    "    print(\"\\n---\", title, \"---\")\n",
    "\n",
    "# Prepare a small deterministic input list.\n",
    "input_values = [1.0, 2.0, 3.0]\n",
    "\n",
    "# Validate the input length before using it.\n",
    "if len(input_values) <= 0:\n",
    "    raise ValueError(\"Input list must not be empty\")\n",
    "\n",
    "# Run the eager style computation.\n",
    "eager_result = eager_square_sum(input_values)\n",
    "\n",
    "# Create an exported style program with fixed length.\n",
    "exported_program = ExportedSquareSum(length=len(input_values))\n",
    "\n",
    "# Run the exported style computation.\n",
    "exported_result = exported_program.run(input_values)\n",
    "\n",
    "# Show that both approaches give the same numeric result.\n",
    "print_separator(\"Eager versus exported results\")\n",
    "print(\"Input values:\", input_values)\n",
    "print(\"Eager result:\", eager_result)\n",
    "print(\"Exported result:\", exported_result)\n",
    "\n",
    "# Demonstrate that exported program expects fixed length.\n",
    "short_input = [1.0, 2.0]\n",
    "\n",
    "# Try running with wrong length and handle the error.\n",
    "print_separator(\"Exported length check behavior\")\n",
    "try:\n",
    "    _ = exported_program.run(short_input)\n",
    "except ValueError as exc:\n",
    "    print(\"Exported program error:\", str(exc))\n",
    "\n",
    "# Final confirmation that eager still runs with any length.\n",
    "print(\"Eager with short input:\", eager_square_sum(short_input))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7618639",
   "metadata": {},
   "source": [
    "### **1.2. Graph Capture Constraints**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c38dd93",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_09/Lecture_A/image_01_02.jpg?v=1769774468\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Export turns flexible Python models into static graphs\n",
    ">* Graphs allow optimization but restrict Python features\n",
    "\n",
    ">* Control flow and shapes must be predictable\n",
    ">* Symbolic tensor logic works; opaque Python breaks export\n",
    "\n",
    ">* Export prefers pure tensor math without side effects\n",
    ">* Move logging and I/O outside the exported graph\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d432ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Graph Capture Constraints\n",
    "\n",
    "# This script illustrates graph capture constraints simply.\n",
    "# We simulate export friendly and unfriendly model behavior.\n",
    "# Focus on predictable control flow and side effects.\n",
    "\n",
    "# Required standard imports only, no extra installations.\n",
    "# !pip install torch.\n",
    "\n",
    "# Import random for deterministic branching demonstration.\n",
    "import random\n",
    "# Import os for potential environment based configuration.\n",
    "import os\n",
    "\n",
    "# Set deterministic seed for reproducible random behavior.\n",
    "random.seed(0)\n",
    "# Define a tiny tensor like structure using Python lists.\n",
    "small_tensor = [1.0, 2.0, 3.0]\n",
    "\n",
    "# Define a function that mimics pure tensor computation.\n",
    "def pure_computation(tensor_values, scale):\n",
    "    # Use predictable loop based only on tensor length.\n",
    "    result = []\n",
    "    # Iterate over values and apply simple scaling.\n",
    "    for value in tensor_values:\n",
    "        result.append(value * scale)\n",
    "    # Return new list without side effects.\n",
    "    return result\n",
    "\n",
    "# Define a function with Python side effects and randomness.\n",
    "def side_effect_computation(tensor_values):\n",
    "    # Mutate external state and use random branching.\n",
    "    global small_tensor\n",
    "    # Random choice makes control flow unpredictable.\n",
    "    if random.random() > 0.5:\n",
    "        small_tensor.append(99.0)\n",
    "    # Return original list without clear relation.\n",
    "    return tensor_values\n",
    "\n",
    "# Demonstrate predictable, export friendly style behavior.\n",
    "scale_factor = 2.0\n",
    "# Call pure computation that would be graph friendly.\n",
    "pure_output = pure_computation(small_tensor, scale_factor)\n",
    "\n",
    "# Demonstrate unpredictable, export unfriendly behavior.\n",
    "# Call side effect computation that changes global state.\n",
    "side_output = side_effect_computation(small_tensor)\n",
    "\n",
    "# Show original tensor values before any mutation.\n",
    "print(\"Initial small_tensor values:\", [1.0, 2.0, 3.0])\n",
    "# Show output from pure computation, stable and predictable.\n",
    "print(\"Pure computation output:\", pure_output)\n",
    "\n",
    "# Show tensor after potential mutation from side effects.\n",
    "print(\"small_tensor after side_effect:\", small_tensor)\n",
    "# Show side effect function output for comparison.\n",
    "print(\"Side effect computation output:\", side_output)\n",
    "\n",
    "# Explain why pure computation is export friendly.\n",
    "print(\"Pure path uses fixed loops and no side effects.\")\n",
    "# Explain why side effect path breaks graph capture.\n",
    "print(\"Side effect path mutates globals and uses randomness.\")\n",
    "\n",
    "# Final confirmation that shapes remain predictable for pure path.\n",
    "print(\"Length of pure_output is\", len(pure_output))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6967040",
   "metadata": {},
   "source": [
    "### **1.3. Supported export patterns**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de7d253",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_09/Lecture_A/image_01_03.jpg?v=1769774616\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Export supports many standard deep learning architectures\n",
    ">* Handles tensor-based control flow as unified graph\n",
    "\n",
    ">* Export handles dynamic shapes and flexible inputs\n",
    ">* One exported model serves many input sizes efficiently\n",
    "\n",
    ">* Export supports multi-stage pipelines across devices\n",
    ">* Preprocessing and postprocessing can live inside exports\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea981090",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Supported export patterns\n",
    "\n",
    "# This script shows supported export patterns simply.\n",
    "# We simulate export ideas without real PyTorch usage.\n",
    "# Focus on control flow and dynamic shapes conceptually.\n",
    "\n",
    "# Required external libraries would be installed here.\n",
    "# !pip install torch torchvision torchaudio.\n",
    "\n",
    "# Import standard modules for numerical work.\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Set deterministic random seed for reproducibility.\n",
    "random.seed(0)\n",
    "\n",
    "# Define a simple feedforward like model using functions.\n",
    "def simple_feedforward(x_list):\n",
    "    # Apply a linear like transform to each element.\n",
    "    return [2.0 * x + 1.0 for x in x_list]\n",
    "\n",
    "# Define a model with tensor like control flow branching.\n",
    "def branching_model(x_list, threshold):\n",
    "    # Choose path depending on average value condition.\n",
    "    avg = sum(x_list) / float(len(x_list))\n",
    "    if avg > threshold:\n",
    "        # If high average, apply feedforward twice.\n",
    "        return simple_feedforward(simple_feedforward(x_list))\n",
    "    else:\n",
    "        # If low average, apply feedforward once only.\n",
    "        return simple_feedforward(x_list)\n",
    "\n",
    "# Define a loop based sequence style processing model.\n",
    "def loop_sequence_model(x_list, max_steps):\n",
    "    # Start with an empty list for outputs.\n",
    "    outputs = []\n",
    "    step = 0\n",
    "    # Iterate until max steps or list end reached.\n",
    "    while step < max_steps and step < len(x_list):\n",
    "        value = x_list[step]\n",
    "        outputs.append(value * 0.5)\n",
    "        step += 1\n",
    "    return outputs\n",
    "\n",
    "# Define a dynamic shape friendly wrapper function.\n",
    "def dynamic_shape_pipeline(x_list, threshold, max_steps):\n",
    "    # Validate input is a non empty list.\n",
    "    if not isinstance(x_list, list) or len(x_list) == 0:\n",
    "        raise ValueError(\"x_list must be non empty list\")\n",
    "    # Apply branching model to full list first.\n",
    "    branch_out = branching_model(x_list, threshold)\n",
    "    # Then apply loop model to branching output.\n",
    "    loop_out = loop_sequence_model(branch_out, max_steps)\n",
    "    return loop_out\n",
    "\n",
    "# Define a tiny feature extraction like stage.\n",
    "def feature_extractor(raw_values):\n",
    "    # Normalize values using simple min max scaling.\n",
    "    min_v = min(raw_values)\n",
    "    max_v = max(raw_values)\n",
    "    if max_v == min_v:\n",
    "        return [0.0 for _ in raw_values]\n",
    "    return [(v - min_v) / (max_v - min_v) for v in raw_values]\n",
    "\n",
    "# Define a tiny prediction head consuming extracted features.\n",
    "def prediction_head(features, bias):\n",
    "    # Compute a simple score using sum and bias.\n",
    "    score = sum(features) + bias\n",
    "    # Apply thresholding to get binary decision.\n",
    "    return 1 if score > 1.5 else 0\n",
    "\n",
    "# Define a pipeline combining two exported style stages.\n",
    "def two_stage_pipeline(raw_values, threshold, max_steps, bias):\n",
    "    # First stage performs feature extraction step.\n",
    "    feats = feature_extractor(raw_values)\n",
    "    # Second stage runs dynamic shape pipeline.\n",
    "    processed = dynamic_shape_pipeline(feats, threshold, max_steps)\n",
    "    # Final stage applies prediction head decision.\n",
    "    decision = prediction_head(processed, bias)\n",
    "    return decision, feats, processed\n",
    "\n",
    "# Prepare two different dynamic length input examples.\n",
    "example_short = [0.2, 0.4, 0.6]\n",
    "example_long = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "\n",
    "# Run pipeline on short example with parameters.\n",
    "short_decision, short_feats, short_proc = two_stage_pipeline(\n",
    "    example_short,\n",
    "    threshold=0.4,\n",
    "    max_steps=5,\n",
    "    bias=0.1,\n",
    ")\n",
    "\n",
    "# Run pipeline on long example with different parameters.\n",
    "long_decision, long_feats, long_proc = two_stage_pipeline(\n",
    "    example_long,\n",
    "    threshold=0.3,\n",
    "    max_steps=3,\n",
    "    bias=0.2,\n",
    ")\n",
    "\n",
    "# Print framework placeholder version information.\n",
    "print(\"PyTorch version placeholder for export concepts.\")\n",
    "\n",
    "# Show that dynamic shapes still follow same pipeline.\n",
    "print(\"Short input length and decision:\", len(example_short), short_decision)\n",
    "\n",
    "# Show processed length for short dynamic example.\n",
    "print(\"Short processed length:\", len(short_proc))\n",
    "\n",
    "# Show that longer input reuses same computation pattern.\n",
    "print(\"Long input length and decision:\", len(example_long), long_decision)\n",
    "\n",
    "# Show processed length for long dynamic example.\n",
    "print(\"Long processed length:\", len(long_proc))\n",
    "\n",
    "# Confirm branching behavior by printing one internal average.\n",
    "print(\"Average of short features:\", sum(short_feats) / len(short_feats))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b64091",
   "metadata": {},
   "source": [
    "## **2. Exporting Models with torchexport**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85adfe8",
   "metadata": {},
   "source": [
    "### **2.1. Defining example inputs**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a3ac0b",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_09/Lecture_A/image_02_01.jpg?v=1769774690\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Example inputs must mirror real deployment data\n",
    ">* Accurate examples ensure complete, robust exported models\n",
    "\n",
    ">* Capture real input variability and structure carefully\n",
    ">* Match example inputs to true production data flows\n",
    "\n",
    ">* Include edge cases and hardware limits in examples\n",
    ">* Use challenging scenarios to expose hidden model issues\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacb21c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Defining example inputs\n",
    "\n",
    "# This script shows defining example inputs.\n",
    "# We use TensorFlow to mimic model behavior.\n",
    "# Focus on shapes and simple consistency checks.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Import TensorFlow and Keras layers.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Define a tiny image classifier model.\n",
    "inputs = keras.Input(shape=(28, 28, 1))\n",
    "x = layers.Conv2D(4, (3, 3), activation=\"relu\")(inputs)\n",
    "x = layers.Flatten()(x)\n",
    "outputs = layers.Dense(10, activation=\"softmax\")(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Compile model with simple settings.\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\")\n",
    "\n",
    "# Create a realistic example input batch.\n",
    "batch_size = 2\n",
    "height, width, channels = 28, 28, 1\n",
    "example_images = np.random.rand(\n",
    "    batch_size,\n",
    "    height,\n",
    "    width,\n",
    "    channels,\n",
    ").astype(\"float32\")\n",
    "\n",
    "# Validate example input shape and dtype.\n",
    "print(\"Example batch shape:\", example_images.shape)\n",
    "print(\"Example batch dtype:\", example_images.dtype)\n",
    "\n",
    "# Run the model once on example inputs.\n",
    "example_outputs = model.predict(example_images, verbose=0)\n",
    "\n",
    "# Check that output shape matches expectations.\n",
    "print(\"Output batch shape:\", example_outputs.shape)\n",
    "\n",
    "# Show first prediction vector length only.\n",
    "print(\"First prediction length:\", len(example_outputs[0]))\n",
    "\n",
    "# Demonstrate a mismatched example input shape.\n",
    "wrong_example = np.random.rand(batch_size, height, width).astype(\"float32\")\n",
    "\n",
    "# Try calling model with wrong shape inside try block.\n",
    "try:\n",
    "    model.predict(wrong_example, verbose=0)\n",
    "except Exception as e:\n",
    "    print(\"Mismatched example error type:\", type(e).__name__)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3bc927",
   "metadata": {},
   "source": [
    "### **2.2. Exporting Neural Networks**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9fb8fb",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_09/Lecture_A/image_02_02.jpg?v=1769774856\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Export freezes the trained model into stable form\n",
    ">* Creates a static graph for optimization and deployment\n",
    "\n",
    ">* Export splits computation graph from fixed parameters\n",
    ">* Same graph reused with different tuned parameter sets\n",
    "\n",
    ">* Exported models must satisfy strict deployment constraints\n",
    ">* Exporting simplifies, stabilizes, and shapes models for portability\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b43d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Exporting Neural Networks\n",
    "\n",
    "# This script shows a tiny neural network export example.\n",
    "# We simulate exporting by saving weights and structure description.\n",
    "# Focus is on concept not heavy training or big data.\n",
    "\n",
    "# Required external installs would be listed here if needed.\n",
    "# No extra installs are required for this simple example.\n",
    "\n",
    "# Import standard libraries for numerical work and reproducibility.\n",
    "import numpy as np\n",
    "import random as pyrandom\n",
    "\n",
    "# Import TensorFlow as our simple neural network framework.\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set deterministic seeds for reproducible behavior and outputs.\n",
    "pyrandom.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# Set TensorFlow random seed for deterministic initialization.\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "# Print TensorFlow version in one concise line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Create a tiny synthetic dataset for demonstration only.\n",
    "inputs = np.linspace(-1.0, 1.0, num=8, dtype=np.float32)\n",
    "\n",
    "# Reshape inputs to match dense layer expected input shape.\n",
    "inputs = inputs.reshape((-1, 1))\n",
    "\n",
    "# Create simple targets using a linear relationship with noise.\n",
    "noise = np.random.normal(loc=0.0, scale=0.05, size=inputs.shape)\n",
    "\n",
    "# Compute targets as two times input plus small noise term.\n",
    "targets = 2.0 * inputs + noise.astype(np.float32)\n",
    "\n",
    "# Define a very small sequential neural network model.\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(1,)),\n",
    "    tf.keras.layers.Dense(units=1, activation=None),\n",
    "])\n",
    "\n",
    "# Compile the model with mean squared error loss function.\n",
    "model.compile(optimizer=\"sgd\", loss=\"mse\")\n",
    "\n",
    "# Train briefly with silent output to keep logs minimal.\n",
    "model.fit(inputs, targets, epochs=50, verbose=0)\n",
    "\n",
    "# Run the trained model on the inputs to get reference outputs.\n",
    "original_outputs = model.predict(inputs, verbose=0)\n",
    "\n",
    "# Validate shapes to ensure safe export operations.\n",
    "assert original_outputs.shape == targets.shape\n",
    "\n",
    "# Prepare a simple dictionary describing model structure.\n",
    "export_structure = {\n",
    "    \"input_shape\": list(model.input_shape[1:]),\n",
    "    \"output_shape\": list(model.output_shape[1:]),\n",
    "    \"layers\": [\"Dense(1, linear)\"],\n",
    "}\n",
    "\n",
    "# Extract trained weights and biases from the dense layer.\n",
    "weights, biases = model.layers[0].get_weights()\n",
    "\n",
    "# Convert weights and biases to lists for JSON friendly export.\n",
    "weights_list = weights.tolist()\n",
    "\n",
    "# Convert biases to list for consistent simple serialization.\n",
    "biases_list = biases.tolist()\n",
    "\n",
    "# Bundle structure and parameters into one export artifact object.\n",
    "export_artifact = {\n",
    "    \"structure\": export_structure,\n",
    "    \"weights\": weights_list,\n",
    "    \"biases\": biases_list,\n",
    "}\n",
    "\n",
    "# Rebuild a new model using the exported structure description.\n",
    "reloaded_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=tuple(export_artifact[\"structure\"][\"input_shape\"])),\n",
    "    tf.keras.layers.Dense(units=1, activation=None),\n",
    "])\n",
    "\n",
    "# Set the reloaded model weights from the exported parameters.\n",
    "reloaded_model.layers[0].set_weights([\n",
    "    np.array(export_artifact[\"weights\"], dtype=np.float32),\n",
    "    np.array(export_artifact[\"biases\"], dtype=np.float32),\n",
    "])\n",
    "\n",
    "# Run the reloaded model on the same inputs for comparison.\n",
    "reloaded_outputs = reloaded_model.predict(inputs, verbose=0)\n",
    "\n",
    "# Compute maximum absolute difference between outputs for validation.\n",
    "max_diff = np.max(np.abs(original_outputs - reloaded_outputs))\n",
    "\n",
    "# Print a short summary of export and reload consistency.\n",
    "print(\"Original outputs shape:\", original_outputs.shape)\n",
    "print(\"Reloaded outputs shape:\", reloaded_outputs.shape)\n",
    "print(\"Maximum absolute difference:\", float(max_diff))\n",
    "print(\"Export structure description:\", export_structure)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1bb73c",
   "metadata": {},
   "source": [
    "### **2.3. Persisting Exported Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5f0f7d",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_09/Lecture_A/image_02_03.jpg?v=1769774923\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Save exported models as reusable serialized artifacts\n",
    ">* Ensures deployed behavior matches tested training environment\n",
    "\n",
    ">* Use naming, metadata, versions to organize models\n",
    ">* Track model history for debugging, audits, compliance\n",
    "\n",
    ">* Persisted exports run across many systems and tools\n",
    ">* They enable portable, consistent, production-ready deployments\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4368b1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Persisting Exported Models\n",
    "\n",
    "# This script shows persisting exported style models.\n",
    "# We simulate export using simple TensorFlow SavedModel.\n",
    "# Focus is on saving loading and validating outputs.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import pathlib\n",
    "import random\n",
    "\n",
    "# Import numpy for small numeric operations.\n",
    "import numpy as np\n",
    "\n",
    "# Import tensorflow as lightweight model framework.\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version in one concise line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Define a tiny dense model for demonstration.\n",
    "model = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Input(shape=(4,)),\n",
    "        tf.keras.layers.Dense(3, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(1, activation=\"linear\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Compile model with simple optimizer and loss.\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "\n",
    "# Create tiny deterministic training data batch.\n",
    "x_train = np.array([[0.1, 0.2, 0.3, 0.4]], dtype=np.float32)\n",
    "y_train = np.array([[0.5]], dtype=np.float32)\n",
    "\n",
    "# Validate shapes before training for safety.\n",
    "assert x_train.shape == (1, 4)\n",
    "assert y_train.shape == (1, 1)\n",
    "\n",
    "# Train for few epochs with silent verbose setting.\n",
    "model.fit(x_train, y_train, epochs=10, verbose=0)\n",
    "\n",
    "# Create a small sample input for export testing.\n",
    "sample_input = np.array([[0.9, 0.8, 0.1, 0.2]], dtype=np.float32)\n",
    "\n",
    "# Validate sample input shape before inference.\n",
    "assert sample_input.shape == (1, 4)\n",
    "\n",
    "# Run original model to get reference prediction.\n",
    "original_output = model.predict(sample_input, verbose=0)\n",
    "\n",
    "# Print original model prediction for comparison.\n",
    "print(\"Original model output:\", original_output.flatten())\n",
    "\n",
    "# Choose directory path for persisted exported model.\n",
    "export_dir = pathlib.Path(\"exported_model_demo\")\n",
    "\n",
    "# Remove existing directory if present for cleanliness.\n",
    "if export_dir.exists():\n",
    "    for item in export_dir.iterdir():\n",
    "        if item.is_file():\n",
    "            item.unlink()\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "# Save model as a stable SavedModel artifact.\n",
    "model.export(export_dir)\n",
    "\n",
    "# Confirm directory exists after saving operation.\n",
    "print(\"Saved model directory exists:\", export_dir.exists())\n",
    "\n",
    "# Load model back to simulate deployment environment.\n",
    "loaded_model = tf.keras.layers.TFSMLayer(str(export_dir), call_endpoint='serving_default')\n",
    "\n",
    "# Run loaded model on same sample input.\n",
    "loaded_output = loaded_model(sample_input)\n",
    "\n",
    "# If the loaded model returns a dict, extract the single tensor for comparison.\n",
    "if isinstance(loaded_output, dict):\n",
    "    # take first value in dict (SavedModel default output)\n",
    "    loaded_output = next(iter(loaded_output.values()))\n",
    "\n",
    "# Compare outputs using small numeric tolerance.\n",
    "are_close = np.allclose(original_output, loaded_output.numpy(), atol=1e-6)\n",
    "\n",
    "# Print comparison result and both outputs briefly.\n",
    "print(\"Loaded model output:\", loaded_output.numpy().flatten())\n",
    "\n",
    "# Print final validation line showing persistence success.\n",
    "print(\"Outputs match after persistence:\", bool(are_close))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e241146",
   "metadata": {},
   "source": [
    "## **3. Model Validation Checks**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fa5420",
   "metadata": {},
   "source": [
    "### **3.1. Output Consistency Checks**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61196d0",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_09/Lecture_A/image_03_01.jpg?v=1769775036\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Compare original and exported models on inputs\n",
    ">* Check predictions and numbers match within acceptable tolerance\n",
    "\n",
    ">* Use diverse, realistic test inputs for comparison\n",
    ">* Measure differences; investigate large or systematic deviations\n",
    "\n",
    ">* Set strict or relaxed tolerances by domain\n",
    ">* Investigate large outliers to find export issues\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4664136",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Output Consistency Checks\n",
    "\n",
    "# This script demonstrates output consistency checks.\n",
    "# We compare original and exported TensorFlow models.\n",
    "# Focus is on simple beginner friendly validation.\n",
    "\n",
    "# !pip install tensorflow.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Import TensorFlow and Keras utilities.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Print TensorFlow version briefly.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Set deterministic random seeds.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "\n",
    "# Set NumPy random seed for reproducibility.\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# Set TensorFlow random seed for reproducibility.\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Load MNIST dataset from Keras.\n",
    "(x_train, y_train), _ = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Select a small subset for quick training.\n",
    "x_train_small = x_train[:2000]\n",
    "\n",
    "# Select corresponding labels subset.\n",
    "y_train_small = y_train[:2000]\n",
    "\n",
    "# Normalize images to range zero one.\n",
    "x_train_small = x_train_small.astype(\"float32\") / 255.0\n",
    "\n",
    "# Add channel dimension for convolution.\n",
    "x_train_small = np.expand_dims(x_train_small, axis=-1)\n",
    "\n",
    "# Validate input shape before modeling.\n",
    "assert x_train_small.shape[1:] == (28, 28, 1)\n",
    "\n",
    "# Build a simple sequential model.\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Input(shape=(28, 28, 1)),\n",
    "    keras.layers.Conv2D(8, (3, 3), activation=\"relu\"),\n",
    "    keras.layers.MaxPooling2D((2, 2)),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(32, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "# Compile model with standard settings.\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Train briefly with silent output.\n",
    "model.fit(\n",
    "    x_train_small,\n",
    "    y_train_small,\n",
    "    epochs=2,\n",
    "    batch_size=64,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Pick a small batch for validation.\n",
    "validation_inputs = x_train_small[100:110]\n",
    "\n",
    "# Confirm validation batch shape.\n",
    "assert validation_inputs.shape[0] == 10\n",
    "\n",
    "# Run original model to get predictions.\n",
    "original_outputs = model.predict(\n",
    "    validation_inputs,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Export model using SavedModel format.\n",
    "export_dir = \"mnist_export_demo.keras\"\n",
    "\n",
    "# Remove existing export directory safely.\n",
    "if tf.io.gfile.exists(export_dir):\n",
    "    tf.io.gfile.rmtree(export_dir)\n",
    "\n",
    "# Save the trained model for deployment.\n",
    "model.save(export_dir, include_optimizer=False)\n",
    "\n",
    "# Load exported model from disk.\n",
    "exported_model = keras.models.load_model(export_dir)\n",
    "\n",
    "# Run exported model on same inputs.\n",
    "exported_outputs = exported_model.predict(\n",
    "    validation_inputs,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Ensure output shapes match exactly.\n",
    "assert original_outputs.shape == exported_outputs.shape\n",
    "\n",
    "# Compute absolute differences per example.\n",
    "abs_diff = np.abs(original_outputs - exported_outputs)\n",
    "\n",
    "# Compute maximum difference across all outputs.\n",
    "max_diff = float(np.max(abs_diff))\n",
    "\n",
    "# Compute mean difference across all outputs.\n",
    "mean_diff = float(np.mean(abs_diff))\n",
    "\n",
    "# Choose a simple tolerance threshold.\n",
    "tolerance = 1e-5\n",
    "\n",
    "# Check if all differences are within tolerance.\n",
    "all_close = bool(max_diff <= tolerance)\n",
    "\n",
    "# Print summary of consistency results.\n",
    "print(\"Max difference between outputs:\", max_diff)\n",
    "\n",
    "# Print mean absolute difference value.\n",
    "print(\"Mean difference between outputs:\", mean_diff)\n",
    "\n",
    "# Print chosen tolerance threshold.\n",
    "print(\"Tolerance used for comparison:\", tolerance)\n",
    "\n",
    "# Print final consistency decision.\n",
    "print(\"Are outputs consistent within tolerance?\", all_close)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d3be76",
   "metadata": {},
   "source": [
    "### **3.2. Shape and dtype checks**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c1fecd",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_09/Lecture_A/image_03_02.jpg?v=1769775120\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Check tensor shapes and dtypes match originals\n",
    ">* Mismatches cause subtle bugs; verify before deployment\n",
    "\n",
    ">* Check shapes and dtypes at model boundaries\n",
    ">* Compare original and exported outputs with real inputs\n",
    "\n",
    ">* Test dynamic shapes across realistic input ranges\n",
    ">* Continuously verify dtype changes donâ€™t break assumptions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e641e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Shape and dtype checks\n",
    "\n",
    "# This script shows simple shape and dtype checks.\n",
    "# We simulate original and exported models using TensorFlow.\n",
    "# Focus on comparing outputs for safety in deployment.\n",
    "\n",
    "# Uncomment the next line if tensorflow is not installed.\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Import tensorflow and set a visible version print.\n",
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "seed_value = 7\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Define a tiny original model for demonstration.\n",
    "original_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(4,), dtype=tf.float32),\n",
    "    tf.keras.layers.Dense(3, activation=\"relu\"),\n",
    "])\n",
    "\n",
    "# Build the model by running one dummy batch.\n",
    "dummy_input = tf.zeros((1, 4), dtype=tf.float32)\n",
    "_ = original_model(dummy_input)\n",
    "\n",
    "# Create a simple exported model copy for comparison.\n",
    "exported_model = tf.keras.models.clone_model(original_model)\n",
    "_ = exported_model(dummy_input)\n",
    "\n",
    "# Define a helper to run a model safely.\n",
    "def run_model_safely(model, x_batch):\n",
    "    # Validate rank and last dimension before running.\n",
    "    if x_batch.ndim != 2:\n",
    "        raise ValueError(\"Input must be rank 2 batch tensor\")\n",
    "    if x_batch.shape[1] != 4:\n",
    "        raise ValueError(\"Second dimension must be size four\")\n",
    "    return model(x_batch)\n",
    "\n",
    "\n",
    "# Create a small batch of representative test inputs.\n",
    "inputs = tf.constant([[1.0, 2.0, 3.0, 4.0],\n",
    "                      [0.5, 0.0, -1.0, 2.0]],\n",
    "                     dtype=tf.float32)\n",
    "\n",
    "# Run both models on the same inputs.\n",
    "orig_out = run_model_safely(original_model, inputs)\n",
    "exp_out = run_model_safely(exported_model, inputs)\n",
    "\n",
    "# Check and print input shape and dtype information.\n",
    "print(\"Input shape:\", inputs.shape, \"dtype:\", inputs.dtype)\n",
    "\n",
    "# Check and print original model output shape and dtype.\n",
    "print(\"Original output shape:\", orig_out.shape,\n",
    "      \"dtype:\", orig_out.dtype)\n",
    "\n",
    "# Check and print exported model output shape and dtype.\n",
    "print(\"Exported output shape:\", exp_out.shape,\n",
    "      \"dtype:\", exp_out.dtype)\n",
    "\n",
    "# Compare shapes and dtypes for consistency.\n",
    "shapes_match = orig_out.shape == exp_out.shape\n",
    "dtypes_match = orig_out.dtype == exp_out.dtype\n",
    "\n",
    "# Print a short validation summary for the student.\n",
    "print(\"Shapes match?\", bool(shapes_match))\n",
    "print(\"Dtypes match?\", bool(dtypes_match))\n",
    "\n",
    "# Also check that values are numerically close enough.\n",
    "max_diff = tf.reduce_max(tf.abs(orig_out - exp_out)).numpy()\n",
    "print(\"Maximum absolute difference between outputs:\", float(max_diff))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ad788b",
   "metadata": {},
   "source": [
    "### **3.3. Debugging Export Failures**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f2d73b",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_09/Lecture_A/image_03_03.jpg?v=1769775172\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Treat export issues as structured debugging tasks\n",
    ">* Find dynamic behaviors and refactor model or inputs\n",
    "\n",
    ">* Compare original and exported models on simple inputs\n",
    ">* Change one factor to find failing conditions\n",
    "\n",
    ">* Refactor dynamic or unsupported patterns into exportable forms\n",
    ">* Re-export, compare outputs, and iterate for reliability\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ece21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Debugging Export Failures\n",
    "\n",
    "# This script shows debugging export failures.\n",
    "# We simulate export and compare model behaviors.\n",
    "# Focus on simple checks and clear printed messages.\n",
    "\n",
    "# TensorFlow is available by default in this environment.\n",
    "# We only use standard library and tensorflow here.\n",
    "# No extra installations are required for this script.\n",
    "\n",
    "# Import required modules for the demonstration.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set deterministic seeds for reproducible behavior.\n",
    "seed_value = 7\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version as framework information.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Define a tiny model with a risky dynamic branch.\n",
    "class DynamicBranchModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dense_small = tf.keras.layers.Dense(2)\n",
    "        self.dense_large = tf.keras.layers.Dense(2)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        mean_value = tf.reduce_mean(inputs, axis=-1, keepdims=True)\n",
    "        condition = mean_value > 0.0\n",
    "        branch_small = self.dense_small(inputs)\n",
    "        branch_large = self.dense_large(inputs)\n",
    "        return tf.where(condition, branch_large, branch_small)\n",
    "\n",
    "\n",
    "# Create a simple wrapper that mimics an exported program.\n",
    "class ExportedProgramWrapper:\n",
    "    def __init__(self, original_model):\n",
    "        self.model = original_model\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        if inputs.shape[-1] != 3:\n",
    "            raise ValueError(\"Exported program expects last dimension three\")\n",
    "        return self.model(inputs, training=False)\n",
    "\n",
    "\n",
    "# Build model and exported wrapper with a dummy call.\n",
    "model = DynamicBranchModel()\n",
    "dummy_input = tf.zeros((1, 3), dtype=tf.float32)\n",
    "_ = model(dummy_input, training=False)\n",
    "exported_program = ExportedProgramWrapper(model)\n",
    "\n",
    "\n",
    "# Define a helper to compare original and exported outputs.\n",
    "def compare_models(inputs, description):\n",
    "    original_out = model(inputs, training=False)\n",
    "    exported_out = exported_program(inputs)\n",
    "    diff = tf.reduce_max(tf.abs(original_out - exported_out))\n",
    "    print(description, \"max difference:\", float(diff))\n",
    "\n",
    "\n",
    "# Create a valid input that matches exported expectations.\n",
    "valid_input = tf.constant([[0.1, -0.2, 0.3]], dtype=tf.float32)\n",
    "print(\"Valid input shape:\", valid_input.shape)\n",
    "compare_models(valid_input, \"Valid input\")\n",
    "\n",
    "\n",
    "# Create an invalid input to trigger a failure scenario.\n",
    "invalid_input = tf.constant([[0.1, -0.2, 0.3]], dtype=tf.float32)\n",
    "print(\"Invalid input shape:\", invalid_input.shape)\n",
    "\n",
    "# Try running exported program and catch the raised error.\n",
    "try:\n",
    "    _ = exported_program(invalid_input)\n",
    "except ValueError as error:\n",
    "    print(\"Caught export style error:\", str(error))\n",
    "\n",
    "# Show that original model still runs but shapes now differ.\n",
    "original_invalid_out = model(invalid_input, training=False)\n",
    "print(\"Original model output shape:\", original_invalid_out.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3079c796",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Model Export**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b516698b",
   "metadata": {},
   "source": [
    "\n",
    "In this lecture, you learned to:\n",
    "- Describe the purpose and capabilities of torch.export in PyTorch 2.10.0. \n",
    "- Export a trained nn.Module to an exported program suitable for deployment or further compilation. \n",
    "- Inspect and validate exported models to ensure they produce consistent outputs with the original model. \n",
    "\n",
    "In the next Lecture (Lecture B), we will go over 'Serving Models'"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
