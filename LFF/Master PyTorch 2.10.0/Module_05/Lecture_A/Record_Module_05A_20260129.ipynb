{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b52992b",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**CNNs from Scratch**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fba3bf",
   "metadata": {},
   "source": [
    ">Last update: 20260129.\n",
    "    \n",
    "By the end of this Lecture, you will be able to:\n",
    "- Construct a convolutional neural network using Conv2d, pooling, and fully connected layers in PyTorch. \n",
    "- Train the CNN on a small image dataset using the standard training loop and appropriate loss and metrics. \n",
    "- Analyze model performance using accuracy, confusion matrices, and simple error inspection. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec40c8d",
   "metadata": {},
   "source": [
    "## **1. Building CNN Layers**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca0ff80",
   "metadata": {},
   "source": [
    "### **1.1. Convolution Layers Essentials**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e175fdc7",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_05/Lecture_A/image_01_01.jpg?v=1769676649\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Small filters scan local pixel neighborhoods for patterns\n",
    ">* Stacked filters capture many features with few parameters\n",
    "\n",
    ">* Conv2d hyperparameters control filters and feature maps\n",
    ">* Kernel, stride, padding set detail and resolution\n",
    "\n",
    ">* Early layers learn simple, generic visual features\n",
    ">* Deeper layers combine features into high-level concepts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5e7308",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Convolution Layers Essentials\n",
    "\n",
    "# This script explains basic convolution layers.\n",
    "# It uses TensorFlow to mimic PyTorch ideas.\n",
    "# Run cells to see shapes and outputs.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Import TensorFlow and Keras layers.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Set deterministic random seeds.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version once.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Define image and batch dimensions.\n",
    "batch_size = 4\n",
    "height = 28\n",
    "width = 28\n",
    "channels = 1\n",
    "\n",
    "# Create a small random image batch.\n",
    "images = tf.random.uniform(\n",
    "    shape=(batch_size, height, width, channels),\n",
    "    minval=0.0,\n",
    "    maxval=1.0,\n",
    ")\n",
    "\n",
    "# Verify the input tensor shape.\n",
    "print(\"Input batch shape:\", images.shape)\n",
    "\n",
    "# Define a simple Conv2D layer.\n",
    "conv_layer = layers.Conv2D(\n",
    "    filters=8,\n",
    "    kernel_size=(3, 3),\n",
    "    strides=(1, 1),\n",
    "    padding=\"same\",\n",
    ")\n",
    "\n",
    "# Apply convolution layer to the images.\n",
    "conv_output = conv_layer(images)\n",
    "\n",
    "# Print the convolution output shape.\n",
    "print(\"Conv output shape:\", conv_output.shape)\n",
    "\n",
    "# Define a max pooling layer.\n",
    "pool_layer = layers.MaxPooling2D(\n",
    "    pool_size=(2, 2),\n",
    "    strides=(2, 2),\n",
    "    padding=\"valid\",\n",
    ")\n",
    "\n",
    "# Apply pooling to convolution output.\n",
    "pool_output = pool_layer(conv_output)\n",
    "\n",
    "# Print the pooled feature map shape.\n",
    "print(\"Pooled output shape:\", pool_output.shape)\n",
    "\n",
    "# Show kernel weights shape for intuition.\n",
    "kernel_weights = conv_layer.kernel\n",
    "print(\"Kernel weights shape:\", kernel_weights.shape)\n",
    "\n",
    "# Select one example feature map.\n",
    "example_map = pool_output[0]\n",
    "\n",
    "# Compute simple statistics for that map.\n",
    "mean_val = tf.reduce_mean(example_map).numpy()\n",
    "max_val = tf.reduce_max(example_map).numpy()\n",
    "min_val = tf.reduce_min(example_map).numpy()\n",
    "\n",
    "# Print summary statistics for understanding.\n",
    "print(\"Example feature map mean:\", float(mean_val))\n",
    "print(\"Example feature map max:\", float(max_val))\n",
    "print(\"Example feature map min:\", float(min_val))\n",
    "\n",
    "# Confirm final tensor rank and size.\n",
    "print(\"Example feature map shape:\", example_map.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edcfcd2",
   "metadata": {},
   "source": [
    "### **1.2. Understanding Pooling Layers**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c76b06",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_05/Lecture_A/image_01_02.jpg?v=1769676715\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Pooling compresses feature maps, keeping key information\n",
    ">* Makes models efficient and robust to small shifts\n",
    "\n",
    ">* Max and average pooling summarize feature map regions\n",
    ">* Max keeps strongest signals; pooling adds spatial invariance\n",
    "\n",
    ">* Pooling size and stride control downsampling strength\n",
    ">* Placement balances detail loss and high-level abstraction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc61991",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Understanding Pooling Layers\n",
    "\n",
    "# This script explains pooling layers visually.\n",
    "# It uses TensorFlow to simulate simple pooling.\n",
    "# Run cells to see how pooling changes images.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Import TensorFlow and check version.\n",
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Set deterministic random seeds.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# Create a small fake image batch.\n",
    "height, width, channels = 8, 8, 1\n",
    "image_array = np.arange(height * width,\n",
    "                        dtype=np.float32).reshape(\n",
    "                        (1, height, width, channels))\n",
    "\n",
    "# Normalize values to range zero one.\n",
    "image_array = image_array / np.max(image_array)\n",
    "print(\"Input batch shape:\", image_array.shape)\n",
    "\n",
    "# Convert numpy array to tensor.\n",
    "image_tensor = tf.convert_to_tensor(image_array,\n",
    "                                    dtype=tf.float32)\n",
    "\n",
    "# Define a simple max pooling layer.\n",
    "max_pool = tf.keras.layers.MaxPool2D(\n",
    "    pool_size=(2, 2), strides=(2, 2))\n",
    "\n",
    "# Define a simple average pooling layer.\n",
    "avg_pool = tf.keras.layers.AveragePooling2D(\n",
    "    pool_size=(2, 2), strides=(2, 2))\n",
    "\n",
    "# Apply max pooling to the image.\n",
    "max_pooled = max_pool(image_tensor)\n",
    "print(\"Max pooled shape:\", max_pooled.shape)\n",
    "\n",
    "# Apply average pooling to the image.\n",
    "avg_pooled = avg_pool(image_tensor)\n",
    "print(\"Avg pooled shape:\", avg_pooled.shape)\n",
    "\n",
    "# Convert pooled tensors back to numpy.\n",
    "max_pooled_np = max_pooled.numpy().reshape(\n",
    "    (max_pooled.shape[1], max_pooled.shape[2]))\n",
    "\n",
    "# Convert average pooled tensor to numpy.\n",
    "avg_pooled_np = avg_pooled.numpy().reshape(\n",
    "    (avg_pooled.shape[1], avg_pooled.shape[2]))\n",
    "\n",
    "# Print original image values summary.\n",
    "print(\"Original top left patch:\\n\",\n",
    "      image_array[0, :4, :4, 0])\n",
    "\n",
    "# Print max pooled values summary.\n",
    "print(\"Max pooled values:\\n\", max_pooled_np)\n",
    "\n",
    "# Print average pooled values summary.\n",
    "print(\"Avg pooled values:\\n\", avg_pooled_np)\n",
    "\n",
    "# Confirm spatial size reduction effect.\n",
    "print(\"Height reduced from\", height, \"to\",\n",
    "      max_pooled.shape[1])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1e7b6e",
   "metadata": {},
   "source": [
    "### **1.3. Flattening and Linear Layers**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7064c19",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_05/Lecture_A/image_01_03.jpg?v=1769676755\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Convolutions create spatial feature maps highlighting patterns\n",
    ">* Flattening turns maps into vectors for classification\n",
    "\n",
    ">* Linear layers weight flattened features for classes\n",
    ">* Stacked linear layers build higher-level feature combinations\n",
    "\n",
    ">* Flatten size controls parameters, cost, overfitting risk\n",
    ">* Balance compression and detail for accurate, efficient decisions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8d222f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Flattening and Linear Layers\n",
    "\n",
    "# This script explains flattening and linear layers.\n",
    "# We use TensorFlow to mimic PyTorch style layers.\n",
    "# Focus is on shapes before and after flattening.\n",
    "\n",
    "# !pip install tensorflow.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Import TensorFlow and Keras layers.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Set deterministic random seeds.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "\n",
    "# Set NumPy random seed.\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# Set TensorFlow random seed.\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version once.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Define image height, width, channels.\n",
    "img_height, img_width, img_channels = 28, 28, 1\n",
    "\n",
    "# Create a small batch of fake images.\n",
    "batch_size = 4\n",
    "\n",
    "# Use random numbers to simulate features.\n",
    "fake_images = np.random.rand(\n",
    "    batch_size,\n",
    "    img_height,\n",
    "    img_width,\n",
    "    img_channels,\n",
    ").astype(\"float32\")\n",
    "\n",
    "# Define a simple convolutional feature extractor.\n",
    "feature_extractor = keras.Sequential([\n",
    "    layers.Conv2D(\n",
    "        filters=8,\n",
    "        kernel_size=3,\n",
    "        activation=\"relu\",\n",
    "        input_shape=(img_height, img_width, img_channels),\n",
    "    ),\n",
    "    layers.MaxPooling2D(pool_size=2),\n",
    "    layers.Conv2D(filters=16, kernel_size=3, activation=\"relu\"),\n",
    "    layers.MaxPooling2D(pool_size=2),\n",
    "])\n",
    "\n",
    "# Pass fake images through feature extractor.\n",
    "feature_maps = feature_extractor(fake_images)\n",
    "\n",
    "# Show shape of feature maps before flattening.\n",
    "print(\"Feature maps shape:\", feature_maps.shape)\n",
    "\n",
    "# Build a small classifier with flattening.\n",
    "classifier = keras.Sequential([\n",
    "    feature_extractor,\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(units=32, activation=\"relu\"),\n",
    "    layers.Dense(units=10, activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "# Get flattened features by calling Flatten directly.\n",
    "flatten_layer = layers.Flatten()\n",
    "\n",
    "# Compute flattened output for inspection.\n",
    "flattened_output = flatten_layer(feature_maps)\n",
    "\n",
    "# Print shape of flattened vector.\n",
    "print(\"Flattened vector shape:\", flattened_output.shape)\n",
    "\n",
    "# Confirm flattened length equals product of dimensions.\n",
    "fm_shape = feature_maps.shape\n",
    "\n",
    "# Compute expected flattened length safely.\n",
    "expected_length = int(fm_shape[1] * fm_shape[2] * fm_shape[3])\n",
    "\n",
    "# Print expected flattened length for comparison.\n",
    "print(\"Expected flattened length:\", expected_length)\n",
    "\n",
    "# Print actual flattened length from tensor.\n",
    "print(\"Actual flattened length:\", int(flattened_output.shape[1]))\n",
    "\n",
    "# Create fake labels for ten classes.\n",
    "fake_labels = np.random.randint(0, 10, size=(batch_size,))\n",
    "\n",
    "# Compile classifier with simple settings.\n",
    "classifier.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Train briefly to show linear layers working.\n",
    "history = classifier.fit(\n",
    "    fake_images,\n",
    "    fake_labels,\n",
    "    epochs=2,\n",
    "    batch_size=batch_size,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Evaluate model on same tiny batch.\n",
    "loss, acc = classifier.evaluate(\n",
    "    fake_images,\n",
    "    fake_labels,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Print final accuracy to confirm pipeline.\n",
    "print(\"Tiny batch accuracy after training:\", float(acc))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adc00f1",
   "metadata": {},
   "source": [
    "## **2. Training Vision CNNs**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b888c0d",
   "metadata": {},
   "source": [
    "### **2.1. Image Normalization Basics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69835d40",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_05/Lecture_A/image_02_01.jpg?v=1769676838\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Raw pixel scales hinder stable CNN training\n",
    ">* Normalization rescales pixels to a consistent range\n",
    "\n",
    ">* Scale pixels and standardize using dataset statistics\n",
    ">* Center channels so CNN learns shapes, not brightness\n",
    "\n",
    ">* Pretrained models require matching normalization statistics\n",
    ">* Consistent normalization stabilizes training and evaluation metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e669b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Image Normalization Basics\n",
    "\n",
    "# This script shows basic image normalization concepts.\n",
    "# We use TensorFlow to load and normalize MNIST images.\n",
    "# Focus on scaling and standardizing pixel intensities.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Import TensorFlow and Keras utilities.\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets\n",
    "\n",
    "# Set deterministic random seeds for reproducibility.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Load MNIST dataset using Keras helper.\n",
    "(x_train, y_train), (x_test, y_test) = datasets.mnist.load_data()\n",
    "\n",
    "# Confirm dataset shapes before normalization.\n",
    "print(\"Train shape:\", x_train.shape, \"Test shape:\", x_test.shape)\n",
    "\n",
    "# Select a small subset for quick demonstration.\n",
    "subset_size = 1000\n",
    "x_train_small = x_train[:subset_size]\n",
    "y_train_small = y_train[:subset_size]\n",
    "\n",
    "# Convert integer pixels to float32 values.\n",
    "x_train_float = x_train_small.astype(\"float32\")\n",
    "x_test_float = x_test.astype(\"float32\")\n",
    "\n",
    "# Show original pixel range using min and max.\n",
    "print(\"Original min, max:\", x_train_float.min(), x_train_float.max())\n",
    "\n",
    "# Scale pixels to range zero to one.\n",
    "x_train_scaled = x_train_float / 255.0\n",
    "x_test_scaled = x_test_float / 255.0\n",
    "\n",
    "# Compute per pixel mean and standard deviation.\n",
    "mean_value = np.mean(x_train_scaled)\n",
    "std_value = np.std(x_train_scaled)\n",
    "\n",
    "# Print computed mean and standard deviation.\n",
    "print(\"Scaled mean:\", float(mean_value))\n",
    "print(\"Scaled std:\", float(std_value))\n",
    "\n",
    "# Standardize data using training statistics.\n",
    "x_train_norm = (x_train_scaled - mean_value) / std_value\n",
    "x_test_norm = (x_test_scaled - mean_value) / std_value\n",
    "\n",
    "# Verify new distribution after normalization.\n",
    "print(\"Norm mean:\", float(np.mean(x_train_norm)))\n",
    "print(\"Norm std:\", float(np.std(x_train_norm)))\n",
    "\n",
    "# Add channel dimension required for CNN inputs.\n",
    "x_train_norm = np.expand_dims(x_train_norm, axis=-1)\n",
    "x_test_norm = np.expand_dims(x_test_norm, axis=-1)\n",
    "\n",
    "# Confirm final shapes are as expected.\n",
    "print(\"Final train shape:\", x_train_norm.shape)\n",
    "\n",
    "# Build a simple CNN model for classification.\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(\n",
    "        8,\n",
    "        (3, 3),\n",
    "        activation=\"relu\",\n",
    "        input_shape=(28, 28, 1),\n",
    "    ),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(32, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "# Compile model with optimizer, loss, and accuracy metric.\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Train briefly on normalized subset with silent output.\n",
    "history = model.fit(\n",
    "    x_train_norm,\n",
    "    y_train_small,\n",
    "    epochs=2,\n",
    "    batch_size=64,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Evaluate model on normalized test data silently.\n",
    "loss, acc = model.evaluate(x_test_norm, y_test, verbose=0)\n",
    "\n",
    "# Print final test accuracy to summarize effect.\n",
    "print(\"Test accuracy with normalization:\", float(acc))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e40d2ad",
   "metadata": {},
   "source": [
    "### **2.2. CrossEntropy Loss for Labels**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df79958",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_05/Lecture_A/image_02_02.jpg?v=1769676910\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Cross entropy measures prediction error against true labels\n",
    ">* Rewards confident correct guesses, penalizes confident mistakes\n",
    "\n",
    ">* Logits become class probabilities using softmax\n",
    ">* Cross entropy penalizes uncertain or wrong predictions\n",
    "\n",
    ">* Large loss on confident mistakes gives strong gradients\n",
    ">* Stable gradients guide CNN improvement across applications\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca006c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - CrossEntropy Loss for Labels\n",
    "\n",
    "# This script shows cross entropy loss usage.\n",
    "# We train a tiny CNN on MNIST digits.\n",
    "# Focus is on labels and loss behavior.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Import tensorflow and keras submodules.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print tensorflow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Select device preferring GPU when available.\n",
    "physical_gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "if physical_gpus:\n",
    "    device_name = \"GPU\"\n",
    "else:\n",
    "    device_name = \"CPU\"\n",
    "\n",
    "# Print selected device type briefly.\n",
    "print(\"Using device:\", device_name)\n",
    "\n",
    "# Load MNIST dataset from keras datasets.\n",
    "(mnist_x_train, mnist_y_train), (mnist_x_test, mnist_y_test) = (\n",
    "    keras.datasets.mnist.load_data()\n",
    ")\n",
    "\n",
    "# Reduce dataset size for quick demonstration.\n",
    "train_limit = 2000\n",
    "test_limit = 500\n",
    "x_train = mnist_x_train[:train_limit]\n",
    "y_train = mnist_y_train[:train_limit]\n",
    "\n",
    "# Slice test data to small subset.\n",
    "x_test = mnist_x_test[:test_limit]\n",
    "y_test = mnist_y_test[:test_limit]\n",
    "\n",
    "# Normalize images to range zero one.\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_test = x_test.astype(\"float32\") / 255.0\n",
    "\n",
    "# Add channel dimension for convolution layers.\n",
    "x_train = np.expand_dims(x_train, axis=-1)\n",
    "x_test = np.expand_dims(x_test, axis=-1)\n",
    "\n",
    "# Validate shapes before building model.\n",
    "print(\"Train shape:\", x_train.shape, y_train.shape)\n",
    "\n",
    "# Define number of classes for digits.\n",
    "num_classes = 10\n",
    "input_shape = x_train.shape[1:]\n",
    "\n",
    "# Build a simple sequential CNN model.\n",
    "model = keras.Sequential([\n",
    "    layers.Input(shape=input_shape),\n",
    "    layers.Conv2D(16, (3, 3), activation=\"relu\"),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(32, (3, 3), activation=\"relu\"),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation=\"relu\"),\n",
    "    layers.Dense(num_classes),\n",
    "])\n",
    "\n",
    "# Explain loss choice using sparse labels.\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Compile model with optimizer and metrics.\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss=loss_fn,\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Train model briefly with silent verbose.\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=3,\n",
    "    batch_size=64,\n",
    "    validation_split=0.1,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Evaluate model on held out test data.\n",
    "test_loss, test_acc = model.evaluate(\n",
    "    x_test,\n",
    "    y_test,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Print concise evaluation results.\n",
    "print(\"Test loss (cross entropy):\", round(test_loss, 4))\n",
    "print(\"Test accuracy:\", round(test_acc, 4))\n",
    "\n",
    "# Take small batch to inspect predictions.\n",
    "sample_images = x_test[:5]\n",
    "sample_labels = y_test[:5]\n",
    "logits = model.predict(sample_images, verbose=0)\n",
    "\n",
    "# Convert logits to probabilities with softmax.\n",
    "probabilities = tf.nn.softmax(logits, axis=-1).numpy()\n",
    "\n",
    "# Print predicted class and true label pairs.\n",
    "for idx in range(len(sample_labels)):\n",
    "    true_label = int(sample_labels[idx])\n",
    "    pred_label = int(np.argmax(probabilities[idx]))\n",
    "    true_prob = float(probabilities[idx, true_label])\n",
    "    print(\n",
    "        \"Sample\",\n",
    "        idx,\n",
    "        \"true:\",\n",
    "        true_label,\n",
    "        \"pred:\",\n",
    "        pred_label,\n",
    "        \"p_true:\",\n",
    "        round(true_prob, 3),\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e42f5e5",
   "metadata": {},
   "source": [
    "### **2.3. Accuracy Metric Setup**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3957a4",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_05/Lecture_A/image_02_03.jpg?v=1769676994\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Accuracy counts how many predictions are correct\n",
    ">* Compute using argmax class, compare, then average\n",
    "\n",
    ">* Track correct predictions per batch, then aggregate\n",
    ">* Epoch-wide averaging gives stable, trustworthy accuracy\n",
    "\n",
    ">* Accuracy is useful but limited, especially imbalanced\n",
    ">* Track train and validation accuracy to detect overfitting\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62590ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Accuracy Metric Setup\n",
    "\n",
    "# This script shows accuracy metric setup.\n",
    "# We use TensorFlow for a tiny example.\n",
    "# Focus is on simple clear accuracy calculation.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Import TensorFlow and Keras utilities.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Set deterministic random seeds.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version once.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Load MNIST dataset from Keras.\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Select a small subset for speed.\n",
    "train_samples = 2000\n",
    "test_samples = 500\n",
    "x_train = x_train[:train_samples]\n",
    "y_train = y_train[:train_samples]\n",
    "\n",
    "# Slice test data subset.\n",
    "x_test = x_test[:test_samples]\n",
    "y_test = y_test[:test_samples]\n",
    "\n",
    "# Normalize pixel values to range zero one.\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_test = x_test.astype(\"float32\") / 255.0\n",
    "\n",
    "# Add channel dimension for convolution layers.\n",
    "x_train = np.expand_dims(x_train, axis=-1)\n",
    "x_test = np.expand_dims(x_test, axis=-1)\n",
    "\n",
    "# Validate shapes before building model.\n",
    "assert x_train.shape[1:] == (28, 28, 1)\n",
    "assert x_test.shape[1:] == (28, 28, 1)\n",
    "\n",
    "# Build a tiny CNN model.\n",
    "model = keras.Sequential([\n",
    "    layers.Conv2D(8, (3, 3), activation=\"relu\", input_shape=(28, 28, 1)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(32, activation=\"relu\"),\n",
    "    layers.Dense(10)\n",
    "])\n",
    "\n",
    "# Define loss function for logits.\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Define optimizer with safe learning rate.\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# Prepare metric variables for accuracy.\n",
    "train_correct = 0\n",
    "train_total = 0\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "# Convert labels to int64 for safety.\n",
    "y_train = y_train.astype(\"int64\")\n",
    "y_test = y_test.astype(\"int64\")\n",
    "\n",
    "# Define batch size and epochs.\n",
    "batch_size = 64\n",
    "epochs = 2\n",
    "\n",
    "# Training loop over epochs.\n",
    "for epoch in range(epochs):\n",
    "    # Reset counters each epoch.\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "\n",
    "    # Iterate over training batches.\n",
    "    for start in range(0, train_samples, batch_size):\n",
    "        end = min(start + batch_size, train_samples)\n",
    "        x_batch = x_train[start:end]\n",
    "        y_batch = y_train[start:end]\n",
    "\n",
    "        # Use GradientTape for training.\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x_batch, training=True)\n",
    "            loss_value = loss_fn(y_batch, logits)\n",
    "\n",
    "        # Apply gradients to model weights.\n",
    "        grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        # Compute predicted class indices.\n",
    "        preds = tf.argmax(logits, axis=1, output_type=tf.int64)\n",
    "\n",
    "        # Count correct predictions in batch.\n",
    "        correct_batch = tf.reduce_sum(tf.cast(preds == y_batch, tf.int32))\n",
    "\n",
    "        # Update running totals for accuracy.\n",
    "        train_correct += int(correct_batch.numpy())\n",
    "        train_total += y_batch.shape[0]\n",
    "\n",
    "    # Compute epoch training accuracy.\n",
    "    train_accuracy = train_correct / train_total\n",
    "\n",
    "    # Evaluate accuracy on test subset.\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "\n",
    "    # Loop over test batches for accuracy.\n",
    "    for start in range(0, test_samples, batch_size):\n",
    "        end = min(start + batch_size, test_samples)\n",
    "        x_batch = x_test[start:end]\n",
    "        y_batch = y_test[start:end]\n",
    "\n",
    "        # Forward pass without gradient.\n",
    "        logits = model(x_batch, training=False)\n",
    "\n",
    "        # Compute predicted classes for test.\n",
    "        preds = tf.argmax(logits, axis=1, output_type=tf.int64)\n",
    "\n",
    "        # Count correct predictions for test.\n",
    "        correct_batch = tf.reduce_sum(tf.cast(preds == y_batch, tf.int32))\n",
    "\n",
    "        # Update running test totals.\n",
    "        test_correct += int(correct_batch.numpy())\n",
    "        test_total += y_batch.shape[0]\n",
    "\n",
    "    # Compute epoch test accuracy.\n",
    "    test_accuracy = test_correct / test_total\n",
    "\n",
    "    # Print concise accuracy summary.\n",
    "    print(\n",
    "        f\"Epoch {epoch + 1}: train_acc={train_accuracy:.3f}, test_acc={test_accuracy:.3f}\"\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbcf655",
   "metadata": {},
   "source": [
    "## **3. Evaluating CNN Performance**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b4bdaf",
   "metadata": {},
   "source": [
    "### **3.1. Reading Confusion Matrices**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770b5e3f",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_05/Lecture_A/image_03_01.jpg?v=1769677083\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Confusion matrices summarize predictions for every class\n",
    ">* Rows show true classes, columns show predicted classes\n",
    "\n",
    ">* Check diagonal versus off-diagonal counts and patterns\n",
    ">* Use asymmetries to judge error types and costs\n",
    "\n",
    ">* Look for repeated confusions between similar classes\n",
    ">* Use these patterns to guide data and improvements\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bb0900",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Reading Confusion Matrices\n",
    "\n",
    "# This script shows confusion matrices for CNN predictions.\n",
    "# It uses TensorFlow to train a tiny image classifier.\n",
    "# Focus is on reading confusion matrices and errors.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Import TensorFlow and Keras utilities.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Import confusion matrix and plotting tools.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set deterministic random seeds for reproducibility.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Load MNIST dataset of handwritten digits.\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Select a small subset for quick training.\n",
    "train_samples = 4000\n",
    "test_samples = 1000\n",
    "x_train = x_train[:train_samples]\n",
    "y_train = y_train[:train_samples]\n",
    "\n",
    "# Slice test data to the chosen subset.\n",
    "x_test = x_test[:test_samples]\n",
    "y_test = y_test[:test_samples]\n",
    "\n",
    "# Normalize pixel values to range zero to one.\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_test = x_test.astype(\"float32\") / 255.0\n",
    "\n",
    "# Add channel dimension required by Conv2D.\n",
    "x_train = np.expand_dims(x_train, axis=-1)\n",
    "x_test = np.expand_dims(x_test, axis=-1)\n",
    "\n",
    "# Confirm shapes are as expected.\n",
    "print(\"Train shape:\", x_train.shape, y_train.shape)\n",
    "print(\"Test shape:\", x_test.shape, y_test.shape)\n",
    "\n",
    "# Build a simple CNN model for classification.\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Conv2D(\n",
    "        8,\n",
    "        (3, 3),\n",
    "        activation=\"relu\",\n",
    "        input_shape=(28, 28, 1),\n",
    "    ),\n",
    "    keras.layers.MaxPooling2D((2, 2)),\n",
    "    keras.layers.Conv2D(16, (3, 3), activation=\"relu\"),\n",
    "    keras.layers.MaxPooling2D((2, 2)),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(32, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "# Compile model with suitable loss and metric.\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Train briefly with silent output for speed.\n",
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=2,\n",
    "    batch_size=64,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Evaluate accuracy on the test subset.\n",
    "loss, acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test accuracy:\", round(float(acc), 4))\n",
    "\n",
    "# Get predicted class probabilities for test images.\n",
    "y_prob = model.predict(x_test, verbose=0)\n",
    "\n",
    "# Convert probabilities to predicted class indices.\n",
    "y_pred = np.argmax(y_prob, axis=1)\n",
    "\n",
    "# Validate prediction and label shapes match.\n",
    "assert y_pred.shape == y_test.shape\n",
    "\n",
    "# Compute confusion matrix using sklearn helper.\n",
    "cm = confusion_matrix(y_test, y_pred, labels=list(range(10)))\n",
    "\n",
    "# Print a small summary of diagonal correctness.\n",
    "correct_per_class = np.diag(cm)\n",
    "print(\"Correct per class:\", correct_per_class)\n",
    "\n",
    "# Create a simple confusion matrix heatmap plot.\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "# Show matrix as an image with color scale.\n",
    "im = ax.imshow(cm, cmap=\"Blues\")\n",
    "\n",
    "# Add axis labels for true and predicted classes.\n",
    "ax.set_xlabel(\"Predicted class\")\n",
    "ax.set_ylabel(\"True class\")\n",
    "\n",
    "# Add a short title explaining the plot.\n",
    "ax.set_title(\"MNIST confusion matrix (subset)\")\n",
    "\n",
    "# Add colorbar to interpret magnitude visually.\n",
    "fig.colorbar(im, ax=ax)\n",
    "\n",
    "# Highlight a few typical mistakes for beginners.\n",
    "mis_idx = np.where(y_pred != y_test)[0]\n",
    "\n",
    "# If there are mistakes, print first few examples.\n",
    "if mis_idx.size > 0:\n",
    "    sample_idx = mis_idx[:5]\n",
    "    print(\"Example errors (true, pred):\")\n",
    "    for i in sample_idx:\n",
    "        print(int(y_test[i]), int(y_pred[i]))\n",
    "\n",
    "# Display the confusion matrix plot once.\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fca8a19",
   "metadata": {},
   "source": [
    "### **3.2. Inspecting Misclassified Images**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273464bf",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_05/Lecture_A/image_03_02.jpg?v=1769677166\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Look at images the CNN predicts incorrectly\n",
    ">* Connect wrong predictions to specific visual challenges\n",
    "\n",
    ">* Look for repeated patterns in wrong predictions\n",
    ">* Group errors to uncover systematic model weaknesses\n",
    "\n",
    ">* Use misclassified images to guide data improvements\n",
    ">* Target risky errors with specialized data and review\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d917e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Inspecting Misclassified Images\n",
    "\n",
    "# This script inspects misclassified CNN predictions visually.\n",
    "# It trains a tiny CNN on MNIST digit images briefly.\n",
    "# Then it plots a few misclassified test images clearly.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries safely.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Import TensorFlow and Keras utilities.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Set deterministic random seeds for reproducibility.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Load MNIST dataset using Keras helper.\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Reduce dataset size for faster execution.\n",
    "x_train = x_train[:8000]\n",
    "y_train = y_train[:8000]\n",
    "x_test = x_test[:2000]\n",
    "y_test = y_test[:2000]\n",
    "\n",
    "# Normalize pixel values to range zero one.\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_test = x_test.astype(\"float32\") / 255.0\n",
    "\n",
    "# Add channel dimension required by Conv2D.\n",
    "x_train = np.expand_dims(x_train, axis=-1)\n",
    "x_test = np.expand_dims(x_test, axis=-1)\n",
    "\n",
    "# Confirm shapes are as expected.\n",
    "assert x_train.shape[1:] == (28, 28, 1)\n",
    "assert x_test.shape[1:] == (28, 28, 1)\n",
    "\n",
    "# Build a simple convolutional neural network.\n",
    "model = keras.Sequential([\n",
    "    layers.Conv2D(16, (3, 3), activation=\"relu\", input_shape=(28, 28, 1)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(32, (3, 3), activation=\"relu\"),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation=\"relu\"),\n",
    "    layers.Dense(10, activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "# Compile model with suitable loss and metric.\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Train briefly with silent output settings.\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=2,\n",
    "    batch_size=64,\n",
    "    verbose=0,\n",
    "    validation_split=0.1,\n",
    ")\n",
    "\n",
    "# Evaluate model accuracy on test set.\n",
    "test_loss, test_acc = model.evaluate(\n",
    "    x_test,\n",
    "    y_test,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Print concise test accuracy information.\n",
    "print(\"Test accuracy:\", round(float(test_acc), 4))\n",
    "\n",
    "# Get predicted class probabilities for test set.\n",
    "probs = model.predict(x_test, verbose=0)\n",
    "\n",
    "# Convert probabilities to predicted class indices.\n",
    "y_pred = np.argmax(probs, axis=1)\n",
    "\n",
    "# Ensure prediction shape matches labels shape.\n",
    "assert y_pred.shape == y_test.shape\n",
    "\n",
    "# Identify indices where predictions are incorrect.\n",
    "mis_idx = np.where(y_pred != y_test)[0]\n",
    "\n",
    "# Handle case with very few misclassifications.\n",
    "num_to_show = min(9, mis_idx.shape[0])\n",
    "\n",
    "# Print how many misclassified images exist.\n",
    "print(\"Total misclassified images:\", int(mis_idx.shape[0]))\n",
    "\n",
    "# Select a small subset of misclassified indices.\n",
    "selected_idx = mis_idx[:num_to_show]\n",
    "\n",
    "# Import matplotlib for plotting images.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a square grid for misclassified images.\n",
    "cols = 3\n",
    "rows = int(np.ceil(num_to_show / cols))\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(6, 6))\n",
    "\n",
    "# Flatten axes array for easier iteration.\n",
    "axes = np.array(axes).reshape(-1)\n",
    "\n",
    "# Loop through selected misclassified examples.\n",
    "for i, ax in enumerate(axes):\n",
    "    # Clear unused subplot axes if needed.\n",
    "    if i >= num_to_show:\n",
    "        ax.axis(\"off\")\n",
    "        continue\n",
    "\n",
    "    # Get index of current misclassified image.\n",
    "    idx = int(selected_idx[i])\n",
    "\n",
    "    # Extract image, true label, and predicted label.\n",
    "    img = x_test[idx].squeeze()\n",
    "    true_label = int(y_test[idx])\n",
    "    pred_label = int(y_pred[idx])\n",
    "\n",
    "    # Show grayscale image without axis ticks.\n",
    "    ax.imshow(img, cmap=\"gray\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "    # Add informative title with labels.\n",
    "    ax.set_title(\n",
    "        f\"T:{true_label} P:{pred_label}\",\n",
    "        fontsize=8,\n",
    "    )\n",
    "\n",
    "# Adjust layout to avoid overlapping titles.\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6549f304",
   "metadata": {},
   "source": [
    "### **3.3. Spotting Overfitting Patterns**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7e001b",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_05/Lecture_A/image_03_03.jpg?v=1769677259\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Compare training and validation performance to detect overfitting\n",
    ">* Watch for widening accuracy and loss gaps over time\n",
    "\n",
    ">* Overfitting shows in loss curves and errors\n",
    ">* Watch training versus validation trends to detect\n",
    "\n",
    ">* Check mistakes and sensitivity to small changes\n",
    ">* Compare subsets to see brittle, non-general features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d28c339",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Spotting Overfitting Patterns\n",
    "\n",
    "# This script shows CNN overfitting patterns clearly.\n",
    "# We use a tiny MNIST subset for quick training.\n",
    "# Focus on training versus validation loss and accuracy.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Import tensorflow and keras submodules.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print tensorflow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Load MNIST dataset from keras datasets.\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize pixel values to range zero one.\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_test = x_test.astype(\"float32\") / 255.0\n",
    "\n",
    "# Add channel dimension for convolution layers.\n",
    "x_train = np.expand_dims(x_train, axis=-1)\n",
    "x_test = np.expand_dims(x_test, axis=-1)\n",
    "\n",
    "# Select small subsets to exaggerate overfitting.\n",
    "train_samples = 2000\n",
    "val_samples = 2000\n",
    "x_train_small = x_train[:train_samples]\n",
    "y_train_small = y_train[:train_samples]\n",
    "\n",
    "# Create validation subset from test data.\n",
    "x_val_small = x_test[:val_samples]\n",
    "y_val_small = y_test[:val_samples]\n",
    "\n",
    "# Confirm shapes are as expected.\n",
    "print(\"Train subset shape:\", x_train_small.shape)\n",
    "print(\"Validation subset shape:\", x_val_small.shape)\n",
    "\n",
    "# Build a small convolutional neural network.\n",
    "model = keras.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation=\"relu\", input_shape=(28, 28, 1)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation=\"relu\"),\n",
    "    layers.Dense(10, activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "# Compile model with suitable loss and metric.\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Train model and store training history.\n",
    "history = model.fit(\n",
    "    x_train_small,\n",
    "    y_train_small,\n",
    "    epochs=15,\n",
    "    batch_size=64,\n",
    "    validation_data=(x_val_small, y_val_small),\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Extract loss and accuracy curves from history.\n",
    "train_loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "train_acc = history.history[\"accuracy\"]\n",
    "val_acc = history.history[\"val_accuracy\"]\n",
    "\n",
    "# Print compact table of epoch metrics.\n",
    "print(\"Epoch  TrainLoss  ValLoss  TrainAcc  ValAcc\")\n",
    "for epoch in range(len(train_loss)):\n",
    "    tl = round(train_loss[epoch], 3)\n",
    "    vl = round(val_loss[epoch], 3)\n",
    "    ta = round(train_acc[epoch], 3)\n",
    "    va = round(val_acc[epoch], 3)\n",
    "    print(epoch + 1, tl, vl, ta, va)\n",
    "\n",
    "# Identify epoch where validation loss starts increasing.\n",
    "best_val_loss = min(val_loss)\n",
    "best_epoch = val_loss.index(best_val_loss) + 1\n",
    "\n",
    "# Print simple interpretation about overfitting start.\n",
    "print(\"Best validation loss at epoch:\", best_epoch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc1c13d",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**CNNs from Scratch**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa60cbb",
   "metadata": {},
   "source": [
    "\n",
    "In this lecture, you learned to:\n",
    "- Construct a convolutional neural network using Conv2d, pooling, and fully connected layers in PyTorch. \n",
    "- Train the CNN on a small image dataset using the standard training loop and appropriate loss and metrics. \n",
    "- Analyze model performance using accuracy, confusion matrices, and simple error inspection. \n",
    "\n",
    "In the next Lecture (Lecture B), we will go over 'Transfer Learning'"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
