{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e5e855e",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**CNNs from Scratch**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd7458f",
   "metadata": {},
   "source": [
    ">Last update: 20260129.\n",
    "    \n",
    "By the end of this Lecture, you will be able to:\n",
    "- Construct a convolutional neural network using Conv2d, pooling, and fully connected layers in PyTorch. \n",
    "- Train the CNN on a small image dataset using the standard training loop and appropriate loss and metrics. \n",
    "- Analyze model performance using accuracy, confusion matrices, and simple error inspection. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9002c17a",
   "metadata": {},
   "source": [
    "## **1. Building CNN Layers**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c56cce",
   "metadata": {},
   "source": [
    "### **1.1. Understanding Conv2d Kernels**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a56dde",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_05/Lecture_A/image_01_01.jpg?v=1769713549\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Kernel is a small sliding numeric stencil\n",
    ">* Sliding kernels learn patterns like edges during training\n",
    "\n",
    ">* Many kernels scan all channels, small regions\n",
    ">* Each kernel detects different patterns and objects\n",
    "\n",
    ">* Kernel size, stride, padding shape feature maps\n",
    ">* These choices control patterns detected and detail\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637fcd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Understanding Conv2d Kernels\n",
    "\n",
    "# This script visualizes simple Conv2d style kernels.\n",
    "# It helps beginners understand kernel sliding behavior.\n",
    "# We use NumPy only and keep outputs small.\n",
    "\n",
    "# import required library for arrays.\n",
    "import numpy as np\n",
    "\n",
    "# set deterministic random seed for reproducibility.\n",
    "np.random.seed(42)\n",
    "\n",
    "# create a tiny grayscale image with simple pattern.\n",
    "image = np.array(\n",
    "    [[0, 0, 0, 0, 0],\n",
    "     [0, 1, 1, 1, 0],\n",
    "     [0, 1, 2, 1, 0],\n",
    "     [0, 1, 1, 1, 0],\n",
    "     [0, 0, 0, 0, 0]],\n",
    "    dtype=float,\n",
    ")\n",
    "\n",
    "# define a simple edge detection style kernel.\n",
    "kernel = np.array(\n",
    "    [[-1, -1, -1],\n",
    "     [0, 0, 0],\n",
    "     [1, 1, 1]],\n",
    "    dtype=float,\n",
    ")\n",
    "\n",
    "# compute output spatial size for valid convolution.\n",
    "out_height = image.shape[0] - kernel.shape[0] + 1\n",
    "out_width = image.shape[1] - kernel.shape[1] + 1\n",
    "\n",
    "# validate that output size is positive.\n",
    "assert out_height > 0 and out_width > 0\n",
    "\n",
    "# allocate output feature map for convolution result.\n",
    "output = np.zeros((out_height, out_width), dtype=float)\n",
    "\n",
    "# perform manual convolution with stride one.\n",
    "for i in range(out_height):\n",
    "    for j in range(out_width):\n",
    "        # extract current image patch under kernel.\n",
    "        patch = image[i : i + 3, j : j + 3]\n",
    "\n",
    "        # compute elementwise product and sum.\n",
    "        value = np.sum(patch * kernel)\n",
    "\n",
    "        # store result in output feature map.\n",
    "        output[i, j] = value\n",
    "\n",
    "# print original image to inspect local pattern.\n",
    "print(\"Input image (5x5 pixels):\")\n",
    "print(image)\n",
    "\n",
    "# print kernel values representing learned stencil.\n",
    "print(\"\\nKernel (3x3 edge detector):\")\n",
    "print(kernel)\n",
    "\n",
    "# print resulting feature map after convolution.\n",
    "print(\"\\nOutput feature map (3x3):\")\n",
    "print(output)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28c2f54",
   "metadata": {},
   "source": [
    "### **1.2. Understanding Pooling Layers**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3750bc",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_05/Lecture_A/image_01_02.jpg?v=1769713603\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Pooling shrinks feature maps while keeping key information\n",
    ">* Sliding windows summarize regions, adding shift robustness\n",
    "\n",
    ">* Max pooling keeps the strongest feature in regions\n",
    ">* Average pooling keeps the mean, giving smoother summaries\n",
    "\n",
    ">* Pooling cuts computation and speeds up models\n",
    ">* Pooling adds robustness to small image changes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd2f109",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Understanding Pooling Layers\n",
    "\n",
    "# This script explains pooling layers visually.\n",
    "# It uses TensorFlow to simulate simple pooling.\n",
    "# Run all cells to see printed comparisons.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required libraries safely.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set deterministic random seeds.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version briefly.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Create a tiny example feature map.\n",
    "example_map = np.array(\n",
    "    [\n",
    "        [1.0, 2.0, 0.5, 0.0],\n",
    "        [3.0, 4.0, 1.0, 0.5],\n",
    "        [0.0, 1.0, 2.0, 3.0],\n",
    "        [0.5, 1.5, 2.5, 4.0],\n",
    "    ],\n",
    "    dtype=np.float32,\n",
    ")\n",
    "\n",
    "# Validate the feature map shape.\n",
    "if example_map.shape != (4, 4):\n",
    "    raise ValueError(\"Example map must be shape (4,4).\")\n",
    "\n",
    "# Add batch and channel dimensions.\n",
    "input_tensor = example_map[np.newaxis, ..., np.newaxis]\n",
    "\n",
    "# Confirm the tensor has correct shape.\n",
    "if input_tensor.shape != (1, 4, 4, 1):\n",
    "    raise ValueError(\"Input tensor must be shape (1,4,4,1).\")\n",
    "\n",
    "# Define a 2x2 max pooling layer.\n",
    "max_pool = tf.keras.layers.MaxPool2D(\n",
    "    pool_size=(2, 2),\n",
    "    strides=(2, 2),\n",
    "    padding=\"valid\",\n",
    ")\n",
    "\n",
    "# Define a 2x2 average pooling layer.\n",
    "avg_pool = tf.keras.layers.AveragePooling2D(\n",
    "    pool_size=(2, 2),\n",
    "    strides=(2, 2),\n",
    "    padding=\"valid\",\n",
    ")\n",
    "\n",
    "# Apply max pooling to the input.\n",
    "max_pooled = max_pool(input_tensor)\n",
    "\n",
    "# Apply average pooling to the input.\n",
    "avg_pooled = avg_pool(input_tensor)\n",
    "\n",
    "# Convert pooled outputs to numpy arrays.\n",
    "max_pooled_np = max_pooled.numpy()[0, :, :, 0]\n",
    "avg_pooled_np = avg_pooled.numpy()[0, :, :, 0]\n",
    "\n",
    "# Print the original feature map.\n",
    "print(\"Original 4x4 feature map:\")\n",
    "print(example_map)\n",
    "\n",
    "# Print the max pooled result.\n",
    "print(\"\\nMax pooled 2x2 feature map:\")\n",
    "print(max_pooled_np)\n",
    "\n",
    "# Print the average pooled result.\n",
    "print(\"\\nAverage pooled 2x2 feature map:\")\n",
    "print(avg_pooled_np)\n",
    "\n",
    "# Explain how pooling reduces spatial size.\n",
    "print(\"\\nEach 2x2 block became one pooled value.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d17a3f",
   "metadata": {},
   "source": [
    "### **1.3. Flattening and Linear Layers**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5bb1f4",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_05/Lecture_A/image_01_03.jpg?v=1769713666\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Flattening turns 2D feature maps into vectors\n",
    ">* These vectors feed linear layers for predictions\n",
    "\n",
    ">* Flattened features feed into fully connected layers\n",
    ">* Linear layers combine patterns to produce class scores\n",
    "\n",
    ">* Flatten size sets first linear layer inputs\n",
    ">* Intermediate linears refine features into final classes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e48fb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Flattening and Linear Layers\n",
    "\n",
    "# This script shows flattening and linear layers.\n",
    "# We build a tiny CNN using TensorFlow Keras.\n",
    "# Focus on connecting conv outputs to dense layers.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Import TensorFlow and Keras layers.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Set deterministic random seeds.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version briefly.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Load MNIST dataset from Keras.\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Select a small subset for speed.\n",
    "train_samples = 2000\n",
    "test_samples = 500\n",
    "x_train = x_train[:train_samples]\n",
    "y_train = y_train[:train_samples]\n",
    "\n",
    "# Slice test data subset.\n",
    "x_test = x_test[:test_samples]\n",
    "y_test = y_test[:test_samples]\n",
    "\n",
    "# Normalize pixel values to range zero one.\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_test = x_test.astype(\"float32\") / 255.0\n",
    "\n",
    "# Add channel dimension for convolution.\n",
    "x_train = np.expand_dims(x_train, axis=-1)\n",
    "x_test = np.expand_dims(x_test, axis=-1)\n",
    "\n",
    "# Confirm shapes before building model.\n",
    "print(\"Train shape:\", x_train.shape)\n",
    "print(\"Test shape:\", x_test.shape)\n",
    "\n",
    "# Define input shape for the model.\n",
    "input_shape = (28, 28, 1)\n",
    "num_classes = 10\n",
    "\n",
    "# Build a simple sequential CNN model.\n",
    "model = keras.Sequential([\n",
    "    layers.Input(shape=input_shape),\n",
    "    layers.Conv2D(8, (3, 3), activation=\"relu\"),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Conv2D(16, (3, 3), activation=\"relu\"),\n",
    "])\n",
    "\n",
    "# Add flatten layer to unroll feature maps.\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "# Add a hidden dense layer with ReLU.\n",
    "model.add(layers.Dense(32, activation=\"relu\"))\n",
    "\n",
    "# Add final dense layer for class scores.\n",
    "model.add(layers.Dense(num_classes, activation=\"softmax\"))\n",
    "\n",
    "# Call model once to ensure inputs/outputs are defined.\n",
    "_ = model(x_train[:1])\n",
    "\n",
    "# Build the model explicitly to define inputs/outputs.\n",
    "model.build((None,) + input_shape)\n",
    "\n",
    "# Show model summary in one short line.\n",
    "model.summary(print_fn=lambda x: None)\n",
    "print(\"Model built with Flatten and Dense layers.\")\n",
    "\n",
    "# Compile model with optimizer and loss.\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Train model briefly with silent output.\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=2,\n",
    "    batch_size=64,\n",
    "    verbose=0,\n",
    "    validation_split=0.1,\n",
    ")\n",
    "\n",
    "# Evaluate model on test subset.\n",
    "test_loss, test_acc = model.evaluate(\n",
    "    x_test,\n",
    "    y_test,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Print evaluation results clearly.\n",
    "print(\"Test loss:\", round(float(test_loss), 4))\n",
    "print(\"Test accuracy:\", round(float(test_acc), 4))\n",
    "\n",
    "# Inspect how flatten changes tensor shapes.\n",
    "conv_output_model = keras.Model(\n",
    "    inputs=model.inputs,\n",
    "    outputs=model.layers[3].output,\n",
    ")\n",
    "\n",
    "# Take one sample batch from test set.\n",
    "sample_batch = x_test[:1]\n",
    "\n",
    "# Get convolutional feature maps.\n",
    "feature_maps = conv_output_model.predict(sample_batch, verbose=0)\n",
    "\n",
    "# Get flattened vector from model layer.\n",
    "flatten_layer = model.layers[4]\n",
    "flattened_vector = flatten_layer(feature_maps).numpy()\n",
    "\n",
    "# Print shapes before and after flattening.\n",
    "print(\"Feature maps shape:\", feature_maps.shape)\n",
    "print(\"Flattened vector shape:\", flattened_vector.shape)\n",
    "\n",
    "# Confirm flattened size equals dense input units.\n",
    "print(\"Dense input units:\", flattened_vector.shape[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41249a09",
   "metadata": {},
   "source": [
    "## **2. Training Vision CNNs**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0bd323",
   "metadata": {},
   "source": [
    "### **2.1. Image Normalization Basics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c47bd8",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_05/Lecture_A/image_02_01.jpg?v=1769713801\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Normalize pixels to stable, consistent value ranges\n",
    ">* This stabilizes training and improves final performance\n",
    "\n",
    ">* Normalize each RGB channel to similar statistics\n",
    ">* Helps model ignore brightness, focus on shapes\n",
    "\n",
    ">* Normalization reduces dataset and deployment distribution shifts\n",
    ">* Preprocessing keeps inputs consistent, stabilizing training over epochs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca761a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Image Normalization Basics\n",
    "\n",
    "# This script shows basic image normalization concepts.\n",
    "# We use TensorFlow to load and normalize images.\n",
    "# Focus is on simple, clear, and short code.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Import TensorFlow and Keras utilities.\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets\n",
    "\n",
    "# Set deterministic random seeds for reproducibility.\n",
    "SEED_VALUE = 42\n",
    "random.seed(SEED_VALUE)\n",
    "np.random.seed(SEED_VALUE)\n",
    "\n",
    "# Set TensorFlow random seed for reproducibility.\n",
    "tf.random.set_seed(SEED_VALUE)\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Load CIFAR10 dataset with small color images.\n",
    "(x_train, y_train), (x_test, y_test) = datasets.cifar10.load_data()\n",
    "\n",
    "# Confirm dataset shapes before normalization.\n",
    "print(\"Train shape:\", x_train.shape, \"Test shape:\", x_test.shape)\n",
    "\n",
    "# Select a tiny subset for quick demonstration.\n",
    "subset_size = 8\n",
    "x_small = x_train[:subset_size]\n",
    "\n",
    "# Copy subset for normalized version comparison.\n",
    "x_small_norm = x_small.astype(\"float32\")\n",
    "\n",
    "# Show original pixel range for the subset.\n",
    "print(\"Original min:\", x_small.min(), \"max:\", x_small.max())\n",
    "\n",
    "# Scale pixels from integers to range zero one.\n",
    "x_small_norm = x_small_norm / 255.0\n",
    "\n",
    "# Compute per channel mean and standard deviation.\n",
    "channel_means = x_small_norm.mean(axis=(0, 1, 2))\n",
    "\n",
    "# Compute standard deviation for each color channel.\n",
    "channel_stds = x_small_norm.std(axis=(0, 1, 2))\n",
    "\n",
    "# Standardize each channel using mean and standard deviation.\n",
    "x_small_norm = (x_small_norm - channel_means) / channel_stds\n",
    "\n",
    "# Verify normalized subset shape matches original subset.\n",
    "assert x_small_norm.shape == x_small.shape\n",
    "\n",
    "# Compute new per channel mean after normalization.\n",
    "new_means = x_small_norm.mean(axis=(0, 1, 2))\n",
    "\n",
    "# Compute new per channel standard deviation values.\n",
    "new_stds = x_small_norm.std(axis=(0, 1, 2))\n",
    "\n",
    "# Print original channel statistics before normalization.\n",
    "print(\"Original means:\", channel_means)\n",
    "\n",
    "# Print new channel statistics after normalization.\n",
    "print(\"New means:\", new_means)\n",
    "\n",
    "# Print new standard deviations after normalization.\n",
    "print(\"New stds:\", new_stds)\n",
    "\n",
    "# Show a few sample normalized pixel values.\n",
    "print(\"Sample normalized pixels:\", x_small_norm[0, 0, 0, :])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2203ed7",
   "metadata": {},
   "source": [
    "### **2.2. CrossEntropy Loss for Labels**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0fea72",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_05/Lecture_A/image_02_02.jpg?v=1769713832\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Cross entropy compares predictions to true class labels\n",
    ">* It outputs a single loss minimized during training\n",
    "\n",
    ">* High loss when model is uncertain\n",
    ">* Loss shrinks as confidence in correct class grows\n",
    "\n",
    ">* Use logits and integer labels; loss handles softmax\n",
    ">* Average batch loss, backpropagate, improve correct-class probabilities\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f8a49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - CrossEntropy Loss for Labels\n",
    "\n",
    "# This script shows cross entropy loss usage.\n",
    "# It uses TensorFlow for a tiny CNN example.\n",
    "# Focus is on labels and logits representation.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Import TensorFlow and Keras submodules.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Set deterministic random seeds.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version briefly.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Load MNIST dataset from Keras.\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Select a small subset for speed.\n",
    "num_train = 2000\n",
    "num_test = 500\n",
    "x_train = x_train[:num_train]\n",
    "y_train = y_train[:num_train]\n",
    "\n",
    "# Slice test data subset.\n",
    "x_test = x_test[:num_test]\n",
    "y_test = y_test[:num_test]\n",
    "\n",
    "# Normalize pixel values to range zero one.\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_test = x_test.astype(\"float32\") / 255.0\n",
    "\n",
    "# Add channel dimension for CNN input.\n",
    "x_train = np.expand_dims(x_train, axis=-1)\n",
    "x_test = np.expand_dims(x_test, axis=-1)\n",
    "\n",
    "# Confirm shapes are as expected.\n",
    "print(\"Train shape:\", x_train.shape, y_train.shape)\n",
    "\n",
    "# Build a simple CNN model.\n",
    "model = keras.Sequential([\n",
    "    layers.Conv2D(8, (3, 3), activation=\"relu\", input_shape=(28, 28, 1)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(32, activation=\"relu\"),\n",
    "    layers.Dense(10)\n",
    "])\n",
    "\n",
    "# Show that final layer outputs logits.\n",
    "print(\"Output shape (logits):\", model.output_shape)\n",
    "\n",
    "# Define sparse categorical crossentropy loss.\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Compile model with accuracy metric.\n",
    "model.compile(optimizer=\"adam\", loss=loss_fn, metrics=[\"accuracy\"])\n",
    "\n",
    "# Train model briefly with silent verbose.\n",
    "history = model.fit(x_train, y_train, epochs=3, batch_size=64, verbose=0)\n",
    "\n",
    "# Evaluate model on test subset.\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "# Print loss and accuracy summary.\n",
    "print(\"Test loss (cross entropy):\", float(test_loss))\n",
    "print(\"Test accuracy:\", float(test_acc))\n",
    "\n",
    "# Take a small batch for manual loss demo.\n",
    "batch_images = x_test[:4]\n",
    "batch_labels = y_test[:4]\n",
    "\n",
    "# Get raw logits from the model.\n",
    "logits = model(batch_images, training=False)\n",
    "\n",
    "# Compute cross entropy loss for batch.\n",
    "batch_loss = loss_fn(batch_labels, logits).numpy()\n",
    "\n",
    "# Print labels and corresponding loss value.\n",
    "print(\"Batch labels:\", batch_labels)\n",
    "print(\"Batch cross entropy loss:\", float(batch_loss))\n",
    "\n",
    "# Confirm logits are not probabilities yet.\n",
    "print(\"Logits sample:\", logits[0].numpy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c10a3d0",
   "metadata": {},
   "source": [
    "### **2.3. Accuracy Metric Setup**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1eaca4d",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_05/Lecture_A/image_02_03.jpg?v=1769713901\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Accuracy measures correct predictions over total images\n",
    ">* Compute batch accuracy and track learning progress\n",
    "\n",
    ">* Track correct predictions across all epoch batches\n",
    ">* Epoch accuracy is stable and checks generalization\n",
    "\n",
    ">* Accuracy is simple but has important limits\n",
    ">* Use extra metrics for imbalanced, high-stakes tasks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98ee4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Accuracy Metric Setup\n",
    "\n",
    "# This script shows accuracy metric setup.\n",
    "# We use TensorFlow to mimic PyTorch ideas.\n",
    "# Focus is computing batch and epoch accuracy.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Import TensorFlow and Keras utilities.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Set deterministic random seeds.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version once.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Load MNIST dataset from Keras.\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Select a small subset for speed.\n",
    "train_samples = 2000\n",
    "test_samples = 500\n",
    "x_train = x_train[:train_samples]\n",
    "y_train = y_train[:train_samples]\n",
    "\n",
    "# Slice test subset for evaluation.\n",
    "x_test = x_test[:test_samples]\n",
    "y_test = y_test[:test_samples]\n",
    "\n",
    "# Normalize pixel values to zero one.\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_test = x_test.astype(\"float32\") / 255.0\n",
    "\n",
    "# Add channel dimension for convolution.\n",
    "x_train = np.expand_dims(x_train, axis=-1)\n",
    "x_test = np.expand_dims(x_test, axis=-1)\n",
    "\n",
    "# Confirm shapes are as expected.\n",
    "assert x_train.shape[1:] == (28, 28, 1)\n",
    "assert x_test.shape[1:] == (28, 28, 1)\n",
    "\n",
    "# Build a small CNN classification model.\n",
    "model = keras.Sequential([\n",
    "    layers.Conv2D(16, (3, 3), activation=\"relu\", input_shape=(28, 28, 1)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(32, (3, 3), activation=\"relu\"),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation=\"relu\"),\n",
    "    layers.Dense(10, activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "# Compile model with loss and optimizer.\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[],\n",
    ")\n",
    "\n",
    "# Create TensorFlow datasets for batching.\n",
    "batch_size = 64\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_ds = train_ds.shuffle(buffer_size=train_samples, seed=seed_value)\n",
    "train_ds = train_ds.batch(batch_size)\n",
    "\n",
    "# Prepare test dataset for evaluation.\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "test_ds = test_ds.batch(batch_size)\n",
    "\n",
    "# Define a function to compute batch accuracy.\n",
    "def batch_accuracy(logits_batch, labels_batch):\n",
    "    # Get predicted class indices.\n",
    "    preds = tf.argmax(logits_batch, axis=1, output_type=tf.int32)\n",
    "    # Compare predictions with true labels.\n",
    "    matches = tf.equal(preds, tf.cast(labels_batch, tf.int32))\n",
    "    # Compute mean accuracy for batch.\n",
    "    return tf.reduce_mean(tf.cast(matches, tf.float32))\n",
    "\n",
    "# Train for a few epochs manually.\n",
    "epochs = 2\n",
    "for epoch in range(epochs):\n",
    "    # Reset running counters each epoch.\n",
    "    epoch_correct = 0\n",
    "    epoch_total = 0\n",
    "    for step, (images, labels) in enumerate(train_ds):\n",
    "        # Run forward pass and compute loss.\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(images, training=True)\n",
    "            loss_value = model.compiled_loss(labels, logits)\n",
    "        # Apply gradients to update weights.\n",
    "        grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "        model.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        # Compute batch accuracy using helper.\n",
    "        acc_value = batch_accuracy(logits, labels)\n",
    "        batch_size_now = int(images.shape[0])\n",
    "        correct_now = float(acc_value) * batch_size_now\n",
    "\n",
    "        # Update running epoch totals.\n",
    "        epoch_correct += correct_now\n",
    "        epoch_total += batch_size_now\n",
    "\n",
    "    # Compute epoch accuracy from totals.\n",
    "    epoch_acc = epoch_correct / epoch_total\n",
    "    print(\"Epoch\", epoch + 1, \"train accuracy:\", round(epoch_acc, 4))\n",
    "\n",
    "# Evaluate accuracy on test split.\n",
    "all_correct = 0\n",
    "all_total = 0\n",
    "for images, labels in test_ds:\n",
    "    # Forward pass in inference mode.\n",
    "    logits = model(images, training=False)\n",
    "    acc_value = batch_accuracy(logits, labels)\n",
    "    batch_size_now = int(images.shape[0])\n",
    "    correct_now = float(acc_value) * batch_size_now\n",
    "\n",
    "    # Update running test totals.\n",
    "    all_correct += correct_now\n",
    "    all_total += batch_size_now\n",
    "\n",
    "# Compute final test accuracy value.\n",
    "test_accuracy = all_correct / all_total\n",
    "print(\"Test accuracy:\", round(test_accuracy, 4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36248be6",
   "metadata": {},
   "source": [
    "## **3. Evaluating CNN Performance**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edaf40e8",
   "metadata": {},
   "source": [
    "### **3.1. Confusion Matrix Basics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38f84e6",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_05/Lecture_A/image_03_01.jpg?v=1769713992\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Confusion matrices show counts of true vs predicted\n",
    ">* They reveal specific classification errors beyond accuracy\n",
    "\n",
    ">* Confusion matrices reveal which classes get mixed up\n",
    ">* Helps prioritize dangerous errors and guide improvements\n",
    "\n",
    ">* Confusion matrices reveal imbalance and rare-class errors\n",
    ">* They guide data, class, and threshold adjustments\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587685fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Confusion Matrix Basics\n",
    "\n",
    "# This script shows confusion matrix basics.\n",
    "# We use a tiny CNN on MNIST digits.\n",
    "# Focus is on evaluation not training.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Import TensorFlow and Keras utilities.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Set deterministic random seeds.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version briefly.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Load MNIST dataset from Keras.\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Select a small subset for speed.\n",
    "train_samples = 4000\n",
    "test_samples = 1000\n",
    "x_train = x_train[:train_samples]\n",
    "y_train = y_train[:train_samples]\n",
    "\n",
    "# Slice test data subset.\n",
    "x_test = x_test[:test_samples]\n",
    "y_test = y_test[:test_samples]\n",
    "\n",
    "# Normalize pixel values to zero one.\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_test = x_test.astype(\"float32\") / 255.0\n",
    "\n",
    "# Add channel dimension for CNN.\n",
    "x_train = np.expand_dims(x_train, axis=-1)\n",
    "x_test = np.expand_dims(x_test, axis=-1)\n",
    "\n",
    "# Validate shapes before modeling.\n",
    "assert x_train.shape[0] == y_train.shape[0]\n",
    "assert x_test.shape[0] == y_test.shape[0]\n",
    "\n",
    "# Build a simple CNN model.\n",
    "model = keras.Sequential([\n",
    "    layers.Conv2D(8, (3, 3), activation=\"relu\", input_shape=(28, 28, 1)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(32, activation=\"relu\"),\n",
    "    layers.Dense(10, activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "# Compile model with optimizer.\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Train briefly with silent output.\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=2,\n",
    "    batch_size=64,\n",
    "    verbose=0,\n",
    "    validation_split=0.1,\n",
    ")\n",
    "\n",
    "# Evaluate accuracy on test subset.\n",
    "test_loss, test_acc = model.evaluate(\n",
    "    x_test,\n",
    "    y_test,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Print overall test accuracy.\n",
    "print(\"Test accuracy on subset:\", round(float(test_acc), 4))\n",
    "\n",
    "# Get predicted class probabilities.\n",
    "y_prob = model.predict(x_test, verbose=0)\n",
    "\n",
    "# Convert probabilities to predicted labels.\n",
    "y_pred = np.argmax(y_prob, axis=1)\n",
    "\n",
    "# Validate prediction shape matches labels.\n",
    "assert y_pred.shape[0] == y_test.shape[0]\n",
    "\n",
    "# Compute confusion matrix manually.\n",
    "num_classes = 10\n",
    "conf_matrix = np.zeros((num_classes, num_classes), dtype=int)\n",
    "\n",
    "# Fill confusion matrix counts.\n",
    "for true_label, pred_label in zip(y_test, y_pred):\n",
    "    conf_matrix[true_label, pred_label] += 1\n",
    "\n",
    "# Print small confusion matrix summary.\n",
    "print(\"Confusion matrix shape:\", conf_matrix.shape)\n",
    "print(\"Diagonal correct predictions:\", np.trace(conf_matrix))\n",
    "\n",
    "# Show confusion matrix for first three classes.\n",
    "print(\"Top left 3x3 confusion block:\")\n",
    "print(conf_matrix[:3, :3])\n",
    "\n",
    "# Count misclassified examples for inspection.\n",
    "mis_idx = np.where(y_pred != y_test)[0]\n",
    "print(\"Total misclassified examples:\", mis_idx.shape[0])\n",
    "\n",
    "# Inspect first few misclassified pairs.\n",
    "for i in mis_idx[:5]:\n",
    "    print(\"Index\", int(i), \"true\", int(y_test[i]), \"pred\", int(y_pred[i]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e702d6",
   "metadata": {},
   "source": [
    "### **3.2. Reviewing Misclassified Images**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87325dd",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_05/Lecture_A/image_03_02.jpg?v=1769714077\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Inspect misclassified images with true and predicted labels\n",
    ">* Decide if errors reflect ambiguity or model weaknesses\n",
    "\n",
    ">* Compare many misclassified images to find patterns\n",
    ">* Use patterns to uncover data and preprocessing biases\n",
    "\n",
    ">* Use misclassified images to guide data improvements\n",
    ">* Target risky failure modes to build safer models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2b6c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Reviewing Misclassified Images\n",
    "\n",
    "# This script reviews misclassified CNN predictions visually.\n",
    "# We use TensorFlow to train a tiny CNN quickly.\n",
    "# Then we plot a few misclassified MNIST images.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Import TensorFlow and Keras utilities.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Load MNIST dataset from Keras datasets.\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Use a small subset for quick training.\n",
    "train_samples = 800\n",
    "test_samples = 200\n",
    "x_train = x_train[:train_samples]\n",
    "y_train = y_train[:train_samples]\n",
    "\n",
    "# Slice test data to a manageable subset.\n",
    "x_test = x_test[:test_samples]\n",
    "y_test = y_test[:test_samples]\n",
    "\n",
    "# Validate shapes before further processing.\n",
    "assert x_train.shape[0] == train_samples\n",
    "assert x_test.shape[0] == test_samples\n",
    "\n",
    "# Normalize pixel values to range zero one.\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_test = x_test.astype(\"float32\") / 255.0\n",
    "\n",
    "# Add channel dimension for CNN input.\n",
    "x_train = np.expand_dims(x_train, axis=-1)\n",
    "x_test = np.expand_dims(x_test, axis=-1)\n",
    "\n",
    "# Confirm final input shape is correct.\n",
    "input_shape = x_train.shape[1:]\n",
    "print(\"Input shape:\", input_shape)\n",
    "\n",
    "# Build a simple sequential CNN model.\n",
    "model = keras.Sequential([\n",
    "    layers.Conv2D(8, (3, 3), activation=\"relu\", input_shape=input_shape),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(16, (3, 3), activation=\"relu\"),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(32, activation=\"relu\"),\n",
    "    layers.Dense(10, activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "# Compile model with suitable loss and metric.\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Detect available device for information only.\n",
    "physical_gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "print(\"GPUs available:\", len(physical_gpus))\n",
    "\n",
    "# Train the model quietly for few epochs.\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=3,\n",
    "    batch_size=64,\n",
    "    verbose=0,\n",
    "    validation_split=0.1,\n",
    ")\n",
    "\n",
    "# Evaluate accuracy on the small test subset.\n",
    "loss, acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test accuracy on subset:\", round(float(acc), 3))\n",
    "\n",
    "# Get predicted class probabilities for test set.\n",
    "probs = model.predict(x_test, verbose=0)\n",
    "\n",
    "# Convert probabilities to predicted class indices.\n",
    "y_pred = np.argmax(probs, axis=1)\n",
    "\n",
    "# Identify indices where predictions are incorrect.\n",
    "mis_idx = np.where(y_pred != y_test)[0]\n",
    "\n",
    "# Handle case with very few misclassifications.\n",
    "if mis_idx.size == 0:\n",
    "    print(\"No misclassified images found in subset.\")\n",
    "    mis_idx = np.array([0])\n",
    "\n",
    "# Select up to nine misclassified indices.\n",
    "max_examples = 9\n",
    "selected_idx = mis_idx[:max_examples]\n",
    "\n",
    "# Print a short textual summary of errors.\n",
    "print(\"Total misclassified examples:\", int(mis_idx.size))\n",
    "print(\"Showing up to\", int(selected_idx.size), \"examples.\")\n",
    "\n",
    "# Import matplotlib for plotting misclassified images.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a square grid for visualization.\n",
    "cols = 3\n",
    "rows = int(np.ceil(selected_idx.size / cols))\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(6, 6))\n",
    "\n",
    "# Ensure axes is always a flat iterable.\n",
    "axes = np.array(axes).reshape(-1)\n",
    "\n",
    "# Loop through selected misclassified indices.\n",
    "for ax, idx in zip(axes, selected_idx):\n",
    "    # Get image, true label, and predicted label.\n",
    "    img = x_test[idx].squeeze()\n",
    "    true_label = int(y_test[idx])\n",
    "    pred_label = int(y_pred[idx])\n",
    "\n",
    "    # Show grayscale image on the subplot.\n",
    "    ax.imshow(img, cmap=\"gray\")\n",
    "\n",
    "    # Set title to compare true and predicted.\n",
    "    ax.set_title(f\"T:{true_label} P:{pred_label}\")\n",
    "\n",
    "    # Hide axis ticks for clarity.\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "# Turn off any unused subplot axes.\n",
    "for ax in axes[len(selected_idx) :]:\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "# Adjust layout and display the figure.\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d66e27",
   "metadata": {},
   "source": [
    "### **3.3. Spotting Overfitting Patterns**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8750b2",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_05/Lecture_A/image_03_03.jpg?v=1769714168\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Overfitting means memorizing training quirks, not general patterns\n",
    ">* Compare train versus validation metrics to detect overfitting\n",
    "\n",
    ">* Watch for training and validation curves diverging\n",
    ">* Use early signs to adjust model or regularization\n",
    "\n",
    ">* Compare performance across varied conditions and subsets\n",
    ">* Use confusion matrices to reveal brittle, memorized behavior\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf87c29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Spotting Overfitting Patterns\n",
    "\n",
    "# This script shows CNN overfitting patterns simply.\n",
    "# We use TensorFlow to train tiny image models.\n",
    "# Then we compare training and validation performance.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Import TensorFlow and Keras utilities.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Set deterministic random seeds.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version briefly.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Load CIFAR10 dataset from Keras.\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Confirm dataset shapes are as expected.\n",
    "assert x_train.shape[1:] == (32, 32, 3)\n",
    "assert y_train.shape[1:] == (1,)\n",
    "\n",
    "# Select small training subset to force overfitting.\n",
    "small_train_size = 500\n",
    "x_train_small = x_train[:small_train_size]\n",
    "y_train_small = y_train[:small_train_size]\n",
    "\n",
    "# Select validation subset with different images.\n",
    "val_size = 2000\n",
    "x_val = x_train[small_train_size:small_train_size + val_size]\n",
    "y_val = y_train[small_train_size:small_train_size + val_size]\n",
    "\n",
    "# Normalize pixel values to range zero one.\n",
    "x_train_small = x_train_small.astype(\"float32\") / 255.0\n",
    "x_val = x_val.astype(\"float32\") / 255.0\n",
    "x_test = x_test.astype(\"float32\") / 255.0\n",
    "\n",
    "# Define a simple CNN model function.\n",
    "def create_cnn_model():\n",
    "    model = keras.Sequential([\n",
    "        layers.Conv2D(16, (3, 3), activation=\"relu\", input_shape=(32, 32, 3)),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(32, (3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(64, activation=\"relu\"),\n",
    "        layers.Dense(10, activation=\"softmax\"),\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Create two identical CNN models.\n",
    "model_overfit = create_cnn_model()\n",
    "model_regularized = create_cnn_model()\n",
    "\n",
    "# Add simple regularization to second model.\n",
    "model_regularized = keras.Sequential([\n",
    "    layers.Conv2D(16, (3, 3), activation=\"relu\", input_shape=(32, 32, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Dropout(0.25),\n",
    "    layers.Conv2D(32, (3, 3), activation=\"relu\"),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation=\"relu\"),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(10, activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "# Compile the regularized model.\n",
    "model_regularized.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Train overfitting model on tiny subset.\n",
    "history_overfit = model_overfit.fit(\n",
    "    x_train_small,\n",
    "    y_train_small,\n",
    "    epochs=10,\n",
    "    batch_size=64,\n",
    "    validation_data=(x_val, y_val),\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Train regularized model on same data.\n",
    "history_reg = model_regularized.fit(\n",
    "    x_train_small,\n",
    "    y_train_small,\n",
    "    epochs=10,\n",
    "    batch_size=64,\n",
    "    validation_data=(x_val, y_val),\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Extract final training and validation metrics.\n",
    "train_acc_overfit = history_overfit.history[\"accuracy\"][-1]\n",
    "val_acc_overfit = history_overfit.history[\"val_accuracy\"][-1]\n",
    "train_loss_overfit = history_overfit.history[\"loss\"][-1]\n",
    "val_loss_overfit = history_overfit.history[\"val_loss\"][-1]\n",
    "\n",
    "# Extract metrics for regularized model.\n",
    "train_acc_reg = history_reg.history[\"accuracy\"][-1]\n",
    "val_acc_reg = history_reg.history[\"val_accuracy\"][-1]\n",
    "train_loss_reg = history_reg.history[\"loss\"][-1]\n",
    "val_loss_reg = history_reg.history[\"val_loss\"][-1]\n",
    "\n",
    "# Print concise comparison to spot overfitting.\n",
    "print(\"Overfit model train acc:\", round(train_acc_overfit, 3))\n",
    "print(\"Overfit model val acc:\", round(val_acc_overfit, 3))\n",
    "print(\"Overfit model train loss:\", round(train_loss_overfit, 3))\n",
    "print(\"Overfit model val loss:\", round(val_loss_overfit, 3))\n",
    "\n",
    "# Print regularized model metrics for contrast.\n",
    "print(\"Regularized model train acc:\", round(train_acc_reg, 3))\n",
    "print(\"Regularized model val acc:\", round(val_acc_reg, 3))\n",
    "print(\"Regularized model train loss:\", round(train_loss_reg, 3))\n",
    "print(\"Regularized model val loss:\", round(val_loss_reg, 3))\n",
    "\n",
    "# Evaluate both models on held out test set.\n",
    "_, test_acc_overfit = model_overfit.evaluate(x_test, y_test, verbose=0)\n",
    "_, test_acc_reg = model_regularized.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "# Print final test accuracies to inspect generalization.\n",
    "print(\"Overfit model test acc:\", round(test_acc_overfit, 3))\n",
    "print(\"Regularized model test acc:\", round(test_acc_reg, 3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3055edb0",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**CNNs from Scratch**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5492f71f",
   "metadata": {},
   "source": [
    "\n",
    "In this lecture, you learned to:\n",
    "- Construct a convolutional neural network using Conv2d, pooling, and fully connected layers in PyTorch. \n",
    "- Train the CNN on a small image dataset using the standard training loop and appropriate loss and metrics. \n",
    "- Analyze model performance using accuracy, confusion matrices, and simple error inspection. \n",
    "\n",
    "In the next Lecture (Lecture B), we will go over 'Transfer Learning'"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
