{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23361883",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Transfer Learning**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122aacd1",
   "metadata": {},
   "source": [
    ">Last update: 20260129.\n",
    "    \n",
    "By the end of this Lecture, you will be able to:\n",
    "- Load pretrained vision models from torchvision and adapt them to new classification tasks. \n",
    "- Configure layer freezing and unfreezing strategies to balance training speed and performance. \n",
    "- Fine‑tune a pretrained model on a custom dataset and compare results to training from scratch. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462086e0",
   "metadata": {},
   "source": [
    "## **1. Using Pretrained Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17bab38",
   "metadata": {},
   "source": [
    "### **1.1. torchvision models overview**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b6d11c",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_05/Lecture_B/image_01_01.jpg?v=1769748490\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Torchvision offers many pretrained vision architectures\n",
    ">* These models transfer easily to new image tasks\n",
    "\n",
    ">* Torchvision offers many models with trade-off options\n",
    ">* Easily switch models to match hardware and goals\n",
    "\n",
    ">* Models split into feature extractor and head\n",
    ">* Reuse features, swap heads for new tasks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a815e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - torchvision models overview\n",
    "\n",
    "# This script introduces torchvision style model concepts.\n",
    "# We simulate a torchvision overview using TensorFlow models.\n",
    "# Focus is on pretrained style loading and adaptation.\n",
    "\n",
    "# TensorFlow is available by default in this environment.\n",
    "# !pip install tensorflow.\n",
    "\n",
    "# Import required TensorFlow and system modules.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version in one concise line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Prepare a tiny synthetic image dataset for demonstration.\n",
    "num_classes = 3\n",
    "image_height = 64\n",
    "image_width = 64\n",
    "channels = 3\n",
    "\n",
    "# Create small random image tensors and labels.\n",
    "num_samples = 60\n",
    "x_data = tf.random.uniform(\n",
    "    shape=(num_samples, image_height, image_width, channels)\n",
    ")\n",
    "y_data = tf.random.uniform(\n",
    "    shape=(num_samples,), maxval=num_classes, dtype=tf.int32\n",
    ")\n",
    "\n",
    "# Convert labels to categorical one hot encoding.\n",
    "y_data_cat = tf.keras.utils.to_categorical(\n",
    "    y_data, num_classes=num_classes\n",
    ")\n",
    "\n",
    "# Split data into simple train and validation subsets.\n",
    "train_size = 40\n",
    "x_train = x_data[:train_size]\n",
    "y_train = y_data_cat[:train_size]\n",
    "\n",
    "# Prepare validation subset from remaining samples.\n",
    "x_val = x_data[train_size:]\n",
    "y_val = y_data_cat[train_size:]\n",
    "\n",
    "# Confirm shapes are as expected before modeling.\n",
    "print(\"Train shape:\", x_train.shape, y_train.shape)\n",
    "print(\"Val shape:\", x_val.shape, y_val.shape)\n",
    "\n",
    "# Build a small base feature extractor like torchvision backbones.\n",
    "base_model = tf.keras.applications.MobileNetV2(\n",
    "    input_shape=(image_height, image_width, channels),\n",
    "    include_top=False,\n",
    "    weights=None\n",
    ")\n",
    "\n",
    "# Add a global pooling layer to compress spatial dimensions.\n",
    "global_pool = tf.keras.layers.GlobalAveragePooling2D()\n",
    "\n",
    "# Create a new classification head for our tiny task.\n",
    "classifier_head = tf.keras.layers.Dense(\n",
    "    num_classes, activation=\"softmax\"\n",
    ")\n",
    "\n",
    "# Connect base model and head into a full model.\n",
    "inputs = tf.keras.Input(\n",
    "    shape=(image_height, image_width, channels)\n",
    ")\n",
    "features = base_model(inputs, training=False)\n",
    "pooled = global_pool(features)\n",
    "outputs = classifier_head(pooled)\n",
    "\n",
    "# Define the complete model object.\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Show a short summary line about trainable layers.\n",
    "print(\"Total layers:\", len(model.layers))\n",
    "\n",
    "# Compile the model with simple optimizer and loss.\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Train briefly to simulate adapting a pretrained backbone.\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    validation_data=(x_val, y_val),\n",
    "    epochs=2,\n",
    "    batch_size=8,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Evaluate model performance on validation subset.\n",
    "val_loss, val_acc = model.evaluate(\n",
    "    x_val, y_val, verbose=0\n",
    ")\n",
    "\n",
    "# Print concise results to connect with overview ideas.\n",
    "print(\"Validation loss:\", round(float(val_loss), 4))\n",
    "print(\"Validation accuracy:\", round(float(val_acc), 4))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f8c1fb",
   "metadata": {},
   "source": [
    "### **1.2. Model Weights Metadata**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e211c3d",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_05/Lecture_B/image_01_02.jpg?v=1769748565\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Metadata describes how pretrained weights were created\n",
    ">* It guides correct inputs and reuse for tasks\n",
    "\n",
    ">* Metadata describes original labels and output mapping\n",
    ">* Guides replacing final layer and choosing models\n",
    "\n",
    ">* Metadata defines required preprocessing and input settings\n",
    ">* Performance metrics provide baselines and reveal training issues\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d257b230",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Model Weights Metadata\n",
    "\n",
    "# This script shows model weights metadata concepts.\n",
    "# We simulate torchvision style metadata using TensorFlow tools.\n",
    "# Focus is understanding metadata not heavy training.\n",
    "\n",
    "# Optional install for TensorFlow datasets if missing.\n",
    "# !pip install tensorflow-datasets.\n",
    "\n",
    "# Import standard libraries for environment checks.\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "\n",
    "# Import TensorFlow for simple image model utilities.\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set deterministic seeds for reproducible behavior.\n",
    "random.seed(42)\n",
    "\n",
    "# Set TensorFlow random seed for reproducibility.\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Print TensorFlow version in one concise line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Define a small dictionary mimicking weights metadata.\n",
    "resnet_like_metadata = {\n",
    "    \"arch\": \"resnet_like\",\n",
    "    \"dataset\": \"CIFAR10_small\",\n",
    "    \"num_classes\": 10,\n",
    "    \"input_size\": (32, 32),\n",
    "    \"input_channels\": 3,\n",
    "}\n",
    "\n",
    "# Add normalization statistics to the metadata dictionary.\n",
    "resnet_like_metadata.update({\n",
    "    \"mean\": (0.4914, 0.4822, 0.4465),\n",
    "    \"std\": (0.2470, 0.2435, 0.2616),\n",
    "})\n",
    "\n",
    "# Add label names and training recipe version.\n",
    "resnet_like_metadata.update({\n",
    "    \"labels\": [\n",
    "        \"airplane\",\n",
    "        \"automobile\",\n",
    "        \"bird\",\n",
    "        \"cat\",\n",
    "        \"deer\",\n",
    "        \"dog\",\n",
    "        \"frog\",\n",
    "        \"horse\",\n",
    "        \"ship\",\n",
    "        \"truck\",\n",
    "    ],\n",
    "    \"recipe\": \"v1_basic_augmentation\",\n",
    "})\n",
    "\n",
    "# Add simple accuracy metrics from original benchmark.\n",
    "resnet_like_metadata.update({\n",
    "    \"top1_acc\": 0.92,\n",
    "    \"top5_acc\": 0.995,\n",
    "})\n",
    "\n",
    "# Show key metadata fields that guide reuse.\n",
    "print(\"Original dataset:\", resnet_like_metadata[\"dataset\"])\n",
    "\n",
    "# Print expected input shape and channels from metadata.\n",
    "print(\"Expected input size:\", resnet_like_metadata[\"input_size\"])\n",
    "\n",
    "# Print normalization statistics for correct preprocessing.\n",
    "print(\"Channel mean:\", resnet_like_metadata[\"mean\"])\n",
    "\n",
    "# Print number of classes and first three labels.\n",
    "print(\"Num classes:\", resnet_like_metadata[\"num_classes\"])\n",
    "\n",
    "# Show a few example labels from original task.\n",
    "print(\"Example labels:\", resnet_like_metadata[\"labels\"][:3])\n",
    "\n",
    "# Print original benchmark accuracy for quick reference.\n",
    "print(\"Top1 accuracy:\", resnet_like_metadata[\"top1_acc\"])\n",
    "\n",
    "# Define a new smaller label space for transfer learning.\n",
    "new_labels = [\"cat\", \"dog\", \"horse\"]\n",
    "\n",
    "# Derive new number of classes from new labels list.\n",
    "new_num_classes = len(new_labels)\n",
    "\n",
    "# Build a tiny model head using metadata dimensions.\n",
    "inputs = tf.keras.Input(\n",
    "    shape=(\n",
    "        resnet_like_metadata[\"input_size\"][0],\n",
    "        resnet_like_metadata[\"input_size\"][1],\n",
    "        resnet_like_metadata[\"input_channels\"],\n",
    "    )\n",
    ")\n",
    "\n",
    "# Simulate frozen backbone with a simple layer.\n",
    "backbone_output = tf.keras.layers.GlobalAveragePooling2D()(inputs)\n",
    "\n",
    "# Create new classification head for custom labels.\n",
    "outputs = tf.keras.layers.Dense(\n",
    "    new_num_classes,\n",
    "    activation=\"softmax\",\n",
    ")(backbone_output)\n",
    "\n",
    "# Build the transfer model combining backbone and head.\n",
    "transfer_model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Verify output shape matches new label space.\n",
    "print(\"New head output shape:\", transfer_model.output_shape)\n",
    "\n",
    "# Confirm that metadata guided our adaptation choices.\n",
    "print(\"Adapted to labels:\", new_labels)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3356cd5",
   "metadata": {},
   "source": [
    "### **1.3. Input Size Constraints**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cc49ec",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_05/Lecture_B/image_01_03.jpg?v=1769748627\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Pretrained models expect specific image size and channels\n",
    ">* Match preprocessing to model requirements to avoid issues\n",
    "\n",
    ">* Input size affects accuracy and computation cost\n",
    ">* Resize images to model’s expected resolution carefully\n",
    "\n",
    ">* Different architectures tolerate different input resolutions\n",
    ">* Standardize size, crop or pad, then fine-tune\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38031773",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Input Size Constraints\n",
    "\n",
    "# This script shows image input size constraints.\n",
    "# We use TensorFlow to inspect resized image tensors.\n",
    "# Focus on how models expect specific input shapes.\n",
    "\n",
    "# !pip install tensorflow.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Import TensorFlow and image utilities.\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Set deterministic random seeds.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version briefly.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Define a simple image size helper function.\n",
    "def describe_tensor(name, tensor):\n",
    "    shape = tensor.shape\n",
    "    print(name, \"shape:\", tuple(shape))\n",
    "\n",
    "# Choose a target input size like common vision models.\n",
    "input_height, input_width = 224, 224\n",
    "num_channels = 3\n",
    "\n",
    "# Create a dummy RGB image with arbitrary size.\n",
    "original_height, original_width = 300, 400\n",
    "random_image = np.random.randint(\n",
    "    0, 256, size=(original_height, original_width, num_channels), dtype=np.uint8\n",
    ")\n",
    "\n",
    "# Convert the NumPy image to a TensorFlow tensor.\n",
    "image_tensor = tf.convert_to_tensor(random_image, dtype=tf.float32)\n",
    "\n",
    "# Show the original tensor shape.\n",
    "describe_tensor(\"Original\", image_tensor)\n",
    "\n",
    "# Build a resizing layer that mimics preprocessing.\n",
    "resize_layer = layers.Resizing(input_height, input_width)\n",
    "\n",
    "# Apply resizing to match model expected input.\n",
    "resized_tensor = resize_layer(tf.expand_dims(image_tensor, axis=0))\n",
    "\n",
    "# Remove batch dimension for easier shape display.\n",
    "resized_tensor_single = tf.squeeze(resized_tensor, axis=0)\n",
    "\n",
    "# Show the resized tensor shape.\n",
    "describe_tensor(\"Resized\", resized_tensor_single)\n",
    "\n",
    "# Validate that resized tensor matches expected size.\n",
    "expected_shape = (input_height, input_width, num_channels)\n",
    "if tuple(resized_tensor_single.shape) != expected_shape:\n",
    "    raise ValueError(\"Resized tensor shape mismatch with expected size\")\n",
    "\n",
    "# Demonstrate what happens with a grayscale single channel image.\n",
    "gray_image = np.random.randint(\n",
    "    0, 256, size=(original_height, original_width, 1), dtype=np.uint8\n",
    ")\n",
    "\n",
    "# Convert grayscale image to tensor.\n",
    "gray_tensor = tf.convert_to_tensor(gray_image, dtype=tf.float32)\n",
    "\n",
    "# Show grayscale tensor shape before adaptation.\n",
    "describe_tensor(\"Grayscale\", gray_tensor)\n",
    "\n",
    "# Convert grayscale to RGB by repeating channels.\n",
    "rgb_from_gray = tf.repeat(gray_tensor, repeats=3, axis=2)\n",
    "\n",
    "# Show adapted grayscale tensor shape.\n",
    "describe_tensor(\"Gray as RGB\", rgb_from_gray)\n",
    "\n",
    "# Resize the adapted grayscale tensor to expected size.\n",
    "resized_gray = resize_layer(tf.expand_dims(rgb_from_gray, axis=0))\n",
    "\n",
    "# Remove batch dimension again.\n",
    "resized_gray_single = tf.squeeze(resized_gray, axis=0)\n",
    "\n",
    "# Final confirmation print about input size handling.\n",
    "print(\"Final resized grayscale shape:\", tuple(resized_gray_single.shape))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1534ac9",
   "metadata": {},
   "source": [
    "## **2. Layer Freezing Techniques**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0144f0fb",
   "metadata": {},
   "source": [
    "### **2.1. Controlling Requires Grad**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc380e90",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_05/Lecture_B/image_02_01.jpg?v=1769748657\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Use requires_grad to choose which layers train\n",
    ">* Freezing layers saves compute and preserves pretrained features\n",
    "\n",
    ">* Freeze early layers to keep generic features\n",
    ">* Train deeper layers to adapt quickly with less data\n",
    "\n",
    ">* Adjust freezing depth to balance speed, performance\n",
    ">* Unfreeze more layers when domain differs significantly\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc238bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Controlling Requires Grad\n",
    "\n",
    "# This script shows controlling requires_grad conceptually.\n",
    "# We simulate freezing and unfreezing layers using simple flags.\n",
    "# Focus is on understanding gradient flow decisions.\n",
    "\n",
    "# TensorFlow is available by default in this environment.\n",
    "# Uncomment next line if running elsewhere and missing tensorflow.\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required modules from TensorFlow.\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set a global random seed for deterministic behavior.\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Print TensorFlow version in one concise line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Create a tiny dummy image batch with fixed values.\n",
    "images = tf.ones((4, 8, 8, 3))\n",
    "\n",
    "# Create tiny dummy labels for a three class task.\n",
    "labels = tf.constant([0, 1, 2, 1], dtype=tf.int32)\n",
    "\n",
    "# Build a simple sequential convolutional vision model.\n",
    "base_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(4, 3, activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Conv2D(8, 3, activation=\"relu\"),\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "])\n",
    "\n",
    "# Build a small classification head for three classes.\n",
    "classifier_head = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(16, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(3, activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "# Define a simple optimizer for demonstration.\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)\n",
    "\n",
    "# Define a sparse categorical crossentropy loss function.\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "# Helper function to run one training step with control flags.\n",
    "def train_step(freeze_base, freeze_head):\n",
    "    # Use GradientTape to record operations for gradients.\n",
    "    with tf.GradientTape(persistent=False) as tape:\n",
    "        # Forward pass through base model.\n",
    "        features = base_model(images, training=True)\n",
    "        \n",
    "        # Forward pass through classifier head.\n",
    "        logits = classifier_head(features, training=True)\n",
    "        \n",
    "        # Compute loss between predictions and labels.\n",
    "        loss = loss_fn(labels, logits)\n",
    "    \n",
    "    # Collect trainable variables from both parts.\n",
    "    base_vars = base_model.trainable_variables\n",
    "    head_vars = classifier_head.trainable_variables\n",
    "    \n",
    "    # Decide which variables should receive gradients.\n",
    "    train_vars = []\n",
    "    \n",
    "    # Append base variables only when not frozen.\n",
    "    if not freeze_base:\n",
    "        train_vars.extend(base_vars)\n",
    "    \n",
    "    # Append head variables only when not frozen.\n",
    "    if not freeze_head:\n",
    "        train_vars.extend(head_vars)\n",
    "    \n",
    "    # Compute gradients only for selected variables.\n",
    "    grads = tape.gradient(loss, train_vars)\n",
    "    \n",
    "    # Apply gradients to update selected parameters.\n",
    "    if train_vars:\n",
    "        optimizer.apply_gradients(zip(grads, train_vars))\n",
    "    \n",
    "    # Return scalar loss value for inspection.\n",
    "    return float(loss.numpy())\n",
    "\n",
    "# Run one step with base frozen and head trainable.\n",
    "loss_freeze_base = train_step(freeze_base=True, freeze_head=False)\n",
    "\n",
    "# Run one step with both base and head trainable.\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)\n",
    "loss_train_all = train_step(freeze_base=False, freeze_head=False)\n",
    "\n",
    "# Run one step with both base and head frozen.\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)\n",
    "loss_freeze_all = train_step(freeze_base=True, freeze_head=True)\n",
    "\n",
    "# Print concise summary of the three scenarios.\n",
    "print(\"Loss with frozen base, trainable head:\", loss_freeze_base)\n",
    "\n",
    "# Show loss when all layers are trainable together.\n",
    "print(\"Loss with all layers trainable:\", loss_train_all)\n",
    "\n",
    "# Show loss when everything is frozen, no updates applied.\n",
    "print(\"Loss with all layers frozen:\", loss_freeze_all)\n",
    "\n",
    "# Print how many variables received gradient updates each case.\n",
    "print(\"Trainable variables when base frozen:\", len(classifier_head.trainable_variables))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ae4f4f",
   "metadata": {},
   "source": [
    "### **2.2. Feature Extractor Mode**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a958b8e",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_05/Lecture_B/image_02_02.jpg?v=1769748736\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Use pretrained network as frozen feature generator\n",
    ">* Train small new classifier, saving time and data\n",
    "\n",
    ">* Freeze backbone, replace and train new head\n",
    ">* Use frozen features while updating final classifier\n",
    "\n",
    ">* Best when domains are similar or data scarce\n",
    ">* Provides strong baseline and enables later fine-tuning\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f67e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Feature Extractor Mode\n",
    "\n",
    "# This script shows feature extractor mode basics.\n",
    "# We use TensorFlow to mimic transfer learning.\n",
    "# Focus on freezing layers and training classifier.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Import TensorFlow and Keras utilities.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Load MNIST dataset for a tiny vision example.\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Select a small subset to keep runtime short.\n",
    "train_samples = 2000\n",
    "test_samples = 500\n",
    "x_train = x_train[:train_samples]\n",
    "y_train = y_train[:train_samples]\n",
    "\n",
    "# Slice test data subset safely.\n",
    "x_test = x_test[:test_samples]\n",
    "y_test = y_test[:test_samples]\n",
    "\n",
    "# Validate shapes before further processing.\n",
    "print(\"Train shape:\", x_train.shape, y_train.shape)\n",
    "print(\"Test shape:\", x_test.shape, y_test.shape)\n",
    "\n",
    "# Normalize pixel values to range zero one.\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_test = x_test.astype(\"float32\") / 255.0\n",
    "\n",
    "# Add channel dimension for convolutional layers.\n",
    "x_train = np.expand_dims(x_train, axis=-1)\n",
    "x_test = np.expand_dims(x_test, axis=-1)\n",
    "\n",
    "# Confirm new shapes after expanding channels.\n",
    "print(\"Train shape after expand:\", x_train.shape)\n",
    "print(\"Test shape after expand:\", x_test.shape)\n",
    "\n",
    "# Build a small convolutional backbone model.\n",
    "backbone_inputs = keras.Input(shape=(28, 28, 1))\n",
    "x = layers.Conv2D(16, (3, 3), activation=\"relu\")(backbone_inputs)\n",
    "x = layers.MaxPooling2D((2, 2))(x)\n",
    "\n",
    "# Add another convolutional block for richer features.\n",
    "x = layers.Conv2D(32, (3, 3), activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D((2, 2))(x)\n",
    "\n",
    "# Flatten features to feed the classifier head.\n",
    "x = layers.Flatten()(x)\n",
    "backbone_outputs = layers.Dense(64, activation=\"relu\")(x)\n",
    "backbone_model = keras.Model(backbone_inputs, backbone_outputs)\n",
    "\n",
    "# Pretend backbone is pretrained by quick training.\n",
    "pretrain_model = keras.Sequential([\n",
    "    backbone_model,\n",
    "    layers.Dense(10, activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "# Compile pretraining model with simple settings.\n",
    "pretrain_model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Pretrain briefly to simulate learned features.\n",
    "pretrain_model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=1,\n",
    "    batch_size=64,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Freeze backbone to enable feature extractor mode.\n",
    "for layer in backbone_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Build new classifier head for a new task.\n",
    "new_inputs = keras.Input(shape=(28, 28, 1))\n",
    "features = backbone_model(new_inputs, training=False)\n",
    "\n",
    "# Add small dense layer and new output layer.\n",
    "features = layers.Dense(32, activation=\"relu\")(features)\n",
    "new_outputs = layers.Dense(10, activation=\"softmax\")(features)\n",
    "feature_extractor_model = keras.Model(new_inputs, new_outputs)\n",
    "\n",
    "# Confirm which layers are trainable or frozen.\n",
    "trainable_status = [(layer.name, layer.trainable)\n",
    "                    for layer in feature_extractor_model.layers]\n",
    "print(\"Layer trainable flags:\")\n",
    "print(trainable_status)\n",
    "\n",
    "# Compile feature extractor model for fine tuning.\n",
    "feature_extractor_model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Train only the new head while backbone stays frozen.\n",
    "feature_extractor_model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=2,\n",
    "    batch_size=64,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Evaluate performance on the small test subset.\n",
    "loss, acc = feature_extractor_model.evaluate(\n",
    "    x_test,\n",
    "    y_test,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Print concise summary of feature extractor results.\n",
    "print(\"Feature extractor test accuracy:\", round(acc, 4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc8c5c8",
   "metadata": {},
   "source": [
    "### **2.3. Progressive Layer Unfreezing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a39f9d",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_05/Lecture_B/image_02_03.jpg?v=1769748808\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Gradually unfreeze deeper layers during training\n",
    ">* Preserves pretrained features while reducing overfitting risk\n",
    "\n",
    ">* Start with higher learning rates on head\n",
    ">* Unfreeze blocks gradually, stopping when validation worsens\n",
    "\n",
    ">* Focus training on most important model layers\n",
    ">* Adapt unfreezing depth to domain shift size\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfaf118",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Progressive Layer Unfreezing\n",
    "\n",
    "# This script shows progressive layer unfreezing.\n",
    "# We use TensorFlow for a tiny vision example.\n",
    "# Focus on freezing and unfreezing convolutional layers.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Import TensorFlow and Keras utilities.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Load MNIST dataset from Keras datasets.\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Use a very small subset for quick training.\n",
    "train_samples = 2000\n",
    "\n",
    "\n",
    "test_samples = 500\n",
    "x_train = x_train[:train_samples]\n",
    "y_train = y_train[:train_samples]\n",
    "\n",
    "# Slice test data subset safely.\n",
    "x_test = x_test[:test_samples]\n",
    "y_test = y_test[:test_samples]\n",
    "\n",
    "# Expand grayscale images to three channels.\n",
    "x_train = np.repeat(x_train[..., np.newaxis], 3, axis=3)\n",
    "x_test = np.repeat(x_test[..., np.newaxis], 3, axis=3)\n",
    "\n",
    "# Resize images to match small CNN expectations.\n",
    "img_size = 32\n",
    "x_train = tf.image.resize(x_train, (img_size, img_size)).numpy()\n",
    "x_test = tf.image.resize(x_test, (img_size, img_size)).numpy()\n",
    "\n",
    "# Normalize pixel values to range zero one.\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_test = x_test.astype(\"float32\") / 255.0\n",
    "\n",
    "# Confirm shapes are as expected.\n",
    "print(\"Train shape:\", x_train.shape, \"Test shape:\", x_test.shape)\n",
    "\n",
    "# Build a small convolutional backbone model.\n",
    "def build_backbone():\n",
    "    inputs = keras.Input(shape=(img_size, img_size, 3))\n",
    "    x = layers.Conv2D(16, 3, activation=\"relu\")(inputs)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "    x = layers.Conv2D(32, 3, activation=\"relu\")(x)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "    x = layers.Conv2D(64, 3, activation=\"relu\")(x)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    model = keras.Model(inputs, x, name=\"backbone\")\n",
    "    return model\n",
    "\n",
    "# Create backbone and classification head model.\n",
    "backbone = build_backbone()\n",
    "backbone.trainable = False\n",
    "inputs = keras.Input(shape=(img_size, img_size, 3))\n",
    "features = backbone(inputs)\n",
    "outputs = layers.Dense(10, activation=\"softmax\")(features)\n",
    "model = keras.Model(inputs, outputs, name=\"mnist_progressive\")\n",
    "\n",
    "# Compile model with simple optimizer and loss.\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Train only the new classification head first.\n",
    "history_stage1 = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=2,\n",
    "    batch_size=64,\n",
    "    verbose=0,\n",
    "    validation_split=0.1,\n",
    ")\n",
    "\n",
    "# Evaluate after first stage training.\n",
    "loss1, acc1 = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Stage1 test accuracy (head only):\", round(acc1, 3))\n",
    "\n",
    "# Progressive unfreezing of deeper backbone layers.\n",
    "for layer in backbone.layers[-2:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Recompile with lower learning rate for stability.\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=5e-4),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Train again with partially unfrozen backbone.\n",
    "history_stage2 = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=2,\n",
    "    batch_size=64,\n",
    "    verbose=0,\n",
    "    validation_split=0.1,\n",
    ")\n",
    "\n",
    "# Evaluate after progressive unfreezing stage.\n",
    "loss2, acc2 = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Stage2 test accuracy (unfrozen tail):\", round(acc2, 3))\n",
    "\n",
    "# Show how many backbone layers are now trainable.\n",
    "trainable_count = np.sum([int(l.trainable) for l in backbone.layers])\n",
    "print(\"Trainable backbone layers after stage2:\", int(trainable_count))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9bfac4",
   "metadata": {},
   "source": [
    "## **3. Fine Tuning Experiments**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4338e406",
   "metadata": {},
   "source": [
    "### **3.1. Dataset Label Mapping**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a370f33d",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_05/Lecture_B/image_03_01.jpg?v=1769748887\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Define consistent mapping from classes to integers\n",
    ">* Use same mapping across pipeline to avoid errors\n",
    "\n",
    ">* Define class list and fixed label indices\n",
    ">* Keep mapping consistent across pipeline to avoid mislabels\n",
    "\n",
    ">* Consistent labels are required for fair comparisons\n",
    ">* Shared canonical mapping improves reliability and reproducibility\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcca9dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Dataset Label Mapping\n",
    "\n",
    "# This script shows dataset label mapping.\n",
    "# We use TensorFlow image dataset utilities.\n",
    "# Focus is on clear class index mapping.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Import TensorFlow and image utilities.\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Set deterministic random seeds.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# Configure TensorFlow global seed.\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version once.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Create a tiny directory dataset structure.\n",
    "base_dir = \"tiny_animals_dataset\"\n",
    "class_names = [\"cat\", \"dog\"]\n",
    "\n",
    "# Ensure base directory exists.\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "# Create one subdirectory per class.\n",
    "for name in class_names:\n",
    "    os.makedirs(os.path.join(base_dir, name), exist_ok=True)\n",
    "\n",
    "# Create small random images and save.\n",
    "height, width, channels = 32, 32, 3\n",
    "num_images_per_class = 3\n",
    "\n",
    "# Helper function to create random image.\n",
    "def create_random_image(path):\n",
    "    array = np.random.randint(0, 256, (height, width, channels))\n",
    "    img = tf.keras.utils.array_to_img(array)\n",
    "    img.save(path)\n",
    "\n",
    "# Generate images for each class folder.\n",
    "for name in class_names:\n",
    "    folder = os.path.join(base_dir, name)\n",
    "    for i in range(num_images_per_class):\n",
    "        filename = f\"img_{i}.png\"\n",
    "        create_random_image(os.path.join(folder, filename))\n",
    "\n",
    "# Build a deterministic label mapping dictionary.\n",
    "label_to_index = {name: idx for idx, name in enumerate(class_names)}\n",
    "\n",
    "# Also build the inverse mapping dictionary.\n",
    "index_to_label = {idx: name for name, idx in label_to_index.items()}\n",
    "\n",
    "# Print both mappings clearly.\n",
    "print(\"Label to index mapping:\", label_to_index)\n",
    "print(\"Index to label mapping:\", index_to_label)\n",
    "\n",
    "# Load dataset using image_dataset_from_directory.\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    base_dir,\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"int\",\n",
    "    class_names=class_names,\n",
    "    image_size=(height, width),\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    "    seed=seed_value,\n",
    ")\n",
    "\n",
    "# Take one small batch from dataset.\n",
    "images, labels = next(iter(train_ds))\n",
    "\n",
    "# Validate shapes before further use.\n",
    "print(\"Batch images shape:\", images.shape)\n",
    "print(\"Batch labels shape:\", labels.shape)\n",
    "\n",
    "# Show how numeric labels map to names.\n",
    "unique_labels = sorted(set(labels.numpy().tolist()))\n",
    "\n",
    "# Print mapping used by the dataset.\n",
    "for idx in unique_labels:\n",
    "    print(\"Dataset label\", idx, \"means\", index_to_label[int(idx)])\n",
    "\n",
    "# Build a tiny model head matching label count.\n",
    "num_classes = len(class_names)\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Input(shape=(height, width, channels)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(8, activation=\"relu\"),\n",
    "    layers.Dense(num_classes, activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "# Compile model with sparse categorical crossentropy.\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Train briefly to show mapping usage.\n",
    "history = model.fit(train_ds, epochs=1, verbose=0)\n",
    "\n",
    "# Evaluate once to complete example.\n",
    "loss, acc = model.evaluate(train_ds, verbose=0)\n",
    "print(\"Tiny model accuracy with mapping:\", float(acc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad7b59d",
   "metadata": {},
   "source": [
    "### **3.2. Choosing Learning Rates**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae371dd",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_05/Lecture_B/image_03_02.jpg?v=1769748948\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Learning rate controls how strongly weights change\n",
    ">* Balance preserving pretrained features with adapting quickly\n",
    "\n",
    ">* Use higher learning rate for new head\n",
    ">* Use lower rate for backbone; differential learning\n",
    "\n",
    ">* Treat learning rate choice as systematic experiments\n",
    ">* Compare runs to balance stability, adaptation, and baselines\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2011f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Choosing Learning Rates\n",
    "\n",
    "# This script compares two learning rates for fine tuning.\n",
    "# We use a tiny CNN on a small MNIST subset.\n",
    "# Focus is on how learning rate changes training.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Import tensorflow and keras utilities.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Load MNIST dataset from keras datasets.\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Select a small subset for quick experiments.\n",
    "num_train = 4000\n",
    "num_val = 1000\n",
    "x_train = x_train[: num_train + num_val]\n",
    "y_train = y_train[: num_train + num_val]\n",
    "\n",
    "# Split subset into train and validation parts.\n",
    "x_val = x_train[num_train:]\n",
    "y_val = y_train[num_train:]\n",
    "x_train = x_train[:num_train]\n",
    "y_train = y_train[:num_train]\n",
    "\n",
    "# Normalize images to range zero one.\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_val = x_val.astype(\"float32\") / 255.0\n",
    "x_test = x_test.astype(\"float32\") / 255.0\n",
    "\n",
    "# Add channel dimension for convolutional layers.\n",
    "x_train = np.expand_dims(x_train, axis=-1)\n",
    "x_val = np.expand_dims(x_val, axis=-1)\n",
    "x_test = np.expand_dims(x_test, axis=-1)\n",
    "\n",
    "# Validate shapes before building models.\n",
    "print(\"Train shape:\", x_train.shape, y_train.shape)\n",
    "\n",
    "# Define a simple convolutional feature extractor.\n",
    "def create_backbone():\n",
    "    inputs = keras.Input(shape=(28, 28, 1))\n",
    "    x = layers.Conv2D(16, 3, activation=\"relu\")(inputs)\n",
    "    x = layers.MaxPooling2D(2)(x)\n",
    "    x = layers.Conv2D(32, 3, activation=\"relu\")(x)\n",
    "    x = layers.MaxPooling2D(2)(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    outputs = layers.Dense(64, activation=\"relu\")(x)\n",
    "    model = keras.Model(inputs, outputs, name=\"backbone\")\n",
    "    return model\n",
    "\n",
    "# Create and train a backbone to simulate pretraining.\n",
    "backbone = create_backbone()\n",
    "backbone.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Attach a temporary head for pretraining classification.\n",
    "pretrain_outputs = layers.Dense(10, activation=\"softmax\")(backbone.output)\n",
    "pretrain_model = keras.Model(backbone.input, pretrain_outputs)\n",
    "pretrain_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Pretrain briefly with silent training settings.\n",
    "pretrain_model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=2,\n",
    "    batch_size=64,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Freeze backbone to simulate starting fine tuning.\n",
    "backbone.trainable = False\n",
    "\n",
    "# Function builds full model with new classification head.\n",
    "def build_finetune_model(backbone_model):\n",
    "    inputs = keras.Input(shape=(28, 28, 1))\n",
    "    x = backbone_model(inputs, training=False)\n",
    "    outputs = layers.Dense(10, activation=\"softmax\")(x)\n",
    "    model = keras.Model(inputs, outputs, name=\"finetune_model\")\n",
    "    return model\n",
    "\n",
    "# Build two identical models for different learning rates.\n",
    "model_low_lr = build_finetune_model(backbone)\n",
    "model_high_lr = build_finetune_model(backbone)\n",
    "\n",
    "# Choose two learning rates for comparison.\n",
    "low_lr = 1e-4\n",
    "high_lr = 1e-2\n",
    "\n",
    "# Compile models with different learning rates.\n",
    "model_low_lr.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=low_lr),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Compile second model with higher learning rate.\n",
    "model_high_lr.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=high_lr),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Train both models briefly on same data.\n",
    "history_low = model_low_lr.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=2,\n",
    "    batch_size=64,\n",
    "    validation_data=(x_val, y_val),\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Train high learning rate model silently.\n",
    "history_high = model_high_lr.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=2,\n",
    "    batch_size=64,\n",
    "    validation_data=(x_val, y_val),\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Evaluate both models on validation data.\n",
    "val_loss_low, val_acc_low = model_low_lr.evaluate(\n",
    "    x_val,\n",
    "    y_val,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Evaluate high learning rate model silently.\n",
    "val_loss_high, val_acc_high = model_high_lr.evaluate(\n",
    "    x_val,\n",
    "    y_val,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Print concise comparison of learning rate effects.\n",
    "print(\"Low lr:\", low_lr, \"val_acc:\", round(val_acc_low, 4))\n",
    "print(\"High lr:\", high_lr, \"val_acc:\", round(val_acc_high, 4))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb54635e",
   "metadata": {},
   "source": [
    "### **3.3. Comparing baselines**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c08e74",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_05/Lecture_B/image_03_03.jpg?v=1769749030\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Compare fine tuned models to strong baselines\n",
    ">* Match setups to isolate benefits of pretraining\n",
    "\n",
    ">* Include realistic lightweight and non-deep-learning baselines\n",
    ">* Compare to baselines to judge real-world benefits\n",
    "\n",
    ">* Compare models across learning speed and robustness\n",
    ">* Evaluate multiple metrics to judge fine tuning benefits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb5e632",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Comparing baselines\n",
    "\n",
    "# This script compares simple vision baselines.\n",
    "# It uses TensorFlow for quick experiments.\n",
    "# Focus is on comparing training from scratch.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Import TensorFlow and Keras utilities.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Load MNIST dataset from Keras datasets.\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Use a small subset for quick experiments.\n",
    "train_samples = 4000\n",
    "test_samples = 1000\n",
    "x_train = x_train[:train_samples]\n",
    "y_train = y_train[:train_samples]\n",
    "\n",
    "# Slice test data subset safely.\n",
    "x_test = x_test[:test_samples]\n",
    "y_test = y_test[:test_samples]\n",
    "\n",
    "# Validate shapes before further processing.\n",
    "print(\"Train shape:\", x_train.shape, y_train.shape)\n",
    "print(\"Test shape:\", x_test.shape, y_test.shape)\n",
    "\n",
    "# Normalize images to range zero one.\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_test = x_test.astype(\"float32\") / 255.0\n",
    "\n",
    "# Add channel dimension for convolution layers.\n",
    "x_train = np.expand_dims(x_train, axis=-1)\n",
    "x_test = np.expand_dims(x_test, axis=-1)\n",
    "\n",
    "# Confirm new shapes after expansion.\n",
    "print(\"Train shape expanded:\", x_train.shape)\n",
    "print(\"Test shape expanded:\", x_test.shape)\n",
    "\n",
    "# Define a simple CNN model builder.\n",
    "def build_cnn_model():\n",
    "    inputs = keras.Input(shape=(28, 28, 1))\n",
    "    x = layers.Conv2D(16, (3, 3), activation=\"relu\")(inputs)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Conv2D(32, (3, 3), activation=\"relu\")(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(64, activation=\"relu\")(x)\n",
    "    outputs = layers.Dense(10, activation=\"softmax\")(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# Build baseline model trained from scratch.\n",
    "baseline_model = build_cnn_model()\n",
    "\n",
    "# Compile baseline model with simple settings.\n",
    "baseline_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Train baseline model silently for few epochs.\n",
    "history_baseline = baseline_model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=3,\n",
    "    batch_size=64,\n",
    "    verbose=0,\n",
    "    validation_split=0.1,\n",
    ")\n",
    "\n",
    "# Evaluate baseline model on test subset.\n",
    "loss_base, acc_base = baseline_model.evaluate(\n",
    "    x_test,\n",
    "    y_test,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Simulate pretrained model by extra pretraining.\n",
    "pretrained_model = build_cnn_model()\n",
    "\n",
    "# Compile pretrained model with same settings.\n",
    "pretrained_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Pretrain on same data for additional epochs.\n",
    "pretrained_model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=2,\n",
    "    batch_size=64,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Fine tune pretrained model for few epochs.\n",
    "history_finetune = pretrained_model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=3,\n",
    "    batch_size=64,\n",
    "    verbose=0,\n",
    "    validation_split=0.1,\n",
    ")\n",
    "\n",
    "# Evaluate fine tuned model on test subset.\n",
    "loss_ft, acc_ft = pretrained_model.evaluate(\n",
    "    x_test,\n",
    "    y_test,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Print concise comparison of both baselines.\n",
    "print(\"Baseline test accuracy:\", round(acc_base, 4))\n",
    "print(\"Fine tuned test accuracy:\", round(acc_ft, 4))\n",
    "print(\"Accuracy difference:\", round(acc_ft - acc_base, 4))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99357848",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Transfer Learning**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15800bc7",
   "metadata": {},
   "source": [
    "\n",
    "In this lecture, you learned to:\n",
    "- Load pretrained vision models from torchvision and adapt them to new classification tasks. \n",
    "- Configure layer freezing and unfreezing strategies to balance training speed and performance. \n",
    "- Fine‑tune a pretrained model on a custom dataset and compare results to training from scratch. \n",
    "\n",
    "In the next Module (Module 6), we will go over 'NLP with PyTorch'"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
