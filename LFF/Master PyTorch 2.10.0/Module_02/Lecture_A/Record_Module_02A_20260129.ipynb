{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f91c3178",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Autograd Mechanics**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b67521",
   "metadata": {},
   "source": [
    ">Last update: 20260129.\n",
    "    \n",
    "By the end of this Lecture, you will be able to:\n",
    "- Describe how PyTorch builds dynamic computation graphs and uses them to compute gradients. \n",
    "- Use requires_grad, backward, and grad attributes to compute and inspect gradients for simple tensor operations. \n",
    "- Control gradient tracking with torch.no_grad and detach to optimize performance and avoid unintended graph creation. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaeb9690",
   "metadata": {},
   "source": [
    "## **1. Dynamic Computation Graphs**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f5f0ec",
   "metadata": {},
   "source": [
    "### **1.1. Dynamic vs Static Graphs**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8418a424",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_02/Lecture_A/image_01_01.jpg?v=1769694953\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Static graphs are fixed blueprints, hard to change\n",
    ">* Dynamic graphs are built step-by-step during execution\n",
    "\n",
    ">* Dynamic graphs follow normal code control flow\n",
    ">* They simplify flexible, data-dependent model behavior\n",
    "\n",
    ">* Dynamic graph records forward pass for backpropagation\n",
    ">* Enables flexible models while keeping gradients correct\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1818441",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Dynamic vs Static Graphs\n",
    "\n",
    "# This script compares dynamic and static graphs.\n",
    "# It uses PyTorch style ideas with TensorFlow.\n",
    "# Focus on control flow and gradient behavior.\n",
    "\n",
    "# !pip install tensorflow.\n",
    "\n",
    "# Import TensorFlow and enable eager execution.\n",
    "import tensorflow as tf\n",
    "\n",
    "# Print TensorFlow version for reference.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Set a global random seed for determinism.\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "# Define a simple function using dynamic control flow.\n",
    "@tf.function(experimental_relax_shapes=True)\n",
    "def static_style_fn(x):\n",
    "    # Use TensorFlow control flow inside tf.function.\n",
    "    y = x * 2.0\n",
    "    \n",
    "    # Add a conditional branch depending on value.\n",
    "    y = tf.cond(x > 0.0, lambda: y + 1.0, lambda: y - 1.0)\n",
    "    \n",
    "    # Return squared result as final output.\n",
    "    return y * y\n",
    "\n",
    "# Define a pure eager function for dynamic style.\n",
    "def dynamic_style_fn(x):\n",
    "    # Use normal Python control flow with tensors.\n",
    "    y = x * 2.0\n",
    "    \n",
    "    # Branch using regular Python if statement.\n",
    "    if x > 0.0:\n",
    "        y = y + 1.0\n",
    "    else:\n",
    "        y = y - 1.0\n",
    "    \n",
    "    # Return squared result as final output.\n",
    "    return y * y\n",
    "\n",
    "# Create a scalar tensor with gradient tracking.\n",
    "x = tf.Variable(2.0)\n",
    "\n",
    "# Use GradientTape to record dynamic operations.\n",
    "with tf.GradientTape() as tape_dynamic:\n",
    "    # Call the eager dynamic style function.\n",
    "    y_dynamic = dynamic_style_fn(x)\n",
    "\n",
    "# Compute gradient dy/dx for dynamic style.\n",
    "grad_dynamic = tape_dynamic.gradient(y_dynamic, x)\n",
    "\n",
    "# Use GradientTape with the static style function.\n",
    "with tf.GradientTape() as tape_static:\n",
    "    # Call the tf.function compiled static style.\n",
    "    y_static = static_style_fn(x)\n",
    "\n",
    "# Compute gradient dy/dx for static style.\n",
    "grad_static = tape_static.gradient(y_static, x)\n",
    "\n",
    "# Print outputs to compare behaviors.\n",
    "print(\"Input x:\", float(x.numpy()))\n",
    "print(\"Dynamic style y:\", float(y_dynamic.numpy()))\n",
    "print(\"Static style y:\", float(y_static.numpy()))\n",
    "print(\"Dynamic style grad:\", float(grad_dynamic.numpy()))\n",
    "print(\"Static style grad:\", float(grad_static.numpy()))\n",
    "\n",
    "# Show that dynamic style can change branches easily.\n",
    "x.assign(-1.0)\n",
    "\n",
    "# Recompute using dynamic style with new value.\n",
    "with tf.GradientTape() as tape_dynamic2:\n",
    "    y_dynamic2 = dynamic_style_fn(x)\n",
    "\n",
    "# Compute gradient for new dynamic path.\n",
    "grad_dynamic2 = tape_dynamic2.gradient(y_dynamic2, x)\n",
    "\n",
    "# Print new results showing different path.\n",
    "print(\"New input x:\", float(x.numpy()))\n",
    "print(\"New dynamic y:\", float(y_dynamic2.numpy()))\n",
    "print(\"New dynamic grad:\", float(grad_dynamic2.numpy()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448ad849",
   "metadata": {},
   "source": [
    "### **1.2. Autograd Function Graph**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de55f84",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_02/Lecture_A/image_01_02.jpg?v=1769695025\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Each tensor operation becomes a node in autograd\n",
    ">* Nodes link functions and tensors, encoding dependencies\n",
    "\n",
    ">* Graph flows from inputs through layered operations\n",
    ">* Each node knows how to run backward for gradients\n",
    "\n",
    ">* Graph records only actually executed operations\n",
    ">* Supports complex control flow while enabling backpropagation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bce48bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Autograd Function Graph\n",
    "\n",
    "# This script explains PyTorch autograd graphs.\n",
    "# It shows how operations create function nodes.\n",
    "# It keeps outputs small and beginner friendly.\n",
    "\n",
    "# !pip install torch torchvision torchaudio.\n",
    "\n",
    "# Import torch for tensor and autograd features.\n",
    "import torch\n",
    "\n",
    "# Print PyTorch version in one short line.\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "\n",
    "# Create a simple input tensor with gradients enabled.\n",
    "x = torch.tensor([2.0, 3.0], requires_grad=True)\n",
    "\n",
    "# Verify tensor shape to avoid unexpected issues.\n",
    "assert x.shape == (2,), \"Input tensor must have length two\"\n",
    "\n",
    "# Build a small computation that uses x twice.\n",
    "y = x[0] ** 2 + 3 * x[1]\n",
    "\n",
    "# Confirm output is a scalar suitable for backward.\n",
    "assert y.dim() == 0, \"Output y must be a scalar\"\n",
    "\n",
    "# Inspect requires_grad flag and value of y.\n",
    "print(\"x requires_grad:\", x.requires_grad)\n",
    "print(\"y value:\", y.item())\n",
    "\n",
    "# Show which function created y in the graph.\n",
    "print(\"y.grad_fn type:\", type(y.grad_fn).__name__)\n",
    "\n",
    "# Call backward to traverse the autograd function graph.\n",
    "y.backward()\n",
    "\n",
    "# Inspect gradients accumulated on the leaf tensor x.\n",
    "print(\"Gradient dy/dx:\", x.grad)\n",
    "\n",
    "# Build another tensor from x without tracking gradients.\n",
    "with torch.no_grad():\n",
    "    z = x * 5.0 + 1.0\n",
    "\n",
    "# Confirm z does not require gradients or graph links.\n",
    "print(\"z requires_grad:\", z.requires_grad)\n",
    "print(\"z grad_fn is:\", z.grad_fn)\n",
    "\n",
    "# Create a detached view that breaks the function graph.\n",
    "x_detached = x.detach()\n",
    "\n",
    "# Show that detached tensor no longer tracks gradients.\n",
    "print(\"Detached requires_grad:\", x_detached.requires_grad)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f754208b",
   "metadata": {},
   "source": [
    "### **1.3. Gradient Accumulation Essentials**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad5f76d",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_02/Lecture_A/image_01_03.jpg?v=1769695052\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Computation graph also stores and updates gradients\n",
    ">* Repeated backward calls sum gradients before parameter updates\n",
    "\n",
    ">* Process small batches, let gradients build up\n",
    ">* Accumulated gradients mimic large batches with less memory\n",
    "\n",
    ">* Each forward pass builds a temporary computation graph\n",
    ">* Gradients accumulate across passes; reset them carefully\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc91a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Gradient Accumulation Essentials\n",
    "\n",
    "# This script explains gradient accumulation basics.\n",
    "# It uses tiny tensors for clear illustration.\n",
    "# Run cells in order to follow comments.\n",
    "\n",
    "# !pip install torch torchvision torchaudio.\n",
    "\n",
    "# Import torch for tensor and autograd operations.\n",
    "import torch\n",
    "\n",
    "# Set a deterministic seed for reproducibility.\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Check and print the PyTorch version briefly.\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "\n",
    "# Create a simple parameter tensor with gradients.\n",
    "param = torch.tensor([2.0], requires_grad=True)\n",
    "\n",
    "# Show initial parameter and its gradient value.\n",
    "print(\"Initial param:\", param.item())\n",
    "print(\"Initial grad:\", param.grad)\n",
    "\n",
    "# Define a helper function to compute a tiny loss.\n",
    "def compute_loss(scale: float) -> torch.Tensor:\n",
    "    # Compute a simple squared loss scaled externally.\n",
    "    loss = (scale * param) ** 2\n",
    "    return loss\n",
    "\n",
    "# First forward pass with scale one for loss.\n",
    "loss1 = compute_loss(scale=1.0)\n",
    "\n",
    "# Backward computes gradient and stores in param.grad.\n",
    "loss1.backward()\n",
    "\n",
    "# Show gradient after first backward call.\n",
    "print(\"Grad after first backward:\", param.grad.item())\n",
    "\n",
    "# Second forward pass with different loss scale.\n",
    "loss2 = compute_loss(scale=0.5)\n",
    "\n",
    "# Backward again accumulates into existing gradient.\n",
    "loss2.backward()\n",
    "\n",
    "# Show gradient after second backward accumulation.\n",
    "print(\"Grad after second backward:\", param.grad.item())\n",
    "\n",
    "# Manually compute expected accumulated gradient value.\n",
    "expected_grad = 2 * 1.0 ** 2 * param.detach() + 2 * (0.5 ** 2) * param.detach()\n",
    "\n",
    "# Print expected gradient for comparison clarity.\n",
    "print(\"Expected accumulated grad:\", expected_grad.item())\n",
    "\n",
    "# Always clear gradients before a fresh accumulation.\n",
    "param.grad.zero_()\n",
    "\n",
    "# Confirm gradient has been reset to zero now.\n",
    "print(\"Grad after manual zero_():\", param.grad.item())\n",
    "\n",
    "# Final line prints a short completion confirmation.\n",
    "print(\"Gradient accumulation demo finished.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4097a3",
   "metadata": {},
   "source": [
    "## **2. Working With Gradients**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afeb0592",
   "metadata": {},
   "source": [
    "### **2.1. Controlling requires_grad**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbc4893",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_02/Lecture_A/image_02_01.jpg?v=1769695084\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* requires_grad marks which tensors need gradients\n",
    ">* use it to track only learnable parameters\n",
    "\n",
    ">* requires_grad spreads through operations, building the graph\n",
    ">* disable gradients for preprocessing; start graph at parameters\n",
    "\n",
    ">* Unnecessary gradient tracking wastes memory and compute time\n",
    ">* Track gradients only for parameters and loss calculations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199393f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Controlling requires_grad\n",
    "\n",
    "# This script explains controlling requires_grad clearly.\n",
    "# It focuses on simple gradient tracking examples.\n",
    "# Run cells stepwise and read printed explanations.\n",
    "\n",
    "# torch is not installed by default in some environments.\n",
    "# Uncomment the next line if torch import fails.\n",
    "# !pip install torch torchvision torchaudio.\n",
    "\n",
    "# Import torch for tensor and autograd operations.\n",
    "import torch\n",
    "\n",
    "# Print torch version to confirm environment details.\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "\n",
    "# Create input tensor without gradient tracking enabled.\n",
    "x_data = torch.tensor([2.0, 3.0])\n",
    "\n",
    "# Create parameter tensor with gradient tracking enabled.\n",
    "w_param = torch.tensor([1.5, -0.5], requires_grad=True)\n",
    "\n",
    "# Confirm which tensors currently require gradients.\n",
    "print(\"x_data requires_grad:\", x_data.requires_grad)\n",
    "print(\"w_param requires_grad:\", w_param.requires_grad)\n",
    "\n",
    "# Compute simple linear combination using both tensors.\n",
    "y_output = (x_data * w_param).sum()\n",
    "\n",
    "# Call backward to compute gradients for w_param only.\n",
    "y_output.backward()\n",
    "\n",
    "# Inspect gradient stored on parameter tensor.\n",
    "print(\"Gradient for w_param:\", w_param.grad)\n",
    "\n",
    "# Reset gradients to zero for a clean next example.\n",
    "w_param.grad.zero_()\n",
    "\n",
    "# Create a tensor with requires_grad set after creation.\n",
    "scale = torch.tensor(10.0)\n",
    "\n",
    "# Enable gradient tracking on scale using requires_grad_ method.\n",
    "scale.requires_grad_()\n",
    "\n",
    "# Confirm gradient tracking status for scale tensor.\n",
    "print(\"scale requires_grad:\", scale.requires_grad)\n",
    "\n",
    "# Build new computation using both tracked tensors.\n",
    "result = (scale * (x_data * w_param).sum())\n",
    "\n",
    "# Compute gradients again with respect to both tensors.\n",
    "result.backward()\n",
    "\n",
    "# Show gradients for w_param and scale after backward.\n",
    "print(\"New gradient w_param:\", w_param.grad)\n",
    "print(\"Gradient for scale:\", scale.grad)\n",
    "\n",
    "# Demonstrate turning off gradient tracking for efficiency.\n",
    "with torch.no_grad():\n",
    "    prediction = (x_data * w_param).sum()\n",
    "\n",
    "# Confirm prediction has gradient tracking disabled.\n",
    "print(\"prediction requires_grad:\", prediction.requires_grad)\n",
    "\n",
    "# Show that w_param still keeps its gradient tracking flag.\n",
    "print(\"w_param still requires_grad:\", w_param.requires_grad)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100e0ad7",
   "metadata": {},
   "source": [
    "### **2.2. Understanding backward and grad**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53236fa8",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_02/Lecture_A/image_02_02.jpg?v=1769695115\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Backward walks the computation graph using chain rule\n",
    ">* It fills leaf tensors’ grad with needed derivatives\n",
    "\n",
    ">* grad stays empty until backward is called\n",
    ">* after backward, grad shows each parameter’s influence\n",
    "\n",
    ">* Backward needs scalar outputs; gradients may accumulate\n",
    ">* Clear grads and interpret accumulated values carefully\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bf230b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Understanding backward and grad\n",
    "\n",
    "# This script explains backward and grad basics.\n",
    "# It uses tiny tensors for clear gradients.\n",
    "# Run cells in order to follow explanations.\n",
    "\n",
    "# Install PyTorch if not already available.\n",
    "# !pip install torch torchvision torchaudio.\n",
    "\n",
    "# Import torch for tensor and autograd features.\n",
    "import torch\n",
    "\n",
    "# Print PyTorch version for reproducibility.\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "\n",
    "# Create a simple tensor marked for gradient tracking.\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "\n",
    "# Confirm that x is a leaf tensor with grad tracking.\n",
    "print(\"x:\", x, \"requires_grad:\", x.requires_grad)\n",
    "\n",
    "# Build a tiny computation that depends on x.\n",
    "y = x ** 2 + 3 * x\n",
    "\n",
    "# Show the computed output value before backward.\n",
    "print(\"y before backward:\", y)\n",
    "\n",
    "# Check that y is not a leaf and has grad_fn.\n",
    "print(\"Is y leaf?\", y.is_leaf, \"grad_fn:\", y.grad_fn)\n",
    "\n",
    "# Ensure x.grad is empty before calling backward.\n",
    "print(\"x.grad before backward:\", x.grad)\n",
    "\n",
    "# Call backward on scalar y to compute dy/dx.\n",
    "y.backward()\n",
    "\n",
    "# After backward, x.grad holds the derivative.\n",
    "print(\"x.grad after backward:\", x.grad)\n",
    "\n",
    "# Manually compute derivative for comparison.\n",
    "manual_grad = 2 * x.detach() + 3\n",
    "\n",
    "# Show that autograd matches manual derivative.\n",
    "print(\"Manual gradient:\", manual_grad)\n",
    "\n",
    "# Reset gradients to zero before another backward.\n",
    "x.grad.zero_()\n",
    "\n",
    "# Build a new output using mean over multiple elements.\n",
    "values = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "\n",
    "# Confirm shape so backward behavior is predictable.\n",
    "print(\"values shape:\", values.shape)\n",
    "\n",
    "# Compute mean to get a scalar output.\n",
    "mean_output = values.mean()\n",
    "\n",
    "# Call backward to get gradient of mean w.r.t values.\n",
    "mean_output.backward()\n",
    "\n",
    "# Inspect gradients stored in values.grad attribute.\n",
    "print(\"values.grad after backward:\", values.grad)\n",
    "\n",
    "# Show that gradients match derivative of mean function.\n",
    "print(\"Expected grad for mean is 1/3 each.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136fa65a",
   "metadata": {},
   "source": [
    "### **2.3. Resetting Gradients Safely**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71702801",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_02/Lecture_A/image_02_03.jpg?v=1769695145\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Gradients accumulate across steps unless manually cleared\n",
    ">* Resetting gradients keeps results fresh and meaningful\n",
    "\n",
    ">* Gradients must be cleared to avoid contamination\n",
    ">* Fresh gradients keep updates stable and interpretable\n",
    "\n",
    ">* Use cleared gradients to read models correctly\n",
    ">* Reset at planned times to control behavior\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b68d2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Resetting Gradients Safely\n",
    "\n",
    "# This script demonstrates resetting gradients safely.\n",
    "# It uses PyTorch tensors with simple scalar operations.\n",
    "# Focus on requires_grad backward and grad attributes.\n",
    "\n",
    "# Install PyTorch if not already available in the environment.\n",
    "# !pip install torch torchvision torchaudio --quiet.\n",
    "\n",
    "# Import torch for tensor and autograd operations.\n",
    "import torch\n",
    "\n",
    "# Print the PyTorch version for quick verification.\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "\n",
    "# Create a scalar parameter with gradient tracking enabled.\n",
    "weight = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "# Confirm that the tensor is a scalar parameter.\n",
    "assert weight.shape == torch.Size([])\n",
    "\n",
    "# Define a simple target value for our tiny example.\n",
    "target = torch.tensor(10.0, requires_grad=False)\n",
    "\n",
    "# Confirm that target has the same shape as predictions.\n",
    "assert target.shape == torch.Size([])\n",
    "\n",
    "# Compute a simple prediction using the parameter.\n",
    "prediction = weight * 3.0\n",
    "\n",
    "# Compute a squared error loss for this prediction.\n",
    "loss = (prediction - target) ** 2\n",
    "\n",
    "# Run backpropagation to compute gradient for weight.\n",
    "loss.backward()\n",
    "\n",
    "# Print the first gradient value for weight.\n",
    "print(\"Gradient after first backward:\", weight.grad.item())\n",
    "\n",
    "# Store the first gradient for comparison later.\n",
    "first_grad = weight.grad.item()\n",
    "\n",
    "# Compute a new prediction without resetting gradients.\n",
    "prediction2 = weight * 3.0\n",
    "\n",
    "# Compute a new loss using the same formula.\n",
    "loss2 = (prediction2 - target) ** 2\n",
    "\n",
    "# Call backward again to accumulate gradients.\n",
    "loss2.backward()\n",
    "\n",
    "# Print the accumulated gradient value for weight.\n",
    "print(\"Gradient after second backward:\", weight.grad.item())\n",
    "\n",
    "# Show that gradient is roughly double the first value.\n",
    "print(\"Accumulated equals first times two:\", weight.grad.item() >= first_grad * 1.9)\n",
    "\n",
    "# Safely reset gradients to zero before next step.\n",
    "weight.grad.zero_()\n",
    "\n",
    "# Confirm that gradients were cleared successfully.\n",
    "print(\"Gradient after zero_ call:\", weight.grad.item())\n",
    "\n",
    "# Compute a fresh prediction after clearing gradients.\n",
    "prediction3 = weight * 3.0\n",
    "\n",
    "# Compute a fresh loss for the cleared state.\n",
    "loss3 = (prediction3 - target) ** 2\n",
    "\n",
    "# Backpropagate again to get a clean gradient.\n",
    "loss3.backward()\n",
    "\n",
    "# Print the new gradient which matches a single step.\n",
    "print(\"Gradient after reset and backward:\", weight.grad.item())\n",
    "\n",
    "# Verify that the new gradient matches the original one.\n",
    "print(\"Reset kept gradient consistent:\", abs(weight.grad.item() - first_grad) < 1e-6)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da6113c",
   "metadata": {},
   "source": [
    "## **3. Controlling Autograd Gradients**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc661f9",
   "metadata": {},
   "source": [
    "### **3.1. No Grad Context**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819fc954",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_02/Lecture_A/image_03_01.jpg?v=1769695177\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* No grad context temporarily disables gradient tracking\n",
    ">* Use it for evaluation to save computation\n",
    "\n",
    ">* Training needs gradients; inference usually does not\n",
    ">* No grad context saves memory and compute\n",
    "\n",
    ">* Use no_grad for evaluation, metrics, visualizations\n",
    ">* Prevents graph bloat, bugs, and memory issues\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ad11de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - No Grad Context\n",
    "\n",
    "# This script shows torch no grad context.\n",
    "# We compare gradients with and without context.\n",
    "# Focus on simple tensors not full models.\n",
    "\n",
    "# Uncomment if torch is not already installed.\n",
    "# !pip install torch torchvision torchaudio.\n",
    "\n",
    "# Import torch for tensor and autograd operations.\n",
    "import torch\n",
    "\n",
    "# Set a manual seed for deterministic behavior.\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Create a tensor with gradient tracking enabled.\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "# Confirm that requires_grad is correctly set.\n",
    "print(\"x requires_grad:\", x.requires_grad)\n",
    "\n",
    "# Build a simple computation that will track gradients.\n",
    "y = x ** 3 + 2 * x\n",
    "\n",
    "# Call backward to compute gradient dy/dx at x.\n",
    "y.backward()\n",
    "\n",
    "# Print the gradient computed for x in this graph.\n",
    "print(\"Gradient of y with grad tracking:\", x.grad.item())\n",
    "\n",
    "# Reset gradient on x to avoid accumulation issues.\n",
    "x.grad.zero_()\n",
    "\n",
    "# Use a no grad context to disable tracking temporarily.\n",
    "with torch.no_grad():\n",
    "    y_no_grad = x ** 3 + 2 * x\n",
    "\n",
    "# Show that result is still a normal tensor value.\n",
    "print(\"Value inside no_grad context:\", y_no_grad.item())\n",
    "\n",
    "# Check that the new tensor does not require gradients.\n",
    "print(\"y_no_grad requires_grad:\", y_no_grad.requires_grad)\n",
    "\n",
    "# Try calling backward on a tensor from no_grad context.\n",
    "try:\n",
    "    y_no_grad.backward()\n",
    "except RuntimeError as e:\n",
    "    print(\"Backward failed inside no_grad:\", type(e).__name__)\n",
    "\n",
    "# Show that x gradient was not changed by no_grad block.\n",
    "print(\"Gradient of x after no_grad block:\", x.grad.item())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bded1cd8",
   "metadata": {},
   "source": [
    "### **3.2. Detach and Inplace Operations**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2ccd50",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_02/Lecture_A/image_03_02.jpg?v=1769695205\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Detach stops gradients, treating tensors as constants\n",
    ">* Lets you control which parameters receive updates\n",
    "\n",
    ">* Inplace ops can break autograd’s gradient tracking\n",
    ">* Use inplace only on detached or unused tensors\n",
    "\n",
    ">* Use detached copies for safe inplace updates\n",
    ">* Avoid inplace changes on tensors tracked by autograd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89db2bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Detach and Inplace Operations\n",
    "\n",
    "# This script demonstrates detach and inplace operations.\n",
    "# It uses PyTorch autograd with simple tensor examples.\n",
    "# Focus on safe gradient control and inplace behavior.\n",
    "\n",
    "# Install PyTorch if not already available in the environment.\n",
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu.\n",
    "\n",
    "# Import torch for tensor and autograd operations.\n",
    "import torch\n",
    "\n",
    "# Set a deterministic seed for reproducible tensor values.\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Print the PyTorch version in one concise line.\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "\n",
    "# Create a tensor with gradients enabled for autograd.\n",
    "x = torch.tensor([2.0, 3.0], requires_grad=True)\n",
    "\n",
    "# Confirm the tensor shape is as expected.\n",
    "assert x.shape == (2,), \"Input tensor must have shape (2,)\"\n",
    "\n",
    "# Build a simple computation graph using x squared.\n",
    "y = x ** 2\n",
    "\n",
    "# Sum the outputs to obtain a scalar loss value.\n",
    "loss = y.sum()\n",
    "\n",
    "# Run backpropagation to compute gradients for x.\n",
    "loss.backward()\n",
    "\n",
    "# Print original gradients for x before any detach operations.\n",
    "print(\"Gradients for x before detach:\", x.grad)\n",
    "\n",
    "# Stop tracking gradients by creating a detached view of x.\n",
    "x_detached = x.detach()\n",
    "\n",
    "# Verify detached tensor shares data but tracks no gradients.\n",
    "print(\"Detached requires_grad flag:\", x_detached.requires_grad)\n",
    "\n",
    "# Perform an inplace operation safely on the detached tensor.\n",
    "x_detached.add_(1.0)\n",
    "\n",
    "# Show values of original and detached tensors after inplace update.\n",
    "print(\"Original x after detached update:\", x)\n",
    "print(\"Detached tensor after inplace add_:\", x_detached)\n",
    "\n",
    "# Clear previous gradients on x before next backward pass.\n",
    "x.grad.zero_()\n",
    "\n",
    "# Recompute loss using original x to see gradient independence.\n",
    "new_loss = (x ** 2).sum()\n",
    "\n",
    "# Backpropagate again to update gradients based on original x.\n",
    "new_loss.backward()\n",
    "\n",
    "# Print gradients to confirm they ignore detached inplace changes.\n",
    "print(\"Gradients for x after second backward:\", x.grad)\n",
    "\n",
    "# Demonstrate unsafe inplace attempt on tracked tensor inside try block.\n",
    "try:\n",
    "    # Create a new tensor that depends on x for demonstration.\n",
    "    z = x * 2.0\n",
    "\n",
    "    # Attempt an inplace operation that may confuse autograd.\n",
    "    z.add_(1.0)\n",
    "\n",
    "    # Print z to show the inplace modification result.\n",
    "    print(\"Inplace on tracked tensor z succeeded:\", z)\n",
    "except RuntimeError as e:\n",
    "    # Catch and report autograd error from unsafe inplace operation.\n",
    "    print(\"RuntimeError from inplace on tracked tensor:\")\n",
    "    print(str(e).split(\"\\n\")[0])\n",
    "\n",
    "# Final confirmation that original x data and gradients remain valid.\n",
    "print(\"Final x values:\", x)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a5bd97",
   "metadata": {},
   "source": [
    "### **3.3. Memory and Speed**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3955ea07",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_02/Lecture_A/image_03_03.jpg?v=1769695272\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Gradient tracking stores graphs and intermediates in memory\n",
    ">* Large models magnify this cost, limiting resources\n",
    "\n",
    ">* Turn off gradients to skip storing intermediates\n",
    ">* Use for eval, logging, visualization to save memory\n",
    "\n",
    ">* Fewer tracked gradients reduce computation and speed training\n",
    ">* Disabling gradients in inference saves memory and compute\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf96a036",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Memory and Speed\n",
    "\n",
    "# This script demonstrates PyTorch autograd memory control.\n",
    "# We compare gradient tracking for speed and memory usage.\n",
    "# Focus on torch.no_grad and detach for beginners.\n",
    "\n",
    "# !pip install torch torchvision torchaudio.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Import torch for tensor and autograd features.\n",
    "import torch\n",
    "\n",
    "# Set deterministic random seeds for reproducibility.\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Select device based on GPU availability.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Print PyTorch version and selected device.\n",
    "print(\"PyTorch version:\", torch.__version__, \"Device:\", device)\n",
    "\n",
    "# Define a helper to measure execution time.\n",
    "def measure_time(fn, label):\n",
    "    start = time.time()\n",
    "    result = fn()\n",
    "    end = time.time()\n",
    "    elapsed = (end - start) * 1000.0\n",
    "    print(label, \"time_ms=\", round(elapsed, 3))\n",
    "    return result\n",
    "\n",
    "# Create an input tensor with gradient tracking enabled.\n",
    "input_size = 1024 * 16\n",
    "x = torch.randn(input_size, device=device, requires_grad=True)\n",
    "\n",
    "# Confirm tensor shape and gradient requirement.\n",
    "assert x.shape[0] == input_size\n",
    "assert x.requires_grad is True\n",
    "\n",
    "# Define a simple computation with gradients enabled.\n",
    "def forward_with_grad():\n",
    "    y = x * 2.0 + 1.0\n",
    "    z = torch.relu(y)\n",
    "    loss = z.mean()\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "# Define the same computation with gradients disabled.\n",
    "def forward_no_grad():\n",
    "    with torch.no_grad():\n",
    "        y = x * 2.0 + 1.0\n",
    "        z = torch.relu(y)\n",
    "        loss = z.mean()\n",
    "    return loss\n",
    "\n",
    "# Run both functions once to warm up device.\n",
    "_ = forward_with_grad()\n",
    "_ = forward_no_grad()\n",
    "\n",
    "# Zero gradients before timing to avoid accumulation.\n",
    "if x.grad is not None:\n",
    "    x.grad.zero_()\n",
    "\n",
    "# Measure time for computation with gradients.\n",
    "loss_grad = measure_time(forward_with_grad, \"With_grad_forward_backward\")\n",
    "\n",
    "# Clear gradients again to free memory.\n",
    "if x.grad is not None:\n",
    "    x.grad.zero_()\n",
    "\n",
    "# Measure time for computation without gradients.\n",
    "loss_nograd = measure_time(forward_no_grad, \"No_grad_forward_only\")\n",
    "\n",
    "# Show that no_grad result has no computation graph.\n",
    "print(\"loss_grad_requires_grad:\", loss_grad.requires_grad)\n",
    "print(\"loss_nograd_requires_grad:\", loss_nograd.requires_grad)\n",
    "\n",
    "# Demonstrate detach to stop gradients from flowing.\n",
    "intermediate = x * 3.0\n",
    "intermediate_detached = intermediate.detach()\n",
    "\n",
    "# Confirm detached tensor does not require gradients.\n",
    "print(\"intermediate_requires_grad:\", intermediate.requires_grad)\n",
    "print(\"detached_requires_grad:\", intermediate_detached.requires_grad)\n",
    "\n",
    "# Use detached tensor in further computation without tracking.\n",
    "with torch.no_grad():\n",
    "    summary_value = intermediate_detached.abs().mean().item()\n",
    "\n",
    "# Print final summary to show script completed.\n",
    "print(\"Summary_value_from_detached_tensor:\", round(summary_value, 4))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c4653c",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Autograd Mechanics**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ae673f",
   "metadata": {},
   "source": [
    "\n",
    "In this lecture, you learned to:\n",
    "- Describe how PyTorch builds dynamic computation graphs and uses them to compute gradients. \n",
    "- Use requires_grad, backward, and grad attributes to compute and inspect gradients for simple tensor operations. \n",
    "- Control gradient tracking with torch.no_grad and detach to optimize performance and avoid unintended graph creation. \n",
    "\n",
    "In the next Lecture (Lecture B), we will go over 'Modules and Layers'"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
