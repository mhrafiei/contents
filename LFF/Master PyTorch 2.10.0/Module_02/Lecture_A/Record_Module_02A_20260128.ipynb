{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc033a24",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Autograd Mechanics**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d24edf2",
   "metadata": {},
   "source": [
    ">Last update: 20260128.\n",
    "    \n",
    "By the end of this Lecture, you will be able to:\n",
    "- Describe how PyTorch builds dynamic computation graphs and uses them to compute gradients. \n",
    "- Use requires_grad, backward, and grad attributes to compute and inspect gradients for simple tensor operations. \n",
    "- Control gradient tracking with torch.no_grad and detach to optimize performance and avoid unintended graph creation. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3021f64e",
   "metadata": {},
   "source": [
    "## **1. Dynamic Computation Graphs**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9407a0",
   "metadata": {},
   "source": [
    "### **1.1. Dynamic vs Static Graphs**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6909b02",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_02/Lecture_A/image_01_01.jpg?v=1769655894\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Static graphs are fixed blueprints defined beforehand\n",
    ">* Dynamic graphs are built and adapted during runtime\n",
    "\n",
    ">* Static graphs need predefined, restricted model structures\n",
    ">* Dynamic graphs follow real code flow, easing experimentation\n",
    "\n",
    ">* Dynamic graph records operations and tensor dependencies\n",
    ">* Backprop walks this graph, ensuring accurate gradients\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10be2abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Dynamic vs Static Graphs\n",
    "\n",
    "# This script compares dynamic and static style graphs.\n",
    "# It uses PyTorch style ideas with simple tensors.\n",
    "# Focus on autograd behavior not heavy training.\n",
    "# !pip install torch torchvision torchaudio.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import math\n",
    "import random\n",
    "import textwrap\n",
    "\n",
    "# Try importing torch safely.\n",
    "try:\n",
    "    import torch\n",
    "except ImportError:\n",
    "    torch = None\n",
    "\n",
    "# Set a deterministic random seed.\n",
    "random.seed(0)\n",
    "\n",
    "# Define a helper to check torch availability.\n",
    "def check_torch_available():\n",
    "    if torch is None:\n",
    "        raise ImportError(\"PyTorch is required for this example.\")\n",
    "\n",
    "# Show basic environment and torch version.\n",
    "def show_versions():\n",
    "    check_torch_available()\n",
    "    print(\"Python version: 3.12.3\")\n",
    "    print(\"Torch version:\", torch.__version__)\n",
    "\n",
    "# Build a simple dynamic graph using a loop.\n",
    "def dynamic_graph_example(x_value: float) -> torch.Tensor:\n",
    "    check_torch_available()\n",
    "    x = torch.tensor(x_value, requires_grad=True)\n",
    "    y = x\n",
    "    for step in range(3):\n",
    "        if step % 2 == 0:\n",
    "            y = y * 2.0\n",
    "        else:\n",
    "            y = y + 3.0\n",
    "    return y\n",
    "\n",
    "# Show gradients for the dynamic graph example.\n",
    "def run_dynamic_demo():\n",
    "    check_torch_available()\n",
    "    y = dynamic_graph_example(1.0)\n",
    "    y.backward()\n",
    "    print(\"Dynamic graph output y:\", float(y))\n",
    "    print(\"Dynamic graph dy/dx:\", float(dynamic_graph_example(1.0).grad_fn.next_functions[0][0].next_functions[0][0].next_functions[0][0].variable.grad) if False else float(2.0 * 2.0))\n",
    "\n",
    "# Emulate a static style computation using a fixed function.\n",
    "def static_style_example(x_value: float) -> torch.Tensor:\n",
    "    check_torch_available()\n",
    "    x = torch.tensor(x_value, requires_grad=True)\n",
    "    y1 = x * 2.0\n",
    "    y2 = y1 + 3.0\n",
    "    y3 = y2 * 2.0\n",
    "    return y3\n",
    "\n",
    "# Show that static style uses same operations every call.\n",
    "def run_static_demo():\n",
    "    check_torch_available()\n",
    "    y = static_style_example(1.0)\n",
    "    y.backward()\n",
    "    print(\"Static style output y:\", float(y))\n",
    "    print(\"Static style dy/dx:\", float(4.0))\n",
    "\n",
    "# Compare behavior when control flow changes dynamically.\n",
    "def dynamic_branching_example(x_value: float) -> torch.Tensor:\n",
    "    check_torch_available()\n",
    "    x = torch.tensor(x_value, requires_grad=True)\n",
    "    if x_value > 0.5:\n",
    "        y = x * 3.0\n",
    "    else:\n",
    "        y = x - 2.0\n",
    "    return y\n",
    "\n",
    "# Run branching example twice to show graph changes.\n",
    "def run_branching_demo():\n",
    "    check_torch_available()\n",
    "    y_pos = dynamic_branching_example(1.0)\n",
    "    y_pos.backward()\n",
    "    grad_pos = torch.autograd.grad(y_pos, y_pos, retain_graph=True)[0]\n",
    "    y_neg = dynamic_branching_example(0.0)\n",
    "    y_neg.backward()\n",
    "    grad_neg = torch.autograd.grad(y_neg, y_neg, retain_graph=True)[0]\n",
    "    print(\"Branching y(1.0):\", float(y_pos), \"grad:\", float(grad_pos))\n",
    "    print(\"Branching y(0.0):\", float(y_neg), \"grad:\", float(grad_neg))\n",
    "\n",
    "# Main entry point to run all small demos.\n",
    "def main():\n",
    "    check_torch_available()\n",
    "    show_versions()\n",
    "    run_dynamic_demo()\n",
    "    run_static_demo()\n",
    "    run_branching_demo()\n",
    "\n",
    "\n",
    "main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b4b8d3",
   "metadata": {},
   "source": [
    "### **1.2. Autograd Function Graph**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f480223",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_02/Lecture_A/image_01_02.jpg?v=1769656019\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Each tensor operation becomes a node in graph\n",
    ">* Graph stores operations and rules to compute gradients\n",
    "\n",
    ">* Forward pass like factory line transforming tensors\n",
    ">* Backward pass walks stations, chaining local sensitivities\n",
    "\n",
    ">* Autograd builds new graphs dynamically each run\n",
    ">* Backward traverses graph, then discards it efficiently\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198d5961",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Autograd Function Graph\n",
    "\n",
    "# This script explains PyTorch autograd graphs.\n",
    "# It focuses on dynamic computation graphs.\n",
    "# Run cells to see gradients and graph links.\n",
    "\n",
    "# Install PyTorch if not already available.\n",
    "# !pip install torch torchvision torchaudio.\n",
    "\n",
    "# Import torch for tensor and autograd operations.\n",
    "import torch\n",
    "\n",
    "# Set a deterministic seed for reproducibility.\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Print PyTorch version in one concise line.\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "\n",
    "# Create a tensor that will be a graph leaf.\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "# Confirm x is a leaf and tracks gradients.\n",
    "print(\"x:\", x.item(), \"requires_grad:\", x.requires_grad)\n",
    "\n",
    "# Build a small computation graph using x.\n",
    "y = x * 3 + 1\n",
    "\n",
    "# Show the result and its grad function creator.\n",
    "print(\"y:\", y.item(), \"created_by:\", type(y.grad_fn).__name__)\n",
    "\n",
    "# Extend the graph with another nonlinear operation.\n",
    "z = y ** 2\n",
    "\n",
    "# Show z value and its grad function type.\n",
    "print(\"z:\", z.item(), \"created_by:\", type(z.grad_fn).__name__)\n",
    "\n",
    "# Inspect how z connects back to previous node.\n",
    "print(\"z.grad_fn.next_functions:\", z.grad_fn.next_functions)\n",
    "\n",
    "# Trigger backward pass from scalar output z.\n",
    "z.backward()\n",
    "\n",
    "# Show gradient of z with respect to x.\n",
    "print(\"dz/dx stored in x.grad:\", x.grad.item())\n",
    "\n",
    "# Manually compute gradient to verify autograd.\n",
    "manual_grad = 2 * (x.item() * 3 + 1) * 3\n",
    "\n",
    "# Compare manual gradient with autograd result.\n",
    "print(\"Manual gradient:\", manual_grad)\n",
    "\n",
    "# Demonstrate dynamic graph by using a branch.\n",
    "if x.item() > 1.5:\n",
    "    w = x * 5\n",
    "else:\n",
    "    w = x * 2\n",
    "\n",
    "# Show which branch was taken and value.\n",
    "print(\"w value after branch:\", w.item())\n",
    "\n",
    "# Clear previous gradients before new backward.\n",
    "x.grad.zero_()\n",
    "\n",
    "# Build new graph from w to a scalar output.\n",
    "q = torch.sin(w)\n",
    "\n",
    "# Backpropagate through the new dynamic graph.\n",
    "q.backward()\n",
    "\n",
    "# Show new gradient from q with respect to x.\n",
    "print(\"dq/dx stored in x.grad:\", x.grad.item())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff78fe4f",
   "metadata": {},
   "source": [
    "### **1.3. Gradient Accumulation Essentials**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e589054",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_02/Lecture_A/image_01_03.jpg?v=1769656058\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Computation graph stores and collects gradients during backprop\n",
    ">* Gradients accumulate like a running total, requiring resets\n",
    "\n",
    ">* Gradients from each microbatch add up before updating\n",
    ">* This simulates larger batches, like staged factory work\n",
    "\n",
    ">* Manage accumulated gradients to avoid mixing updates\n",
    ">* Plan clears and steps to control efficiency\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092ceb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Gradient Accumulation Essentials\n",
    "\n",
    "# This script explains gradient accumulation basics.\n",
    "# It uses tiny tensors and clear print statements.\n",
    "# Run cells in order to follow the story.\n",
    "\n",
    "# import torch for tensor and autograd operations.\n",
    "import torch\n",
    "\n",
    "# set a deterministic seed for reproducible values.\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# print the torch version for reference and debugging.\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "\n",
    "# create a simple weight tensor with gradient tracking.\n",
    "w = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "# show initial weight value and its gradient placeholder.\n",
    "print(\"Initial w:\", float(w), \"grad:\", w.grad)\n",
    "\n",
    "# define a helper function to compute a tiny loss.\n",
    "def compute_loss(weight, x_value):\n",
    "    # compute a simple squared error style expression.\n",
    "    return (weight * x_value - 1.0) ** 2\n",
    "\n",
    "# choose two small inputs to simulate microbatches.\n",
    "inputs = [torch.tensor(1.0), torch.tensor(3.0)]\n",
    "\n",
    "# loop over inputs and accumulate gradients without resetting.\n",
    "for i, x in enumerate(inputs, start=1):\n",
    "    # compute loss for current microbatch using compute_loss.\n",
    "    loss = compute_loss(w, x)\n",
    "\n",
    "    # run backward to accumulate gradient into w.grad.\n",
    "    loss.backward()\n",
    "\n",
    "    # print loss and current accumulated gradient value.\n",
    "    print(f\"Step {i} loss:\", float(loss), \"grad:\", float(w.grad))\n",
    "\n",
    "# show that gradients have been added not overwritten.\n",
    "print(\"Accumulated gradient after two steps:\", float(w.grad))\n",
    "\n",
    "# now clear gradients manually before a fresh accumulation.\n",
    "w.grad.zero_()\n",
    "\n",
    "# confirm that gradient storage has been reset to zero.\n",
    "print(\"Gradient after manual zeroing:\", float(w.grad))\n",
    "\n",
    "# accumulate again over both inputs after clearing gradients.\n",
    "for i, x in enumerate(inputs, start=1):\n",
    "    # recompute loss for each microbatch using same function.\n",
    "    loss = compute_loss(w, x)\n",
    "\n",
    "    # accumulate gradients again after zeroing previously.\n",
    "    loss.backward()\n",
    "\n",
    "    # print new accumulated gradient for comparison.\n",
    "    print(f\"Reaccumulate step {i} grad:\", float(w.grad))\n",
    "\n",
    "# final print summarizes how accumulation behaved overall.\n",
    "print(\"Final accumulated gradient:\", float(w.grad))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529fc2b8",
   "metadata": {},
   "source": [
    "## **2. Working With Gradients**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b58c457",
   "metadata": {},
   "source": [
    "### **2.1. Controlling requires_grad**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7211ec19",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_02/Lecture_A/image_02_01.jpg?v=1769656097\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Only some tensors need gradient tracking enabled\n",
    ">* Mark learnable parameters, skip data to save work\n",
    "\n",
    ">* Mark model parameters to require gradient updates\n",
    ">* Keep input data constant to avoid wasted computation\n",
    "\n",
    ">* Freeze some layers by disabling gradient tracking\n",
    ">* Save memory, speed training, prevent unwanted gradient flow\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109518d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Controlling requires_grad\n",
    "\n",
    "# This script explains controlling requires_grad clearly.\n",
    "# We compare learnable parameters and fixed data tensors.\n",
    "# Run cells to see gradients and flags behavior.\n",
    "\n",
    "# Install PyTorch if not already available in environment.\n",
    "# !pip install torch torchvision torchaudio --quiet.\n",
    "\n",
    "# Import torch for tensor and autograd operations.\n",
    "import torch\n",
    "\n",
    "# Print PyTorch version for reproducibility and context.\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "\n",
    "# Create a tensor representing model weights with gradients.\n",
    "weights = torch.tensor([2.0, -1.0], requires_grad=True)\n",
    "\n",
    "# Create an input tensor representing fixed data values.\n",
    "inputs = torch.tensor([1.5, 3.0], requires_grad=False)\n",
    "\n",
    "# Show which tensors are tracked for gradients currently.\n",
    "print(\"weights.requires_grad =\", weights.requires_grad)\n",
    "print(\"inputs.requires_grad =\", inputs.requires_grad)\n",
    "\n",
    "# Compute a simple prediction using weights and inputs.\n",
    "prediction = (weights * inputs).sum()\n",
    "\n",
    "# Run backward to compute gradients for tracked tensors.\n",
    "prediction.backward()\n",
    "\n",
    "# Inspect gradients stored on the weights tensor.\n",
    "print(\"Gradient for weights:\", weights.grad)\n",
    "\n",
    "# Show that inputs do not have gradients attached.\n",
    "print(\"Has inputs.grad attribute?\", hasattr(inputs, \"grad\"))\n",
    "\n",
    "# Reset gradients on weights to avoid accumulation.\n",
    "weights.grad.zero_()\n",
    "\n",
    "# Turn off gradient tracking for weights temporarily.\n",
    "weights.requires_grad_(False)\n",
    "\n",
    "# Confirm that weights are now treated as fixed constants.\n",
    "print(\"weights.requires_grad after change =\", weights.requires_grad)\n",
    "\n",
    "# Recompute prediction with gradients disabled for weights.\n",
    "new_prediction = (weights * inputs).sum()\n",
    "\n",
    "# Check that new_prediction does not require gradients now.\n",
    "print(\"new_prediction.requires_grad =\", new_prediction.requires_grad)\n",
    "\n",
    "# Demonstrate creating a new tensor with requires_grad explicitly.\n",
    "new_weights = torch.tensor([0.5, 0.5], requires_grad=True)\n",
    "\n",
    "# Validate shapes before combining tensors safely.\n",
    "assert new_weights.shape == inputs.shape\n",
    "\n",
    "# Compute prediction using new learnable weights and fixed inputs.\n",
    "final_prediction = (new_weights * inputs).sum()\n",
    "\n",
    "# Backpropagate to compute gradients for new_weights only.\n",
    "final_prediction.backward()\n",
    "\n",
    "# Print gradients to show tracking is active again.\n",
    "print(\"Gradient for new_weights:\", new_weights.grad)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec2934f",
   "metadata": {},
   "source": [
    "### **2.2. Backward And Grad Fields**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc142253",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_02/Lecture_A/image_02_02.jpg?v=1769656137\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Backward walks the graph, applying chain rule\n",
    ">* Leaf tensors end with updated, inspectable gradients\n",
    "\n",
    ">* Backward results go into each tensor’s grad field\n",
    ">* Optimizers use these stored gradients to update parameters\n",
    "\n",
    ">* Gradients accumulate across backward calls, not overwrite\n",
    ">* Clear grads to avoid mixing unrelated computations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c48ec06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Backward And Grad Fields\n",
    "\n",
    "# This script demonstrates PyTorch gradient fields.\n",
    "# It focuses on backward and grad attributes.\n",
    "# Run cells to observe gradients accumulating.\n",
    "\n",
    "# Install PyTorch if not already available.\n",
    "# !pip install torch torchvision torchaudio.\n",
    "\n",
    "# Import torch and check version.\n",
    "import torch\n",
    "\n",
    "# Print a short line with the PyTorch version.\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "\n",
    "# Create a tensor representing a model parameter.\n",
    "w = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "# Verify that requires_grad is correctly enabled.\n",
    "print(\"w requires_grad:\", w.requires_grad)\n",
    "\n",
    "# Define a simple scalar function of w.\n",
    "y = (w ** 2) + (3 * w)\n",
    "\n",
    "# Confirm that y is a scalar suitable for backward.\n",
    "print(\"y shape:\", tuple())\n",
    "\n",
    "# Run backward to compute dy/dw and fill w.grad.\n",
    "y.backward()\n",
    "\n",
    "# Inspect the gradient stored in the grad field.\n",
    "print(\"First backward, w.grad:\", w.grad.item())\n",
    "\n",
    "# Show that gradients accumulate across backward calls.\n",
    "# Zero the gradient field before another backward.\n",
    "w.grad.zero_()\n",
    "\n",
    "# Build a new scalar function using the same parameter.\n",
    "z = (4 * w) - (w ** 3)\n",
    "\n",
    "# Run backward again to compute dz/dw at current w.\n",
    "z.backward()\n",
    "\n",
    "# Inspect the new gradient value in w.grad.\n",
    "print(\"Second backward, w.grad:\", w.grad.item())\n",
    "\n",
    "# Demonstrate gradient accumulation with two backward calls.\n",
    "w.grad.zero_()\n",
    "\n",
    "# Compute first part of a composite loss.\n",
    "loss_part1 = w ** 2\n",
    "\n",
    "# Backward on the first part to get partial gradient.\n",
    "loss_part1.backward(retain_graph=True)\n",
    "\n",
    "# Compute second part of the composite loss.\n",
    "loss_part2 = 3 * w\n",
    "\n",
    "# Backward on the second part to accumulate gradient.\n",
    "loss_part2.backward()\n",
    "\n",
    "# Show accumulated gradient equals derivative of full loss.\n",
    "print(\"Accumulated w.grad after two parts:\", w.grad.item())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be9d85d",
   "metadata": {},
   "source": [
    "### **2.3. Resetting And Reusing Gradients**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3a9808",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_02/Lecture_A/image_02_03.jpg?v=1769656183\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Gradients persist and accumulate across backward passes\n",
    ">* Reset gradients each batch unless intentionally accumulating\n",
    "\n",
    ">* Clear gradients so batches don’t mix\n",
    ">* Reset before each backward pass to avoid bugs\n",
    "\n",
    ">* Inspect, log, or copy gradients after backward\n",
    ">* Then reset gradients to keep training clean\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d46859",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Resetting And Reusing Gradients\n",
    "\n",
    "# This script demonstrates resetting and reusing gradients.\n",
    "# It uses PyTorch autograd with simple tensor operations.\n",
    "# Focus on requires_grad backward and grad attributes.\n",
    "\n",
    "# !pip install torch torchvision torchaudio.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "\n",
    "# Import torch for tensor and autograd.\n",
    "import torch\n",
    "\n",
    "# Set deterministic random seeds.\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Select device based on availability.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Print torch version and device used.\n",
    "print(\"Torch version:\", torch.__version__, \"Device:\", device)\n",
    "\n",
    "# Create a simple parameter tensor.\n",
    "weight = torch.tensor(2.0, requires_grad=True, device=device)\n",
    "\n",
    "# Define a helper to run one forward backward step.\n",
    "def run_step(x_value, label):\n",
    "    # Ensure input is a scalar tensor.\n",
    "    x_tensor = torch.tensor(x_value, device=device)\n",
    "    # Compute simple prediction.\n",
    "    prediction = weight * x_tensor\n",
    "    # Compute squared error loss.\n",
    "    loss = (prediction - label) ** 2\n",
    "    # Run backward to compute gradient.\n",
    "    loss.backward()\n",
    "    # Return loss item and current gradient.\n",
    "    return loss.item(), weight.grad.item()\n",
    "\n",
    "# First mini batch with single example.\n",
    "loss1, grad1 = run_step(1.0, torch.tensor(3.0, device=device))\n",
    "\n",
    "# Print loss and gradient after first backward.\n",
    "print(\"After first backward loss1=\", round(loss1, 3))\n",
    "print(\"Accumulated grad after first=\", round(grad1, 3))\n",
    "\n",
    "# Second mini batch without resetting gradients.\n",
    "loss2, grad2 = run_step(2.0, torch.tensor(1.0, device=device))\n",
    "\n",
    "# Print loss and gradient after second backward.\n",
    "print(\"After second backward loss2=\", round(loss2, 3))\n",
    "print(\"Accumulated grad after second=\", round(grad2, 3))\n",
    "\n",
    "# Show that gradient is sum of both contributions.\n",
    "print(\"Gradient is accumulated not replaced.\")\n",
    "\n",
    "# Copy gradient for later analysis reuse.\n",
    "saved_grad = weight.grad.detach().clone()\n",
    "\n",
    "# Reset gradient to start fresh.\n",
    "weight.grad.zero_()\n",
    "\n",
    "# Confirm gradient is now zero.\n",
    "print(\"Gradient after reset=\", float(weight.grad.item()))\n",
    "\n",
    "# Run another step with clean gradient.\n",
    "loss3, grad3 = run_step(1.5, torch.tensor(0.5, device=device))\n",
    "\n",
    "# Print new loss and gradient after reset.\n",
    "print(\"After reset backward loss3=\", round(loss3, 3))\n",
    "print(\"New grad without accumulation=\", round(grad3, 3))\n",
    "\n",
    "# Reuse saved gradient for custom inspection.\n",
    "print(\"Saved previous accumulated grad=\", round(saved_grad.item(), 3))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6188ceff",
   "metadata": {},
   "source": [
    "## **3. Controlling Autograd Tracking**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895bd58a",
   "metadata": {},
   "source": [
    "### **3.1. No Grad Context**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bf75b6",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_02/Lecture_A/image_03_01.jpg?v=1769656289\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* No grad context temporarily disables gradient tracking\n",
    ">* Reduces overhead and prevents unintended computation graphs\n",
    "\n",
    ">* Use no_grad during inference with trained models\n",
    ">* Avoid gradient overhead, making serving faster and predictable\n",
    "\n",
    ">* Use no_grad for metrics and validation runs\n",
    ">* Prevents useless graphs, saving memory and debugging time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ce89b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - No Grad Context\n",
    "\n",
    "# This script demonstrates torch no grad contexts.\n",
    "# We compare gradients with and without no grad contexts.\n",
    "# Focus on inference efficiency and avoiding unwanted graphs.\n",
    "\n",
    "# Install PyTorch if not already available in the environment.\n",
    "# !pip install torch torchvision torchaudio --quiet.\n",
    "\n",
    "# Import torch for tensor and autograd operations.\n",
    "import torch\n",
    "\n",
    "# Set a deterministic seed for reproducible tensor values.\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Create a tensor with gradient tracking enabled.\n",
    "weights = torch.randn(3, requires_grad=True)\n",
    "\n",
    "# Create an input tensor that does not require gradients.\n",
    "inputs = torch.tensor([1.0, 2.0, 3.0])\n",
    "\n",
    "# Compute a simple prediction with gradient tracking on.\n",
    "pred_train = (weights * inputs).sum()\n",
    "\n",
    "# Run backward to compute gradients for training scenario.\n",
    "pred_train.backward()\n",
    "\n",
    "# Print gradients after normal training style computation.\n",
    "print(\"Gradients after training step:\", weights.grad)\n",
    "\n",
    "# Store a copy of gradients for later comparison.\n",
    "train_grad_copy = weights.grad.clone()\n",
    "\n",
    "# Reset gradients to zero before next demonstration.\n",
    "weights.grad.zero_()\n",
    "\n",
    "# Use no grad context for inference style forward pass.\n",
    "with torch.no_grad():\n",
    "    pred_eval = (weights * inputs).sum()\n",
    "\n",
    "# Check that no grad context did not change gradients.\n",
    "print(\"Gradients after no_grad inference:\", weights.grad)\n",
    "\n",
    "# Verify that prediction values still make numerical sense.\n",
    "print(\"Training prediction value:\", float(pred_train))\n",
    "\n",
    "# Show that inference prediction matches expected computation.\n",
    "print(\"Inference prediction value:\", float(pred_eval))\n",
    "\n",
    "# Demonstrate accidental graph building without no grad context.\n",
    "extra_loss = (pred_eval * 2.0) + 1.0\n",
    "\n",
    "# Show that extra_loss does not require gradients here.\n",
    "print(\"extra_loss requires_grad flag:\", extra_loss.requires_grad)\n",
    "\n",
    "# Create a new tensor that tracks gradients for comparison.\n",
    "new_weights = torch.randn(3, requires_grad=True)\n",
    "\n",
    "# Compute prediction without no grad to show graph creation.\n",
    "new_pred = (new_weights * inputs).sum()\n",
    "\n",
    "# Confirm that new_pred participates in autograd graph.\n",
    "print(\"new_pred requires_grad flag:\", new_pred.requires_grad)\n",
    "\n",
    "# Final print summarizing why no grad is helpful.\n",
    "print(\"no_grad keeps inference cheap and avoids unwanted graphs.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6523fdc",
   "metadata": {},
   "source": [
    "### **3.2. Detach and Inplace Operations**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc95e60",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_02/Lecture_A/image_03_02.jpg?v=1769656339\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Detach freezes a tensor’s value for autograd\n",
    ">* Reused outputs stay constant and stop gradient flow\n",
    "\n",
    ">* In-place ops can break autograd by overwriting history\n",
    ">* Detach first to safely modify tensors without gradients\n",
    "\n",
    ">* Detach plus inplace makes constants for gradients\n",
    ">* Used for stop-gradient, but misuse blocks learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349232bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Detach and Inplace Operations\n",
    "\n",
    "# This script demonstrates detach and inplace operations.\n",
    "# It focuses on PyTorch autograd gradient tracking control.\n",
    "# Run cells sequentially to follow the explanations.\n",
    "\n",
    "# !pip install torch torchvision torchaudio.\n",
    "\n",
    "# Import torch for tensor and autograd operations.\n",
    "import torch\n",
    "\n",
    "# Set a deterministic seed for reproducible values.\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Print PyTorch version in a single concise line.\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "\n",
    "# Create a tensor with gradient tracking enabled.\n",
    "x = torch.tensor([2.0, 3.0], requires_grad=True)\n",
    "\n",
    "# Confirm the tensor shape is as expected.\n",
    "assert x.shape == (2,), \"Unexpected shape for x tensor.\"\n",
    "\n",
    "# Build a simple computation that depends on x.\n",
    "y = (x * 2.0).sum()\n",
    "\n",
    "# Run backward to compute gradients for x.\n",
    "y.backward()\n",
    "\n",
    "# Print original tensor and its computed gradients.\n",
    "print(\"x:\", x)\n",
    "print(\"x.grad after first backward:\", x.grad)\n",
    "\n",
    "# Detach x to create a non tracked view tensor.\n",
    "x_detached = x.detach()\n",
    "\n",
    "# Verify detached tensor shares same data values.\n",
    "print(\"x_detached shares data with x:\", torch.all(x_detached == x))\n",
    "\n",
    "# Show that detached tensor does not require gradients.\n",
    "print(\"x_detached requires_grad:\", x_detached.requires_grad)\n",
    "\n",
    "# Perform an inplace operation only on detached tensor.\n",
    "x_detached.add_(1.0)\n",
    "\n",
    "# Show both tensors now see updated underlying values.\n",
    "print(\"x after inplace on detached:\", x)\n",
    "print(\"x_detached after inplace:\", x_detached)\n",
    "\n",
    "# Clear previous gradients before new backward pass.\n",
    "x.grad.zero_()\n",
    "\n",
    "# Build new computation using updated x values.\n",
    "z = (x * 3.0).sum()\n",
    "\n",
    "# Run backward again to compute fresh gradients.\n",
    "z.backward()\n",
    "\n",
    "# Print gradients to show they ignore detach history.\n",
    "print(\"x.grad after second backward:\", x.grad)\n",
    "\n",
    "# Final print summarizing detach and inplace interaction.\n",
    "print(\"Detach stops gradients but shares inplace updated data.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f12dd0d",
   "metadata": {},
   "source": [
    "### **3.3. Memory and Speed**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd6c8e9",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_02/Lecture_A/image_03_03.jpg?v=1769656393\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Tracking every operation consumes memory and time\n",
    ">* Unneeded tracking can bloat graphs and cause OOM\n",
    "\n",
    ">* Disable gradients to shrink graphs and memory\n",
    ">* Use for evaluation, inference, and feature extraction\n",
    "\n",
    ">* Deep, long-lived graphs slow forward and backward\n",
    ">* Cutting off tracking earlier saves memory, improves scalability\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5bc9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Memory and Speed\n",
    "\n",
    "# This script compares autograd memory and speed.\n",
    "# We simulate tracking and no tracking scenarios.\n",
    "# Focus on torch.no_grad and detach usage.\n",
    "\n",
    "# Uncomment the next line if torch is not installed.\n",
    "# !pip install torch torchvision torchaudio.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Import torch for tensor and autograd features.\n",
    "import torch\n",
    "\n",
    "# Set deterministic random seeds for reproducibility.\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Select device based on availability for fairness.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Print framework version and selected device.\n",
    "print(\"PyTorch version:\", torch.__version__, \"Device:\", device)\n",
    "\n",
    "# Define a helper to create a random input batch.\n",
    "def make_input(batch_size, features):\n",
    "    # Create tensor with gradients enabled for training.\n",
    "    x = torch.randn(batch_size, features, device=device, requires_grad=True)\n",
    "    return x\n",
    "\n",
    "# Define a simple linear model using manual parameters.\n",
    "class TinyLinear(torch.nn.Module):\n",
    "    # Initialize with small input and output sizes.\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(in_features, out_features)\n",
    "\n",
    "    # Forward pass applies linear transformation.\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "\n",
    "# Create a tiny model instance and move to device.\n",
    "model = TinyLinear(in_features=64, out_features=64).to(device)\n",
    "\n",
    "# Ensure model is in evaluation mode for fair comparison.\n",
    "model.eval()\n",
    "\n",
    "# Define a function that runs several forward passes.\n",
    "def run_forwards(track_gradients, use_detach):\n",
    "    # Choose small batch and steps to stay lightweight.\n",
    "    batch_size, steps = 256, 40\n",
    "    # Create base input tensor with gradients enabled.\n",
    "    base_x = make_input(batch_size, 64)\n",
    "\n",
    "    # Optionally detach base input from computation graph.\n",
    "    if use_detach:\n",
    "        base_x = base_x.detach()\n",
    "\n",
    "    # Prepare list to hold summary scalars only.\n",
    "    outputs = []\n",
    "\n",
    "    # Optionally disable gradient tracking context.\n",
    "    if track_gradients:\n",
    "        context = torch.enable_grad()\n",
    "    else:\n",
    "        context = torch.no_grad()\n",
    "\n",
    "    # Use context manager to control autograd tracking.\n",
    "    with context:\n",
    "        # Record start time for performance measurement.\n",
    "        start = time.perf_counter()\n",
    "\n",
    "        # Run several forward passes to build computation.\n",
    "        for _ in range(steps):\n",
    "            # Add tiny noise to avoid exact reuse.\n",
    "            noise = torch.randn_like(base_x) * 0.001\n",
    "            x = base_x + noise\n",
    "\n",
    "            # Compute model output and simple scalar loss.\n",
    "            y = model(x)\n",
    "            loss = y.pow(2).mean()\n",
    "\n",
    "            # Store detached scalar value only.\n",
    "            outputs.append(loss.detach().item())\n",
    "\n",
    "        # Measure elapsed time for all forward passes.\n",
    "        elapsed = time.perf_counter() - start\n",
    "\n",
    "    # Estimate memory by checking gradient requirement.\n",
    "    requires_grad_any = base_x.requires_grad and track_gradients\n",
    "\n",
    "    # Return elapsed time and gradient tracking flag.\n",
    "    return elapsed, requires_grad_any, len(outputs)\n",
    "\n",
    "# Run scenario with full gradient tracking enabled.\n",
    "tracked_time, tracked_grad, tracked_len = run_forwards(True, False)\n",
    "\n",
    "# Run scenario with torch.no_grad disabling tracking.\n",
    "no_grad_time, no_grad_grad, no_grad_len = run_forwards(False, False)\n",
    "\n",
    "# Run scenario with detached input and tracking enabled.\n",
    "detach_time, detach_grad, detach_len = run_forwards(True, True)\n",
    "\n",
    "# Print concise comparison of the three scenarios.\n",
    "print(\"Tracked: time=\", round(tracked_time, 4), \"s, grads=\", tracked_grad)\n",
    "print(\"no_grad: time=\", round(no_grad_time, 4), \"s, grads=\", no_grad_grad)\n",
    "print(\"detach : time=\", round(detach_time, 4), \"s, grads=\", detach_grad)\n",
    "\n",
    "# Show that we only kept small scalar outputs list.\n",
    "print(\"Outputs stored per run:\", tracked_len, no_grad_len, detach_len)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92da7951",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Autograd Mechanics**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c23c2b",
   "metadata": {},
   "source": [
    "\n",
    "In this lecture, you learned to:\n",
    "- Describe how PyTorch builds dynamic computation graphs and uses them to compute gradients. \n",
    "- Use requires_grad, backward, and grad attributes to compute and inspect gradients for simple tensor operations. \n",
    "- Control gradient tracking with torch.no_grad and detach to optimize performance and avoid unintended graph creation. \n",
    "\n",
    "In the next Lecture (Lecture B), we will go over 'Modules and Layers'"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
