{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1e82500",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Modules and Layers**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91e673e",
   "metadata": {},
   "source": [
    ">Last update: 20260129.\n",
    "    \n",
    "By the end of this Lecture, you will be able to:\n",
    "- Define custom neural network components by subclassing nn.Module and implementing forward methods. \n",
    "- Use common nn layers such as Linear, Conv2d, and Dropout to assemble simple models. \n",
    "- Inspect and manage model parameters, including initialization and parameter counting. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe73b177",
   "metadata": {},
   "source": [
    "## **1. Building Custom Modules**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07aeec3",
   "metadata": {},
   "source": [
    "### **1.1. Creating Custom Modules**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e04b220",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_02/Lecture_B/image_01_01.jpg?v=1769698282\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Custom modules group related operations and parameters\n",
    ">* They simplify complex models, improving clarity and reuse\n",
    "\n",
    ">* Decide module behavior, inputs, and outputs clearly\n",
    ">* Hide internal steps; keep interface stable, flexible\n",
    "\n",
    ">* Design modules that match your problem domain\n",
    ">* Swap high-level components to experiment and reuse\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3fb6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Creating Custom Modules\n",
    "\n",
    "# This script shows creating simple custom modules.\n",
    "# We use TensorFlow Keras to define custom layers.\n",
    "# Focus on subclassing Layer and implementing call.\n",
    "\n",
    "# !pip install tensorflow.\n",
    "\n",
    "# Import required TensorFlow modules.\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set deterministic random seeds for reproducibility.\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Define a simple custom dense like layer.\n",
    "class MyDenseLayer(tf.keras.layers.Layer):\n",
    "\n",
    "    # Initialize layer with output units argument.\n",
    "    def __init__(self, units):\n",
    "        super().__init__()\n",
    "        self.units = units\n",
    "\n",
    "    # Build method creates weights once input shape known.\n",
    "    def build(self, input_shape):\n",
    "        last_dim = int(input_shape[-1])\n",
    "        assert last_dim > 0\n",
    "        self.w = self.add_weight(\n",
    "            shape=(last_dim, self.units),\n",
    "            initializer=\"glorot_uniform\",\n",
    "            trainable=True,\n",
    "            name=\"kernel\",\n",
    "        )\n",
    "        self.b = self.add_weight(\n",
    "            shape=(self.units,),\n",
    "            initializer=\"zeros\",\n",
    "            trainable=True,\n",
    "            name=\"bias\",\n",
    "        )\n",
    "\n",
    "    # Call method defines forward computation logic.\n",
    "    def call(self, inputs):\n",
    "        return tf.nn.relu(tf.matmul(inputs, self.w) + self.b)\n",
    "\n",
    "# Create a small model using the custom layer.\n",
    "inputs = tf.keras.Input(shape=(4,))\n",
    "\n",
    "# Apply custom dense layer to the inputs.\n",
    "x = MyDenseLayer(units=3)(inputs)\n",
    "\n",
    "# Add a final built in dense output layer.\n",
    "outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "# Build the Keras model object.\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Show a short model summary to inspect shapes.\n",
    "model.summary()\n",
    "\n",
    "# Create a tiny batch of example input data.\n",
    "example_batch = tf.constant([[1.0, 2.0, 3.0, 4.0]])\n",
    "\n",
    "# Validate input shape before running the model.\n",
    "assert example_batch.shape == (1, 4)\n",
    "\n",
    "# Run a forward pass through the model.\n",
    "output_batch = model(example_batch)\n",
    "\n",
    "# Print the model output tensor values.\n",
    "print(\"Model output:\", output_batch.numpy())\n",
    "\n",
    "# Print the number of trainable parameters.\n",
    "print(\"Trainable parameters:\", model.count_params())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2f6643",
   "metadata": {},
   "source": [
    "### **1.2. Init and Forward Methods**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8732d934",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_02/Lecture_B/image_01_02.jpg?v=1769698346\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Initializer defines module structure and components\n",
    ">* Forward method specifies data flow and computation\n",
    "\n",
    ">* Initializer builds and connects the module’s parts\n",
    ">* Forward defines how inputs move through those parts\n",
    "\n",
    ">* Init defines complex model parts and structure\n",
    ">* Forward controls data flow, improving reuse and debugging\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270b9d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Init and Forward Methods\n",
    "\n",
    "# This script shows custom module initialization and forward computation.\n",
    "# We use TensorFlow Keras layers to mimic PyTorch style modules.\n",
    "# Focus on __init__ like setup and forward style call method.\n",
    "# !pip install tensorflow.\n",
    "# Import required TensorFlow modules.\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set a deterministic random seed.\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Print TensorFlow version briefly.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Define a simple custom dense block class.\n",
    "class SimpleDenseBlock(tf.keras.layers.Layer):\n",
    "\n",
    "    # Initialize internal layers and configuration.\n",
    "    def __init__(self, units, dropout_rate=0.0):\n",
    "        super().__init__()\n",
    "        self.units = int(units)\n",
    "        self.dropout_rate = float(dropout_rate)\n",
    "\n",
    "        # Create a dense layer for linear transformation.\n",
    "        self.dense = tf.keras.layers.Dense(\n",
    "            units=self.units,\n",
    "            activation=\"relu\",\n",
    "        )\n",
    "\n",
    "        # Create a dropout layer for regularization.\n",
    "        self.dropout = tf.keras.layers.Dropout(\n",
    "            rate=self.dropout_rate,\n",
    "        )\n",
    "\n",
    "    # Define the forward computation for one input batch.\n",
    "    def call(self, inputs, training=False):\n",
    "        dense_out = self.dense(inputs)\n",
    "        outputs = self.dropout(dense_out, training=training)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "# Create a tiny model using the custom block.\n",
    "class TinyModel(tf.keras.Model):\n",
    "\n",
    "    # Initialize submodules inside the model.\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.block1 = SimpleDenseBlock(units=8, dropout_rate=0.1)\n",
    "        self.output_layer = tf.keras.layers.Dense(\n",
    "            units=1,\n",
    "            activation=\"sigmoid\",\n",
    "        )\n",
    "\n",
    "    # Define how data flows through the model.\n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.block1(inputs, training=training)\n",
    "        outputs = self.output_layer(x)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "# Create a small batch of dummy input data.\n",
    "inputs = tf.random.uniform(shape=(4, 5), minval=-1.0, maxval=1.0)\n",
    "\n",
    "# Validate the input shape before using the model.\n",
    "if inputs.shape[1] != 5:\n",
    "    raise ValueError(\"Expected input features equal to five.\")\n",
    "\n",
    "# Instantiate the tiny model.\n",
    "model = TinyModel()\n",
    "\n",
    "# Run a forward pass to build the model.\n",
    "outputs = model(inputs, training=False)\n",
    "\n",
    "# Print input and output shapes to inspect behavior.\n",
    "print(\"Input shape:\", inputs.shape)\n",
    "print(\"Output shape:\", outputs.shape)\n",
    "\n",
    "# Count and print the total number of trainable parameters.\n",
    "trainable_count = sum(\n",
    "    int(tf.size(v)) for v in model.trainable_variables\n",
    ")\n",
    "print(\"Trainable parameters:\", int(trainable_count))\n",
    "\n",
    "# Show names of internal variables for clarity.\n",
    "for var in model.trainable_variables:\n",
    "    print(\"Variable:\", var.name, \"shape:\", var.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707daac2",
   "metadata": {},
   "source": [
    "### **1.3. Mastering super in Modules**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a73a516",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_02/Lecture_B/image_01_03.jpg?v=1769698412\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Calling super activates nn.Module’s built-in features\n",
    ">* It prepares infrastructure so custom layers work seamlessly\n",
    "\n",
    ">* Overriding forward customizes computation within framework rules\n",
    ">* Sometimes reuse parent forward, then add extra steps\n",
    "\n",
    ">* Calling parent classes prevents subtle training bugs\n",
    ">* Consistent super use keeps models compatible and reliable\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c18c8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Mastering super in Modules\n",
    "\n",
    "# This script shows mastering super in modules.\n",
    "# We use TensorFlow to mimic PyTorch style modules.\n",
    "# Focus on subclassing and calling parent initializers.\n",
    "\n",
    "# !pip install tensorflow-2.20.0.\n",
    "\n",
    "# Import required TensorFlow module.\n",
    "import tensorflow as tf\n",
    "\n",
    "# Print TensorFlow version briefly.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Set deterministic random seed value.\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Define a simple custom dense like layer.\n",
    "class MyDense(tf.keras.layers.Layer):\n",
    "\n",
    "    # Initialize layer and call parent initializer.\n",
    "    def __init__(self, units, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = int(units)\n",
    "\n",
    "    # Build weights once input shape is known.\n",
    "    def build(self, input_shape):\n",
    "        last_dim = int(input_shape[-1])\n",
    "        assert last_dim > 0, \"Input features must be positive\"\n",
    "        self.w = self.add_weight(\n",
    "            shape=(last_dim, self.units), initializer=\"glorot_uniform\"\n",
    "        )\n",
    "        self.b = self.add_weight(\n",
    "            shape=(self.units,), initializer=\"zeros\"\n",
    "        )\n",
    "        super().build(input_shape)\n",
    "\n",
    "    # Define forward computation for the layer.\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b\n",
    "\n",
    "# Define a custom module that reuses MyDense.\n",
    "class MyBlock(tf.keras.layers.Layer):\n",
    "\n",
    "    # Call parent initializer before creating sublayers.\n",
    "    def __init__(self, units, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.base = MyDense(units)\n",
    "        self.extra_bias = self.add_weight(\n",
    "            shape=(units,), initializer=\"zeros\"\n",
    "        )\n",
    "\n",
    "    # Forward pass extends base layer behavior.\n",
    "    def call(self, inputs):\n",
    "        base_out = self.base(inputs)\n",
    "        return base_out + self.extra_bias\n",
    "\n",
    "# Create a tiny input batch tensor.\n",
    "inputs = tf.constant([[1.0, 2.0], [3.0, 4.0]])\n",
    "\n",
    "# Instantiate custom block with specific units.\n",
    "block = MyBlock(units=3, name=\"my_block\")\n",
    "\n",
    "# Run a forward pass to build weights.\n",
    "outputs = block(inputs)\n",
    "\n",
    "# Print input and output shapes clearly.\n",
    "print(\"Input shape:\", inputs.shape)\n",
    "print(\"Output shape:\", outputs.shape)\n",
    "\n",
    "# Show that variables were registered correctly.\n",
    "print(\"Number of trainable variables:\", len(block.trainable_variables))\n",
    "\n",
    "# Print each variable name and shape briefly.\n",
    "for var in block.trainable_variables:\n",
    "    print(\"Var:\", var.name, \"Shape:\", var.shape)\n",
    "\n",
    "# Confirm that calling block again reuses same weights.\n",
    "print(\"Second call output equal:\", tf.reduce_all(block(inputs) == outputs).numpy())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd83ed1",
   "metadata": {},
   "source": [
    "## **2. Core Neural Layers**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8067e3f",
   "metadata": {},
   "source": [
    "### **2.1. Linear Layers and Activations**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c8c472",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_02/Lecture_B/image_02_01.jpg?v=1769698478\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Linear layers turn input features into outputs\n",
    ">* Stacked layers build increasingly abstract learned representations\n",
    "\n",
    ">* Linear layers alone stay a simple transformation\n",
    ">* Nonlinear activations give networks rich expressive power\n",
    "\n",
    ">* Alternate linear layers and activations to build features\n",
    ">* Layer sizes and activations adapt models to tasks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb35f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Linear Layers and Activations\n",
    "\n",
    "# This script shows linear layers and activations.\n",
    "# It uses TensorFlow dense layers for clarity.\n",
    "# Run cells to see shapes and simple outputs.\n",
    "\n",
    "# !pip install tensorflow.\n",
    "\n",
    "# Import TensorFlow and NumPy libraries.\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Set deterministic random seeds for reproducibility.\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Create a small batch of input feature vectors.\n",
    "inputs = np.random.randn(4, 3).astype(\"float32\")\n",
    "\n",
    "# Show the input shape and a small preview.\n",
    "print(\"Input shape:\", inputs.shape)\n",
    "print(\"First input row:\", inputs[0])\n",
    "\n",
    "# Define a simple model with two dense layers.\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(5, activation=None, input_shape=(3,)),\n",
    "    tf.keras.layers.ReLU(),\n",
    "    tf.keras.layers.Dense(2, activation=\"sigmoid\"),\n",
    "])\n",
    "\n",
    "# Run a forward pass to get model outputs.\n",
    "outputs = model(inputs)\n",
    "\n",
    "# Show the output shape and a small preview.\n",
    "print(\"Output shape:\", outputs.shape)\n",
    "print(\"First output row:\", outputs[0].numpy())\n",
    "\n",
    "# Access the first dense layer from the model.\n",
    "first_dense = model.layers[0]\n",
    "\n",
    "# Get weights and biases from the first dense layer.\n",
    "weights, biases = first_dense.get_weights()\n",
    "\n",
    "# Print shapes of weights and biases for inspection.\n",
    "print(\"First layer weights shape:\", weights.shape)\n",
    "print(\"First layer biases shape:\", biases.shape)\n",
    "\n",
    "# Count total trainable parameters in the model.\n",
    "model_params = model.count_params()\n",
    "\n",
    "# Print the total number of trainable parameters.\n",
    "print(\"Total trainable parameters:\", model_params)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8fa620",
   "metadata": {},
   "source": [
    "### **2.2. Convolutions and Pooling**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56050e69",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_02/Lecture_B/image_02_02.jpg?v=1769698535\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Convolutional layers slide shared filters over structured data\n",
    ">* Stacked convolutions build hierarchical features with fewer parameters\n",
    "\n",
    ">* Pooling summarizes local regions, reducing feature map size\n",
    ">* It adds translation invariance and saves computation\n",
    "\n",
    ">* Stack conv and pooling blocks to extract features\n",
    ">* Flatten features, use linear layers for tasks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b467509e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Convolutions and Pooling\n",
    "\n",
    "# This script shows basic convolutions and pooling.\n",
    "# It uses TensorFlow to build tiny models.\n",
    "# Focus on images with simple feature maps.\n",
    "\n",
    "# !pip install tensorflow.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Import TensorFlow and Keras layers.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Set deterministic random seeds.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version briefly.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Load MNIST dataset from Keras.\n",
    "(x_train, y_train), _ = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Select a small subset for speed.\n",
    "num_samples = 64\n",
    "x_small = x_train[:num_samples]\n",
    "y_small = y_train[:num_samples]\n",
    "\n",
    "# Normalize pixel values to range zero one.\n",
    "x_small = x_small.astype(\"float32\") / 255.0\n",
    "\n",
    "# Add channel dimension for convolutions.\n",
    "x_small = np.expand_dims(x_small, axis=-1)\n",
    "\n",
    "# Validate input shape before modeling.\n",
    "assert x_small.shape[1:] == (28, 28, 1)\n",
    "\n",
    "# Build a simple conv and pooling model.\n",
    "model = keras.Sequential([\n",
    "    layers.Input(shape=(28, 28, 1)),\n",
    "    layers.Conv2D(filters=4, kernel_size=3, activation=\"relu\"),\n",
    "    layers.MaxPooling2D(pool_size=2),\n",
    "    layers.Conv2D(filters=8, kernel_size=3, activation=\"relu\"),\n",
    "    layers.AveragePooling2D(pool_size=2),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(10, activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "# Show model summary in one line.\n",
    "model.summary(print_fn=lambda x: None)\n",
    "\n",
    "# Count total trainable parameters.\n",
    "trainable_params = np.sum([\n",
    "    np.prod(v.shape) for v in model.trainable_weights\n",
    "])\n",
    "\n",
    "# Compile model with simple settings.\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Train briefly on the tiny subset.\n",
    "history = model.fit(\n",
    "    x_small,\n",
    "    y_small,\n",
    "    epochs=2,\n",
    "    batch_size=16,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Evaluate on the same subset silently.\n",
    "loss, acc = model.evaluate(\n",
    "    x_small,\n",
    "    y_small,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Get one example and its prediction.\n",
    "example = x_small[0:1]\n",
    "true_label = int(y_small[0])\n",
    "probs = model.predict(example, verbose=0)[0]\n",
    "\n",
    "# Find predicted class index.\n",
    "pred_label = int(np.argmax(probs))\n",
    "\n",
    "# Print key results in few lines.\n",
    "print(\"Trainable parameters:\", int(trainable_params))\n",
    "print(\"Subset accuracy:\", round(float(acc), 3))\n",
    "print(\"True label:\", true_label, \"Predicted:\", pred_label)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c0c44a",
   "metadata": {},
   "source": [
    "### **2.3. Regularization with Dropout BatchNorm**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b29ce48",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_02/Lecture_B/image_02_03.jpg?v=1769698604\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Dropout randomly disables units to reduce overfitting\n",
    ">* BatchNorm stabilizes activations, speeding deeper network training\n",
    "\n",
    ">* Combine Conv, BatchNorm, activations, and dropout layers\n",
    ">* They stabilize training and improve generalization on data\n",
    "\n",
    ">* Dropout and BatchNorm act differently during inference\n",
    ">* Correct modes give stable, reliable real-world predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1d2b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Regularization with Dropout BatchNorm\n",
    "\n",
    "# This script shows dropout and batch normalization.\n",
    "# We build a tiny image model using TensorFlow layers.\n",
    "# Focus on regularization behavior during training evaluation.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Import TensorFlow and Keras layers.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Set deterministic random seeds.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version briefly.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Load MNIST dataset from Keras.\n",
    "(x_train, y_train), _ = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Select a small subset for speed.\n",
    "num_samples = 512\n",
    "x_train = x_train[:num_samples]\n",
    "y_train = y_train[:num_samples]\n",
    "\n",
    "# Normalize pixel values to range zero one.\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "\n",
    "# Add channel dimension for Conv2D.\n",
    "x_train = np.expand_dims(x_train, axis=-1)\n",
    "\n",
    "# Validate input shape dimensions.\n",
    "assert x_train.shape[1:] == (28, 28, 1)\n",
    "\n",
    "# Build a small model with Conv2D.\n",
    "inputs = keras.Input(shape=(28, 28, 1))\n",
    "x = layers.Conv2D(16, (3, 3), activation=\"relu\")(inputs)\n",
    "x = layers.BatchNormalization()(x)\n",
    "\n",
    "# Add dropout after convolution block.\n",
    "x = layers.Dropout(0.3)(x)\n",
    "\n",
    "# Flatten and add dense layer.\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(32, activation=\"relu\")(x)\n",
    "\n",
    "# Add another dropout for regularization.\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "# Output layer for ten digit classes.\n",
    "outputs = layers.Dense(10, activation=\"softmax\")(x)\n",
    "\n",
    "# Create the Keras model object.\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Compile model with simple optimizer.\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Show model summary in one short line.\n",
    "print(\"Model has\", model.count_params(), \"trainable parameters.\")\n",
    "\n",
    "# Train briefly with silent verbose setting.\n",
    "history = model.fit(x_train, y_train, epochs=2, batch_size=64, verbose=0)\n",
    "\n",
    "# Take a small batch for demonstration.\n",
    "x_batch = x_train[:4]\n",
    "\n",
    "# Get predictions in training mode.\n",
    "train_preds = model(x_batch, training=True)\n",
    "\n",
    "# Get predictions in inference mode.\n",
    "eval_preds = model(x_batch, training=False)\n",
    "\n",
    "# Convert predictions to numpy arrays.\n",
    "train_preds_np = train_preds.numpy()\n",
    "eval_preds_np = eval_preds.numpy()\n",
    "\n",
    "# Print shapes to confirm behavior.\n",
    "print(\"Batch shape:\", x_batch.shape)\n",
    "print(\"Train preds shape:\", train_preds_np.shape)\n",
    "print(\"Eval preds shape:\", eval_preds_np.shape)\n",
    "\n",
    "# Compute mean difference between predictions.\n",
    "diff = np.mean(np.abs(train_preds_np - eval_preds_np))\n",
    "\n",
    "# Print explanation of dropout effect.\n",
    "print(\"Mean abs difference train vs eval:\", float(diff))\n",
    "print(\"Larger difference shows active dropout during training.\")\n",
    "print(\"BatchNorm uses running statistics during evaluation.\")\n",
    "print(\"This combination helps regularize the small model.\")\n",
    "print(\"Script finished successfully with regularization demo.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948cd761",
   "metadata": {},
   "source": [
    "## **3. Managing Model Parameters**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7ddce7",
   "metadata": {},
   "source": [
    "### **3.1. Iterating Model Parameters**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd01be0f",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_02/Lecture_B/image_03_01.jpg?v=1769698676\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Iterating parameters reveals all learnable weights and biases\n",
    ">* Gives transparency for debugging, optimization, and design checks\n",
    "\n",
    ">* See parameter values, names, shapes, trainable flags\n",
    ">* Group parameters for different training or regularization strategies\n",
    "\n",
    ">* Use parameter iteration for freezing and custom optimization\n",
    ">* Compute stats for deployment and precise control\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c53523e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Iterating Model Parameters\n",
    "\n",
    "# This script shows iterating model parameters.\n",
    "# We use TensorFlow Keras dense layers.\n",
    "# Focus is on inspecting trainable weights.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required TensorFlow and NumPy modules.\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Set deterministic random seeds for reproducibility.\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Define a simple sequential model with dense layers.\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(4,)),\n",
    "    tf.keras.layers.Dense(8, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(3, activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "# Build the model by calling it on dummy data.\n",
    "dummy_input = tf.zeros((1, 4), dtype=tf.float32)\n",
    "_ = model(dummy_input)\n",
    "\n",
    "# Confirm the model output shape is as expected.\n",
    "assert _.shape == (1, 3)\n",
    "\n",
    "# Print a short header for parameter inspection.\n",
    "print(\"\\nIterating over trainable parameters:\")\n",
    "\n",
    "# Iterate over each trainable variable in the model.\n",
    "for var in model.trainable_variables:\n",
    "    name = var.name\n",
    "    shape = var.shape\n",
    "    size = np.prod(shape)\n",
    "    trainable = var.trainable\n",
    "\n",
    "    # Print a concise summary for each parameter.\n",
    "    print(\n",
    "        f\"Name: {name}, shape: {shape}, size: {size}, trainable: {trainable}\"\n",
    "    )\n",
    "\n",
    "# Compute total number of trainable parameters.\n",
    "param_counts = [int(np.prod(v.shape)) for v in model.trainable_variables]\n",
    "\n",
    "# Sum parameter counts to get the total size.\n",
    "total_params = int(np.sum(param_counts))\n",
    "\n",
    "# Print the final total parameter count clearly.\n",
    "print(\"\\nTotal trainable parameters in model:\", total_params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d471b1",
   "metadata": {},
   "source": [
    "### **3.2. Initializing Model Parameters**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5af0783",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_02/Lecture_B/image_03_02.jpg?v=1769698741\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Initialization sets learning’s starting point and stability\n",
    ">* Choose layer-aware weight scales to keep signals balanced\n",
    "\n",
    ">* Defaults work, but complex models need control\n",
    ">* Choose schemes by activation, task, and stability\n",
    "\n",
    ">* Decide when to reinitialize and reuse weights\n",
    ">* Use domain knowledge; treat initialization as ongoing design\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955cbed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Initializing Model Parameters\n",
    "\n",
    "# This script shows simple parameter initialization concepts.\n",
    "# We use TensorFlow to build tiny dense layers.\n",
    "# Focus on inspecting and reinitializing layer weights.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required modules from TensorFlow.\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set a deterministic random seed value.\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Create a simple dense layer with default initialization.\n",
    "default_layer = tf.keras.layers.Dense(\n",
    "    units=3, activation=\"relu\", input_shape=(4,)\n",
    ")\n",
    "\n",
    "# Build the layer by calling it on dummy input.\n",
    "dummy_input = tf.zeros((1, 4))\n",
    "\n",
    "# Run a forward pass to ensure weights are created.\n",
    "_ = default_layer(dummy_input)\n",
    "\n",
    "# Get the weights and biases from the layer.\n",
    "default_weights, default_biases = default_layer.get_weights()\n",
    "\n",
    "# Print basic information about default initialized parameters.\n",
    "print(\"Default weights shape:\", default_weights.shape)\n",
    "\n",
    "# Show a small sample of default weight values.\n",
    "print(\"First row default weights:\", default_weights[0])\n",
    "\n",
    "# Show default bias values for the dense layer.\n",
    "print(\"Default biases:\", default_biases)\n",
    "\n",
    "# Now create a new dense layer with custom initializer.\n",
    "custom_layer = tf.keras.layers.Dense(\n",
    "    units=3, activation=\"relu\", input_shape=(4,)\n",
    ")\n",
    "\n",
    "# Build the custom layer using another dummy input.\n",
    "_ = custom_layer(dummy_input)\n",
    "\n",
    "# Manually create new weight and bias tensors.\n",
    "custom_w = tf.random.normal(shape=(4, 3), stddev=0.05)\n",
    "\n",
    "# Initialize biases to small positive constant values.\n",
    "custom_b = tf.ones(shape=(3,)) * 0.1\n",
    "\n",
    "# Validate shapes before assigning new weights.\n",
    "if custom_w.shape == custom_layer.kernel.shape:\n",
    "    custom_layer.set_weights([custom_w.numpy(), custom_b.numpy()])\n",
    "\n",
    "# Retrieve the updated weights and biases.\n",
    "new_weights, new_biases = custom_layer.get_weights()\n",
    "\n",
    "# Print shapes to confirm successful reinitialization.\n",
    "print(\"Custom weights shape:\", new_weights.shape)\n",
    "\n",
    "# Show a small sample of custom initialized weights.\n",
    "print(\"First row custom weights:\", new_weights[0])\n",
    "\n",
    "# Show custom bias values for the dense layer.\n",
    "print(\"Custom biases:\", new_biases)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78091f80",
   "metadata": {},
   "source": [
    "### **3.3. Counting Model Parameters**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bda879a",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_02/Lecture_B/image_03_03.jpg?v=1769698776\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Counting parameters reveals model size and cost\n",
    ">* Layer design choices directly change total parameters\n",
    "\n",
    ">* View each layer as weights plus biases\n",
    ">* Sum layer contributions to compare model designs\n",
    "\n",
    ">* Parameter limits control memory, speed, and scalability\n",
    ">* Regular parameter checks balance accuracy and overfitting\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba3ac91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Counting Model Parameters\n",
    "\n",
    "# This script explores counting model parameters.\n",
    "# It uses TensorFlow dense layers for illustration.\n",
    "# Focus on understanding shapes and parameter formulas.\n",
    "\n",
    "# !pip install tensorflow.\n",
    "\n",
    "# Import TensorFlow and NumPy for this lesson.\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Set a deterministic random seed for reproducibility.\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Print TensorFlow version in one concise line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Define a simple sequential model with dense layers.\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(8,)),\n",
    "    tf.keras.layers.Dense(4, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(3, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1, activation=\"linear\"),\n",
    "])\n",
    "\n",
    "# Build the model by calling it on dummy input.\n",
    "dummy_input = tf.zeros((1, 8), dtype=tf.float32)\n",
    "_ = model(dummy_input)\n",
    "\n",
    "# Confirm the dummy input shape is as expected.\n",
    "assert dummy_input.shape == (1, 8)\n",
    "\n",
    "# Function to count parameters in a single variable.\n",
    "def count_params_in_variable(variable):\n",
    "    shape = variable.shape\n",
    "    size = np.prod(shape)\n",
    "    return int(size)\n",
    "\n",
    "# Function to count parameters for each layer.\n",
    "def count_params_per_layer(model):\n",
    "    layer_param_info = []\n",
    "    for layer in model.layers:\n",
    "        layer_params = 0\n",
    "        for var in layer.trainable_variables:\n",
    "            layer_params += count_params_in_variable(var)\n",
    "        layer_param_info.append((layer.name, layer_params))\n",
    "    return layer_param_info\n",
    "\n",
    "# Get parameter counts for each layer in the model.\n",
    "layer_info = count_params_per_layer(model)\n",
    "\n",
    "# Compute the total number of trainable parameters.\n",
    "trainable_variables = model.trainable_variables\n",
    "total_params = sum(count_params_in_variable(v) for v in trainable_variables)\n",
    "\n",
    "# Print a short header for the parameter table.\n",
    "print(\"\\nLayer name and parameter counts:\")\n",
    "\n",
    "# Loop through layers and print their parameter counts.\n",
    "for name, params in layer_info:\n",
    "    print(f\"{name:15s} -> {params:4d} params\")\n",
    "\n",
    "# Explain the manual formula for the first dense layer.\n",
    "input_units = 8\n",
    "output_units_layer1 = 4\n",
    "weights_layer1 = input_units * output_units_layer1\n",
    "biases_layer1 = output_units_layer1\n",
    "\n",
    "# Compute the manual parameter count for first dense layer.\n",
    "manual_params_layer1 = weights_layer1 + biases_layer1\n",
    "\n",
    "# Print manual and model counts for comparison.\n",
    "print(\"\\nManual params for first dense layer:\", manual_params_layer1)\n",
    "\n",
    "# Print the total number of trainable parameters.\n",
    "print(\"Total trainable parameters:\", total_params)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7982efe1",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Modules and Layers**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b890f8d0",
   "metadata": {},
   "source": [
    "\n",
    "In this lecture, you learned to:\n",
    "- Define custom neural network components by subclassing nn.Module and implementing forward methods. \n",
    "- Use common nn layers such as Linear, Conv2d, and Dropout to assemble simple models. \n",
    "- Inspect and manage model parameters, including initialization and parameter counting. \n",
    "\n",
    "In the next Module (Module 3), we will go over 'Training Workflow'"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
