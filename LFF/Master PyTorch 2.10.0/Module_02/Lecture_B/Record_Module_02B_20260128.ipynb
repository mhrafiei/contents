{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af90993b",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Modules and Layers**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92d84f2",
   "metadata": {},
   "source": [
    ">Last update: 20260128.\n",
    "    \n",
    "By the end of this Lecture, you will be able to:\n",
    "- Define custom neural network components by subclassing nn.Module and implementing forward methods. \n",
    "- Use common nn layers such as Linear, Conv2d, and Dropout to assemble simple models. \n",
    "- Inspect and manage model parameters, including initialization and parameter counting. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9500eb4a",
   "metadata": {},
   "source": [
    "## **1. Building Custom Modules**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6396cbab",
   "metadata": {},
   "source": [
    "### **1.1. Creating Custom Modules**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bb7e4c",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_02/Lecture_B/image_01_01.jpg?v=1769659961\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Custom modules bundle computations, parameters, and configuration\n",
    ">* They enable reusable, understandable, testable neural components\n",
    "\n",
    ">* Decide what parameters and layers to store\n",
    ">* Define how inputs are transformed into outputs\n",
    "\n",
    ">* Custom modules plug into training like built-ins\n",
    ">* Internal design can change while interface stays\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c001e299",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Creating Custom Modules\n",
    "\n",
    "# This script shows creating simple custom modules.\n",
    "# We use TensorFlow Keras layers to build modules.\n",
    "# Focus is on subclassing and defining call methods.\n",
    "\n",
    "# !pip install tensorflow.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Import TensorFlow and Keras components.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Create tiny synthetic input data for demonstration.\n",
    "num_samples = 8\n",
    "input_dim = 4\n",
    "x_data = np.random.randn(num_samples, input_dim).astype(\"float32\")\n",
    "\n",
    "# Create tiny synthetic targets for regression.\n",
    "y_data = np.random.randn(num_samples, 1).astype(\"float32\")\n",
    "\n",
    "# Define a custom dense block as a reusable module.\n",
    "class DenseBlock(keras.Model):\n",
    "\n",
    "    # Initialize internal layers and configuration.\n",
    "    def __init__(self, units, activation=\"relu\"):\n",
    "        super().__init__()\n",
    "        self.units = int(units)\n",
    "        self.activation = activation\n",
    "        self.linear = layers.Dense(self.units)\n",
    "        self.dropout = layers.Dropout(0.1)\n",
    "\n",
    "    # Define the forward computation for this module.\n",
    "    def call(self, inputs, training=False):\n",
    "        if inputs.shape[-1] != input_dim:\n",
    "            raise ValueError(\"Unexpected input feature size.\")\n",
    "        x = self.linear(inputs)\n",
    "        x = tf.nn.relu(x) if self.activation == \"relu\" else x\n",
    "        x = self.dropout(x, training=training)\n",
    "        return x\n",
    "\n",
    "# Define a small custom model using the DenseBlock.\n",
    "class SmallRegressor(keras.Model):\n",
    "    # Initialize with one block and an output layer.\n",
    "    def __init__(self, hidden_units):\n",
    "        super().__init__()\n",
    "        self.block = DenseBlock(hidden_units)\n",
    "        self.out_layer = layers.Dense(1)\n",
    "\n",
    "    # Implement the forward pass using submodules.\n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.block(inputs, training=training)\n",
    "        output = self.out_layer(x)\n",
    "        return output\n",
    "\n",
    "# Instantiate the custom model with chosen hidden size.\n",
    "model = SmallRegressor(hidden_units=6)\n",
    "\n",
    "# Build the model by calling once on sample data.\n",
    "_ = model(x_data[:2])\n",
    "\n",
    "# Compile the model with simple optimizer and loss.\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "\n",
    "# Train briefly with silent output for speed.\n",
    "history = model.fit(x_data, y_data, epochs=5, verbose=0)\n",
    "\n",
    "# Count total trainable parameters in the model.\n",
    "param_count = model.count_params()\n",
    "\n",
    "# Print a few informative lines about the custom modules.\n",
    "print(\"Custom model trainable parameters:\", param_count)\n",
    "print(\"DenseBlock internal layers:\", len(model.block.layers))\n",
    "print(\"Example prediction shape:\", model(x_data[:1]).shape)\n",
    "print(\"Final training loss:\", float(history.history[\"loss\"][-1]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b491d50",
   "metadata": {},
   "source": [
    "### **1.2. Init and forward basics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a964e4",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_02/Lecture_B/image_01_02.jpg?v=1769660011\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Initialization defines layers, parameters, and configuration\n",
    ">* Forward reuses this setup to process inputs repeatedly\n",
    "\n",
    ">* Forward pass defines operations transforming inputs to outputs\n",
    ">* Must be clear, deterministic for debugging and gradients\n",
    "\n",
    ">* Init builds the module; forward runs data\n",
    ">* Separation improves reuse, clarity, and integration\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d7ef9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Init and forward basics\n",
    "\n",
    "# This script shows basic module initialization concepts.\n",
    "# It uses TensorFlow to mimic PyTorch style modules.\n",
    "# Focus on __init__ and call forward style methods.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Import TensorFlow and Keras layers.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Set deterministic random seeds.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version briefly.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Define a simple custom dense block class.\n",
    "class SimpleDenseBlock(keras.Model):\n",
    "\n",
    "    # Initialize layers and configuration here.\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.input_dim = int(input_dim)\n",
    "        self.hidden_dim = int(hidden_dim)\n",
    "        self.output_dim = int(output_dim)\n",
    "\n",
    "        # Validate basic dimension arguments.\n",
    "        if self.input_dim <= 0 or self.hidden_dim <= 0:\n",
    "            raise ValueError(\"Dimensions must be positive integers\")\n",
    "\n",
    "        # Define internal dense layers once in __init__.\n",
    "        self.hidden_layer = layers.Dense(\n",
    "            units=self.hidden_dim,\n",
    "            activation=\"relu\",\n",
    "            name=\"hidden_layer\",\n",
    "        )\n",
    "\n",
    "        # Define output dense layer without activation.\n",
    "        self.output_layer = layers.Dense(\n",
    "            units=self.output_dim,\n",
    "            activation=None,\n",
    "            name=\"output_layer\",\n",
    "        )\n",
    "\n",
    "    # Define the forward computation using call.\n",
    "    def call(self, inputs, training=False):\n",
    "        inputs = tf.convert_to_tensor(inputs)\n",
    "        if inputs.shape[-1] != self.input_dim:\n",
    "            raise ValueError(\"Last dimension must match input_dim\")\n",
    "        x = self.hidden_layer(inputs)\n",
    "        outputs = self.output_layer(x)\n",
    "        return outputs\n",
    "\n",
    "# Create a small instance of the custom block.\n",
    "model = SimpleDenseBlock(input_dim=4, hidden_dim=3, output_dim=2)\n",
    "\n",
    "# Build the model by calling it once.\n",
    "dummy_input = tf.zeros(shape=(1, 4), dtype=tf.float32)\n",
    "_ = model(dummy_input, training=False)\n",
    "\n",
    "# Print a short summary like parameter overview.\n",
    "model.summary(expand_nested=False)\n",
    "\n",
    "# Create a tiny batch of example data.\n",
    "example_batch = tf.constant(\n",
    "    [[1.0, 0.0, -1.0, 2.0], [0.5, 0.5, 0.5, 0.5]],\n",
    "    dtype=tf.float32,\n",
    ")\n",
    "\n",
    "# Run the forward pass on the example batch.\n",
    "outputs = model(example_batch, training=False)\n",
    "\n",
    "# Print input and output shapes clearly.\n",
    "print(\"Input shape:\", example_batch.shape)\n",
    "print(\"Output shape:\", outputs.shape)\n",
    "\n",
    "# Show the actual numeric outputs for inspection.\n",
    "print(\"Model outputs:\\n\", outputs.numpy())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc03478",
   "metadata": {},
   "source": [
    "### **1.3. Mastering super in Modules**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675721e2",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_02/Lecture_B/image_01_03.jpg?v=1769660072\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* super ensures standard module setup and registration\n",
    ">* Misusing super breaks training, saving, and devices\n",
    "\n",
    ">* super registers nested modules and parameters correctly\n",
    ">* Ensures saving, moving, and freezing models works\n",
    "\n",
    ">* Use super in forward to extend layers\n",
    ">* Reuse proven behavior while adding custom logic\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b669ed0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Mastering super in Modules\n",
    "\n",
    "# This script shows mastering super in modules.\n",
    "# We use TensorFlow to mimic PyTorch style modules.\n",
    "# Focus on subclassing and calling parent constructors.\n",
    "\n",
    "# !pip install tensorflow-2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Import TensorFlow and Keras layers.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Set deterministic random seeds.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "\n",
    "# Set numpy random seed.\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# Set tensorflow random seed.\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version briefly.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Define a simple custom dense like layer.\n",
    "class MyDense(layers.Layer):\n",
    "    # Initialize the custom layer correctly.\n",
    "    def __init__(self, units, **kwargs):\n",
    "        # Call parent constructor using super.\n",
    "        super().__init__(**kwargs)\n",
    "        # Store configuration for later use.\n",
    "        self.units = int(units)\n",
    "\n",
    "    # Build layer weights when input shape is known.\n",
    "    def build(self, input_shape):\n",
    "        # Validate input feature dimension.\n",
    "        assert int(input_shape[-1]) > 0\n",
    "        # Create kernel weight using add_weight.\n",
    "        self.kernel = self.add_weight(\n",
    "            shape=(int(input_shape[-1]), self.units),\n",
    "            initializer=\"glorot_uniform\",\n",
    "            trainable=True,\n",
    "            name=\"kernel\",\n",
    "        )\n",
    "        \n",
    "        # Create bias weight using add_weight.\n",
    "        self.bias = self.add_weight(\n",
    "            shape=(self.units,),\n",
    "            initializer=\"zeros\",\n",
    "            trainable=True,\n",
    "            name=\"bias\",\n",
    "        )\n",
    "        \n",
    "        # Call parent build to finalize.\n",
    "        super().build(input_shape)\n",
    "\n",
    "    # Define the forward computation call.\n",
    "    def call(self, inputs, training=False):\n",
    "        # Compute linear transformation manually.\n",
    "        outputs = tf.linalg.matmul(inputs, self.kernel)\n",
    "        \n",
    "        # Add bias term to outputs.\n",
    "        outputs = outputs + self.bias\n",
    "        \n",
    "        # Return activated outputs using relu.\n",
    "        return tf.nn.relu(outputs)\n",
    "\n",
    "# Define a custom block composed of layers.\n",
    "class MyBlock(keras.Model):\n",
    "    # Initialize block and internal layers.\n",
    "    def __init__(self, units, dropout_rate=0.2):\n",
    "        # Call parent constructor using super.\n",
    "        super().__init__()\n",
    "        # Create first dense layer using MyDense.\n",
    "        self.dense_one = MyDense(units)\n",
    "        \n",
    "        # Create dropout layer for regularization.\n",
    "        self.dropout = layers.Dropout(dropout_rate)\n",
    "        \n",
    "        # Create second dense layer using MyDense.\n",
    "        self.dense_two = MyDense(units)\n",
    "\n",
    "    # Define forward pass for the block.\n",
    "    def call(self, inputs, training=False):\n",
    "        # Pass inputs through first dense layer.\n",
    "        x = self.dense_one(inputs, training=training)\n",
    "        \n",
    "        # Apply dropout only during training.\n",
    "        x = self.dropout(x, training=training)\n",
    "        \n",
    "        # Pass through second dense layer.\n",
    "        return self.dense_two(x, training=training)\n",
    "\n",
    "# Create a small instance of the custom block.\n",
    "model = MyBlock(units=4, dropout_rate=0.1)\n",
    "\n",
    "# Create a tiny batch of dummy input data.\n",
    "dummy_inputs = tf.ones(shape=(2, 3), dtype=tf.float32)\n",
    "\n",
    "# Run a forward pass to build the model.\n",
    "outputs = model(dummy_inputs, training=False)\n",
    "\n",
    "# Validate output shape matches expectations.\n",
    "assert outputs.shape == (2, 4)\n",
    "\n",
    "# Count total trainable parameters safely.\n",
    "param_count = np.sum([np.prod(v.shape) for v in model.trainable_variables])\n",
    "\n",
    "# Print a short summary of key information.\n",
    "print(\"Dummy input shape:\", dummy_inputs.shape)\n",
    "print(\"Output shape:\", outputs.shape)\n",
    "print(\"Trainable parameter count:\", int(param_count))\n",
    "print(\"Registered variable names:\")\n",
    "for v in model.trainable_variables:\n",
    "    print(\" \", v.name, \"shape\", v.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b6a2d8",
   "metadata": {},
   "source": [
    "## **2. Core Neural Layers**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eee4d9a",
   "metadata": {},
   "source": [
    "### **2.1. Linear layers and activations**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b704e7ba",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_02/Lecture_B/image_02_01.jpg?v=1769660142\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Linear layers map input vectors to outputs\n",
    ">* Weights and bias learn feature importance for tasks\n",
    "\n",
    ">* Activation functions add nonlinearity beyond stacked linear layers\n",
    ">* They enable complex features and decision boundaries hierarchies\n",
    "\n",
    ">* Choose layer counts, sizes, and activations thoughtfully\n",
    ">* Linear layers combine features; activations add complexity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3444db85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Linear layers and activations\n",
    "\n",
    "# This script shows linear layers and activations.\n",
    "# It uses TensorFlow dense layers for clarity.\n",
    "# Run cells to see shapes and simple outputs.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Import TensorFlow and Keras layers.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Set deterministic random seeds.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version briefly.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Create small dummy input features.\n",
    "num_samples = 4\n",
    "num_features = 3\n",
    "x_input = np.array(\n",
    "    [[0.2, 0.5, 0.1],\n",
    "     [0.9, 0.1, 0.3],\n",
    "     [0.4, 0.7, 0.8],\n",
    "     [0.0, 0.2, 0.9]],\n",
    "    dtype=np.float32,\n",
    ")\n",
    "\n",
    "# Validate input shape before building model.\n",
    "assert x_input.shape == (num_samples, num_features)\n",
    "\n",
    "# Build a simple model with linear layers.\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        layers.Input(shape=(num_features,)),\n",
    "        layers.Dense(5, activation=None),\n",
    "        layers.Activation(\"relu\"),\n",
    "        layers.Dense(1, activation=None),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Build model by calling it once so weights are created.\n",
    "_ = model(x_input[:1], training=False)\n",
    "\n",
    "# Show a short model summary safely.\n",
    "model.summary(print_fn=lambda x: print(x))\n",
    "\n",
    "# Get model output for dummy inputs.\n",
    "outputs = model(x_input, training=False)\n",
    "\n",
    "# Confirm output shape is as expected.\n",
    "assert outputs.shape == (num_samples, 1)\n",
    "\n",
    "# Print input and output to see transformation.\n",
    "print(\"Input features shape:\", x_input.shape)\n",
    "print(\"Output predictions shape:\", outputs.shape)\n",
    "\n",
    "# Access first dense layer weights and biases.\n",
    "first_dense = model.layers[0]\n",
    "weights, biases = first_dense.get_weights()\n",
    "\n",
    "# Print shapes of weights and biases.\n",
    "print(\"First dense weights shape:\", weights.shape)\n",
    "print(\"First dense biases shape:\", biases.shape)\n",
    "\n",
    "# Count total trainable parameters manually.\n",
    "param_count = np.sum([np.prod(v.shape) for v in model.trainable_variables])\n",
    "\n",
    "# Print total parameter count clearly.\n",
    "print(\"Total trainable parameters:\", int(param_count))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf39ce2",
   "metadata": {},
   "source": [
    "### **2.2. Convolutions and Pooling**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091b3f8e",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_02/Lecture_B/image_02_02.jpg?v=1769660221\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Convolution layers scan locally to detect patterns\n",
    ">* Stacked convolutions learn complex features across modalities\n",
    "\n",
    ">* Pooling downsamples feature maps, keeping key information\n",
    ">* Convolution plus pooling builds robust, compressed hierarchies\n",
    "\n",
    ">* Stack conv, activation, pooling blocks for features\n",
    ">* These blocks enable robust, simple models across domains\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9900dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Convolutions and Pooling\n",
    "\n",
    "# This script shows basic convolutions and pooling.\n",
    "# We use TensorFlow to build tiny image layers.\n",
    "# Focus is on shapes not on serious training.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Import TensorFlow and Keras layers.\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Set deterministic random seeds.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version briefly.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Define image height and width.\n",
    "img_height = 8\n",
    "img_width = 8\n",
    "img_channels = 1\n",
    "\n",
    "# Create a tiny batch of random images.\n",
    "batch_size = 2\n",
    "images = np.random.rand(\n",
    "    batch_size,\n",
    "    img_height,\n",
    "    img_width,\n",
    "    img_channels,\n",
    ").astype(\"float32\")\n",
    "\n",
    "# Validate the created image batch shape.\n",
    "assert images.shape == (\n",
    "    batch_size,\n",
    "    img_height,\n",
    "    img_width,\n",
    "    img_channels,\n",
    ")\n",
    "\n",
    "# Build a simple convolution and pooling model.\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Input(shape=(img_height,\n",
    "                        img_width,\n",
    "                        img_channels,)),\n",
    "    layers.Conv2D(filters=4,\n",
    "                  kernel_size=(3,\n",
    "                               3,),\n",
    "                  padding=\"same\",\n",
    "                  activation=\"relu\"),\n",
    "    layers.MaxPooling2D(pool_size=(2,\n",
    "                                   2,)),\n",
    "])\n",
    "\n",
    "# Show a short model summary.\n",
    "model.summary(print_fn=lambda x: print(x))\n",
    "\n",
    "# Pass the images through the model.\n",
    "conv_pooled = model(images)\n",
    "\n",
    "# Validate output spatial dimensions.\n",
    "expected_height = img_height // 2\n",
    "expected_width = img_width // 2\n",
    "assert conv_pooled.shape[1] == expected_height\n",
    "assert conv_pooled.shape[2] == expected_width\n",
    "\n",
    "# Print input and output shapes for clarity.\n",
    "print(\"Input batch shape:\", images.shape)\n",
    "print(\"Output batch shape:\", conv_pooled.shape)\n",
    "\n",
    "# Inspect one pixel location across channels.\n",
    "sample_tensor = conv_pooled[0,\n",
    "                             0,\n",
    "                             0]\n",
    "print(\"One pooled location values:\", sample_tensor.numpy())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e108ae4",
   "metadata": {},
   "source": [
    "### **2.3. Regularization with Dropout BatchNorm**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee07f90c",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_02/Lecture_B/image_02_03.jpg?v=1769660269\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Dropout randomly disables neurons to reduce overfitting\n",
    ">* Encourages robust, shared representations in dense layers\n",
    "\n",
    ">* BatchNorm normalizes layer outputs within each batch\n",
    ">* Stabilizes training, improves gradients, adds mild regularization\n",
    "\n",
    ">* Place BatchNorm after layers, dropout near output\n",
    ">* This combo builds stable, robust, generalizing models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55f4d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Regularization with Dropout BatchNorm\n",
    "\n",
    "# This script shows dropout and batchnorm together.\n",
    "# We use TensorFlow to build a tiny model.\n",
    "# Focus on regularization layers and simple outputs.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Import TensorFlow and Keras layers.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Set deterministic random seeds.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version briefly.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Create a small synthetic classification dataset.\n",
    "num_samples = 256\n",
    "num_features = 20\n",
    "num_classes = 3\n",
    "\n",
    "# Generate random input features.\n",
    "X = np.random.randn(num_samples, num_features).astype(\"float32\")\n",
    "\n",
    "# Generate random integer labels.\n",
    "y_int = np.random.randint(num_classes, size=num_samples)\n",
    "\n",
    "# One hot encode labels for training.\n",
    "y = keras.utils.to_categorical(y_int, num_classes=num_classes)\n",
    "\n",
    "# Validate shapes before building the model.\n",
    "assert X.shape == (num_samples, num_features)\n",
    "assert y.shape == (num_samples, num_classes)\n",
    "\n",
    "# Build a simple model without regularization.\n",
    "inputs = keras.Input(shape=(num_features,))\n",
    "x_plain = layers.Dense(32, activation=\"relu\")(inputs)\n",
    "outputs_plain = layers.Dense(num_classes, activation=\"softmax\")(x_plain)\n",
    "model_plain = keras.Model(inputs, outputs_plain, name=\"plain_model\")\n",
    "\n",
    "# Build a similar model with BatchNorm and Dropout.\n",
    "inputs_reg = keras.Input(shape=(num_features,))\n",
    "x_reg = layers.Dense(32, use_bias=False)(inputs_reg)\n",
    "x_reg = layers.BatchNormalization()(x_reg)\n",
    "x_reg = layers.Activation(\"relu\")(x_reg)\n",
    "\n",
    "# Apply dropout for regularization.\n",
    "x_reg = layers.Dropout(0.5)(x_reg)\n",
    "outputs_reg = layers.Dense(num_classes, activation=\"softmax\")(x_reg)\n",
    "model_reg = keras.Model(inputs_reg, outputs_reg, name=\"reg_model\")\n",
    "\n",
    "# Compile both models with same settings.\n",
    "optimizer_plain = keras.optimizers.Adam(learning_rate=0.01)\n",
    "optimizer_reg = keras.optimizers.Adam(learning_rate=0.01)\n",
    "loss_fn = keras.losses.CategoricalCrossentropy()\n",
    "metrics_list = [\"accuracy\"]\n",
    "\n",
    "# Compile plain model.\n",
    "model_plain.compile(optimizer=optimizer_plain, loss=loss_fn, metrics=metrics_list)\n",
    "\n",
    "# Compile regularized model.\n",
    "model_reg.compile(optimizer=optimizer_reg, loss=loss_fn, metrics=metrics_list)\n",
    "\n",
    "# Train both models briefly and silently.\n",
    "history_plain = model_plain.fit(\n",
    "    X,\n",
    "    y,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Train the regularized model.\n",
    "history_reg = model_reg.fit(\n",
    "    X,\n",
    "    y,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Evaluate both models on the same data.\n",
    "plain_loss, plain_acc = model_plain.evaluate(\n",
    "    X,\n",
    "    y,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Evaluate regularized model.\n",
    "reg_loss, reg_acc = model_reg.evaluate(\n",
    "    X,\n",
    "    y,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Print a short comparison of results.\n",
    "print(\"Plain model accuracy:\", round(float(plain_acc), 3))\n",
    "print(\"Reg model accuracy:\", round(float(reg_acc), 3))\n",
    "print(\"Plain model loss:\", round(float(plain_loss), 3))\n",
    "print(\"Reg model loss:\", round(float(reg_loss), 3))\n",
    "\n",
    "# Show how dropout behaves in training mode.\n",
    "sample_batch = X[:3]\n",
    "plain_pred = model_plain(sample_batch, training=False)\n",
    "reg_pred_train = model_reg(sample_batch, training=True)\n",
    "reg_pred_eval = model_reg(sample_batch, training=False)\n",
    "\n",
    "# Print a few prediction summaries.\n",
    "print(\"Plain predictions sum row0:\", float(tf.reduce_sum(plain_pred[0])))\n",
    "print(\"Reg train predictions sum row0:\", float(tf.reduce_sum(reg_pred_train[0])))\n",
    "print(\"Reg eval predictions sum row0:\", float(tf.reduce_sum(reg_pred_eval[0])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353ce619",
   "metadata": {},
   "source": [
    "## **3. Managing Model Parameters**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb974ec",
   "metadata": {},
   "source": [
    "### **3.1. Iterating Model Parameters**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56dd559",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_02/Lecture_B/image_03_01.jpg?v=1769660353\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Systematically step through every model weight and bias\n",
    ">* Use parameter iteration for insight, debugging, customization\n",
    "\n",
    ">* Parameter names and shapes reveal model structure\n",
    ">* This info aids debugging, design checks, and counting\n",
    "\n",
    ">* Use iteration to freeze or tune layers\n",
    ">* Group parameters for custom training, monitoring, protection\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e2f21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Iterating Model Parameters\n",
    "\n",
    "# This script shows how to inspect model parameters.\n",
    "# We use TensorFlow Keras layers to build a model.\n",
    "# Focus on iterating parameters and counting them.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required TensorFlow and NumPy modules.\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Set a deterministic random seed for reproducibility.\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Print TensorFlow version in one concise line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Define a simple sequential model with common layers.\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(16,)),\n",
    "    tf.keras.layers.Dense(8, activation=\"relu\"),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(4, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(2, activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "# Build model by calling it once on dummy data.\n",
    "dummy_input = tf.zeros((1, 16), dtype=tf.float32)\n",
    "\n",
    "# Run a forward pass to ensure weights are created.\n",
    "_ = model(dummy_input)\n",
    "\n",
    "# Prepare containers for parameter statistics.\n",
    "layer_param_counts = []\n",
    "\n",
    "# Iterate over each layer and its trainable variables.\n",
    "for layer in model.layers:\n",
    "    params = layer.trainable_variables\n",
    "    layer_count = 0\n",
    "    for var in params:\n",
    "        shape = var.shape\n",
    "        count = np.prod(shape)\n",
    "        layer_count += int(count)\n",
    "    layer_param_counts.append((layer.name, layer_count))\n",
    "\n",
    "# Compute total number of trainable parameters.\n",
    "total_params = sum(count for _, count in layer_param_counts)\n",
    "\n",
    "# Print a clear header for parameter listing.\n",
    "print(\"\\nTrainable parameters by layer:\")\n",
    "\n",
    "# Print each layer name and its parameter count.\n",
    "for name, count in layer_param_counts:\n",
    "    print(f\"Layer {name:15s} -> {count:4d} params\")\n",
    "\n",
    "# Print the total number of trainable parameters.\n",
    "print(\"\\nTotal trainable parameters:\", total_params)\n",
    "\n",
    "# Show an example of accessing a specific parameter.\n",
    "first_layer = model.layers[1]\n",
    "\n",
    "# Safely access the first trainable variable of this layer.\n",
    "if first_layer.trainable_variables:\n",
    "    first_weight = first_layer.trainable_variables[0]\n",
    "    print(\"\\nFirst layer weight shape:\", first_weight.shape)\n",
    "\n",
    "# Confirm that model output shape matches expectation.\n",
    "print(\"Model output shape on dummy input:\", _.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3db59a",
   "metadata": {},
   "source": [
    "### **3.2. Initializing Model Parameters**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be62316d",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_02/Lecture_B/image_03_02.jpg?v=1769660407\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Initialization strongly affects learning speed and quality\n",
    ">* Control weight initialization to stabilize and improve training\n",
    "\n",
    ">* Default inits keep activations and gradients stable\n",
    ">* Complex architectures may need custom initialization choices\n",
    "\n",
    ">* Use custom initialization to encode task knowledge\n",
    ">* Initialization choices strongly influence training stability, performance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1ed7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Initializing Model Parameters\n",
    "\n",
    "# This script shows parameter initialization basics.\n",
    "# We use TensorFlow dense layers for clarity.\n",
    "# Focus on inspecting and customizing layer weights.\n",
    "\n",
    "# !pip install tensorflow.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Import TensorFlow and Keras layers.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Print TensorFlow version for reproducibility.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Set deterministic random seeds for stability.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "\n",
    "# Set NumPy and TensorFlow seeds also.\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Define a simple dense model building function.\n",
    "\n",
    "def build_model(initializer):\n",
    "    # Create a small sequential model.\n",
    "    model = keras.Sequential(\n",
    "        [\n",
    "            keras.layers.Input(shape=(4,)),\n",
    "            keras.layers.Dense(\n",
    "                3,\n",
    "                activation=\"relu\",\n",
    "                kernel_initializer=initializer,\n",
    "                bias_initializer=\"zeros\",\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Return the constructed model.\n",
    "    return model\n",
    "\n",
    "# Choose two different kernel initializers.\n",
    "init_random_normal = keras.initializers.RandomNormal(\n",
    "    mean=0.0,\n",
    "    stddev=0.05,\n",
    ")\n",
    "\n",
    "# Define a glorot uniform initializer.\n",
    "init_glorot_uniform = keras.initializers.GlorotUniform()\n",
    "\n",
    "# Build two models with different initializations.\n",
    "model_normal = build_model(init_random_normal)\n",
    "model_glorot = build_model(init_glorot_uniform)\n",
    "\n",
    "# Get weights from the first dense layer of each.\n",
    "weights_normal, biases_normal = model_normal.layers[0].get_weights()\n",
    "\n",
    "# Extract weights and biases for second model.\n",
    "weights_glorot, biases_glorot = model_glorot.layers[0].get_weights()\n",
    "\n",
    "# Validate shapes to avoid unexpected issues.\n",
    "assert weights_normal.shape == (4, 3)\n",
    "assert weights_glorot.shape == (4, 3)\n",
    "\n",
    "# Compute simple statistics for comparison.\n",
    "mean_normal = float(weights_normal.mean())\n",
    "std_normal = float(weights_normal.std())\n",
    "\n",
    "# Compute statistics for glorot initialized weights.\n",
    "mean_glorot = float(weights_glorot.mean())\n",
    "std_glorot = float(weights_glorot.std())\n",
    "\n",
    "# Print a short header for clarity.\n",
    "print(\"\\nDense layer kernel statistics by initializer:\")\n",
    "\n",
    "# Show statistics for random normal initializer.\n",
    "print(\"RandomNormal -> mean:\", round(mean_normal, 5), \"std:\", round(std_normal, 5))\n",
    "\n",
    "# Show statistics for glorot uniform initializer.\n",
    "print(\"GlorotUniform -> mean:\", round(mean_glorot, 5), \"std:\", round(std_glorot, 5))\n",
    "\n",
    "# Print a few sample weights from each initializer.\n",
    "print(\"\\nFirst row weights (RandomNormal):\", weights_normal[0])\n",
    "print(\"First row weights (GlorotUniform):\", weights_glorot[0])\n",
    "\n",
    "# Confirm bias initialization behavior explicitly.\n",
    "print(\"\\nBias vector (RandomNormal):\", biases_normal)\n",
    "print(\"Bias vector (GlorotUniform):\", biases_glorot)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60771640",
   "metadata": {},
   "source": [
    "### **3.3. Counting model size**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51e706c",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_02/Lecture_B/image_03_03.jpg?v=1769660467\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Model size means counting all scalar parameters\n",
    ">* Parameter count affects memory, speed, and deployment\n",
    "\n",
    ">* Different layers contribute parameters in specific ways\n",
    ">* Sum all parameter tensors to estimate model size\n",
    "\n",
    ">* Relate parameter counts to hardware and goals\n",
    ">* Use counts to compare, control, and refine models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f882c68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Counting model size\n",
    "\n",
    "# This script explores counting model parameters.\n",
    "# It uses TensorFlow to build simple models.\n",
    "# Focus on understanding model size and parameters.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Import TensorFlow and Keras layers.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Set deterministic random seeds.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version briefly.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Define a helper to count parameters.\n",
    "def count_parameters(model):\n",
    "    trainable = np.sum([\n",
    "        np.prod(v.shape) for v in model.trainable_weights\n",
    "    ])\n",
    "    non_trainable = np.sum([\n",
    "        np.prod(v.shape) for v in model.non_trainable_weights\n",
    "    ])\n",
    "    return int(trainable), int(non_trainable)\n",
    "\n",
    "# Build a small dense model.\n",
    "small_model = keras.Sequential([\n",
    "    layers.Input(shape=(16,)),\n",
    "    layers.Dense(8, activation=\"relu\"),\n",
    "    layers.Dense(4, activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "# Build a slightly larger dense model.\n",
    "large_model = keras.Sequential([\n",
    "    layers.Input(shape=(16,)),\n",
    "    layers.Dense(32, activation=\"relu\"),\n",
    "    layers.Dense(32, activation=\"relu\"),\n",
    "    layers.Dense(4, activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "# Initialize models by running a dummy forward pass.\n",
    "dummy_input = np.zeros((1, 16), dtype=np.float32)\n",
    "_ = small_model(dummy_input)\n",
    "_ = large_model(dummy_input)\n",
    "\n",
    "# Count parameters for the small model.\n",
    "small_trainable, small_non_trainable = count_parameters(small_model)\n",
    "\n",
    "# Count parameters for the large model.\n",
    "large_trainable, large_non_trainable = count_parameters(large_model)\n",
    "\n",
    "# Print parameter counts for both models.\n",
    "print(\"Small model trainable parameters:\", small_trainable)\n",
    "print(\"Small model non_trainable parameters:\", small_non_trainable)\n",
    "print(\"Large model trainable parameters:\", large_trainable)\n",
    "print(\"Large model non_trainable parameters:\", large_non_trainable)\n",
    "\n",
    "# Show how many times larger the big model is.\n",
    "ratio = large_trainable / max(small_trainable, 1)\n",
    "print(\"Large model has\", round(ratio, 2), \"times more parameters.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d3feab",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Modules and Layers**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a6e638",
   "metadata": {},
   "source": [
    "\n",
    "In this lecture, you learned to:\n",
    "- Define custom neural network components by subclassing nn.Module and implementing forward methods. \n",
    "- Use common nn layers such as Linear, Conv2d, and Dropout to assemble simple models. \n",
    "- Inspect and manage model parameters, including initialization and parameter counting. \n",
    "\n",
    "In the next Module (Module 3), we will go over 'Training Workflow'"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
