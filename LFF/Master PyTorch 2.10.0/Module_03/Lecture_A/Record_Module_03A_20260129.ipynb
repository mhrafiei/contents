{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6abd0443",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Losses and Optimizers**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17ae6ad",
   "metadata": {},
   "source": [
    ">Last update: 20260129.\n",
    "    \n",
    "By the end of this Lecture, you will be able to:\n",
    "- Select and configure appropriate loss functions for common supervised learning tasks in PyTorch. \n",
    "- Use optimizers such as SGD and Adam to update model parameters within a training loop. \n",
    "- Inspect and debug optimization behavior using learning rate settings, gradient norms, and loss curves. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcacbdd5",
   "metadata": {},
   "source": [
    "## **1. PyTorch Loss Functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2200b213",
   "metadata": {},
   "source": [
    "### **1.1. Regression and Classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492a77c8",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_03/Lecture_A/image_01_01.jpg?v=1769663895\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Decide if your task is regression or classification\n",
    ">* Regression predicts numbers; classification predicts class probabilities\n",
    "\n",
    ">* Regression losses reflect how you value errors\n",
    ">* They use continuous outputs, penalizing large deviations\n",
    "\n",
    ">* Classification loss compares class scores and labels\n",
    ">* It rewards high probability on the correct class\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532b4eb5",
   "metadata": {},
   "source": [
    "### **1.2. MSE and L1 Losses**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8444cb48",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_03/Lecture_A/image_01_02.jpg?v=1769663912\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* MSE and L1 measure regression prediction errors\n",
    ">* MSE punishes big errors; L1 is steadier\n",
    "\n",
    ">* MSE highlights large errors and smooth optimization\n",
    ">* Can overreact to noisy labels and outliers\n",
    "\n",
    ">* L1 loss resists outliers and noisy labels\n",
    ">* Choose L1 or MSE based on error tradeoffs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c6693d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - MSE and L1 Losses\n",
    "\n",
    "# This script compares MSE and L1 losses.\n",
    "# It uses tiny tensors for clear intuition.\n",
    "# Run cells in order inside Google Colab.\n",
    "\n",
    "# Optional install line for PyTorch in Colab.\n",
    "# !pip install torch torchvision torchaudio.\n",
    "\n",
    "# Import torch for tensor and loss operations.\n",
    "import torch\n",
    "\n",
    "# Set deterministic seed for reproducible results.\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Create small batch of target regression values.\n",
    "true_targets = torch.tensor([[2.0], [0.0], [1.0]])\n",
    "\n",
    "# Create predictions with one large error included.\n",
    "predictions = torch.tensor([[2.5], [3.0], [0.5]])\n",
    "\n",
    "# Validate shapes to avoid broadcasting mistakes.\n",
    "assert predictions.shape == true_targets.shape\n",
    "\n",
    "# Define mean squared error loss function object.\n",
    "mse_loss_fn = torch.nn.MSELoss(reduction=\"mean\")\n",
    "\n",
    "# Define L1 loss function object for comparison.\n",
    "l1_loss_fn = torch.nn.L1Loss(reduction=\"mean\")\n",
    "\n",
    "# Compute MSE loss value for current predictions.\n",
    "mse_value = mse_loss_fn(predictions, true_targets)\n",
    "\n",
    "# Compute L1 loss value for current predictions.\n",
    "l1_value = l1_loss_fn(predictions, true_targets)\n",
    "\n",
    "# Print both loss values with short explanations.\n",
    "print(\"MSE loss value:\", float(mse_value))\n",
    "\n",
    "# Show L1 loss value for the same predictions.\n",
    "print(\"L1 loss value:\", float(l1_value))\n",
    "\n",
    "# Compute elementwise squared errors for illustration.\n",
    "squared_errors = (predictions - true_targets) ** 2\n",
    "\n",
    "# Compute elementwise absolute errors for illustration.\n",
    "absolute_errors = (predictions - true_targets).abs()\n",
    "\n",
    "# Print per example squared and absolute errors.\n",
    "print(\"Squared errors per example:\", squared_errors.view(-1).tolist())\n",
    "\n",
    "# Print per example absolute errors for comparison.\n",
    "print(\"Absolute errors per example:\", absolute_errors.view(-1).tolist())\n",
    "\n",
    "# Show that large errors dominate MSE more strongly.\n",
    "print(\"MSE emphasizes large errors more than L1.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7c550b",
   "metadata": {},
   "source": [
    "### **1.3. Classification Loss Choices**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9771796",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_03/Lecture_A/image_01_03.jpg?v=1769663937\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Classification losses depend on outputs and labels\n",
    ">* Use logits with cross-entropy; avoid pre-softmax\n",
    "\n",
    ">* Use cross-entropy for single-label classification tasks\n",
    ">* Use sigmoid plus BCE for multi-label; wrong choice hurts\n",
    "\n",
    ">* Adjust loss for imbalance and noisy labels\n",
    ">* Use weighting or focal losses to prioritize errors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9afb50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Classification Loss Choices\n",
    "\n",
    "# This script compares basic classification loss choices.\n",
    "# It uses tiny tensors to keep things simple.\n",
    "# Focus on logits targets and loss configuration.\n",
    "\n",
    "# Optional install for PyTorch if missing.\n",
    "# !pip install torch torchvision torchaudio.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "\n",
    "# Try importing torch and handle absence gracefully.\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "except ImportError:\n",
    "    raise SystemExit(\"PyTorch is required for this example.\")\n",
    "\n",
    "# Set deterministic random seeds for reproducibility.\n",
    "random.seed(0)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Print PyTorch version in one short line.\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "\n",
    "# Create logits for single label three class example.\n",
    "logits_single = torch.tensor([[2.0, 0.5, -1.0]])\n",
    "\n",
    "# Create integer target for single label classification.\n",
    "target_single = torch.tensor([0])\n",
    "\n",
    "# Validate shapes for single label example.\n",
    "assert logits_single.shape == (1, 3)\n",
    "\n",
    "# Define cross entropy loss for single label case.\n",
    "ce_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "# Compute loss using logits and integer target.\n",
    "loss_single = ce_loss(logits_single, target_single)\n",
    "\n",
    "# Print single label cross entropy loss value.\n",
    "print(\"Single label CE loss:\", float(loss_single))\n",
    "\n",
    "# Create logits for multi label three class example.\n",
    "logits_multi = torch.tensor([[2.0, -0.5, 1.0]])\n",
    "\n",
    "# Create multi hot targets for multi label example.\n",
    "target_multi = torch.tensor([[1.0, 0.0, 1.0]])\n",
    "\n",
    "# Validate shapes for multi label example.\n",
    "assert logits_multi.shape == target_multi.shape\n",
    "\n",
    "# Define BCE with logits loss for multi label.\n",
    "bce_loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Compute loss using logits and multi hot targets.\n",
    "loss_multi = bce_loss(logits_multi, target_multi)\n",
    "\n",
    "# Print multi label BCE with logits loss value.\n",
    "print(\"Multi label BCE loss:\", float(loss_multi))\n",
    "\n",
    "# Define class weights for imbalanced single label case.\n",
    "class_weights = torch.tensor([3.0, 1.0, 1.0])\n",
    "\n",
    "# Validate weights length matches class count.\n",
    "assert class_weights.shape[0] == logits_single.shape[1]\n",
    "\n",
    "# Create weighted cross entropy loss instance.\n",
    "weighted_ce = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "# Compute weighted loss for same single label example.\n",
    "loss_weighted = weighted_ce(logits_single, target_single)\n",
    "\n",
    "# Print weighted cross entropy loss value.\n",
    "print(\"Weighted single label CE loss:\", float(loss_weighted))\n",
    "\n",
    "# Show how probabilities differ for softmax versus sigmoid.\n",
    "softmax_probs = torch.softmax(logits_single, dim=1)\n",
    "\n",
    "# Compute sigmoid probabilities for multi label logits.\n",
    "sigmoid_probs = torch.sigmoid(logits_multi)\n",
    "\n",
    "# Print both probability style outputs briefly.\n",
    "print(\"Softmax probs single:\", softmax_probs.tolist())\n",
    "\n",
    "# Print sigmoid probabilities for multi label example.\n",
    "print(\"Sigmoid probs multi:\", sigmoid_probs.tolist())\n",
    "\n",
    "# Final line confirms script finished without issues.\n",
    "print(\"Finished comparing classification loss choices.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7fc0c2",
   "metadata": {},
   "source": [
    "## **2. PyTorch Optimizers Overview**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e92a82",
   "metadata": {},
   "source": [
    "### **2.1. Momentum in SGD**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b45ebcf",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_03/Lecture_A/image_02_01.jpg?v=1769663995\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Momentum makes SGD faster and more stable\n",
    ">* It smooths noisy gradients using past helpful directions\n",
    "\n",
    ">* Momentum stores a velocity from past gradients\n",
    ">* It speeds movement and smooths zigzagging updates\n",
    "\n",
    ">* Too much momentum can overshoot and oscillate\n",
    ">* Start with moderate momentum, tune for stability\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d58f4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Momentum in SGD\n",
    "\n",
    "# This script demonstrates momentum in SGD.\n",
    "# We compare SGD with and without momentum.\n",
    "# Focus on a tiny linear regression example.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Import TensorFlow and check version.\n",
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Set deterministic random seeds.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# Set TensorFlow random seed.\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Create simple synthetic linear regression data.\n",
    "true_w = 2.0\n",
    "true_b = -1.0\n",
    "num_samples = 64\n",
    "\n",
    "# Generate input features.\n",
    "X = np.linspace(-1.0, 1.0, num_samples).astype(np.float32)\n",
    "\n",
    "# Generate targets with small noise.\n",
    "noise = 0.05 * np.random.randn(num_samples).astype(np.float32)\n",
    "y = true_w * X + true_b + noise\n",
    "\n",
    "# Reshape data for TensorFlow models.\n",
    "X_tf = X.reshape(-1, 1)\n",
    "y_tf = y.reshape(-1, 1)\n",
    "\n",
    "# Build a tiny linear model function.\n",
    "def build_model():\n",
    "    # Create a simple sequential model.\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(1,)),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Create two identical models for fair comparison.\n",
    "model_sgd = build_model()\n",
    "model_mom = build_model()\n",
    "\n",
    "# Configure optimizer without momentum.\n",
    "opt_sgd = tf.keras.optimizers.SGD(learning_rate=0.1)\n",
    "\n",
    "# Configure optimizer with momentum enabled.\n",
    "opt_mom = tf.keras.optimizers.SGD(\n",
    "    learning_rate=0.1,\n",
    "    momentum=0.9\n",
    ")\n",
    "\n",
    "# Define mean squared error loss function.\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "# Prepare dataset as TensorFlow tensors.\n",
    "X_tensor = tf.convert_to_tensor(X_tf)\n",
    "y_tensor = tf.convert_to_tensor(y_tf)\n",
    "\n",
    "# Validate shapes before training.\n",
    "assert X_tensor.shape[0] == y_tensor.shape[0]\n",
    "\n",
    "# Training settings for both optimizers.\n",
    "epochs = 20\n",
    "\n",
    "# Containers to store loss history.\n",
    "loss_history_sgd = []\n",
    "loss_history_mom = []\n",
    "\n",
    "# Training loop comparing both optimizers.\n",
    "for epoch in range(epochs):\n",
    "    # Record gradients and update plain SGD model.\n",
    "    with tf.GradientTape() as tape_sgd:\n",
    "        preds_sgd = model_sgd(X_tensor, training=True)\n",
    "        loss_sgd = loss_fn(y_tensor, preds_sgd)\n",
    "    grads_sgd = tape_sgd.gradient(loss_sgd, model_sgd.trainable_variables)\n",
    "    opt_sgd.apply_gradients(zip(grads_sgd, model_sgd.trainable_variables))\n",
    "\n",
    "    # Record gradients and update momentum model.\n",
    "    with tf.GradientTape() as tape_mom:\n",
    "        preds_mom = model_mom(X_tensor, training=True)\n",
    "        loss_mom = loss_fn(y_tensor, preds_mom)\n",
    "    grads_mom = tape_mom.gradient(loss_mom, model_mom.trainable_variables)\n",
    "    opt_mom.apply_gradients(zip(grads_mom, model_mom.trainable_variables))\n",
    "\n",
    "    # Store scalar losses for later inspection.\n",
    "    loss_history_sgd.append(float(loss_sgd.numpy()))\n",
    "    loss_history_mom.append(float(loss_mom.numpy()))\n",
    "\n",
    "# Print a compact comparison of losses.\n",
    "print(\"Epoch  Loss_SGD  Loss_Momentum\")\n",
    "for i in range(epochs):\n",
    "    # Format each epoch line clearly.\n",
    "    print(i + 1, round(loss_history_sgd[i], 4), round(loss_history_mom[i], 4))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f93d36",
   "metadata": {},
   "source": [
    "### **2.2. Adaptive Adam Optimizers**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65521521",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_03/Lecture_A/image_02_02.jpg?v=1769664054\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Adam adapts learning rates per parameter automatically\n",
    ">* Helps different model parts learn faster, more stably\n",
    "\n",
    ">* Adam fits into the usual training loop\n",
    ">* Uses adaptive per-parameter steps, forgiving learning rates\n",
    "\n",
    ">* Tune learning rate, betas, and weight decay\n",
    ">* Monitor loss curves to compare and refine Adam\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf24860",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Adaptive Adam Optimizers\n",
    "\n",
    "# This script demonstrates adaptive Adam optimizer basics.\n",
    "# We compare SGD and Adam on a tiny regression task.\n",
    "# Focus on training loop and optimizer configuration.\n",
    "\n",
    "# !pip install torch torchvision torchaudio.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "\n",
    "# Import torch and check availability.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "random.seed(0)\n",
    "\n",
    "# Set torch manual seed for reproducibility.\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Select device based on GPU availability.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Print framework version and selected device.\n",
    "print(\"PyTorch version:\", torch.__version__, \"Device:\", device)\n",
    "\n",
    "# Create small synthetic regression dataset.\n",
    "true_w, true_b = 2.0, -1.0\n",
    "\n",
    "# Generate input features as a column vector.\n",
    "x = torch.linspace(-1.0, 1.0, steps=40).unsqueeze(1)\n",
    "\n",
    "# Generate targets with a simple linear relationship.\n",
    "y = true_w * x + true_b\n",
    "\n",
    "# Move data tensors to the selected device.\n",
    "x, y = x.to(device), y.to(device)\n",
    "\n",
    "# Validate shapes before training loop.\n",
    "assert x.shape == y.shape\n",
    "\n",
    "# Define a tiny linear regression model.\n",
    "model_sgd = nn.Linear(in_features=1, out_features=1).to(device)\n",
    "\n",
    "# Create a separate model copy for Adam optimizer.\n",
    "model_adam = nn.Linear(in_features=1, out_features=1).to(device)\n",
    "\n",
    "# Initialize Adam model with same parameters as SGD model.\n",
    "model_adam.load_state_dict(model_sgd.state_dict())\n",
    "\n",
    "# Define mean squared error loss function.\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Configure SGD optimizer with fixed learning rate.\n",
    "optimizer_sgd = torch.optim.SGD(model_sgd.parameters(), lr=0.05)\n",
    "\n",
    "# Configure Adam optimizer with typical hyperparameters.\n",
    "optimizer_adam = torch.optim.Adam(\n",
    "    model_adam.parameters(), lr=0.05, betas=(0.9, 0.999)\n",
    ")\n",
    "\n",
    "# Set number of training epochs small for speed.\n",
    "num_epochs = 40\n",
    "\n",
    "# Prepare lists to store loss values for inspection.\n",
    "sgd_losses, adam_losses = [], []\n",
    "\n",
    "# Training loop comparing SGD and Adam side by side.\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # Zero gradients for SGD model.\n",
    "    optimizer_sgd.zero_grad()\n",
    "\n",
    "    # Forward pass for SGD model.\n",
    "    preds_sgd = model_sgd(x)\n",
    "\n",
    "    # Compute loss for SGD model.\n",
    "    loss_sgd = criterion(preds_sgd, y)\n",
    "\n",
    "    # Backward pass to compute gradients.\n",
    "    loss_sgd.backward()\n",
    "\n",
    "    # Optional gradient norm inspection for SGD.\n",
    "    total_norm_sgd = 0.0\n",
    "\n",
    "    # Accumulate squared gradient norms for SGD.\n",
    "    for p in model_sgd.parameters():\n",
    "        if p.grad is not None:\n",
    "            total_norm_sgd += p.grad.data.norm().item() ** 2\n",
    "\n",
    "    # Take square root to get overall gradient norm.\n",
    "    total_norm_sgd = math.sqrt(total_norm_sgd)\n",
    "\n",
    "    # Update parameters using SGD step.\n",
    "    optimizer_sgd.step()\n",
    "\n",
    "    # Zero gradients for Adam model.\n",
    "    optimizer_adam.zero_grad()\n",
    "\n",
    "    # Forward pass for Adam model.\n",
    "    preds_adam = model_adam(x)\n",
    "\n",
    "    # Compute loss for Adam model.\n",
    "    loss_adam = criterion(preds_adam, y)\n",
    "\n",
    "    # Backward pass to compute gradients.\n",
    "    loss_adam.backward()\n",
    "\n",
    "    # Optional gradient norm inspection for Adam.\n",
    "    total_norm_adam = 0.0\n",
    "\n",
    "    # Accumulate squared gradient norms for Adam.\n",
    "    for p in model_adam.parameters():\n",
    "        if p.grad is not None:\n",
    "            total_norm_adam += p.grad.data.norm().item() ** 2\n",
    "\n",
    "    # Take square root to get overall gradient norm.\n",
    "    total_norm_adam = math.sqrt(total_norm_adam)\n",
    "\n",
    "    # Update parameters using Adam step.\n",
    "    optimizer_adam.step()\n",
    "\n",
    "    # Store detached loss values for later printing.\n",
    "    sgd_losses.append(loss_sgd.item())\n",
    "\n",
    "    # Store Adam loss values for comparison.\n",
    "    adam_losses.append(loss_adam.item())\n",
    "\n",
    "# Print a few selected epochs to avoid spam.\n",
    "print(\"Epoch 0 SGD loss:\", round(sgd_losses[0], 4))\n",
    "\n",
    "# Print Adam loss at first epoch.\n",
    "print(\"Epoch 0 Adam loss:\", round(adam_losses[0], 4))\n",
    "\n",
    "# Print middle epoch losses for both optimizers.\n",
    "mid = num_epochs // 2\n",
    "\n",
    "# Show SGD loss at middle epoch.\n",
    "print(\"Epoch\", mid, \"SGD loss:\", round(sgd_losses[mid], 4))\n",
    "\n",
    "# Show Adam loss at middle epoch.\n",
    "print(\"Epoch\", mid, \"Adam loss:\", round(adam_losses[mid], 4))\n",
    "\n",
    "# Print final epoch losses for both optimizers.\n",
    "print(\"Epoch\", num_epochs - 1, \"SGD loss:\", round(sgd_losses[-1], 4))\n",
    "\n",
    "# Print final Adam loss to compare convergence.\n",
    "print(\"Epoch\", num_epochs - 1, \"Adam loss:\", round(adam_losses[-1], 4))\n",
    "\n",
    "# Show learned parameters for SGD model.\n",
    "print(\"SGD learned w, b:\", model_sgd.weight.item(), model_sgd.bias.item())\n",
    "\n",
    "# Show learned parameters for Adam model.\n",
    "print(\"Adam learned w, b:\", model_adam.weight.item(), model_adam.bias.item())\n",
    "\n",
    "# Print true underlying parameters for reference.\n",
    "print(\"True w, b:\", true_w, true_b)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfa53d6",
   "metadata": {},
   "source": [
    "### **2.3. Tuning Optimizer Settings**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2448268d",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_03/Lecture_A/image_02_03.jpg?v=1769664088\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Learning rate balances speed and training stability\n",
    ">* Watch loss curves to adjust update step sizes\n",
    "\n",
    ">* Optimizer hyperparameters shape update speed and smoothness\n",
    ">* Tuning momentum and Adam settings improves stability\n",
    "\n",
    ">* Use changing learning rates instead of fixed\n",
    ">* Schedules balance fast early learning and stable convergence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e4867e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Tuning Optimizer Settings\n",
    "\n",
    "# This script shows basic optimizer tuning concepts.\n",
    "# We compare SGD and Adam with different learning rates.\n",
    "# Focus on simple training loops and clear outputs.\n",
    "\n",
    "# !pip install torch torchvision.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "\n",
    "# Import torch and check availability.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "\n",
    "# Set seeds for torch random generators.\n",
    "torch.manual_seed(seed_value)\n",
    "\n",
    "# Select device based on GPU availability.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Print torch version and selected device.\n",
    "print(\"PyTorch version:\", torch.__version__, \"Device:\", device)\n",
    "\n",
    "# Create a tiny synthetic regression dataset.\n",
    "true_w = torch.tensor([[2.0], [-3.0]])\n",
    "\n",
    "# Create true bias for the synthetic data.\n",
    "true_b = torch.tensor([0.5])\n",
    "\n",
    "# Generate input features on CPU for simplicity.\n",
    "X = torch.randn(200, 2)\n",
    "\n",
    "# Generate targets with a simple linear rule.\n",
    "y = X @ true_w + true_b\n",
    "\n",
    "# Move data to the selected device.\n",
    "X = X.to(device)\n",
    "\n",
    "# Move targets to the selected device.\n",
    "y = y.to(device)\n",
    "\n",
    "# Confirm shapes are as expected.\n",
    "assert X.shape == (200, 2)\n",
    "\n",
    "# Confirm target shape matches expectations.\n",
    "assert y.shape == (200, 1)\n",
    "\n",
    "# Define a simple linear regression model.\n",
    "class TinyRegressor(nn.Module):\n",
    "    # Initialize with one linear layer.\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(2, 1)\n",
    "\n",
    "    # Define forward computation for inputs.\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "\n",
    "# Helper function to train for a few epochs.\n",
    "def train_model(optimizer_name, learning_rate, momentum=None):\n",
    "    # Create a fresh model for each run.\n",
    "    model = TinyRegressor().to(device)\n",
    "\n",
    "    # Use mean squared error loss function.\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Choose optimizer based on requested name.\n",
    "    if optimizer_name == \"SGD\":\n",
    "        if momentum is None:\n",
    "            optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "        else:\n",
    "            optimizer = optim.SGD(\n",
    "                model.parameters(), lr=learning_rate, momentum=momentum\n",
    "            )\n",
    "    else:\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Store losses for each epoch.\n",
    "    losses = []\n",
    "\n",
    "    # Run a short deterministic training loop.\n",
    "    for epoch in range(15):\n",
    "        # Set model to training mode.\n",
    "        model.train()\n",
    "\n",
    "        # Forward pass to compute predictions.\n",
    "        preds = model(X)\n",
    "\n",
    "        # Compute loss between predictions and targets.\n",
    "        loss = criterion(preds, y)\n",
    "\n",
    "        # Zero gradients before backward pass.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Backward pass to compute gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        # Optional gradient norm check for stability.\n",
    "        total_norm = 0.0\n",
    "        for p in model.parameters():\n",
    "            if p.grad is not None:\n",
    "                param_norm = p.grad.data.norm(2).item()\n",
    "                total_norm += param_norm ** 2\n",
    "        total_norm = math.sqrt(total_norm)\n",
    "\n",
    "        # Take one optimization step.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Record loss value for later inspection.\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    # Return final loss and last gradient norm.\n",
    "    return losses[-1], total_norm\n",
    "\n",
    "\n",
    "# Define different optimizer settings to compare.\n",
    "settings = [\n",
    "    (\"SGD\", 0.001, 0.0),\n",
    "    (\"SGD\", 0.1, 0.0),\n",
    "    (\"SGD\", 0.1, 0.9),\n",
    "    (\"Adam\", 0.001, None),\n",
    "]\n",
    "\n",
    "# Run experiments and collect results.\n",
    "results = []\n",
    "for name, lr, mom in settings:\n",
    "    # Train model with given hyperparameters.\n",
    "    final_loss, grad_norm = train_model(name, lr, mom)\n",
    "\n",
    "    # Store a summary tuple for printing.\n",
    "    results.append((name, lr, mom, final_loss, grad_norm))\n",
    "\n",
    "# Print a short header for clarity.\n",
    "print(\"Optimizer, lr, momentum, final_loss, grad_norm\")\n",
    "\n",
    "# Print one summary line per configuration.\n",
    "for name, lr, mom, loss_val, gnorm in results:\n",
    "    print(\n",
    "        name,\n",
    "        \"lr=\",\n",
    "        lr,\n",
    "        \"mom=\",\n",
    "        mom,\n",
    "        \"loss=\",\n",
    "        round(loss_val, 4),\n",
    "        \"g_norm=\",\n",
    "        round(gnorm, 4),\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540bce90",
   "metadata": {},
   "source": [
    "## **3. Monitoring Optimization Progress**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43651531",
   "metadata": {},
   "source": [
    "### **3.1. Tracking Loss Trends**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2c879e",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_03/Lecture_A/image_03_01.jpg?v=1769664158\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Track loss over epochs to judge learning\n",
    ">* Flat or erratic loss suggests training problems\n",
    "\n",
    ">* Compare training and validation loss to assess generalization\n",
    ">* Use loss patterns to detect overfitting or underfitting\n",
    "\n",
    ">* Loss curve shape reveals learning rate issues\n",
    ">* Visual patterns guide debugging and training adjustments\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f713ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Tracking Loss Trends\n",
    "\n",
    "# This script shows how loss trends behave.\n",
    "# We train a tiny model and record losses.\n",
    "# Then we plot and briefly inspect them.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Import TensorFlow and Keras utilities.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Print TensorFlow version once.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Set deterministic random seeds.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Load MNIST dataset from Keras.\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Use a small subset for quick training.\n",
    "train_samples = 2000\n",
    "test_samples = 1000\n",
    "x_train = x_train[:train_samples]\n",
    "y_train = y_train[:train_samples]\n",
    "\n",
    "# Reduce test set size.\n",
    "x_test = x_test[:test_samples]\n",
    "y_test = y_test[:test_samples]\n",
    "\n",
    "# Normalize pixel values to range zero one.\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_test = x_test.astype(\"float32\") / 255.0\n",
    "\n",
    "# Flatten images into vectors.\n",
    "input_shape = (28 * 28,)\n",
    "x_train = x_train.reshape((-1, input_shape[0]))\n",
    "x_test = x_test.reshape((-1, input_shape[0]))\n",
    "\n",
    "# Validate shapes before building model.\n",
    "assert x_train.shape[1] == input_shape[0]\n",
    "assert x_test.shape[1] == input_shape[0]\n",
    "\n",
    "# Build a simple dense neural network.\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Input(shape=input_shape),\n",
    "    keras.layers.Dense(64, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "# Choose optimizer and loss function.\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "# Prepare metrics for monitoring.\n",
    "train_loss_metric = keras.metrics.Mean(name=\"train_loss\")\n",
    "val_loss_metric = keras.metrics.Mean(name=\"val_loss\")\n",
    "\n",
    "# Create TensorFlow datasets for batching.\n",
    "batch_size = 64\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_ds = train_ds.shuffle(buffer_size=train_samples, seed=seed_value)\n",
    "train_ds = train_ds.batch(batch_size)\n",
    "\n",
    "# Create validation dataset.\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "val_ds = val_ds.batch(batch_size)\n",
    "\n",
    "# Prepare lists to store epoch losses.\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Define number of epochs for demonstration.\n",
    "num_epochs = 5\n",
    "\n",
    "# Training loop with manual loss tracking.\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss_metric.reset_state()\n",
    "    val_loss_metric.reset_state()\n",
    "\n",
    "    # Iterate over training batches.\n",
    "    for step, (images, labels) in enumerate(train_ds):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(images, training=True)\n",
    "            loss_value = loss_fn(labels, logits)\n",
    "        gradients = tape.gradient(loss_value, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        train_loss_metric.update_state(loss_value)\n",
    "\n",
    "    # Iterate over validation batches.\n",
    "    for val_images, val_labels in val_ds:\n",
    "        val_logits = model(val_images, training=False)\n",
    "        val_loss_value = loss_fn(val_labels, val_logits)\n",
    "        val_loss_metric.update_state(val_loss_value)\n",
    "\n",
    "    # Store average losses for this epoch.\n",
    "    epoch_train_loss = float(train_loss_metric.result().numpy())\n",
    "    epoch_val_loss = float(val_loss_metric.result().numpy())\n",
    "    train_losses.append(epoch_train_loss)\n",
    "    val_losses.append(epoch_val_loss)\n",
    "\n",
    "    # Print concise summary for this epoch.\n",
    "    print(\n",
    "        f\"Epoch {epoch + 1}: train_loss={epoch_train_loss:.4f}, \"\n",
    "        f\"val_loss={epoch_val_loss:.4f}\"\n",
    "    )\n",
    "\n",
    "# Import matplotlib for plotting.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a simple loss curve plot.\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(range(1, num_epochs + 1), train_losses, label=\"Train loss\")\n",
    "plt.plot(range(1, num_epochs + 1), val_losses, label=\"Validation loss\")\n",
    "\n",
    "# Label axes and add legend.\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and validation loss trends\")\n",
    "plt.legend()\n",
    "\n",
    "# Display the plot to inspect trends.\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d700a9",
   "metadata": {},
   "source": [
    "### **3.2. Gradient Norm Monitoring**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f31cbe",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_03/Lecture_A/image_03_02.jpg?v=1769664232\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Gradient norms show overall strength of parameter updates\n",
    ">* Healthy training shows large then gradually shrinking norms\n",
    "\n",
    ">* Huge gradient norms signal unstable, exploding training\n",
    ">* Fix by lowering learning rate or clipping\n",
    "\n",
    ">* Tiny gradients mean stalled or vanishing learning\n",
    ">* Log norms to intervene and improve optimization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862fd6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Gradient Norm Monitoring\n",
    "\n",
    "# This script shows gradient norm monitoring simply.\n",
    "# We train a tiny model and track gradient magnitudes.\n",
    "# Use this to debug unstable or stagnant optimization.\n",
    "\n",
    "# Install PyTorch if needed in your environment.\n",
    "# !pip install torch torchvision torchaudio.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "\n",
    "# Import torch and check version and device.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "\n",
    "# Set seeds for torch random number generators.\n",
    "torch.manual_seed(seed_value)\n",
    "\n",
    "# Select device based on GPU availability.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Print framework version and selected device.\n",
    "print(\"PyTorch version:\", torch.__version__, \"Device:\", device)\n",
    "\n",
    "# Create a tiny synthetic regression dataset.\n",
    "num_samples = 64\n",
    "\n",
    "# Generate input features from normal distribution.\n",
    "X = torch.randn(num_samples, 1)\n",
    "\n",
    "# Generate targets with simple linear relationship.\n",
    "y = 3.0 * X + 0.5\n",
    "\n",
    "# Move data to selected device safely.\n",
    "X = X.to(device)\n",
    "\n",
    "y = y.to(device)\n",
    "\n",
    "# Validate shapes before building model.\n",
    "assert X.shape[0] == y.shape[0]\n",
    "\n",
    "# Define a tiny linear regression model.\n",
    "model = nn.Linear(1, 1).to(device)\n",
    "\n",
    "# Choose mean squared error loss function.\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Configure optimizer with slightly high learning rate.\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.2)\n",
    "\n",
    "# Function to compute total gradient norm value.\n",
    "def compute_grad_norm(parameters):\n",
    "    total_norm_sq = 0.0\n",
    "    for p in parameters:\n",
    "        if p.grad is None:\n",
    "            continue\n",
    "        param_norm = p.grad.detach().data.norm(2).item()\n",
    "        total_norm_sq += param_norm ** 2\n",
    "    return math.sqrt(total_norm_sq)\n",
    "\n",
    "# Training loop with gradient norm monitoring.\n",
    "num_epochs = 8\n",
    "\n",
    "grad_norm_history = []\n",
    "\n",
    "# Run several epochs and record loss and gradient norms.\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X)\n",
    "    loss = criterion(outputs, y)\n",
    "    loss.backward()\n",
    "    grad_norm = compute_grad_norm(model.parameters())\n",
    "    grad_norm_history.append(grad_norm)\n",
    "    optimizer.step()\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}: loss={loss.item():.4f}, grad_norm={grad_norm:.4f}\"\n",
    "    )\n",
    "\n",
    "# Show final parameters and last gradient norm.\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, \"value:\", param.data.view(-1).tolist())\n",
    "\n",
    "# Print short summary of gradient norm trend.\n",
    "print(\"Gradient norms per epoch:\", [round(g, 4) for g in grad_norm_history])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcb02d0",
   "metadata": {},
   "source": [
    "### **3.3. Learning Rate Dynamics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cfd6ce",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_03/Lecture_A/image_03_03.jpg?v=1769664278\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Learning rate controls how fast parameters change\n",
    ">* Too high or low harms stable loss improvement\n",
    "\n",
    ">* Track schedule changes with loss and validation\n",
    ">* Relate loss patterns to too high or low learning rates\n",
    "\n",
    ">* Correlate learning rate, loss, and gradients carefully\n",
    ">* Use logs to tune schedules and reduce instability\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6ce51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Learning Rate Dynamics\n",
    "\n",
    "# This script visualizes simple learning rate dynamics.\n",
    "# It compares high and low learning rate behaviors.\n",
    "# Use it to connect curves with optimization stability.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "\n",
    "# Import TensorFlow and check version.\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set deterministic random seeds for reproducibility.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "\n",
    "# Set TensorFlow random seed for reproducibility.\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Create a tiny synthetic regression dataset.\n",
    "true_w = tf.constant([[2.0]], dtype=tf.float32)\n",
    "\n",
    "# Define true bias for the synthetic data.\n",
    "true_b = tf.constant([0.5], dtype=tf.float32)\n",
    "\n",
    "# Generate simple input features as a column vector.\n",
    "x_values = tf.linspace(-1.0, 1.0, 64)\n",
    "\n",
    "# Reshape inputs to match linear model expectations.\n",
    "x_values = tf.reshape(x_values, (-1, 1))\n",
    "\n",
    "# Generate noiseless targets using the true parameters.\n",
    "y_values = tf.matmul(x_values, true_w) + true_b\n",
    "\n",
    "# Confirm shapes are as expected before training.\n",
    "assert x_values.shape == (64, 1)\n",
    "\n",
    "# Confirm target shape matches input batch dimension.\n",
    "assert y_values.shape == (64, 1)\n",
    "\n",
    "# Define a simple linear regression model function.\n",
    "def linear_model(x, w, b):\n",
    "    return tf.matmul(x, w) + b\n",
    "\n",
    "# Define mean squared error loss function.\n",
    "def mse_loss(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "\n",
    "# Create two sets of trainable parameters.\n",
    "w_low = tf.Variable(tf.random.normal((1, 1)))\n",
    "\n",
    "# Initialize bias for low learning rate model.\n",
    "b_low = tf.Variable(tf.zeros((1,)))\n",
    "\n",
    "# Initialize parameters for high learning rate model.\n",
    "w_high = tf.Variable(tf.random.normal((1, 1)))\n",
    "\n",
    "# Initialize bias for high learning rate model.\n",
    "b_high = tf.Variable(tf.zeros((1,)))\n",
    "\n",
    "# Create optimizers with different learning rates.\n",
    "optimizer_low = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "# Define a more aggressive learning rate optimizer.\n",
    "optimizer_high = tf.keras.optimizers.SGD(learning_rate=0.5)\n",
    "\n",
    "# Prepare containers to store loss and gradient norms.\n",
    "loss_history_low = []\n",
    "\n",
    "# Store high learning rate loss values per epoch.\n",
    "loss_history_high = []\n",
    "\n",
    "# Store gradient norms for low learning rate model.\n",
    "grad_norms_low = []\n",
    "\n",
    "# Store gradient norms for high learning rate model.\n",
    "grad_norms_high = []\n",
    "\n",
    "# Define a single training step function.\n",
    "@tf.function\n",
    "\n",
    "\n",
    "def train_step(x_batch, y_batch, w_var, b_var, optimizer):\n",
    "    with tf.GradientTape() as tape:\n",
    "        preds = linear_model(x_batch, w_var, b_var)\n",
    "        loss = mse_loss(y_batch, preds)\n",
    "    grads = tape.gradient(loss, [w_var, b_var])\n",
    "    optimizer.apply_gradients(zip(grads, [w_var, b_var]))\n",
    "    grad_norm = tf.sqrt(\n",
    "        tf.add(tf.reduce_sum(tf.square(grads[0])), tf.reduce_sum(tf.square(grads[1])))\n",
    "    )\n",
    "    return loss, grad_norm\n",
    "\n",
    "# Set number of epochs for the demonstration.\n",
    "num_epochs = 20\n",
    "\n",
    "# Run a short training loop for both learning rates.\n",
    "for epoch in range(num_epochs):\n",
    "    loss_low, grad_low = train_step(\n",
    "        x_values, y_values, w_low, b_low, optimizer_low\n",
    "    )\n",
    "    loss_high, grad_high = train_step(\n",
    "        x_values, y_values, w_high, b_high, optimizer_high\n",
    "    )\n",
    "    loss_history_low.append(float(loss_low.numpy()))\n",
    "    loss_history_high.append(float(loss_high.numpy()))\n",
    "    grad_norms_low.append(float(grad_low.numpy()))\n",
    "    grad_norms_high.append(float(grad_high.numpy()))\n",
    "\n",
    "# Import matplotlib for plotting learning curves.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a figure with two vertically stacked subplots.\n",
    "fig, axes = plt.subplots(2, 1, figsize=(6, 6))\n",
    "\n",
    "# Plot loss curves for both learning rates.\n",
    "axes[0].plot(loss_history_low, label=\"low lr 0.01\")\n",
    "\n",
    "# Plot high learning rate loss curve for comparison.\n",
    "axes[0].plot(loss_history_high, label=\"high lr 0.5\")\n",
    "\n",
    "# Add labels and legend to the loss subplot.\n",
    "axes[0].set_ylabel(\"MSE loss\")\n",
    "\n",
    "# Add a simple title describing the loss behavior.\n",
    "axes[0].set_title(\"Loss versus epoch for two learning rates\")\n",
    "\n",
    "# Show legend to distinguish the two curves.\n",
    "axes[0].legend()\n",
    "\n",
    "# Plot gradient norm curves for both learning rates.\n",
    "axes[1].plot(grad_norms_low, label=\"low lr grad norm\")\n",
    "\n",
    "# Plot gradient norms for the high learning rate model.\n",
    "axes[1].plot(grad_norms_high, label=\"high lr grad norm\")\n",
    "\n",
    "# Label the y axis for gradient norms.\n",
    "axes[1].set_ylabel(\"Gradient norm\")\n",
    "\n",
    "# Label the x axis shared by both subplots.\n",
    "axes[1].set_xlabel(\"Epoch\")\n",
    "\n",
    "# Add legend to the gradient norm subplot.\n",
    "axes[1].legend()\n",
    "\n",
    "# Adjust layout to prevent overlapping labels.\n",
    "plt.tight_layout()\n",
    "\n",
    "# Print a short summary connecting curves to dynamics.\n",
    "print(\"Low lr shows smooth loss and stable gradients.\")\n",
    "\n",
    "# Print a second line describing high learning rate behavior.\n",
    "print(\"High lr may oscillate with larger gradient norms.\")\n",
    "\n",
    "# Display the combined plot for visual inspection.\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3f66d8",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Losses and Optimizers**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b82d04b",
   "metadata": {},
   "source": [
    "\n",
    "In this lecture, you learned to:\n",
    "- Select and configure appropriate loss functions for common supervised learning tasks in PyTorch. \n",
    "- Use optimizers such as SGD and Adam to update model parameters within a training loop. \n",
    "- Inspect and debug optimization behavior using learning rate settings, gradient norms, and loss curves. \n",
    "\n",
    "In the next Lecture (Lecture B), we will go over 'Training Loop Design'"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
