{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b07aeef",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Losses and Optimizers**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97466293",
   "metadata": {},
   "source": [
    ">Last update: 20260129.\n",
    "    \n",
    "By the end of this Lecture, you will be able to:\n",
    "- Select and configure appropriate loss functions for common supervised learning tasks in PyTorch. \n",
    "- Use optimizers such as SGD and Adam to update model parameters within a training loop. \n",
    "- Inspect and debug optimization behavior using learning rate settings, gradient norms, and loss curves. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01675b56",
   "metadata": {},
   "source": [
    "## **1. Choosing Loss Functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48bc7f3",
   "metadata": {},
   "source": [
    "### **1.1. Regression and Classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94bc01b",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_03/Lecture_A/image_01_01.jpg?v=1769701742\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* First choose between regression or classification task\n",
    ">* Their outputs and loss definitions differ fundamentally\n",
    "\n",
    ">* Regression loss measures distance between predictions and targets\n",
    ">* Continuous outputs make loss scale and outlier sensitive\n",
    "\n",
    ">* Classification losses use probabilities over discrete classes\n",
    ">* They align with output layers and probability transforms\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5218fc",
   "metadata": {},
   "source": [
    "### **1.2. MSE and L1 Losses**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55306cbd",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_03/Lecture_A/image_01_02.jpg?v=1769701760\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* MSE squares errors, heavily penalizing large outliers\n",
    ">* L1 uses absolute errors, giving robust behavior\n",
    "\n",
    ">* MSE suits clean data, smooth fast training\n",
    ">* L1 handles noisy outliers, targets median behavior\n",
    "\n",
    ">* Configure averaging, summing, or weighting across batches\n",
    ">* Combine MSE and L1 for robustness, smoothness\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0017ce0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - MSE and L1 Losses\n",
    "\n",
    "# This script compares MSE and L1 losses.\n",
    "# It uses tiny synthetic regression data.\n",
    "# It prints simple values for clear intuition.\n",
    "\n",
    "# Optional install for plotting library if missing.\n",
    "# !pip install matplotlib.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import math\n",
    "import random\n",
    "import statistics\n",
    "\n",
    "# Import matplotlib for a small plot.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set deterministic random seeds for reproducibility.\n",
    "random.seed(42)\n",
    "\n",
    "# Create a tiny synthetic regression dataset.\n",
    "true_weights = 2.0\n",
    "true_bias = 1.0\n",
    "\n",
    "# Generate simple input features and targets.\n",
    "xs = [float(x) for x in range(-3, 4)]\n",
    "ys = [true_weights * x + true_bias for x in xs]\n",
    "\n",
    "# Add one strong outlier to the targets.\n",
    "ys_noisy = ys.copy()\n",
    "ys_noisy[-1] = ys_noisy[-1] + 10.0\n",
    "\n",
    "# Define a helper to compute MSE loss.\n",
    "def mse_loss(preds, targets):\n",
    "    assert len(preds) == len(targets)\n",
    "    squared_errors = [(p - t) ** 2 for p, t in zip(preds, targets)]\n",
    "    return sum(squared_errors) / len(squared_errors)\n",
    "\n",
    "# Define a helper to compute L1 loss.\n",
    "def l1_loss(preds, targets):\n",
    "    assert len(preds) == len(targets)\n",
    "    abs_errors = [abs(p - t) for p, t in zip(preds, targets)]\n",
    "    return sum(abs_errors) / len(abs_errors)\n",
    "\n",
    "# Build two simple prediction sets for comparison.\n",
    "preds_perfect = ys\n",
    "preds_shifted = [y + 1.5 for y in ys]\n",
    "\n",
    "# Compute losses on clean data without outlier.\n",
    "mse_clean = mse_loss(preds_shifted, ys)\n",
    "l1_clean = l1_loss(preds_shifted, ys)\n",
    "\n",
    "# Compute losses on noisy data with outlier.\n",
    "mse_noisy = mse_loss(preds_shifted, ys_noisy)\n",
    "l1_noisy = l1_loss(preds_shifted, ys_noisy)\n",
    "\n",
    "# Print a short summary of loss values.\n",
    "print(\"Clean MSE:\", round(mse_clean, 3))\n",
    "print(\"Clean L1:\", round(l1_clean, 3))\n",
    "print(\"Noisy MSE:\", round(mse_noisy, 3))\n",
    "print(\"Noisy L1:\", round(l1_noisy, 3))\n",
    "\n",
    "# Prepare values for a tiny bar plot.\n",
    "labels = [\"MSE clean\", \"L1 clean\", \"MSE noisy\", \"L1 noisy\"]\n",
    "values = [mse_clean, l1_clean, mse_noisy, l1_noisy]\n",
    "\n",
    "# Create a simple bar chart to visualize sensitivity.\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar(labels, values, color=[\"C0\", \"C1\", \"C0\", \"C1\"])\n",
    "\n",
    "# Add basic labels and title for clarity.\n",
    "plt.ylabel(\"Average loss value\")\n",
    "plt.title(\"MSE reacts more to the outlier than L1\")\n",
    "\n",
    "# Display the plot to compare both losses.\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f524421f",
   "metadata": {},
   "source": [
    "### **1.3. CrossEntropy and BCE**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65c630f",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_03/Lecture_A/image_01_03.jpg?v=1769701791\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Cross entropy handles single-label multi-class classification\n",
    ">* BCE handles binary and multi-label yes/no tasks\n",
    "\n",
    ">* Cross entropy scores multi-class predictions as probabilities\n",
    ">* PyTorch takes raw logits and integer class labels\n",
    "\n",
    ">* BCE handles independent yes or no predictions\n",
    ">* Use sigmoid outputs and 0–1 float targets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca015cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - CrossEntropy and BCE\n",
    "\n",
    "# This script compares cross entropy and BCE losses.\n",
    "# It uses tiny tensors to keep things simple.\n",
    "# Run it in Colab to see printed outputs.\n",
    "\n",
    "# Uncomment if tensorflow is missing in your environment.\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import tensorflow and numpy for basic tensors.\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Set deterministic random seeds for reproducibility.\n",
    "tf.random.set_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# Print tensorflow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Create tiny logits for a three class example.\n",
    "logits_multi = tf.constant([[2.0, 0.5, -1.0]], dtype=tf.float32)\n",
    "\n",
    "# Create integer label for the correct class index.\n",
    "labels_multi = tf.constant([0], dtype=tf.int32)\n",
    "\n",
    "# Check shapes to ensure they are compatible.\n",
    "print(\"Multi logits shape:\", logits_multi.shape)\n",
    "\n",
    "# Compute sparse categorical cross entropy from logits.\n",
    "ce_loss = tf.keras.losses.sparse_categorical_crossentropy(\n",
    "    labels_multi, logits_multi, from_logits=True\n",
    ")\n",
    "\n",
    "# Print the scalar cross entropy loss value.\n",
    "print(\"Cross entropy loss value:\", float(ce_loss.numpy()[0]))\n",
    "\n",
    "# Convert logits to probabilities using softmax function.\n",
    "probs_multi = tf.nn.softmax(logits_multi, axis=1)\n",
    "\n",
    "# Print the resulting probability distribution values.\n",
    "print(\"Softmax probabilities:\", probs_multi.numpy())\n",
    "\n",
    "# Create logits for a binary classification example.\n",
    "logits_binary = tf.constant([[1.5]], dtype=tf.float32)\n",
    "\n",
    "# Create binary label as float between zero and one.\n",
    "labels_binary = tf.constant([[1.0]], dtype=tf.float32)\n",
    "\n",
    "# Validate shapes for binary loss computation.\n",
    "print(\"Binary logits shape:\", logits_binary.shape)\n",
    "\n",
    "# Compute binary cross entropy using logits directly.\n",
    "bce_loss = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "    labels=labels_binary, logits=logits_binary\n",
    ")\n",
    "\n",
    "# Print the scalar binary cross entropy loss.\n",
    "print(\"Binary cross entropy loss:\", float(bce_loss.numpy()[0][0]))\n",
    "\n",
    "# Convert binary logits to probabilities with sigmoid.\n",
    "probs_binary = tf.nn.sigmoid(logits_binary)\n",
    "\n",
    "# Print the predicted positive class probability.\n",
    "print(\"Sigmoid probability:\", float(probs_binary.numpy()[0][0]))\n",
    "\n",
    "# Show both losses together for quick comparison.\n",
    "print(\"CE and BCE losses:\", float(ce_loss.numpy()[0]), float(bce_loss.numpy()[0][0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fe7865",
   "metadata": {},
   "source": [
    "## **2. Training Optimizers**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7221a4b",
   "metadata": {},
   "source": [
    "### **2.1. Momentum Based SGD**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91c7131",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_03/Lecture_A/image_02_01.jpg?v=1769701824\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Momentum stores a velocity from recent gradients\n",
    ">* Updates follow consistent downhill directions, smoothing noise\n",
    "\n",
    ">* Momentum smooths noisy updates and loss zigzags\n",
    ">* Helps traverse narrow valleys faster, improving convergence\n",
    "\n",
    ">* Momentum coefficient controls memory of past gradients\n",
    ">* Tune learning rate and momentum based on loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f0e999",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Momentum Based SGD\n",
    "\n",
    "# This script compares SGD with and without momentum.\n",
    "# It trains a tiny model on synthetic regression data.\n",
    "# Focus on optimizer behavior and loss trajectories.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Import tensorflow and check version.\n",
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# Set tensorflow random seed for reproducibility.\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Generate simple one dimensional regression data.\n",
    "num_samples = 200\n",
    "x = np.linspace(-1.0, 1.0, num_samples)\n",
    "\n",
    "# Create noisy targets using a linear relationship.\n",
    "true_w, true_b = 2.0, -0.5\n",
    "y = true_w * x + true_b\n",
    "\n",
    "# Add small gaussian noise to targets.\n",
    "noise = 0.1 * np.random.randn(num_samples)\n",
    "y_noisy = y + noise\n",
    "\n",
    "# Reshape data for keras dense layer.\n",
    "X = x.reshape(-1, 1).astype(\"float32\")\n",
    "Y = y_noisy.reshape(-1, 1).astype(\"float32\")\n",
    "\n",
    "# Validate shapes before building models.\n",
    "assert X.shape[0] == Y.shape[0]\n",
    "\n",
    "# Build a tiny sequential regression model.\n",
    "def build_model():\n",
    "    # Use single dense unit with linear activation.\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(1, input_shape=(1,))\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Create two identical models for fair comparison.\n",
    "model_plain = build_model()\n",
    "model_momentum = build_model()\n",
    "\n",
    "# Define mean squared error loss function.\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "# Configure plain SGD optimizer without momentum.\n",
    "optimizer_plain = tf.keras.optimizers.SGD(\n",
    "    learning_rate=0.1, momentum=0.0\n",
    ")\n",
    "\n",
    "# Configure SGD optimizer with momentum enabled.\n",
    "optimizer_momentum = tf.keras.optimizers.SGD(\n",
    "    learning_rate=0.1, momentum=0.9\n",
    ")\n",
    "\n",
    "# Prepare dataset as small batches for training.\n",
    "batch_size = 32\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X, Y))\n",
    "\n",
    "# Shuffle and batch the dataset deterministically.\n",
    "dataset = dataset.shuffle(buffer_size=num_samples, seed=seed_value)\n",
    "\n",
    "# Batch and prefetch for efficient iteration.\n",
    "dataset = dataset.batch(batch_size).prefetch(1)\n",
    "\n",
    "# Training parameters for short demonstration.\n",
    "num_epochs = 15\n",
    "\n",
    "# Containers to store epoch losses for both optimizers.\n",
    "plain_losses = []\n",
    "momentum_losses = []\n",
    "\n",
    "# Training loop comparing both optimizers side by side.\n",
    "for epoch in range(num_epochs):\n",
    "    # Reset epoch loss accumulators.\n",
    "    epoch_loss_plain = 0.0\n",
    "    epoch_loss_momentum = 0.0\n",
    "\n",
    "    # Counter for number of processed batches.\n",
    "    batch_count = 0\n",
    "\n",
    "    # Iterate over mini batches from dataset.\n",
    "    for batch_x, batch_y in dataset:\n",
    "        batch_count += 1\n",
    "\n",
    "        # Use gradient tape for plain optimizer.\n",
    "        with tf.GradientTape() as tape_plain:\n",
    "            preds_plain = model_plain(batch_x, training=True)\n",
    "            loss_plain = loss_fn(batch_y, preds_plain)\n",
    "\n",
    "        # Compute gradients for plain optimizer.\n",
    "        grads_plain = tape_plain.gradient(\n",
    "            loss_plain, model_plain.trainable_variables\n",
    "        )\n",
    "\n",
    "        # Apply gradients using plain SGD.\n",
    "        optimizer_plain.apply_gradients(\n",
    "            zip(grads_plain, model_plain.trainable_variables)\n",
    "        )\n",
    "\n",
    "        # Use gradient tape for momentum optimizer.\n",
    "        with tf.GradientTape() as tape_mom:\n",
    "            preds_mom = model_momentum(batch_x, training=True)\n",
    "            loss_mom = loss_fn(batch_y, preds_mom)\n",
    "\n",
    "        # Compute gradients for momentum optimizer.\n",
    "        grads_mom = tape_mom.gradient(\n",
    "            loss_mom, model_momentum.trainable_variables\n",
    "        )\n",
    "\n",
    "        # Apply gradients using momentum SGD.\n",
    "        optimizer_momentum.apply_gradients(\n",
    "            zip(grads_mom, model_momentum.trainable_variables)\n",
    "        )\n",
    "\n",
    "        # Accumulate batch losses for averaging.\n",
    "        epoch_loss_plain += float(loss_plain)\n",
    "        epoch_loss_momentum += float(loss_mom)\n",
    "\n",
    "    # Compute mean loss over all batches.\n",
    "    mean_plain = epoch_loss_plain / batch_count\n",
    "    mean_mom = epoch_loss_momentum / batch_count\n",
    "\n",
    "    # Store losses for later inspection.\n",
    "    plain_losses.append(mean_plain)\n",
    "    momentum_losses.append(mean_mom)\n",
    "\n",
    "    # Print concise epoch summary for both optimizers.\n",
    "    print(\n",
    "        f\"Epoch {epoch+1:02d} - SGD loss: {mean_plain:.4f}, \"\n",
    "        f\"Momentum SGD loss: {mean_mom:.4f}\"\n",
    "    )\n",
    "\n",
    "# Import matplotlib for a simple loss curve plot.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a small figure for loss comparison.\n",
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "# Plot plain SGD loss trajectory.\n",
    "plt.plot(plain_losses, label=\"SGD no momentum\")\n",
    "\n",
    "# Plot momentum SGD loss trajectory.\n",
    "plt.plot(momentum_losses, label=\"SGD with momentum\")\n",
    "\n",
    "# Label axes and add legend for clarity.\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Mean training loss\")\n",
    "\n",
    "# Add title emphasizing momentum effect.\n",
    "plt.title(\"Effect of momentum on SGD convergence\")\n",
    "\n",
    "# Show legend and tight layout for readability.\n",
    "plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bacf1b",
   "metadata": {},
   "source": [
    "### **2.2. Adaptive Optimizers Overview**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737836c5",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_03/Lecture_A/image_02_02.jpg?v=1769701944\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Adaptive optimizers adjust each parameter’s learning rate\n",
    ">* Helps deep networks where layers learn at different speeds\n",
    "\n",
    ">* RMSProp scales learning rates using recent gradient sizes\n",
    ">* Adam adds momentum, stabilizing and speeding training\n",
    "\n",
    ">* Training loop stays the same with adaptives\n",
    ">* Optimizer tracks gradient history, adds tunable hyperparameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34faf910",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Adaptive Optimizers Overview\n",
    "\n",
    "# This script compares SGD and Adam optimizers.\n",
    "# It shows how adaptive optimizers change updates.\n",
    "# We use a tiny regression example in PyTorch.\n",
    "\n",
    "# Install PyTorch if not already available.\n",
    "# !pip install torch torchvision torchaudio.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Import torch and check availability.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Select device based on GPU availability.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Print framework version and selected device.\n",
    "print(\"PyTorch version:\", torch.__version__, \"Device:\", device)\n",
    "\n",
    "# Create simple synthetic regression data.\n",
    "true_w = 2.0\n",
    "true_b = -1.0\n",
    "\n",
    "# Generate small input tensor on CPU.\n",
    "x = torch.linspace(-1.0, 1.0, steps=40).unsqueeze(1)\n",
    "\n",
    "# Generate targets with a little noise.\n",
    "noise = 0.1 * torch.randn_like(x)\n",
    "y = true_w * x + true_b + noise\n",
    "\n",
    "# Move data to selected device.\n",
    "x = x.to(device)\n",
    "y = y.to(device)\n",
    "\n",
    "# Define a tiny linear regression model.\n",
    "class TinyRegressor(nn.Module):\n",
    "    # Initialize with one linear layer.\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "\n",
    "    # Define forward computation step.\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# Create two identical models for fair comparison.\n",
    "model_sgd = TinyRegressor().to(device)\n",
    "model_adam = TinyRegressor().to(device)\n",
    "\n",
    "# Copy parameters from SGD model to Adam model.\n",
    "model_adam.load_state_dict(model_sgd.state_dict())\n",
    "\n",
    "# Define mean squared error loss function.\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Configure SGD optimizer with fixed learning rate.\n",
    "optimizer_sgd = optim.SGD(model_sgd.parameters(), lr=0.1)\n",
    "\n",
    "# Configure Adam optimizer with default betas.\n",
    "optimizer_adam = optim.Adam(model_adam.parameters(), lr=0.1)\n",
    "\n",
    "# Helper function to run one training step.\n",
    "def train_step(model, optimizer, x_batch, y_batch):\n",
    "    # Set gradients to zero before backward.\n",
    "    optimizer.zero_grad()\n",
    "    # Forward pass through the model.\n",
    "    preds = model(x_batch)\n",
    "    # Compute loss between predictions and targets.\n",
    "    loss = criterion(preds, y_batch)\n",
    "    # Backpropagate gradients through the graph.\n",
    "    loss.backward()\n",
    "    # Compute gradient norm for monitoring.\n",
    "    total_norm = 0.0\n",
    "    for p in model.parameters():\n",
    "        if p.grad is not None:\n",
    "            param_norm = p.grad.data.norm(2).item()\n",
    "            total_norm += param_norm ** 2\n",
    "    total_norm = math.sqrt(total_norm)\n",
    "    # Update parameters using optimizer step.\n",
    "    optimizer.step()\n",
    "    # Return scalar loss and gradient norm.\n",
    "    return loss.item(), total_norm\n",
    "\n",
    "# Train both models for a few epochs.\n",
    "num_epochs = 15\n",
    "\n",
    "# Store history for selected epochs.\n",
    "record_epochs = [1, 5, 10, 15]\n",
    "\n",
    "# Print header for comparison table.\n",
    "print(\"Epoch  Optim  Loss       GradNorm\")\n",
    "\n",
    "# Loop over epochs and train both optimizers.\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    # Run one step for SGD model.\n",
    "    loss_sgd, grad_sgd = train_step(model_sgd, optimizer_sgd, x, y)\n",
    "    # Run one step for Adam model.\n",
    "    loss_adam, grad_adam = train_step(model_adam, optimizer_adam, x, y)\n",
    "    # Print only selected epochs for clarity.\n",
    "    if epoch in record_epochs:\n",
    "        print(\n",
    "            f\"{epoch:5d}  SGD   {loss_sgd:8.4f}  {grad_sgd:8.4f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"{epoch:5d}  Adam  {loss_adam:8.4f}  {grad_adam:8.4f}\"\n",
    "        )\n",
    "\n",
    "# Show final learned parameters for both optimizers.\n",
    "print(\"True w, b:\", true_w, true_b)\n",
    "print(\"SGD w, b:\", list(model_sgd.parameters())[0].item(), list(model_sgd.parameters())[1].item())\n",
    "print(\"Adam w, b:\", list(model_adam.parameters())[0].item(), list(model_adam.parameters())[1].item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0747edd0",
   "metadata": {},
   "source": [
    "### **2.3. Tuning Optimizer Settings**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d7c74d",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_03/Lecture_A/image_02_03.jpg?v=1769702036\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Learning rate size controls training speed and stability\n",
    ">* Start with a default, then adjust from loss\n",
    "\n",
    ">* Optimizer hyperparameters shape update behavior and stability\n",
    ">* Adjust settings per task using experiments and feedback\n",
    "\n",
    ">* Use loss curves and metrics to guide tuning\n",
    ">* Run experiments, adjust schedules, build optimization intuition\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82ec8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Tuning Optimizer Settings\n",
    "\n",
    "# This script shows basic optimizer tuning concepts.\n",
    "# We compare SGD and Adam on a tiny regression task.\n",
    "# Focus on learning rate and loss behavior over epochs.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "\n",
    "# Import TensorFlow and NumPy for modeling.\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Print TensorFlow version briefly.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Select device based on GPU availability.\n",
    "physical_gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "if physical_gpus:\n",
    "    device_name = \"GPU\"\n",
    "else:\n",
    "    device_name = \"CPU\"\n",
    "\n",
    "# Print which device type is used.\n",
    "print(\"Using device type:\", device_name)\n",
    "\n",
    "# Create a tiny synthetic regression dataset.\n",
    "num_samples = 64\n",
    "x_values = np.linspace(-1.0, 1.0, num_samples).astype(\"float32\")\n",
    "true_w, true_b = 2.0, -0.5\n",
    "\n",
    "# Generate noisy targets for y = 2x - 0.5.\n",
    "noise = 0.1 * np.random.randn(num_samples).astype(\"float32\")\n",
    "y_values = true_w * x_values + true_b + noise\n",
    "\n",
    "# Reshape features and targets to column vectors.\n",
    "x_train = x_values.reshape(-1, 1)\n",
    "y_train = y_values.reshape(-1, 1)\n",
    "\n",
    "# Validate shapes before building models.\n",
    "assert x_train.shape == (num_samples, 1)\n",
    "assert y_train.shape == (num_samples, 1)\n",
    "\n",
    "# Build a simple one layer regression model.\n",
    "def build_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(1, input_shape=(1,))\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Create two identical models for fair comparison.\n",
    "model_sgd = build_model()\n",
    "model_adam = build_model()\n",
    "\n",
    "# Define mean squared error loss function.\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "# Configure SGD optimizer with moderate learning rate.\n",
    "optimizer_sgd = tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.0)\n",
    "\n",
    "# Configure Adam optimizer with smaller learning rate.\n",
    "optimizer_adam = tf.keras.optimizers.Adam(learning_rate=0.05)\n",
    "\n",
    "# Prepare lists to store loss history for each optimizer.\n",
    "sgd_losses = []\n",
    "adam_losses = []\n",
    "\n",
    "# Define a small number of training epochs.\n",
    "num_epochs = 20\n",
    "batch_size = 16\n",
    "\n",
    "# Helper function to compute gradient norm safely.\n",
    "def gradient_norm(gradients):\n",
    "    squared_sum = 0.0\n",
    "    for g in gradients:\n",
    "        if g is not None:\n",
    "            squared_sum += tf.reduce_sum(tf.square(g))\n",
    "    return float(tf.sqrt(squared_sum))\n",
    "\n",
    "# Training loop comparing SGD and Adam side by side.\n",
    "for epoch in range(num_epochs):\n",
    "    indices = np.arange(num_samples)\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    # Shuffle data for this epoch.\n",
    "    x_shuffled = x_train[indices]\n",
    "    y_shuffled = y_train[indices]\n",
    "\n",
    "    # Iterate over mini batches.\n",
    "    for start in range(0, num_samples, batch_size):\n",
    "        end = start + batch_size\n",
    "        xb = x_shuffled[start:end]\n",
    "        yb = y_shuffled[start:end]\n",
    "\n",
    "        # One training step for SGD model.\n",
    "        with tf.GradientTape() as tape_sgd:\n",
    "            preds_sgd = model_sgd(xb, training=True)\n",
    "            loss_sgd = loss_fn(yb, preds_sgd)\n",
    "        grads_sgd = tape_sgd.gradient(loss_sgd, model_sgd.trainable_variables)\n",
    "        optimizer_sgd.apply_gradients(zip(grads_sgd, model_sgd.trainable_variables))\n",
    "\n",
    "        # One training step for Adam model.\n",
    "        with tf.GradientTape() as tape_adam:\n",
    "            preds_adam = model_adam(xb, training=True)\n",
    "            loss_adam = loss_fn(yb, preds_adam)\n",
    "        grads_adam = tape_adam.gradient(loss_adam, model_adam.trainable_variables)\n",
    "        optimizer_adam.apply_gradients(zip(grads_adam, model_adam.trainable_variables))\n",
    "\n",
    "    # Compute full batch loss after epoch.\n",
    "    full_preds_sgd = model_sgd(x_train, training=False)\n",
    "    full_preds_adam = model_adam(x_train, training=False)\n",
    "\n",
    "    # Calculate losses for logging and analysis.\n",
    "    epoch_loss_sgd = float(loss_fn(y_train, full_preds_sgd))\n",
    "    epoch_loss_adam = float(loss_fn(y_train, full_preds_adam))\n",
    "\n",
    "    # Store losses for later inspection.\n",
    "    sgd_losses.append(epoch_loss_sgd)\n",
    "    adam_losses.append(epoch_loss_adam)\n",
    "\n",
    "    # Compute gradient norms on full batch for insight.\n",
    "    with tf.GradientTape() as tape_sgd_full:\n",
    "        preds_sgd_full = model_sgd(x_train, training=True)\n",
    "        loss_sgd_full = loss_fn(y_train, preds_sgd_full)\n",
    "    grads_sgd_full = tape_sgd_full.gradient(\n",
    "        loss_sgd_full, model_sgd.trainable_variables\n",
    "    )\n",
    "\n",
    "    with tf.GradientTape() as tape_adam_full:\n",
    "        preds_adam_full = model_adam(x_train, training=True)\n",
    "        loss_adam_full = loss_fn(y_train, preds_adam_full)\n",
    "    grads_adam_full = tape_adam_full.gradient(\n",
    "        loss_adam_full, model_adam.trainable_variables\n",
    "    )\n",
    "\n",
    "    # Measure gradient norms for both optimizers.\n",
    "    sgd_grad_norm = gradient_norm(grads_sgd_full)\n",
    "    adam_grad_norm = gradient_norm(grads_adam_full)\n",
    "\n",
    "    # Print compact training summary for this epoch.\n",
    "    print(\n",
    "        f\"Epoch {epoch+1:02d} | \"\n",
    "        f\"SGD loss {epoch_loss_sgd:.4f}, grad {sgd_grad_norm:.4f} | \"\n",
    "        f\"Adam loss {epoch_loss_adam:.4f}, grad {adam_grad_norm:.4f}\"\n",
    "    )\n",
    "\n",
    "# Print final learned parameters for both optimizers.\n",
    "sgd_w, sgd_b = model_sgd.layers[0].get_weights()\n",
    "adam_w, adam_b = model_adam.layers[0].get_weights()\n",
    "\n",
    "# Show how close each optimizer came to true parameters.\n",
    "print(\"True w, b:\", true_w, true_b)\n",
    "print(\"SGD w, b:\", float(sgd_w[0][0]), float(sgd_b[0]))\n",
    "print(\"Adam w, b:\", float(adam_w[0][0]), float(adam_b[0]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6b5199",
   "metadata": {},
   "source": [
    "## **3. Monitoring Optimization Progress**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6677810",
   "metadata": {},
   "source": [
    "### **3.1. Tracking Loss Trends**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d7008a",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_03/Lecture_A/image_03_01.jpg?v=1769702168\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Loss curves show if the model learns\n",
    ">* Unusual loss patterns signal optimization setup problems\n",
    "\n",
    ">* Compare training and validation loss to assess generalization\n",
    ">* Use curves to spot overfitting, underfitting, tune hyperparameters\n",
    "\n",
    ">* Loss curves reveal subtle learning rate problems\n",
    ">* Logging and comparing curves guides optimization debugging\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cc4b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Tracking Loss Trends\n",
    "\n",
    "# This script shows how to track loss trends.\n",
    "# We use TensorFlow to simulate a training loop.\n",
    "# Focus on plotting simple training and validation losses.\n",
    "\n",
    "# !pip install tensorflow.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "\n",
    "# Import TensorFlow and NumPy.\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Set deterministic random seeds.\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "# Print TensorFlow version once.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Choose device based on GPU availability.\n",
    "physical_gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "if physical_gpus:\n",
    "    device_name = \"/GPU:0\"\n",
    "else:\n",
    "    device_name = \"/CPU:0\"\n",
    "\n",
    "# Briefly report selected device.\n",
    "print(\"Using device:\", device_name)\n",
    "\n",
    "# Create a small synthetic regression dataset.\n",
    "num_samples = 256\n",
    "x_data = np.linspace(-2.0, 2.0, num_samples).astype(\"float32\")\n",
    "\n",
    "# Generate targets with a simple nonlinear pattern.\n",
    "y_true = (0.5 * x_data ** 3 - 0.3 * x_data).astype(\"float32\")\n",
    "\n",
    "# Add small Gaussian noise.\n",
    "noise = 0.05 * np.random.randn(num_samples).astype(\"float32\")\n",
    "y_data = y_true + noise\n",
    "\n",
    "# Split into training and validation sets.\n",
    "train_size = 200\n",
    "x_train = x_data[:train_size]\n",
    "y_train = y_data[:train_size]\n",
    "\n",
    "# Prepare validation subset.\n",
    "x_val = x_data[train_size:]\n",
    "y_val = y_data[train_size:]\n",
    "\n",
    "# Validate shapes before building model.\n",
    "assert x_train.shape[0] == y_train.shape[0]\n",
    "assert x_val.shape[0] == y_val.shape[0]\n",
    "\n",
    "# Build a tiny regression model.\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(1,)),\n",
    "    tf.keras.layers.Dense(8, activation=\"tanh\"),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# Choose mean squared error loss.\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "# Use Adam optimizer with moderate learning rate.\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.05)\n",
    "\n",
    "# Prepare lists to store loss history.\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Prepare list to store gradient norms.\n",
    "grad_norms = []\n",
    "\n",
    "# Define batch size and epochs.\n",
    "batch_size = 32\n",
    "epochs = 20\n",
    "\n",
    "# Create TensorFlow datasets for batching.\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_ds = train_ds.shuffle(buffer_size=train_size, seed=0)\n",
    "\n",
    "# Batch the training dataset.\n",
    "train_ds = train_ds.batch(batch_size)\n",
    "\n",
    "# Use context manager for selected device.\n",
    "with tf.device(device_name):\n",
    "    for epoch in range(epochs):\n",
    "        epoch_losses = []\n",
    "        epoch_grad_norms = []\n",
    "\n",
    "        # Iterate over mini batches.\n",
    "        for batch_x, batch_y in train_ds:\n",
    "            batch_x = tf.reshape(batch_x, (-1, 1))\n",
    "            batch_y = tf.reshape(batch_y, (-1, 1))\n",
    "\n",
    "            # Record gradients with GradientTape.\n",
    "            with tf.GradientTape() as tape:\n",
    "                preds = model(batch_x, training=True)\n",
    "                loss_value = loss_fn(batch_y, preds)\n",
    "\n",
    "            # Compute gradients of loss w.r.t parameters.\n",
    "            grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "\n",
    "            # Apply gradients using optimizer.\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "            # Store batch loss for averaging.\n",
    "            epoch_losses.append(float(loss_value.numpy()))\n",
    "\n",
    "            # Compute gradient norm for monitoring.\n",
    "            squared_sum = 0.0\n",
    "            for g in grads:\n",
    "                if g is not None:\n",
    "                    squared_sum += float(tf.reduce_sum(g ** 2).numpy())\n",
    "\n",
    "            # Take square root for L2 norm.\n",
    "            grad_norm = math.sqrt(squared_sum)\n",
    "            epoch_grad_norms.append(grad_norm)\n",
    "\n",
    "        # Compute mean training loss for epoch.\n",
    "        mean_train_loss = float(np.mean(epoch_losses))\n",
    "        train_losses.append(mean_train_loss)\n",
    "\n",
    "        # Compute mean gradient norm for epoch.\n",
    "        mean_grad_norm = float(np.mean(epoch_grad_norms))\n",
    "        grad_norms.append(mean_grad_norm)\n",
    "\n",
    "        # Evaluate validation loss without gradient tracking.\n",
    "        x_val_tensor = tf.reshape(x_val, (-1, 1))\n",
    "        y_val_tensor = tf.reshape(y_val, (-1, 1))\n",
    "        val_preds = model(x_val_tensor, training=False)\n",
    "        val_loss_value = loss_fn(y_val_tensor, val_preds)\n",
    "\n",
    "        # Store validation loss.\n",
    "        val_losses.append(float(val_loss_value.numpy()))\n",
    "\n",
    "# Import matplotlib for plotting.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a figure with two subplots.\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "# Plot training and validation loss trends.\n",
    "axes[0].plot(range(1, epochs + 1), train_losses, label=\"train\")\n",
    "axes[0].plot(range(1, epochs + 1), val_losses, label=\"val\")\n",
    "\n",
    "# Label the first subplot clearly.\n",
    "axes[0].set_xlabel(\"Epoch\")\n",
    "axes[0].set_ylabel(\"Loss\")\n",
    "axes[0].set_title(\"Loss trends over epochs\")\n",
    "axes[0].legend()\n",
    "\n",
    "# Plot gradient norm trend.\n",
    "axes[1].plot(range(1, epochs + 1), grad_norms, label=\"grad_norm\")\n",
    "\n",
    "# Label the second subplot clearly.\n",
    "axes[1].set_xlabel(\"Epoch\")\n",
    "axes[1].set_ylabel(\"Gradient L2 norm\")\n",
    "axes[1].set_title(\"Gradient norm over epochs\")\n",
    "axes[1].legend()\n",
    "\n",
    "# Adjust layout for readability.\n",
    "plt.tight_layout()\n",
    "\n",
    "# Print a short numeric summary.\n",
    "print(\"Final train loss:\", round(train_losses[-1], 4))\n",
    "print(\"Final val loss:\", round(val_losses[-1], 4))\n",
    "print(\"Final grad norm:\", round(grad_norms[-1], 4))\n",
    "\n",
    "# Display the plots to visually inspect trends.\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db53fe1",
   "metadata": {},
   "source": [
    "### **3.2. Gradient Norm Monitoring**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5525e420",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_03/Lecture_A/image_03_02.jpg?v=1769702282\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Gradient norms summarize overall gradient strength during training\n",
    ">* Tracking them reveals unstable, vanishing, or exploding updates\n",
    "\n",
    ">* Small or huge norms signal learning problems\n",
    ">* Plot norms with loss to spot issues\n",
    "\n",
    ">* Use gradient norms to choose tuning actions\n",
    ">* Build intuition to prevent failures and speed experiments\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6dedc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Gradient Norm Monitoring\n",
    "\n",
    "# This script shows gradient norm monitoring basics.\n",
    "# It uses TensorFlow to simulate a tiny training.\n",
    "# Focus on computing and printing simple gradient norms.\n",
    "\n",
    "# Install TensorFlow only if missing in your environment.\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "\n",
    "# Import TensorFlow and NumPy for computation.\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Set deterministic random seeds for reproducibility.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "\n",
    "# Set NumPy and TensorFlow seeds deterministically.\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Select device string based on GPU availability.\n",
    "physical_gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "\n",
    "# Choose CPU if no GPU is detected.\n",
    "if physical_gpus:\n",
    "    device_name = \"/GPU:0\"\n",
    "else:\n",
    "    device_name = \"/CPU:0\"\n",
    "\n",
    "# Create a tiny synthetic regression dataset.\n",
    "num_samples = 64\n",
    "input_dim = 3\n",
    "\n",
    "# Generate random inputs with small normal noise.\n",
    "X = np.random.randn(num_samples, input_dim).astype(np.float32)\n",
    "\n",
    "# Define true weights and bias for synthetic targets.\n",
    "true_w = np.array([[2.0], [-1.0], [0.5]], dtype=np.float32)\n",
    "\n",
    "# Compute targets with linear rule plus small noise.\n",
    "y = X @ true_w + 0.1 * np.random.randn(num_samples, 1).astype(np.float32)\n",
    "\n",
    "# Validate shapes before building the model.\n",
    "assert X.shape == (num_samples, input_dim)\n",
    "\n",
    "# Validate target shape for safety.\n",
    "assert y.shape == (num_samples, 1)\n",
    "\n",
    "# Build a tiny sequential model for regression.\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(input_dim,)),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# Choose mean squared error loss function.\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "# Create an SGD optimizer with moderate learning rate.\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)\n",
    "\n",
    "# Prepare lists to store loss and gradient norms.\n",
    "loss_history = []\n",
    "grad_norm_history = []\n",
    "\n",
    "# Define a small number of training epochs.\n",
    "num_epochs = 8\n",
    "\n",
    "# Use selected device context for training loop.\n",
    "with tf.device(device_name):\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        # Record operations for automatic differentiation.\n",
    "        with tf.GradientTape() as tape:\n",
    "            preds = model(X, training=True)\n",
    "\n",
    "            # Compute scalar loss value for this batch.\n",
    "            loss_value = loss_fn(y, preds)\n",
    "\n",
    "        # Compute gradients of loss with respect to weights.\n",
    "        grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "\n",
    "        # Filter out any None gradients defensively.\n",
    "        valid_grads = [g for g in grads if g is not None]\n",
    "\n",
    "        # Compute global L2 norm of all gradients.\n",
    "        squared_sums = [tf.reduce_sum(tf.square(g)) for g in valid_grads]\n",
    "\n",
    "        # Add small epsilon to avoid numerical issues.\n",
    "        total_squared = tf.add_n(squared_sums) + 1e-12\n",
    "\n",
    "        # Take square root to obtain gradient L2 norm.\n",
    "        grad_norm = tf.sqrt(total_squared)\n",
    "\n",
    "        # Apply gradients to update model parameters.\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        # Store scalar values for later inspection.\n",
    "        loss_history.append(float(loss_value.numpy()))\n",
    "\n",
    "        # Store gradient norm as Python float value.\n",
    "        grad_norm_history.append(float(grad_norm.numpy()))\n",
    "\n",
    "# Print a compact header for monitoring results.\n",
    "print(\"Epoch | Loss | Gradient L2 Norm\")\n",
    "\n",
    "# Loop through history and print few summary lines.\n",
    "for epoch in range(num_epochs):\n",
    "    loss_val = loss_history[epoch]\n",
    "\n",
    "    # Retrieve corresponding gradient norm value.\n",
    "    gnorm_val = grad_norm_history[epoch]\n",
    "\n",
    "    # Print rounded values for readability.\n",
    "    print(epoch + 1, \"|\", round(loss_val, 4), \"|\", round(gnorm_val, 4))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe778a0",
   "metadata": {},
   "source": [
    "### **3.3. Learning Rate Dynamics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f3f21a",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_03/Lecture_A/image_03_03.jpg?v=1769702319\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Learning rate controls how fast parameters change\n",
    ">* Watch loss behavior to judge healthy learning rates\n",
    "\n",
    ">* Schedulers change learning rate during training\n",
    ">* Visualizing rate and loss reveals exploration, refinement\n",
    "\n",
    ">* Watch loss behavior to judge learning rate\n",
    ">* Adjust schedules to stabilize, refine, or stop training\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b25605f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Learning Rate Dynamics\n",
    "\n",
    "# This script visualizes simple learning rate dynamics.\n",
    "# It compares constant and decaying learning rates.\n",
    "# Use it to connect curves with optimization behavior.\n",
    "\n",
    "# Optional TensorFlow install for environments without it.\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "\n",
    "# Import numpy for numeric operations.\n",
    "import numpy as np\n",
    "\n",
    "# Import matplotlib for plotting results.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import tensorflow and set logging level.\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set seeds for reproducible behavior.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Create a tiny synthetic regression dataset.\n",
    "true_w = 2.0\n",
    "true_b = -1.0\n",
    "x_data = np.linspace(-1.0, 1.0, 64).astype(np.float32)\n",
    "noise = 0.1 * np.random.randn(64).astype(np.float32)\n",
    "y_data = true_w * x_data + true_b + noise\n",
    "\n",
    "# Validate shapes before creating tensors.\n",
    "assert x_data.shape == y_data.shape\n",
    "\n",
    "# Convert numpy arrays to TensorFlow tensors.\n",
    "x_tensor = tf.convert_to_tensor(x_data.reshape(-1, 1))\n",
    "y_tensor = tf.convert_to_tensor(y_data.reshape(-1, 1))\n",
    "\n",
    "# Define a simple linear regression model.\n",
    "class SimpleLinear(tf.Module):\n",
    "\n",
    "    # Initialize trainable parameters.\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.w = tf.Variable(tf.random.normal([1, 1]))\n",
    "        self.b = tf.Variable(tf.zeros([1]))\n",
    "\n",
    "    # Forward pass computing predictions.\n",
    "    def __call__(self, x):\n",
    "        return tf.matmul(x, self.w) + self.b\n",
    "\n",
    "# Define mean squared error loss function.\n",
    "def mse_loss(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "\n",
    "# Training step using given optimizer and model.\n",
    "@tf.function\n",
    "def train_step(model, optimizer, x_batch, y_batch):\n",
    "    with tf.GradientTape() as tape:\n",
    "        preds = model(x_batch)\n",
    "        loss = mse_loss(y_batch, preds)\n",
    "    grads = tape.gradient(loss, [model.w, model.b])\n",
    "    optimizer.apply_gradients(zip(grads, [model.w, model.b]))\n",
    "    grad_norm = tf.sqrt(sum(tf.reduce_sum(g * g) for g in grads))\n",
    "    return loss, grad_norm\n",
    "\n",
    "# Helper to run training with a schedule.\n",
    "def run_training(lr_schedule, label, epochs):\n",
    "    model = SimpleLinear()\n",
    "    losses = []\n",
    "    grad_norms = []\n",
    "    lrs = []\n",
    "    for epoch in range(epochs):\n",
    "        lr = lr_schedule(epoch)\n",
    "        optimizer = tf.keras.optimizers.SGD(learning_rate=lr)\n",
    "        loss, grad_norm = train_step(model, optimizer, x_tensor, y_tensor)\n",
    "        losses.append(float(loss.numpy()))\n",
    "        grad_norms.append(float(grad_norm.numpy()))\n",
    "        lrs.append(lr)\n",
    "    return {\n",
    "        \"label\": label,\n",
    "        \"losses\": losses,\n",
    "        \"grad_norms\": grad_norms,\n",
    "        \"lrs\": lrs,\n",
    "    }\n",
    "\n",
    "# Define a constant learning rate schedule.\n",
    "def constant_lr(epoch):\n",
    "    return 0.1\n",
    "\n",
    "# Define a simple exponential decay schedule.\n",
    "def decaying_lr(epoch):\n",
    "    return 0.1 * (0.9 ** epoch)\n",
    "\n",
    "# Set a small number of epochs for clarity.\n",
    "num_epochs = 25\n",
    "\n",
    "# Run training with constant learning rate.\n",
    "result_const = run_training(constant_lr, \"constant\", num_epochs)\n",
    "\n",
    "# Run training with decaying learning rate.\n",
    "result_decay = run_training(decaying_lr, \"decay\", num_epochs)\n",
    "\n",
    "# Print a brief numeric summary for both schedules.\n",
    "print(\"Final loss constant:\", round(result_const[\"losses\"][-1], 4))\n",
    "print(\"Final loss decay:\", round(result_decay[\"losses\"][-1], 4))\n",
    "print(\"First three learning rates constant:\", result_const[\"lrs\"][:3])\n",
    "print(\"First three learning rates decay:\", [round(v, 4) for v in result_decay[\"lrs\"][:3]])\n",
    "print(\"First three grad norms constant:\", [round(v, 4) for v in result_const[\"grad_norms\"][:3]])\n",
    "print(\"First three grad norms decay:\", [round(v, 4) for v in result_decay[\"grad_norms\"][:3]])\n",
    "\n",
    "# Create a figure with two subplots.\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "# Plot loss curves for both learning rate schedules.\n",
    "axes[0].plot(result_const[\"losses\"], label=\"constant lr\")\n",
    "axes[0].plot(result_decay[\"losses\"], label=\"decaying lr\")\n",
    "axes[0].set_title(\"Loss vs epoch\")\n",
    "axes[0].set_xlabel(\"epoch\")\n",
    "axes[0].set_ylabel(\"loss\")\n",
    "axes[0].legend()\n",
    "\n",
    "# Plot learning rate values over epochs.\n",
    "axes[1].plot(result_const[\"lrs\"], label=\"constant lr\")\n",
    "axes[1].plot(result_decay[\"lrs\"], label=\"decaying lr\")\n",
    "axes[1].set_title(\"Learning rate vs epoch\")\n",
    "axes[1].set_xlabel(\"epoch\")\n",
    "axes[1].set_ylabel(\"learning rate\")\n",
    "axes[1].legend()\n",
    "\n",
    "# Adjust layout and display the single figure.\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fd26f9",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Losses and Optimizers**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4014cea3",
   "metadata": {},
   "source": [
    "\n",
    "In this lecture, you learned to:\n",
    "- Select and configure appropriate loss functions for common supervised learning tasks in PyTorch. \n",
    "- Use optimizers such as SGD and Adam to update model parameters within a training loop. \n",
    "- Inspect and debug optimization behavior using learning rate settings, gradient norms, and loss curves. \n",
    "\n",
    "In the next Lecture (Lecture B), we will go over 'Training Loop Design'"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
