{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ece2542",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Training Loop Design**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9f42c3",
   "metadata": {},
   "source": [
    ">Last update: 20260129.\n",
    "    \n",
    "By the end of this Lecture, you will be able to:\n",
    "- Implement a standard PyTorch training loop that iterates over DataLoader batches and updates model parameters. \n",
    "- Add evaluation and metric computation to the training workflow without leaking gradients or mixing modes. \n",
    "- Handle device placement and optional mixed precision to improve performance while maintaining correctness. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78919a5",
   "metadata": {},
   "source": [
    "## **1. Training Epoch Loop**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949aacae",
   "metadata": {},
   "source": [
    "### **1.1. Iterating Over Batches**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc6d9c6",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_03/Lecture_B/image_01_01.jpg?v=1769704620\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Training uses small batches instead of whole dataset\n",
    ">* Batches flow sequentially, refining parameters efficiently\n",
    "\n",
    ">* DataLoader yields shuffled input-target batches each epoch\n",
    ">* Loop runs batches, computes loss, updates model parameters\n",
    "\n",
    ">* Varied batches prevent bias in model learning\n",
    ">* Correct iteration ensures fairness, efficiency, and reproducibility\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c48fc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Iterating Over Batches\n",
    "\n",
    "# This script shows iterating over training batches.\n",
    "# We use TensorFlow to simulate a batch training loop.\n",
    "# Focus on the rhythm of batches per epoch.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Import TensorFlow and check version.\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set deterministic random seeds.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Choose device string based on GPU availability.\n",
    "physical_gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "use_gpu = bool(physical_gpus)\n",
    "device_name = \"GPU\" if use_gpu else \"CPU\"\n",
    "\n",
    "# Inform user which device is used.\n",
    "print(\"Using device:\", device_name)\n",
    "\n",
    "# Create a tiny synthetic regression dataset.\n",
    "num_samples = 64\n",
    "num_features = 3\n",
    "X = np.random.randn(num_samples, num_features).astype(\"float32\")\n",
    "\n",
    "# Create targets with a simple linear rule.\n",
    "true_w = np.array([[2.0], [-1.0], [0.5]], dtype=\"float32\")\n",
    "y = X @ true_w + 0.1\n",
    "\n",
    "# Validate shapes before building dataset.\n",
    "assert X.shape == (num_samples, num_features)\n",
    "assert y.shape == (num_samples, 1)\n",
    "\n",
    "# Build a tf.data.Dataset and batch it.\n",
    "batch_size = 8\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "\n",
    "# Shuffle slightly then batch for training.\n",
    "dataset = dataset.shuffle(buffer_size=num_samples, seed=seed_value)\n",
    "dataset = dataset.batch(batch_size)\n",
    "\n",
    "# Define a simple dense regression model.\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(num_features,)),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# Create optimizer and loss function.\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "# Training parameters for one short epoch.\n",
    "num_epochs = 1\n",
    "print(\"Starting training for\", num_epochs, \"epoch.\")\n",
    "\n",
    "# Iterate over epochs and batches explicitly.\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    batch_count = 0\n",
    "\n",
    "    # Loop over batches from the dataset.\n",
    "    for step, (batch_x, batch_y) in enumerate(dataset):\n",
    "        batch_count += 1\n",
    "\n",
    "        # Validate batch shapes before forward pass.\n",
    "        assert batch_x.shape[0] <= batch_size\n",
    "        assert batch_y.shape[0] <= batch_size\n",
    "\n",
    "        # Record operations for automatic differentiation.\n",
    "        with tf.GradientTape() as tape:\n",
    "            preds = model(batch_x, training=True)\n",
    "            loss_value = loss_fn(batch_y, preds)\n",
    "\n",
    "        # Compute gradients of loss with respect to weights.\n",
    "        grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        # Accumulate loss for simple reporting.\n",
    "        epoch_loss += float(loss_value.numpy())\n",
    "\n",
    "        # Print a short message for the first few batches.\n",
    "        if step < 2:\n",
    "            print(\"Epoch\", epoch + 1, \"batch\", step + 1, \"loss\", round(float(loss_value), 4))\n",
    "\n",
    "    # Compute average loss over all batches.\n",
    "    avg_loss = epoch_loss / max(batch_count, 1)\n",
    "    print(\"Epoch\", epoch + 1, \"average loss\", round(avg_loss, 4))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18354e81",
   "metadata": {},
   "source": [
    "### **1.2. Forward Pass and Loss**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7065ae",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_03/Lecture_B/image_01_02.jpg?v=1769704665\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Forward pass turns input batches into predictions\n",
    ">* Layers run sequentially; output reflects learned understanding\n",
    "\n",
    ">* Compute loss measuring predictions versus true targets\n",
    ">* Loss is scalar, differentiable, usually batch-averaged\n",
    "\n",
    ">* Treat forward pass and loss as one\n",
    ">* Loss drives gradients, enabling iterative performance improvement\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fa7d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Forward Pass and Loss\n",
    "\n",
    "# This script demonstrates forward pass and loss.\n",
    "# It uses TensorFlow to mimic training logic.\n",
    "# Focus on batches predictions and scalar loss.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Import TensorFlow and Keras utilities.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Set deterministic random seeds.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Detect available device type.\n",
    "physical_gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "use_gpu = bool(physical_gpus)\n",
    "device_type = \"GPU\" if use_gpu else \"CPU\"\n",
    "\n",
    "# Print framework version and device.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Using device type:\", device_type)\n",
    "\n",
    "# Create a tiny synthetic classification dataset.\n",
    "num_samples = 64\n",
    "num_features = 20\n",
    "num_classes = 3\n",
    "\n",
    "# Generate random feature data.\n",
    "X = np.random.randn(num_samples, num_features).astype(\"float32\")\n",
    "Y = np.random.randint(num_classes, size=(num_samples,))\n",
    "\n",
    "# Validate shapes before building dataset.\n",
    "assert X.shape == (num_samples, num_features)\n",
    "assert Y.shape == (num_samples,)\n",
    "\n",
    "# Build a tf.data Dataset with small batch size.\n",
    "batch_size = 8\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X, Y))\n",
    "\n",
    "dataset = dataset.shuffle(buffer_size=num_samples, seed=seed_value)\n",
    "\n",
    "dataset = dataset.batch(batch_size)\n",
    "\n",
    "# Define a simple dense neural network model.\n",
    "model = keras.Sequential([\n",
    "    layers.Input(shape=(num_features,)),\n",
    "    layers.Dense(16, activation=\"relu\"),\n",
    "    layers.Dense(num_classes, activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "# Choose an optimizer and loss function.\n",
    "optimizer = keras.optimizers.SGD(learning_rate=0.1)\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "# Run one training epoch focusing on forward pass.\n",
    "print(\"\\nRunning one epoch with manual loop...\")\n",
    "\n",
    "# Iterate over batches from the dataset.\n",
    "for step, (batch_x, batch_y) in enumerate(dataset):\n",
    "    # Validate batch shapes defensively.\n",
    "    assert batch_x.shape[1] == num_features\n",
    "    assert len(batch_y.shape) == 1\n",
    "\n",
    "    # Record operations for automatic differentiation.\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Forward pass to compute predictions.\n",
    "        logits = model(batch_x, training=True)\n",
    "\n",
    "        # Compute scalar loss for current batch.\n",
    "        loss_value = loss_fn(batch_y, logits)\n",
    "\n",
    "    # Compute gradients of loss with respect to weights.\n",
    "    grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "\n",
    "    # Apply gradients to update model parameters.\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    # Print a short summary for first few steps.\n",
    "    if step < 3:\n",
    "        print(\n",
    "            \"Step\", int(step),\n",
    "            \"batch_loss:\", float(loss_value.numpy()),\n",
    "        )\n",
    "\n",
    "# Compute predictions on a single batch without gradients.\n",
    "first_batch_x, first_batch_y = next(iter(dataset))\n",
    "\n",
    "# Disable gradient tracking for evaluation.\n",
    "pred_probs = model(first_batch_x, training=False)\n",
    "\n",
    "# Compute evaluation loss on that batch.\n",
    "eval_loss = loss_fn(first_batch_y, pred_probs).numpy()\n",
    "\n",
    "# Print final evaluation loss for the batch.\n",
    "print(\"\\nEvaluation loss on first batch:\", float(eval_loss))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1f15e8",
   "metadata": {},
   "source": [
    "### **1.3. Backward pass and update**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5579b95",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_03/Lecture_B/image_01_03.jpg?v=1769704749\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Clear old gradients, then call loss.backward\n",
    ">* Backward pass computes each weight’s effect on loss\n",
    "\n",
    ">* Optimizer uses gradients to adjust each parameter\n",
    ">* Small weight changes accumulate, improving model performance\n",
    "\n",
    ">* Backward and updates must follow batch-wise order\n",
    ">* Correct sequence keeps gradients clean and learning stable\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0085353",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Backward pass and update\n",
    "\n",
    "# This script shows a simple training loop.\n",
    "# We focus on backward pass and updates.\n",
    "# Run it in Colab to follow along.\n",
    "\n",
    "# Install PyTorch if not already available.\n",
    "# !pip install torch torchvision torchaudio.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "\n",
    "# Import torch and related submodules.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Print PyTorch version in one short line.\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "\n",
    "# Set deterministic random seeds for reproducibility.\n",
    "random.seed(0)\n",
    "\n",
    "# Set numpy seed through torch generator for simplicity.\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Select device based on GPU availability.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Print which device will be used for training.\n",
    "print(\"Using device:\", device.type)\n",
    "\n",
    "# Create a tiny synthetic regression dataset.\n",
    "num_samples = 64\n",
    "\n",
    "# Generate input features with two dimensions.\n",
    "X = torch.linspace(-1.0, 1.0, num_samples).unsqueeze(1)\n",
    "\n",
    "# Create targets using a simple linear relationship.\n",
    "y = 3.0 * X + 0.5\n",
    "\n",
    "# Move data to the selected device.\n",
    "X = X.to(device)\n",
    "\n",
    "y = y.to(device)\n",
    "\n",
    "# Define a very small linear regression model.\n",
    "model = nn.Linear(in_features=1, out_features=1)\n",
    "\n",
    "# Move model parameters to the selected device.\n",
    "model = model.to(device)\n",
    "\n",
    "# Define mean squared error loss function.\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Create a simple stochastic gradient descent optimizer.\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# Wrap data into a TensorDataset for DataLoader.\n",
    "dataset = torch.utils.data.TensorDataset(X, y)\n",
    "\n",
    "# Create DataLoader to iterate over mini batches.\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Set number of epochs for the training loop.\n",
    "num_epochs = 5\n",
    "\n",
    "# Loop over epochs to repeatedly see the data.\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # Set model to training mode for this epoch.\n",
    "    model.train()\n",
    "\n",
    "    # Track running loss for simple monitoring.\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Iterate over mini batches from the DataLoader.\n",
    "    for batch_inputs, batch_targets in loader:\n",
    "\n",
    "        # Ensure batch tensors are on the correct device.\n",
    "        batch_inputs = batch_inputs.to(device)\n",
    "\n",
    "        # Move targets to the same device as inputs.\n",
    "        batch_targets = batch_targets.to(device)\n",
    "\n",
    "        # Validate shapes before forward computation.\n",
    "        assert batch_inputs.shape[1] == 1\n",
    "\n",
    "        # Clear old gradients so they do not accumulate.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass to compute model predictions.\n",
    "        predictions = model(batch_inputs)\n",
    "\n",
    "        # Compute loss comparing predictions and targets.\n",
    "        loss = criterion(predictions, batch_targets)\n",
    "\n",
    "        # Backward pass computes gradients for all parameters.\n",
    "        loss.backward()\n",
    "\n",
    "        # Optimizer step updates parameters using gradients.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate loss value for reporting later.\n",
    "        running_loss += loss.item() * batch_inputs.size(0)\n",
    "\n",
    "    # Compute average loss over all samples.\n",
    "    epoch_loss = running_loss / num_samples\n",
    "\n",
    "    # Print a short summary line for this epoch.\n",
    "    print(f\"Epoch {epoch + 1}, loss {epoch_loss:.4f}\")\n",
    "\n",
    "# Evaluate model on the full dataset without gradients.\n",
    "with torch.no_grad():\n",
    "\n",
    "    # Switch model to evaluation mode for inference.\n",
    "    model.eval()\n",
    "\n",
    "    # Compute predictions on the full dataset.\n",
    "    preds = model(X)\n",
    "\n",
    "    # Compute final mean squared error on dataset.\n",
    "    final_loss = criterion(preds, y).item()\n",
    "\n",
    "# Print final loss to confirm training effectiveness.\n",
    "print(\"Final MSE on tiny dataset:\", round(final_loss, 4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fa8af3",
   "metadata": {},
   "source": [
    "## **2. Evaluation and Metrics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4664a6a7",
   "metadata": {},
   "source": [
    "### **2.1. Training vs Evaluation Modes**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aced7aa",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_03/Lecture_B/image_02_01.jpg?v=1769704788\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Training and evaluation modes behave differently by design\n",
    ">* Wrong mode choice corrupts metrics and generalization\n",
    "\n",
    ">* Switch between train and eval at boundaries\n",
    ">* Eval mode gives stable, deployment-like, deterministic behavior\n",
    "\n",
    ">* Metrics must reflect true inference-time behavior\n",
    ">* Switch modes correctly to ensure fair, reliable evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123356c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Training vs Evaluation Modes\n",
    "\n",
    "# This script shows training and evaluation modes.\n",
    "# We use TensorFlow to mimic PyTorch behavior.\n",
    "# Focus on metrics without mixing gradient modes.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Import TensorFlow and Keras utilities.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Select device preferring GPU when available.\n",
    "physical_gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "use_gpu = bool(physical_gpus)\n",
    "print(\"Using GPU:\" if use_gpu else \"Using GPU:\", use_gpu)\n",
    "\n",
    "# Load a small subset of MNIST digits.\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Reduce dataset size for quick demonstration.\n",
    "train_samples = 2000\n",
    "\n",
    "# Number of test samples for evaluation.\n",
    "\n",
    "test_samples = 500\n",
    "x_train = x_train[:train_samples]\n",
    "y_train = y_train[:train_samples]\n",
    "\n",
    "# Slice test data for faster evaluation.\n",
    "x_test = x_test[:test_samples]\n",
    "y_test = y_test[:test_samples]\n",
    "\n",
    "# Normalize images to range zero one.\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_test = x_test.astype(\"float32\") / 255.0\n",
    "\n",
    "# Add channel dimension for convolution layers.\n",
    "x_train = np.expand_dims(x_train, axis=-1)\n",
    "x_test = np.expand_dims(x_test, axis=-1)\n",
    "\n",
    "# Validate shapes before building model.\n",
    "assert x_train.shape[0] == train_samples\n",
    "assert x_test.shape[0] == test_samples\n",
    "\n",
    "# Build a simple convolutional classification model.\n",
    "model = keras.Sequential([\n",
    "    layers.Conv2D(8, (3, 3), activation=\"relu\", input_shape=(28, 28, 1)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(32, activation=\"relu\"),\n",
    "    layers.Dense(10, activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "# Compile model with optimizer loss and accuracy.\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Train briefly with silent logs for speed.\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=2,\n",
    "    batch_size=64,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Define a helper to compute accuracy manually.\n",
    "def compute_accuracy(pred_probs, true_labels):\n",
    "    # Convert probabilities to predicted classes.\n",
    "    pred_classes = np.argmax(pred_probs, axis=1)\n",
    "    # Ensure shapes match before comparison.\n",
    "    assert pred_classes.shape == true_labels.shape\n",
    "    # Compute mean accuracy as float.\n",
    "    return float(np.mean(pred_classes == true_labels))\n",
    "\n",
    "# Run evaluation with model in training mode.\n",
    "model.trainable = True\n",
    "# model.train()  # Not available in Keras; training behavior is controlled via the `training` argument.\n",
    "train_mode_probs = model(x_test, training=True).numpy()\n",
    "train_mode_acc = compute_accuracy(train_mode_probs, y_test)\n",
    "\n",
    "# Run evaluation with model in evaluation mode.\n",
    "model.eval = lambda: None\n",
    "model.training = False\n",
    "model_probs = model(x_test, training=False).numpy()\n",
    "clean_eval_acc = compute_accuracy(model_probs, y_test)\n",
    "\n",
    "# Print both accuracies to compare modes.\n",
    "print(\"Accuracy with training behavior on validation:\", round(train_mode_acc, 4))\n",
    "print(\"Accuracy with proper evaluation mode:\", round(clean_eval_acc, 4))\n",
    "\n",
    "# Show that Keras evaluate uses evaluation behavior.\n",
    "loss_eval, acc_eval = model.evaluate(\n",
    "    x_test,\n",
    "    y_test,\n",
    "    verbose=0,\n",
    ")\n",
    "print(\"Keras evaluate accuracy (eval mode):\", round(acc_eval, 4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adb63d5",
   "metadata": {},
   "source": [
    "### **2.2. Safe Inference with no_grad**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db17253b",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_03/Lecture_B/image_02_02.jpg?v=1769704889\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* no_grad stops tracking graphs during evaluation\n",
    ">* acts like read-only mode, saving memory\n",
    "\n",
    ">* no_grad prevents wasted graphs and GPU memory\n",
    ">* enables larger, more frequent, stable evaluations\n",
    "\n",
    ">* Separates training from evaluation as pure observation\n",
    ">* Prevents parameter changes and unnecessary computation graphs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118ce7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Safe Inference with no_grad\n",
    "\n",
    "# This script demonstrates safe inference with no_grad.\n",
    "# We compare training and evaluation behaviors for gradients.\n",
    "# Focus on evaluation metrics without leaking training information.\n",
    "\n",
    "# !pip install torch torchvision.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "\n",
    "# Import torch and related submodules.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Set deterministic random seeds for reproducibility.\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Detect device preference for computation.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Print torch version and selected device.\n",
    "print(\"Torch version:\", torch.__version__, \"Device:\", device)\n",
    "\n",
    "# Define a tiny model for classification.\n",
    "class TinyNet(nn.Module):\n",
    "    # Initialize linear layer and activation.\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(input_dim, num_classes)\n",
    "\n",
    "    # Define forward computation for inputs.\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Set simple problem dimensions.\n",
    "input_dim = 4\n",
    "num_classes = 3\n",
    "batch_size = 8\n",
    "\n",
    "# Create a tiny random training batch.\n",
    "train_inputs = torch.randn(batch_size, input_dim)\n",
    "train_labels = torch.randint(0, num_classes, (batch_size,))\n",
    "\n",
    "# Create a tiny random validation batch.\n",
    "val_inputs = torch.randn(batch_size, input_dim)\n",
    "val_labels = torch.randint(0, num_classes, (batch_size,))\n",
    "\n",
    "# Move tensors to the selected device.\n",
    "train_inputs = train_inputs.to(device)\n",
    "train_labels = train_labels.to(device)\n",
    "val_inputs = val_inputs.to(device)\n",
    "val_labels = val_labels.to(device)\n",
    "\n",
    "# Initialize model, loss function, and optimizer.\n",
    "model = TinyNet(input_dim, num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# Perform one training step with gradients.\n",
    "model.train()\n",
    "optimizer.zero_grad()\n",
    "train_logits = model(train_inputs)\n",
    "train_loss = criterion(train_logits, train_labels)\n",
    "train_loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "# Check that gradients exist after training.\n",
    "grad_norm = 0.0\n",
    "for p in model.parameters():\n",
    "    if p.grad is not None:\n",
    "        grad_norm += p.grad.norm().item()\n",
    "\n",
    "# Print training loss and gradient norm.\n",
    "print(\"Training loss after one step:\", float(train_loss))\n",
    "print(\"Gradient norm after training step:\", round(grad_norm, 4))\n",
    "\n",
    "# Switch model to evaluation mode.\n",
    "model.eval()\n",
    "\n",
    "# Run validation without no_grad to show graph.\n",
    "val_logits_graph = model(val_inputs)\n",
    "val_loss_graph = criterion(val_logits_graph, val_labels)\n",
    "\n",
    "# Confirm that validation loss requires gradients.\n",
    "print(\"Validation loss requires grad (no context):\", val_loss_graph.requires_grad)\n",
    "\n",
    "# Compute accuracy from logits with graph.\n",
    "val_preds_graph = val_logits_graph.argmax(dim=1)\n",
    "val_acc_graph = (val_preds_graph == val_labels).float().mean().item()\n",
    "\n",
    "# Print accuracy computed with gradient tracking.\n",
    "print(\"Validation accuracy with graph tracking:\", round(val_acc_graph, 3))\n",
    "\n",
    "# Now run safe inference using no_grad context.\n",
    "with torch.no_grad():\n",
    "    val_logits_safe = model(val_inputs)\n",
    "    val_loss_safe = criterion(val_logits_safe, val_labels)\n",
    "    val_preds_safe = val_logits_safe.argmax(dim=1)\n",
    "\n",
    "# Compute accuracy from predictions without gradients.\n",
    "val_acc_safe = (val_preds_safe == val_labels).float().mean().item()\n",
    "\n",
    "# Show that loss no longer requires gradients.\n",
    "print(\"Validation loss requires grad (no_grad):\", val_loss_safe.requires_grad)\n",
    "\n",
    "# Print accuracy computed under no_grad context.\n",
    "print(\"Validation accuracy with no_grad:\", round(val_acc_safe, 3))\n",
    "\n",
    "# Demonstrate that no_grad saves memory by disabling graph.\n",
    "print(\"Safe inference uses no_grad for lightweight evaluation.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dadcb2d",
   "metadata": {},
   "source": [
    "### **2.3. Accuracy and F1 Metrics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5dd244",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_03/Lecture_B/image_02_03.jpg?v=1769704963\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Accuracy measures fraction of correctly classified examples\n",
    ">* It’s simple but unreliable with imbalanced classes\n",
    "\n",
    ">* Use precision, recall, F1 for imbalanced data\n",
    ">* F1 balances missed positives and false alarms\n",
    "\n",
    ">* Aggregate predictions before computing accuracy and F1\n",
    ">* Use class-wise or averaged scores to guide decisions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869a8da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Accuracy and F1 Metrics\n",
    "\n",
    "# This script shows accuracy and F1 metrics.\n",
    "# We use TensorFlow to simulate evaluation workflow.\n",
    "# Focus is on safe metric computation without gradients.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Import TensorFlow and Keras utilities.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Select device string based on GPU availability.\n",
    "physical_gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "use_gpu = bool(physical_gpus)\n",
    "device_name = \"GPU\" if use_gpu else \"CPU\"\n",
    "\n",
    "# Print which device will be used.\n",
    "print(\"Using device:\", device_name)\n",
    "\n",
    "# Load MNIST dataset from Keras datasets.\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Use a small subset for quick demonstration.\n",
    "train_samples = 2000\n",
    "test_samples = 500\n",
    "x_train = x_train[:train_samples]\n",
    "y_train = y_train[:train_samples]\n",
    "\n",
    "# Slice test data subset for evaluation.\n",
    "x_test = x_test[:test_samples]\n",
    "y_test = y_test[:test_samples]\n",
    "\n",
    "# Normalize images to range [0,1].\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_test = x_test.astype(\"float32\") / 255.0\n",
    "\n",
    "# Add channel dimension for convolutional layers.\n",
    "x_train = np.expand_dims(x_train, axis=-1)\n",
    "x_test = np.expand_dims(x_test, axis=-1)\n",
    "\n",
    "# Validate shapes before building model.\n",
    "assert x_train.shape[0] == y_train.shape[0]\n",
    "assert x_test.shape[0] == y_test.shape[0]\n",
    "\n",
    "# Build a small CNN classification model.\n",
    "model = keras.Sequential([\n",
    "    layers.Conv2D(8, (3, 3), activation=\"relu\", input_shape=(28, 28, 1)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(32, activation=\"relu\"),\n",
    "    layers.Dense(10, activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "# Compile model with optimizer and loss.\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Train briefly with silent output for speed.\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=2,\n",
    "    batch_size=64,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Create dataset for evaluation without shuffling.\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "val_ds = val_ds.batch(64)\n",
    "\n",
    "# Define function to compute accuracy from predictions.\n",
    "def compute_accuracy(y_true, y_pred):\n",
    "    correct = np.sum(y_true == y_pred)\n",
    "    total = y_true.shape[0]\n",
    "    return correct / float(total)\n",
    "\n",
    "# Define function to compute precision, recall, F1.\n",
    "def compute_f1(y_true, y_pred, num_classes):\n",
    "    f1_scores = []\n",
    "    for c in range(num_classes):\n",
    "        tp = np.sum((y_pred == c) & (y_true == c))\n",
    "        fp = np.sum((y_pred == c) & (y_true != c))\n",
    "        fn = np.sum((y_pred != c) & (y_true == c))\n",
    "        precision = tp / float(tp + fp + 1e-8)\n",
    "        recall = tp / float(tp + fn + 1e-8)\n",
    "        f1 = 2.0 * precision * recall / float(precision + recall + 1e-8)\n",
    "        f1_scores.append(f1)\n",
    "    macro_f1 = float(np.mean(f1_scores))\n",
    "    return macro_f1\n",
    "\n",
    "# Run evaluation in no gradient context.\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "# Use inference mode to avoid training behavior.\n",
    "for batch_images, batch_labels in val_ds:\n",
    "    logits = model(batch_images, training=False)\n",
    "    batch_pred = tf.argmax(logits, axis=1)\n",
    "    all_preds.append(batch_pred.numpy())\n",
    "    all_labels.append(batch_labels.numpy())\n",
    "\n",
    "# Concatenate all predictions and labels.\n",
    "all_preds = np.concatenate(all_preds, axis=0)\n",
    "all_labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "# Validate final concatenated shapes.\n",
    "assert all_preds.shape == all_labels.shape\n",
    "\n",
    "# Compute accuracy on full validation subset.\n",
    "val_accuracy = compute_accuracy(all_labels, all_preds)\n",
    "\n",
    "# Compute macro F1 across all classes.\n",
    "num_classes = 10\n",
    "val_macro_f1 = compute_f1(all_labels, all_preds, num_classes)\n",
    "\n",
    "# Print concise metric summary lines.\n",
    "print(\"Validation accuracy:\", round(val_accuracy, 4))\n",
    "print(\"Validation macro F1:\", round(val_macro_f1, 4))\n",
    "print(\"Difference F1 minus accuracy:\", round(val_macro_f1 - val_accuracy, 4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0032533",
   "metadata": {},
   "source": [
    "## **3. Devices and Mixed Precision**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62b57ea",
   "metadata": {},
   "source": [
    "### **3.1. Moving Data to CUDA**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0f2f82",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_03/Lecture_B/image_03_01.jpg?v=1769705063\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Keep model and data on same device\n",
    ">* Move model once, move each batch before forward\n",
    "\n",
    ">* Move every batch tensor onto the GPU\n",
    ">* Consistent device placement prevents bugs and slowdowns\n",
    "\n",
    ">* Minimize slow CPU–GPU transfers per batch\n",
    ">* Keep computation on GPU; return results only when needed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238ed1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Moving Data to CUDA\n",
    "\n",
    "# This script shows moving data to devices.\n",
    "# It uses TensorFlow to mimic CUDA placement.\n",
    "# Focus is on safe device aware training.\n",
    "\n",
    "# !pip install tensorflow-2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Import TensorFlow and check version.\n",
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Set deterministic random seeds everywhere.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Detect GPU availability for this session.\n",
    "physical_gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "use_gpu = bool(physical_gpus)\n",
    "print(\"GPU available:\", use_gpu)\n",
    "\n",
    "# Choose device string based on availability.\n",
    "if use_gpu:\n",
    "    device_name = \"/GPU:0\"\n",
    "else:\n",
    "    device_name = \"/CPU:0\"\n",
    "print(\"Using device:\", device_name)\n",
    "\n",
    "# Create a tiny synthetic dataset on CPU.\n",
    "num_samples = 64\n",
    "num_features = 8\n",
    "x_cpu = np.random.randn(num_samples, num_features).astype(\"float32\")\n",
    "\n",
    "# Create simple linear targets on CPU.\n",
    "true_w = np.arange(1, num_features + 1, dtype=\"float32\")\n",
    "y_cpu = x_cpu @ true_w + 0.5\n",
    "\n",
    "# Wrap numpy arrays as TensorFlow tensors.\n",
    "x_tensor = tf.convert_to_tensor(x_cpu)\n",
    "y_tensor = tf.convert_to_tensor(y_cpu)\n",
    "\n",
    "# Validate shapes before training begins.\n",
    "assert x_tensor.shape[0] == y_tensor.shape[0]\n",
    "assert x_tensor.shape[1] == num_features\n",
    "\n",
    "# Build a tiny model inside chosen device.\n",
    "with tf.device(device_name):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(num_features,)),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "# Compile model with mean squared error loss.\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "\n",
    "# Decide whether to use mixed precision.\n",
    "use_mixed_precision = use_gpu\n",
    "print(\"Using mixed precision:\", use_mixed_precision)\n",
    "\n",
    "# Configure mixed precision policy if enabled.\n",
    "if use_mixed_precision:\n",
    "    from tensorflow.keras import mixed_precision\n",
    "    policy = mixed_precision.Policy(\"mixed_float16\")\n",
    "    mixed_precision.set_global_policy(policy)\n",
    "\n",
    "# Create a small tf.data.Dataset from tensors.\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x_tensor, y_tensor))\n",
    "\n",
    "# Shuffle and batch the dataset safely.\n",
    "dataset = dataset.shuffle(buffer_size=num_samples, seed=seed_value)\n",
    "dataset = dataset.batch(16)\n",
    "\n",
    "# Define one manual training step with device placement.\n",
    "@tf.function\n",
    "def train_step(batch_x, batch_y):\n",
    "    with tf.device(device_name):\n",
    "        with tf.GradientTape() as tape:\n",
    "            preds = model(batch_x, training=True)\n",
    "            loss = tf.reduce_mean(tf.keras.losses.mse(batch_y, preds))\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        model.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "# Run a short training loop over few epochs.\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_losses = []\n",
    "    for batch_x, batch_y in dataset:\n",
    "        loss_value = train_step(batch_x, batch_y)\n",
    "        epoch_losses.append(float(loss_value.numpy()))\n",
    "    mean_loss = float(np.mean(epoch_losses))\n",
    "    print(\"Epoch\", epoch + 1, \"mean loss:\", round(mean_loss, 4))\n",
    "\n",
    "# Move a small batch explicitly to chosen device.\n",
    "first_batch = next(iter(dataset))\n",
    "with tf.device(device_name):\n",
    "    batch_x_device = tf.identity(first_batch[0])\n",
    "    batch_y_device = tf.identity(first_batch[1])\n",
    "\n",
    "# Show that tensors now live on the same device.\n",
    "print(\"Batch x device:\", batch_x_device.device)\n",
    "print(\"Batch y device:\", batch_y_device.device)\n",
    "print(\"Model variables device:\", model.trainable_variables[0].device if hasattr(model.trainable_variables[0], \"device\") else \"N/A\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd7a93b",
   "metadata": {},
   "source": [
    "### **3.2. Automatic Mixed Precision**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979e734f",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_03/Lecture_B/image_03_02.jpg?v=1769705165\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Runs many operations in faster low precision\n",
    ">* Keeps sensitive parts full precision, code unchanged\n",
    "\n",
    ">* AMP runs safe ops in full precision\n",
    ">* Other ops use half precision for speed, memory\n",
    "\n",
    ">* Monitor metrics; mixed precision can change convergence\n",
    ">* Keep sensitive parts full precision for reliability\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ded236b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Automatic Mixed Precision\n",
    "\n",
    "# This script shows TensorFlow automatic mixed precision usage.\n",
    "# It compares float32 and mixed precision training on MNIST.\n",
    "# Focus is on devices and safe performance improvements.\n",
    "\n",
    "# !pip install tensorflow.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Import TensorFlow and Keras utilities.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version and device information.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"GPU available:\", tf.config.list_physical_devices(\"GPU\") != [])\n",
    "\n",
    "# Load MNIST dataset from Keras datasets.\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Use a small subset for quick demonstration.\n",
    "x_train = x_train[:8000]\n",
    "y_train = y_train[:8000]\n",
    "x_test = x_test[:2000]\n",
    "y_test = y_test[:2000]\n",
    "\n",
    "# Normalize images to range [0,1].\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_test = x_test.astype(\"float32\") / 255.0\n",
    "\n",
    "# Add channel dimension for convolutional layers.\n",
    "x_train = np.expand_dims(x_train, axis=-1)\n",
    "x_test = np.expand_dims(x_test, axis=-1)\n",
    "\n",
    "# Validate shapes before building models.\n",
    "assert x_train.shape[1:] == (28, 28, 1)\n",
    "assert x_test.shape[1:] == (28, 28, 1)\n",
    "\n",
    "# Define a simple CNN model builder function.\n",
    "def build_model():\n",
    "    inputs = keras.Input(shape=(28, 28, 1))\n",
    "    x = layers.Conv2D(16, 3, activation=\"relu\")(inputs)\n",
    "    x = layers.MaxPooling2D(2)(x)\n",
    "    x = layers.Conv2D(32, 3, activation=\"relu\")(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(64, activation=\"relu\")(x)\n",
    "    outputs = layers.Dense(10, activation=\"softmax\")(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "# Create a float32 baseline model on default device.\n",
    "baseline_model = build_model()\n",
    "\n",
    "# Compile baseline model with standard settings.\n",
    "baseline_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Train baseline model quietly for few epochs.\n",
    "history_fp32 = baseline_model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    validation_split=0.1,\n",
    "    epochs=2,\n",
    "    batch_size=64,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Evaluate baseline model on test data.\n",
    "loss_fp32, acc_fp32 = baseline_model.evaluate(\n",
    "    x_test,\n",
    "    y_test,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Enable mixed precision policy if GPU supports it.\n",
    "if tf.config.list_physical_devices(\"GPU\"):\n",
    "    policy = tf.keras.mixed_precision.Policy(\"mixed_float16\")\n",
    "    tf.keras.mixed_precision.set_global_policy(policy)\n",
    "else:\n",
    "    policy = tf.keras.mixed_precision.Policy(\"float32\")\n",
    "    tf.keras.mixed_precision.set_global_policy(policy)\n",
    "\n",
    "# Build a new model under the current policy.\n",
    "mp_model = build_model()\n",
    "\n",
    "# Use an optimizer wrapped for mixed precision.\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "optimizer = tf.keras.mixed_precision.LossScaleOptimizer(optimizer)\n",
    "\n",
    "# Compile mixed precision model with same loss and metrics.\n",
    "mp_model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Train mixed precision model quietly for few epochs.\n",
    "history_mp = mp_model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    validation_split=0.1,\n",
    "    epochs=2,\n",
    "    batch_size=64,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Evaluate mixed precision model on test data.\n",
    "loss_mp, acc_mp = mp_model.evaluate(\n",
    "    x_test,\n",
    "    y_test,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Print concise comparison of results.\n",
    "print(\"Baseline float32 test accuracy:\", round(acc_fp32, 4))\n",
    "print(\"Mixed precision test accuracy:\", round(acc_mp, 4))\n",
    "print(\"Baseline float32 test loss:\", round(loss_fp32, 4))\n",
    "print(\"Mixed precision test loss:\", round(loss_mp, 4))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a757f3",
   "metadata": {},
   "source": [
    "### **3.3. Stable GradScaler Usage**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310e9750",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_03/Lecture_B/image_03_03.jpg?v=1769705247\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Gradient scaler rescales loss to protect small gradients\n",
    ">* Prevents underflow in half precision, keeping training stable\n",
    "\n",
    ">* Scaler detects bad gradients and adjusts scale\n",
    ">* Defaults usually give stable, efficient mixed-precision training\n",
    "\n",
    ">* Integrate scaler consistently; treat skipped steps normally\n",
    ">* Track skip frequency to tune hyperparameters and stability\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc8f793",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Stable GradScaler Usage\n",
    "\n",
    "# This script shows stable GradScaler usage.\n",
    "# We simulate mixed precision training with TensorFlow.\n",
    "# Focus on safe scaling and clear device placement.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Import TensorFlow and Keras components.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Choose device string based on GPU availability.\n",
    "physical_gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "use_gpu = bool(physical_gpus)\n",
    "device_name = \"/GPU:0\" if use_gpu else \"/CPU:0\"\n",
    "\n",
    "# Enable mixed precision policy when GPU is available.\n",
    "if use_gpu:\n",
    "    from tensorflow.keras import mixed_precision\n",
    "    policy = mixed_precision.Policy(\"mixed_float16\")\n",
    "    mixed_precision.set_global_policy(policy)\n",
    "\n",
    "# Create a tiny synthetic regression dataset.\n",
    "num_samples = 256\n",
    "num_features = 8\n",
    "x_data = np.random.randn(num_samples, num_features).astype(\"float32\")\n",
    "\n",
    "# Create targets with a simple linear relationship.\n",
    "true_w = np.arange(1, num_features + 1, dtype=\"float32\")\n",
    "true_b = 0.5\n",
    "y_data = x_data @ true_w + true_b\n",
    "\n",
    "# Add small noise for realism.\n",
    "noise = 0.1 * np.random.randn(num_samples).astype(\"float32\")\n",
    "y_data = y_data + noise\n",
    "\n",
    "# Validate shapes before building dataset.\n",
    "assert x_data.shape == (num_samples, num_features)\n",
    "assert y_data.shape == (num_samples,)\n",
    "\n",
    "# Build a small tf.data.Dataset for iteration.\n",
    "batch_size = 32\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x_data, y_data))\n",
    "dataset = dataset.shuffle(num_samples, seed=seed_value).batch(batch_size)\n",
    "\n",
    "# Define a simple sequential regression model.\n",
    "with tf.device(device_name):\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(16, activation=\"relu\", input_shape=(num_features,)),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "# Choose optimizer and loss function.\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
    "loss_fn = keras.losses.MeanSquaredError()\n",
    "\n",
    "# Create a gradient scaler only when using mixed precision.\n",
    "if use_gpu:\n",
    "    scaler = mixed_precision.LossScaleOptimizer(optimizer)\n",
    "else:\n",
    "    scaler = None\n",
    "\n",
    "# Define one training step with optional scaling.\n",
    "@tf.function\n",
    "def train_step(inputs, targets):\n",
    "    # Ensure shapes are as expected.\n",
    "    tf.debugging.assert_shapes([(inputs, (None, num_features)), (targets, (None,))])\n",
    "\n",
    "    # Use mixed precision path when scaler exists.\n",
    "    if scaler is not None:\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = model(inputs, training=True)\n",
    "            loss = loss_fn(targets, tf.squeeze(predictions))\n",
    "            scaled_loss = scaler.get_scaled_loss(loss)\n",
    "        scaled_grads = tape.gradient(scaled_loss, model.trainable_variables)\n",
    "        grads = scaler.get_unscaled_gradients(scaled_grads)\n",
    "        scaler.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        return loss\n",
    "    else:\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = model(inputs, training=True)\n",
    "            loss = loss_fn(targets, tf.squeeze(predictions))\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        return loss\n",
    "\n",
    "# Define a simple evaluation step without gradient tracking.\n",
    "@tf.function\n",
    "def eval_step(inputs, targets):\n",
    "    predictions = model(inputs, training=False)\n",
    "    loss = loss_fn(targets, tf.squeeze(predictions))\n",
    "    return loss\n",
    "\n",
    "# Run a short training loop with clear logging.\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_losses = []\n",
    "    for batch_inputs, batch_targets in dataset:\n",
    "        loss_value = train_step(batch_inputs, batch_targets)\n",
    "        epoch_losses.append(loss_value)\n",
    "\n",
    "    # Compute mean loss for the epoch.\n",
    "    mean_loss = tf.reduce_mean(epoch_losses)\n",
    "\n",
    "    # Run evaluation on the full dataset once per epoch.\n",
    "    eval_loss = eval_step(x_data, y_data)\n",
    "\n",
    "    # Print concise information about training stability.\n",
    "    print(\n",
    "        f\"Epoch {epoch + 1}: train_loss={mean_loss:.4f}, eval_loss={eval_loss:.4f}\"\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471796ab",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Training Loop Design**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2327ff",
   "metadata": {},
   "source": [
    "\n",
    "In this lecture, you learned to:\n",
    "- Implement a standard PyTorch training loop that iterates over DataLoader batches and updates model parameters. \n",
    "- Add evaluation and metric computation to the training workflow without leaking gradients or mixing modes. \n",
    "- Handle device placement and optional mixed precision to improve performance while maintaining correctness. \n",
    "\n",
    "In the next Module (Module 4), we will go over 'Data and Dataloaders'"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
