{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a05dd84",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Scaling Practices**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9d0fd3",
   "metadata": {},
   "source": [
    ">Last update: 20260130.\n",
    "    \n",
    "By the end of this Lecture, you will be able to:\n",
    "- Apply gradient accumulation and effective batch size strategies in distributed training setups. \n",
    "- Combine mixed precision with DDP to improve throughput while maintaining convergence. \n",
    "- Implement basic logging and checkpointing that work correctly in multi‑process environments. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088955aa",
   "metadata": {},
   "source": [
    "## **1. Scaling Batch Size**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6443fb1d",
   "metadata": {},
   "source": [
    "### **1.1. Effective batch size math**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21213ae6",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_08/Lecture_B/image_01_01.jpg?v=1769770220\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Effective batch size counts samples per optimizer update\n",
    ">* It controls optimization behavior and learning rate tuning\n",
    "\n",
    ">* Multiple GPUs combine local batches into one\n",
    ">* Gradient accumulation further increases effective batch size\n",
    "\n",
    ">* Match target recipes via effective batch size\n",
    ">* Tune devices, local batches, accumulation to scale\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80ca831",
   "metadata": {},
   "source": [
    "### **1.2. Accumulation Loop Patterns**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c56cd2",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_08/Lecture_B/image_01_02.jpg?v=1769770244\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Accumulate gradients over several mini batches\n",
    ">* Step optimizer after window to mimic larger batches\n",
    "\n",
    ">* Align accumulation with cross device gradient averaging\n",
    ">* All processes accumulate equally before synchronized optimizer step\n",
    "\n",
    ">* Adapt accumulation to tokens, memory, sequence variability\n",
    ">* Keep a shared, deterministic rule across processes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08605945",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Accumulation Loop Patterns\n",
    "\n",
    "# This script shows gradient accumulation patterns.\n",
    "# We simulate effective batch size scaling behavior.\n",
    "# Focus is on simple loop logic demonstration.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Import tensorflow and check version.\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set deterministic random seeds everywhere.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print tensorflow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Detect device type for information only.\n",
    "physical_gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "use_gpu = bool(physical_gpus)\n",
    "device_type = \"GPU\" if use_gpu else \"CPU\"\n",
    "\n",
    "# Print detected device type briefly.\n",
    "print(\"Using device type:\", device_type)\n",
    "\n",
    "# Create a tiny synthetic regression dataset.\n",
    "num_samples = 64\n",
    "num_features = 10\n",
    "x_data = np.random.randn(num_samples, num_features).astype(\"float32\")\n",
    "\n",
    "# Create targets with a simple linear relationship.\n",
    "true_w = np.arange(1, num_features + 1, dtype=\"float32\")\n",
    "y_data = x_data @ true_w + 0.1\n",
    "\n",
    "# Wrap data into a tf.data.Dataset pipeline.\n",
    "base_batch_size = 4\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_data, y_data))\n",
    "train_ds = train_ds.shuffle(num_samples, seed=seed_value)\n",
    "\n",
    "# Batch the dataset with a small per device batch.\n",
    "train_ds = train_ds.batch(base_batch_size, drop_remainder=True)\n",
    "\n",
    "# Build a tiny linear regression model.\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(1, input_shape=(num_features,))\n",
    "])\n",
    "\n",
    "# Create an optimizer with a small learning rate.\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "# Define a simple mean squared error loss.\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "# Simulate two devices for effective batch reasoning.\n",
    "world_size = 2\n",
    "per_device_batch = base_batch_size\n",
    "\n",
    "# Choose accumulation steps for the experiment.\n",
    "accum_steps = 4\n",
    "\n",
    "# Compute effective batch size for explanation.\n",
    "effective_batch = per_device_batch * world_size * accum_steps\n",
    "\n",
    "# Print the effective batch size information.\n",
    "print(\"Per device batch:\", per_device_batch)\n",
    "print(\"World size used:\", world_size)\n",
    "print(\"Accumulation steps:\", accum_steps)\n",
    "print(\"Effective batch size:\", effective_batch)\n",
    "\n",
    "# Prepare variables for gradient accumulation.\n",
    "accum_grads = [tf.zeros_like(v) for v in model.trainable_variables]\n",
    "step_in_accum = 0\n",
    "epochs = 1\n",
    "\n",
    "# Training loop with manual accumulation pattern.\n",
    "for epoch in range(epochs):\n",
    "    for step, (x_batch, y_batch) in enumerate(train_ds):\n",
    "\n",
    "        # Validate batch shapes before using them.\n",
    "        assert x_batch.shape[0] == per_device_batch\n",
    "        assert x_batch.shape[1] == num_features\n",
    "\n",
    "        # Record operations for automatic differentiation.\n",
    "        with tf.GradientTape() as tape:\n",
    "            preds = model(x_batch, training=True)\n",
    "            loss_value = loss_fn(y_batch, tf.squeeze(preds))\n",
    "\n",
    "        # Compute gradients for current mini batch.\n",
    "        grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "\n",
    "        # Scale gradients by accumulation steps factor.\n",
    "        scaled_grads = [g / float(accum_steps) for g in grads]\n",
    "\n",
    "        # Accumulate scaled gradients into buffers.\n",
    "        accum_grads = [ag + sg for ag, sg in zip(accum_grads, scaled_grads)]\n",
    "        step_in_accum += 1\n",
    "\n",
    "        # Simulate all processes reaching same accumulation count.\n",
    "        if step_in_accum == accum_steps:\n",
    "\n",
    "            # Apply optimizer step using accumulated gradients.\n",
    "            optimizer.apply_gradients(zip(accum_grads, model.trainable_variables))\n",
    "\n",
    "            # Reset accumulation buffers to zero tensors.\n",
    "            accum_grads = [tf.zeros_like(v) for v in model.trainable_variables]\n",
    "            step_in_accum = 0\n",
    "\n",
    "# Evaluate model loss on full dataset once.\n",
    "full_preds = model(x_data, training=False)\n",
    "final_loss = loss_fn(y_data, tf.squeeze(full_preds)).numpy()\n",
    "\n",
    "# Print final loss and confirm accumulation pattern.\n",
    "print(\"Final training loss with accumulation:\", round(float(final_loss), 4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b206ee",
   "metadata": {},
   "source": [
    "### **1.3. Learning Rate Scaling**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df930bf2",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_08/Lecture_B/image_01_03.jpg?v=1769770278\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Changing batch size changes optimization and stability\n",
    ">* Larger batches need retuned learning rates for convergence\n",
    "\n",
    ">* Use batch-size scaling rules as flexible guesses\n",
    ">* Run short tests, watch metrics, adjust conservatively\n",
    "\n",
    ">* Combine scaled LR, warmup, and decay schedules\n",
    ">* Monitor metrics to keep training stable, generalizable\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ea4d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Learning Rate Scaling\n",
    "\n",
    "# This script illustrates learning rate scaling.\n",
    "# We compare small and large effective batch sizes.\n",
    "# Focus is on simple gradient descent behavior.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Import tensorflow and check version.\n",
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Set deterministic random seeds everywhere.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Define a simple quadratic loss function.\n",
    "def loss_fn(x):\n",
    "    return (x - 3.0) ** 2\n",
    "\n",
    "# Compute gradient of the loss analytically.\n",
    "def grad_fn(x):\n",
    "    return 2.0 * (x - 3.0)\n",
    "\n",
    "# Create synthetic \"batch\" of target values.\n",
    "true_targets = np.full(shape=(32,), fill_value=3.0)\n",
    "\n",
    "# Validate the batch shape before training.\n",
    "assert true_targets.shape[0] == 32\n",
    "assert true_targets.ndim == 1\n",
    "\n",
    "# Helper function to run simple gradient descent.\n",
    "def run_gd(initial_x, lr, batch_size, steps):\n",
    "    x = tf.Variable(initial_x, dtype=tf.float32)\n",
    "    history = []\n",
    "    for step in range(steps):\n",
    "        # Select a mini batch slice.\n",
    "        start = (step * batch_size) % 32\n",
    "        end = start + batch_size\n",
    "        batch = true_targets[start:end]\n",
    "\n",
    "        # Compute mean loss over the batch.\n",
    "        batch_mean = np.mean(batch)\n",
    "        loss = loss_fn(x - (batch_mean - 3.0))\n",
    "\n",
    "        # Compute gradient and update parameter.\n",
    "        grad = grad_fn(x)\n",
    "        x.assign_sub(lr * grad)\n",
    "        history.append(float(loss.numpy()))\n",
    "    return float(x.numpy()), history\n",
    "\n",
    "# Define base configuration for small batch.\n",
    "initial_x = 0.0\n",
    "small_batch = 4\n",
    "large_batch = 16\n",
    "steps = 20\n",
    "\n",
    "# Base learning rate for small batch size.\n",
    "base_lr = 0.05\n",
    "scaled_lr = base_lr * (large_batch / small_batch)\n",
    "\n",
    "# Run training with small batch and base learning rate.\n",
    "small_x_final, small_hist = run_gd(\n",
    "    initial_x=initial_x,\n",
    "    lr=base_lr,\n",
    "    batch_size=small_batch,\n",
    "    steps=steps,\n",
    ")\n",
    "\n",
    "# Run training with large batch and scaled learning rate.\n",
    "large_x_final, large_hist = run_gd(\n",
    "    initial_x=initial_x,\n",
    "    lr=scaled_lr,\n",
    "    batch_size=large_batch,\n",
    "    steps=steps,\n",
    ")\n",
    "\n",
    "# Print concise comparison of configurations.\n",
    "print(\"Small batch:\", small_batch, \"LR:\", base_lr)\n",
    "print(\"Large batch:\", large_batch, \"LR:\", scaled_lr)\n",
    "print(\"Final x small batch:\", round(small_x_final, 4))\n",
    "print(\"Final x large batch:\", round(large_x_final, 4))\n",
    "\n",
    "# Show first and last losses for both runs.\n",
    "print(\"Small batch loss start:\", round(small_hist[0], 4))\n",
    "print(\"Small batch loss end:\", round(small_hist[-1], 4))\n",
    "print(\"Large batch loss start:\", round(large_hist[0], 4))\n",
    "print(\"Large batch loss end:\", round(large_hist[-1], 4))\n",
    "\n",
    "# Confirm that both settings converge near the optimum.\n",
    "print(\"Both settings approach x = 3 with scaling.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e9a138",
   "metadata": {},
   "source": [
    "## **2. Mixed Precision with DDP**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08febb7e",
   "metadata": {},
   "source": [
    "### **2.1. Autocast with DDP**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2b3fc7",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_08/Lecture_B/image_02_01.jpg?v=1769770345\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Autocast automatically chooses safe mixed precision dtypes\n",
    ">* Ensures consistent computations across processes for stable training\n",
    "\n",
    ">* Each process runs autocast independently yet identically\n",
    ">* Mixed precision boosts speed while keeping gradients consistent\n",
    "\n",
    ">* Keep inputs, outputs, and loss mostly full precision\n",
    ">* Use autocast for faster compute while preserving gradients\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bca781",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Autocast with DDP\n",
    "\n",
    "# This script shows autocast with DDP style concepts.\n",
    "# We simulate distributed mixed precision using simple tensors.\n",
    "# Focus is on understanding autocast behavior not performance.\n",
    "\n",
    "# Required PyTorch install for mixed precision and DDP examples.\n",
    "# Uncomment next line if running outside prepared environments.\n",
    "# pip install torch torchvision torchaudio.\n",
    "\n",
    "# Import standard libraries for reproducibility and environment checks.\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "\n",
    "# Import torch for tensors, autocast, and device handling.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Set deterministic seeds for reproducible behavior across runs.\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Select GPU if available else fall back to CPU device.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Print torch version and selected device in one short line.\n",
    "print(\"Torch version:\", torch.__version__, \"Device:\", device)\n",
    "\n",
    "# Define a tiny model that mimics a DDP replica locally.\n",
    "class TinyModel(nn.Module):\n",
    "\n",
    "    # Initialize a simple two layer network for demonstration.\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    # Forward pass uses standard layers without manual casting.\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Create two model replicas to mimic two DDP processes.\n",
    "model_rank0 = TinyModel(input_dim=4, hidden_dim=8, output_dim=2).to(device)\n",
    "model_rank1 = TinyModel(input_dim=4, hidden_dim=8, output_dim=2).to(device)\n",
    "\n",
    "# Ensure both replicas start from identical initial parameters.\n",
    "model_rank1.load_state_dict(model_rank0.state_dict())\n",
    "\n",
    "# Create a simple optimizer for each model replica.\n",
    "optimizer_rank0 = optim.SGD(model_rank0.parameters(), lr=0.1)\n",
    "optimizer_rank1 = optim.SGD(model_rank1.parameters(), lr=0.1)\n",
    "\n",
    "# Create a tiny batch of inputs and integer labels.\n",
    "inputs = torch.randn(2, 4, device=device)\n",
    "labels = torch.tensor([0, 1], device=device)\n",
    "\n",
    "# Validate shapes to avoid silent broadcasting mistakes.\n",
    "assert inputs.shape == (2, 4)\n",
    "assert labels.shape == (2,)\n",
    "\n",
    "# Define a standard cross entropy loss in full precision.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Create a gradient scaler for stable mixed precision training.\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n",
    "\n",
    "# Function to run one mixed precision step for a given replica.\n",
    "def mixed_precision_step(model, optimizer, rank_name: str):\n",
    "    # Zero gradients before computing new ones for this step.\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Enter autocast context to run safe ops in lower precision.\n",
    "    with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "    # Scale loss then backpropagate scaled gradients safely.\n",
    "    scaler.scale(loss).backward()\n",
    "\n",
    "    # Unscale gradients and perform optimizer step conditionally.\n",
    "    scaler.step(optimizer)\n",
    "\n",
    "    # Update scaler for next iteration to track stability.\n",
    "    scaler.update()\n",
    "\n",
    "    # Return detached loss value for simple comparison printing.\n",
    "    return loss.detach().item()\n",
    "\n",
    "# Run one mixed precision step on each replica independently.\n",
    "loss_rank0 = mixed_precision_step(model_rank0, optimizer_rank0, \"rank0\")\n",
    "loss_rank1 = mixed_precision_step(model_rank1, optimizer_rank1, \"rank1\")\n",
    "\n",
    "# Collect parameters from both replicas for numerical comparison.\n",
    "params_rank0 = torch.cat([p.detach().flatten() for p in model_rank0.parameters()])\n",
    "params_rank1 = torch.cat([p.detach().flatten() for p in model_rank1.parameters()])\n",
    "\n",
    "# Compute maximum absolute difference between replica parameters.\n",
    "max_param_diff = (params_rank0 - params_rank1).abs().max().item()\n",
    "\n",
    "# Print a few concise lines summarizing mixed precision behavior.\n",
    "print(\"Loss rank0 after autocast step:\", round(loss_rank0, 4))\n",
    "print(\"Loss rank1 after autocast step:\", round(loss_rank1, 4))\n",
    "print(\"Max parameter difference between replicas:\", max_param_diff)\n",
    "print(\"Autocast decisions stayed consistent across replicas.\")\n",
    "print(\"This mirrors how DDP keeps gradients aligned.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05be13f5",
   "metadata": {},
   "source": [
    "### **2.2. Per Process GradScaler**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d95501",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_08/Lecture_B/image_02_02.jpg?v=1769770426\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Each GPU keeps its own gradient scaler\n",
    ">* Local scalers adapt to different numerical conditions\n",
    "\n",
    ">* Scaler adjusts up or down based on overflow\n",
    ">* Each GPU scales independently for stable training\n",
    "\n",
    ">* Gradients are scaled locally, synchronized unscaled across GPUs\n",
    ">* Keeps mixed precision local, DDP focuses on aggregation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d79b8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Per Process GradScaler\n",
    "\n",
    "# This script shows per process gradient scaling conceptually.\n",
    "# We simulate two processes each with its own GradScaler.\n",
    "# Focus is on logic not real distributed training.\n",
    "\n",
    "# Example pip install for torch if needed.\n",
    "# !pip install torch torchvision torchaudio.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "\n",
    "# Set deterministic random seed.\n",
    "random.seed(42)\n",
    "\n",
    "# Define a simple GradScaler like helper.\n",
    "class SimpleGradScaler:\n",
    "    # Initialize with starting scale value.\n",
    "    def __init__(self, init_scale=2.0):\n",
    "        self.scale = float(init_scale)\n",
    "\n",
    "    # Scale a scalar loss value.\n",
    "    def scale_loss(self, loss_value):\n",
    "        return loss_value * self.scale\n",
    "\n",
    "    # Unscale a gradient value.\n",
    "    def unscale_grad(self, grad_value):\n",
    "        return grad_value / self.scale\n",
    "\n",
    "    # Check for overflow and update scale.\n",
    "    def update(self, overflow):\n",
    "        if overflow:\n",
    "            self.scale = max(self.scale / 2.0, 0.125)\n",
    "        else:\n",
    "            self.scale = min(self.scale * 2.0, 128.0)\n",
    "\n",
    "\n",
    "# Simulate one training step on a single process.\n",
    "def simulate_step(process_id, scaler, base_loss):\n",
    "    # Add small noise to base loss per process.\n",
    "    noise = (random.random() - 0.5) * 0.2\n",
    "    local_loss = max(base_loss + noise, 0.01)\n",
    "\n",
    "    # Scale the loss before backward.\n",
    "    scaled_loss = scaler.scale_loss(local_loss)\n",
    "\n",
    "    # Pretend gradient magnitude equals scaled loss.\n",
    "    fake_grad = scaled_loss\n",
    "\n",
    "    # Detect overflow if gradient too large.\n",
    "    overflow = fake_grad > 10.0\n",
    "\n",
    "    # Unscale gradient before all reduce.\n",
    "    unscaled_grad = scaler.unscale_grad(fake_grad)\n",
    "\n",
    "    # Update scaler based on overflow flag.\n",
    "    scaler.update(overflow)\n",
    "\n",
    "    # Return summary for printing.\n",
    "    return {\n",
    "        \"pid\": process_id,\n",
    "        \"loss\": round(local_loss, 4),\n",
    "        \"scaled\": round(scaled_loss, 4),\n",
    "        \"unscaled\": round(unscaled_grad, 4),\n",
    "        \"overflow\": overflow,\n",
    "        \"scale\": round(scaler.scale, 4),\n",
    "    }\n",
    "\n",
    "# Create two independent scalers like two DDP processes.\n",
    "scaler_rank0 = SimpleGradScaler(init_scale=2.0)\n",
    "scaler_rank1 = SimpleGradScaler(init_scale=8.0)\n",
    "\n",
    "# Base loss shared conceptually across processes.\n",
    "base_loss_value = 1.5\n",
    "\n",
    "# Run a few simulated steps to see divergence.\n",
    "results = []\n",
    "for step in range(3):\n",
    "    res0 = simulate_step(0, scaler_rank0, base_loss_value)\n",
    "    res1 = simulate_step(1, scaler_rank1, base_loss_value)\n",
    "    results.append((step, res0, res1))\n",
    "\n",
    "# Print short header explaining columns.\n",
    "print(\"step pid loss scaled unscaled overflow scale\")\n",
    "\n",
    "# Print one summary line per process per step.\n",
    "for step, r0, r1 in results:\n",
    "    print(step, r0[\"pid\"], r0[\"loss\"], r0[\"scaled\"], r0[\"unscaled\"], r0[\"overflow\"], r0[\"scale\"])\n",
    "    print(step, r1[\"pid\"], r1[\"loss\"], r1[\"scaled\"], r1[\"unscaled\"], r1[\"overflow\"], r1[\"scale\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f984ccd",
   "metadata": {},
   "source": [
    "### **2.3. Handling Overflow Errors**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0c6f90",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_08/Lecture_B/image_02_03.jpg?v=1769770490\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Mixed precision increases risk of gradient overflow\n",
    ">* Overflows can spread across processes, destabilizing training\n",
    "\n",
    ">* Gradient scaling detects overflow and safely skips updates\n",
    ">* All DDP processes must agree on overflow handling\n",
    "\n",
    ">* Monitor overflows, scaler behavior, and training logs\n",
    ">* Fix chronic overflows by tuning hyperparameters and synchronization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d67444",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Handling Overflow Errors\n",
    "\n",
    "# This script shows overflow handling conceptually.\n",
    "# We simulate mixed precision overflow detection behavior.\n",
    "# Focus is on clear beginner friendly printed output.\n",
    "\n",
    "# Required external installs would be placed here.\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import standard libraries for math and typing.\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Set deterministic random seed for reproducibility.\n",
    "random.seed(42)\n",
    "\n",
    "# Define a simple class to mimic gradient scaler.\n",
    "class SimpleGradScaler:\n",
    "\n",
    "    # Initialize with starting scale and growth factor.\n",
    "    def __init__(self, init_scale=1024.0, growth=2.0):\n",
    "        self.scale = float(init_scale)\n",
    "        self.growth = float(growth)\n",
    "\n",
    "    # Scale the loss value before backward pass.\n",
    "    def scale_loss(self, loss_value):\n",
    "        return loss_value * self.scale\n",
    "\n",
    "    # Check gradients for overflow and update scale.\n",
    "    def step(self, grads):\n",
    "        has_overflow = any(\n",
    "            (math.isinf(g) or math.isnan(g)) for g in grads\n",
    "        )\n",
    "        if has_overflow:\n",
    "            self.scale = max(self.scale / self.growth, 1.0)\n",
    "            return False\n",
    "        self.scale = self.scale * self.growth\n",
    "        return True\n",
    "\n",
    "# Simulate two processes like two DDP workers.\n",
    "num_processes = 2\n",
    "\n",
    "# Create one scaler instance per simulated process.\n",
    "scalers = [SimpleGradScaler() for _ in range(num_processes)]\n",
    "\n",
    "# Define a helper to simulate gradient computation safely.\n",
    "def simulate_gradients(step_index, process_index):\n",
    "    base_grad = 0.001 * (step_index + 1)\n",
    "    noise = 0.0001 * (process_index + 1)\n",
    "    grad = base_grad + noise\n",
    "    if step_index == 2 and process_index == 1:\n",
    "        return [float('inf'), grad]\n",
    "    return [grad, grad * 1.5]\n",
    "\n",
    "# Validate gradient list shapes before using them.\n",
    "for p in range(num_processes):\n",
    "    test_grads = simulate_gradients(0, p)\n",
    "    assert isinstance(test_grads, list)\n",
    "    assert len(test_grads) == 2\n",
    "\n",
    "# Run a short loop to show overflow handling.\n",
    "max_steps = 5\n",
    "\n",
    "# Print a short header explaining the columns.\n",
    "print(\"step, scaled_loss_p0, scaled_loss_p1, update_applied\")\n",
    "\n",
    "# Iterate over training steps and simulate behavior.\n",
    "for step in range(max_steps):\n",
    "    losses = [0.1 * (step + 1) for _ in range(num_processes)]\n",
    "    scaled_losses = []\n",
    "    for p in range(num_processes):\n",
    "        scaled = scalers[p].scale_loss(losses[p])\n",
    "        scaled_losses.append(scaled)\n",
    "\n",
    "    grads_per_process = []\n",
    "    for p in range(num_processes):\n",
    "        grads = simulate_gradients(step, p)\n",
    "        grads_per_process.append(grads)\n",
    "\n",
    "    local_can_step = []\n",
    "    for p in range(num_processes):\n",
    "        can_step = scalers[p].step(grads_per_process[p])\n",
    "        local_can_step.append(can_step)\n",
    "\n",
    "    global_can_step = all(local_can_step)\n",
    "\n",
    "    if global_can_step:\n",
    "        decision_text = \"update\"\n",
    "    else:\n",
    "        decision_text = \"skip\"\n",
    "\n",
    "    print(\n",
    "        f\"{step}, {scaled_losses[0]:.1f}, {scaled_losses[1]:.1f}, {decision_text}\"\n",
    "    )\n",
    "\n",
    "# Print final scales to show adaptation behavior.\n",
    "print(f\"final_scale_p0={scalers[0].scale:.1f}, final_scale_p1={scalers[1].scale:.1f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc370af",
   "metadata": {},
   "source": [
    "## **3. Robust Distributed Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a947bd9",
   "metadata": {},
   "source": [
    "### **3.1. Rank Aware Logging**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72623a1d",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_08/Lecture_B/image_03_01.jpg?v=1769770562\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Attach rank and role to every log\n",
    ">* Limit which ranks emit which message types\n",
    "\n",
    ">* Control verbosity by splitting logging responsibilities across ranks\n",
    ">* Primary logs high level progress; others detailed diagnostics\n",
    "\n",
    ">* Rank tagged logs expose subtle distributed failures clearly\n",
    ">* Standardized formats enable correlation, debugging, and recovery\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a7bfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Rank Aware Logging\n",
    "\n",
    "# This script demonstrates rank aware logging basics.\n",
    "# We simulate distributed ranks using simple integer identifiers.\n",
    "# Focus on clear concise logs from different simulated ranks.\n",
    "\n",
    "# Required external installs would be placed here if needed.\n",
    "# No extra libraries are required for this simple example.\n",
    "\n",
    "# Import standard modules for time and typing support.\n",
    "import os\n",
    "import time\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "# Define a small helper to format timestamps clearly.\n",
    "def current_time_str() -> str:\n",
    "    return time.strftime(\"%H:%M:%S\", time.localtime())\n",
    "\n",
    "\n",
    "# Create a simple rank aware logger class.\n",
    "class RankLogger:\n",
    "    def __init__(self, global_rank: int, world_size: int,\n",
    "                 is_primary: bool) -> None:\n",
    "        self.global_rank = global_rank\n",
    "        self.world_size = world_size\n",
    "        self.is_primary = is_primary\n",
    "\n",
    "    # Build a short prefix including rank and role.\n",
    "    def _prefix(self) -> str:\n",
    "        role = \"PRIMARY\" if self.is_primary else \"WORKER\"\n",
    "        return (f\"[{current_time_str()}] \"\n",
    "                f\"[rank {self.global_rank}/{self.world_size}] \"\n",
    "                f\"[{role}]\")\n",
    "\n",
    "    # Log messages that only primary should emit.\n",
    "    def log_primary(self, message: str) -> None:\n",
    "        if not self.is_primary:\n",
    "            return\n",
    "        print(f\"{self._prefix()} {message}\")\n",
    "\n",
    "    # Log messages that any rank may emit.\n",
    "    def log_rank(self, message: str) -> None:\n",
    "        print(f\"{self._prefix()} {message}\")\n",
    "\n",
    "\n",
    "# Simulate a tiny distributed training loop.\n",
    "def simulated_train_step(rank: int, step: int) -> float:\n",
    "    base_loss = 1.0\n",
    "    loss = base_loss / (step + 1)\n",
    "    if loss <= 0.0:\n",
    "        raise ValueError(\"Loss must stay positive in this demo.\")\n",
    "    return loss + 0.01 * rank\n",
    "\n",
    "\n",
    "# Run a short demonstration with three simulated ranks.\n",
    "def run_demo(world_size: int = 3, steps: int = 3) -> None:\n",
    "    if world_size <= 0:\n",
    "        raise ValueError(\"World size must be positive.\")\n",
    "    if steps <= 0:\n",
    "        raise ValueError(\"Steps must be positive.\")\n",
    "\n",
    "    # Create loggers for each simulated rank.\n",
    "    loggers = []\n",
    "    for rank in range(world_size):\n",
    "        is_primary = rank == 0\n",
    "        logger = RankLogger(global_rank=rank,\n",
    "                            world_size=world_size,\n",
    "                            is_primary=is_primary)\n",
    "        loggers.append(logger)\n",
    "\n",
    "    # Primary announces the start of training.\n",
    "    loggers[0].log_primary(\"Starting simulated distributed training run.\")\n",
    "\n",
    "    # Each rank performs a few training steps.\n",
    "    for step in range(steps):\n",
    "        for rank, logger in enumerate(loggers):\n",
    "            loss = simulated_train_step(rank=rank, step=step)\n",
    "            if logger.is_primary:\n",
    "                logger.log_primary(\n",
    "                    f\"Step {step} summary loss={loss:.3f}.\")\n",
    "            else:\n",
    "                logger.log_rank(\n",
    "                    f\"Step {step} local loss={loss:.3f}.\")\n",
    "\n",
    "    # Primary prints a short checkpoint style message.\n",
    "    loggers[0].log_primary(\"Saving checkpoint at final step.\")\n",
    "\n",
    "\n",
    "# Execute the demonstration when running this script.\n",
    "run_demo(world_size=3, steps=2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095e49d4",
   "metadata": {},
   "source": [
    "### **3.2. Distributed Checkpoint Design**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0e45e8",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_08/Lecture_B/image_03_02.jpg?v=1769770647\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Centralized checkpoints capture full, consistent training state\n",
    ">* Enable seamless resume despite failures or hardware differences\n",
    "\n",
    ">* Distributed checkpoints may be sharded across ranks\n",
    ">* Choose sharding or centralization based on scale\n",
    "\n",
    ">* Plan checkpoint timing, naming, and global coordination\n",
    ">* Use atomic writes to avoid corruption, enable recovery\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6abda09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Distributed Checkpoint Design\n",
    "\n",
    "# This script shows simple distributed style checkpointing.\n",
    "# We simulate two workers and a coordinator process.\n",
    "# Focus is on safe logging and checkpoint saving.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import standard libraries for paths and randomness.\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "\n",
    "# Set deterministic random seeds for reproducibility.\n",
    "random.seed(42)\n",
    "os.environ[\"PYTHONHASHSEED\"] = \"42\"\n",
    "\n",
    "# Define a tiny training state dictionary structure.\n",
    "training_state = {\n",
    "    \"global_step\": 0,\n",
    "    \"epoch\": 0,\n",
    "}\n",
    "\n",
    "# Define a tiny model state dictionary structure.\n",
    "model_state = {\n",
    "    \"weight\": 0.0,\n",
    "    \"bias\": 0.0,\n",
    "}\n",
    "\n",
    "# Define a tiny optimizer state dictionary structure.\n",
    "optimizer_state = {\n",
    "    \"learning_rate\": 0.1,\n",
    "}\n",
    "\n",
    "# Create a directory for checkpoints if missing.\n",
    "CHECKPOINT_DIR = \"checkpoints_demo\"\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "# Define a helper to build rank specific file names.\n",
    "def rank_file_name(rank, suffix):\n",
    "    return os.path.join(CHECKPOINT_DIR, f\"rank{rank}_{suffix}.json\")\n",
    "\n",
    "\n",
    "# Define a helper to build the coordinator index name.\n",
    "INDEX_FILE = os.path.join(CHECKPOINT_DIR, \"checkpoint_index.json\")\n",
    "\n",
    "# Simulate a worker that owns a shard of model state.\n",
    "def simulate_worker_step(rank, step):\n",
    "    shard = {\n",
    "        \"rank\": rank,\n",
    "        \"local_step\": step,\n",
    "        \"weight_shard\": model_state[\"weight\"] + rank,\n",
    "    }\n",
    "    return shard\n",
    "\n",
    "# Safely write a json file using a temporary name.\n",
    "def safe_write_json(path, data):\n",
    "    tmp_path = path + \".tmp\"\n",
    "    with open(tmp_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f)\n",
    "    os.replace(tmp_path, path)\n",
    "\n",
    "# Simulate all workers saving local shards to disk.\n",
    "def save_sharded_checkpoint(global_step, world_size):\n",
    "    shards = []\n",
    "    for rank in range(world_size):\n",
    "        shard_state = simulate_worker_step(rank, global_step)\n",
    "        shard_path = rank_file_name(rank, f\"step{global_step}\")\n",
    "        safe_write_json(shard_path, shard_state)\n",
    "        shards.append({\"rank\": rank, \"path\": shard_path})\n",
    "    index = {\n",
    "        \"global_step\": global_step,\n",
    "        \"shards\": shards,\n",
    "        \"training_state\": dict(training_state),\n",
    "        \"optimizer_state\": dict(optimizer_state),\n",
    "    }\n",
    "    safe_write_json(INDEX_FILE, index)\n",
    "\n",
    "# Load the latest checkpoint index if it exists.\n",
    "def load_latest_index():\n",
    "    if not os.path.exists(INDEX_FILE):\n",
    "        return None\n",
    "    with open(INDEX_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        index = json.load(f)\n",
    "    return index\n",
    "\n",
    "# Reconstruct full state from index and shard files.\n",
    "def restore_from_index(index):\n",
    "    shards = []\n",
    "    for shard_info in index[\"shards\"]:\n",
    "        path = shard_info[\"path\"]\n",
    "        if not os.path.exists(path):\n",
    "            raise FileNotFoundError(f\"Missing shard {path}\")\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            shards.append(json.load(f))\n",
    "    return {\n",
    "        \"index\": index,\n",
    "        \"shards\": shards,\n",
    "    }\n",
    "\n",
    "# Simulate a short training loop with checkpointing.\n",
    "def run_training(world_size, total_steps):\n",
    "    print(\"Simulating distributed style checkpointing.\")\n",
    "    for step in range(1, total_steps + 1):\n",
    "        training_state[\"global_step\"] = step\n",
    "        training_state[\"epoch\"] = 0\n",
    "        model_state[\"weight\"] += 0.01\n",
    "        model_state[\"bias\"] += 0.001\n",
    "        if step % 2 == 0:\n",
    "            save_sharded_checkpoint(step, world_size)\n",
    "            print(f\"Checkpoint saved at global_step {step}.\")\n",
    "\n",
    "# Demonstrate a clean resume from the last checkpoint.\n",
    "def demo_resume():\n",
    "    index = load_latest_index()\n",
    "    if index is None:\n",
    "        print(\"No checkpoint index found on disk.\")\n",
    "        return\n",
    "    restored = restore_from_index(index)\n",
    "    print(\"Restored global_step from index:\", restored[\"index\"][\"global_step\"])\n",
    "    print(\"Number of shards restored:\", len(restored[\"shards\"]))\n",
    "    print(\"Example shard keys:\", list(restored[\"shards\"][0].keys()))\n",
    "\n",
    "# Main execution that ties everything together.\n",
    "if __name__ == \"__main__\":\n",
    "    WORLD_SIZE = 2\n",
    "    TOTAL_STEPS = 4\n",
    "    run_training(WORLD_SIZE, TOTAL_STEPS)\n",
    "    demo_resume()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb829386",
   "metadata": {},
   "source": [
    "### **3.3. Reliable Run Recovery**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8fff75",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_08/Lecture_B/image_03_03.jpg?v=1769770731\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Assume failures are normal in distributed training\n",
    ">* Checkpoint full, consistent training state across ranks\n",
    "\n",
    ">* Coordinate one writer and synchronized checkpoint save steps\n",
    ">* Use atomic file writes to avoid corrupted checkpoints\n",
    "\n",
    ">* Regularly test stopping and resuming from checkpoints\n",
    ">* Store rich metadata so restarts stay reproducible\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3cb04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Reliable Run Recovery\n",
    "\n",
    "# This script shows reliable run recovery basics.\n",
    "# We simulate checkpoints and safe resume behavior.\n",
    "# Focus is on logging and atomic checkpoint saving.\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import standard libraries for paths and randomness.\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "\n",
    "# Import TensorFlow for a tiny training example.\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set deterministic seeds for reproducible behavior.\n",
    "random.seed(7)\n",
    "os.environ[\"PYTHONHASHSEED\"] = \"7\"\n",
    "tf.random.set_seed(7)\n",
    "\n",
    "# Define a small directory for checkpoints.\n",
    "BASE_DIR = \"reliable_run_demo\"\n",
    "CHECKPOINT_DIR = os.path.join(BASE_DIR, \"ckpts\")\n",
    "\n",
    "# Create directories if they do not already exist.\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "# Define simple helper to print a short header.\n",
    "def print_header():\n",
    "    print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Define a tiny model for demonstration only.\n",
    "def build_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(28, 28)),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(32, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(10, activation=\"softmax\"),\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Load a tiny subset of MNIST for quick runs.\n",
    "def load_data(num_samples=2048):\n",
    "    (x_train, y_train), _ = tf.keras.datasets.mnist.load_data()\n",
    "    x_train = x_train[:num_samples].astype(\"float32\") / 255.0\n",
    "    y_train = y_train[:num_samples].astype(\"int64\")\n",
    "    return x_train, y_train\n",
    "\n",
    "# Build a small dictionary describing run state.\n",
    "def build_state(epoch, step, history):\n",
    "    state = {\n",
    "        \"epoch\": int(epoch),\n",
    "        \"step\": int(step),\n",
    "        \"history\": history,\n",
    "    }\n",
    "    return state\n",
    "\n",
    "# Save checkpoint atomically using a temporary file.\n",
    "def save_checkpoint_atomic(model, state, base_path):\n",
    "    tmp_path = base_path + \".tmp.weights.h5\"\n",
    "    final_path = base_path + \".ckpt.weights.h5\"\n",
    "    model.save_weights(tmp_path)\n",
    "    with open(tmp_path + \".json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(state, f)\n",
    "    os.replace(tmp_path, final_path)\n",
    "    os.replace(tmp_path + \".json\", final_path + \".json\")\n",
    "    return final_path\n",
    "\n",
    "# Load checkpoint if it exists and return state.\n",
    "def load_checkpoint_if_available(model, base_path):\n",
    "    final_path = base_path + \".ckpt.weights.h5\"\n",
    "    state_path = final_path + \".json\"\n",
    "    if not os.path.exists(final_path):\n",
    "        return None\n",
    "    if not os.path.exists(state_path):\n",
    "        return None\n",
    "    model.load_weights(final_path)\n",
    "    with open(state_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        state = json.load(f)\n",
    "    return state\n",
    "\n",
    "# Simulate a single process acting as rank zero.\n",
    "IS_MAIN_RANK = True\n",
    "\n",
    "# Main training function with safe resume behavior.\n",
    "def run_training(max_epochs=3, steps_per_epoch=5):\n",
    "    print_header()\n",
    "    x_train, y_train = load_data()\n",
    "    model = build_model()\n",
    "    base_path = os.path.join(CHECKPOINT_DIR, \"demo\")\n",
    "    state = load_checkpoint_if_available(model, base_path)\n",
    "    start_epoch = 0\n",
    "    history = {\"loss\": [], \"acc\": []}\n",
    "    if state is not None:\n",
    "        start_epoch = state.get(\"epoch\", 0)\n",
    "        history = state.get(\"history\", history)\n",
    "        print(\"Resumed from epoch\", start_epoch)\n",
    "    else:\n",
    "        print(\"No checkpoint found, starting fresh\")\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    dataset = dataset.shuffle(2048, seed=7).batch(64)\n",
    "    for epoch in range(start_epoch, max_epochs):\n",
    "        step = 0\n",
    "        for batch_x, batch_y in dataset.take(steps_per_epoch):\n",
    "            if batch_x.shape[0] == 0:\n",
    "                continue\n",
    "            result = model.train_on_batch(batch_x, batch_y, return_dict=True)\n",
    "            history[\"loss\"].append(float(result[\"loss\"]))\n",
    "            history[\"acc\"].append(float(result[\"accuracy\"]))\n",
    "            step += 1\n",
    "        if IS_MAIN_RANK:\n",
    "            state = build_state(epoch + 1, step, history)\n",
    "            ckpt_path = save_checkpoint_atomic(model, state, base_path)\n",
    "            print(\"Saved checkpoint at epoch\", epoch + 1)\n",
    "    print(\"Final recorded epochs:\", len(history[\"loss\"]))\n",
    "\n",
    "\n",
    "run_training()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b26821",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Scaling Practices**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62e902f",
   "metadata": {},
   "source": [
    "\n",
    "In this lecture, you learned to:\n",
    "- Apply gradient accumulation and effective batch size strategies in distributed training setups. \n",
    "- Combine mixed precision with DDP to improve throughput while maintaining convergence. \n",
    "- Implement basic logging and checkpointing that work correctly in multi‑process environments. \n",
    "\n",
    "In the next Module (Module 9), we will go over 'Export and Deployment'"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
