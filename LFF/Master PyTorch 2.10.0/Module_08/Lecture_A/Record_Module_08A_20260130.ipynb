{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28d02cc2",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**DDP Fundamentals**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05a421f",
   "metadata": {},
   "source": [
    ">Last update: 20260130.\n",
    "    \n",
    "By the end of this Lecture, you will be able to:\n",
    "- Describe the data‑parallel training paradigm and how DDP synchronizes gradients across processes. \n",
    "- Configure a basic DDP training script using torchrun, process groups, and DistributedDataParallel wrappers. \n",
    "- Use DistributedSampler with DataLoader to ensure each process sees a unique subset of data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d04442",
   "metadata": {},
   "source": [
    "## **1. Data Parallel Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32313bd0",
   "metadata": {},
   "source": [
    "### **1.1. Model Replication Basics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c7b843",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_08/Lecture_A/image_01_01.jpg?v=1769766636\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Same model copied to many devices, data split\n",
    ">* Replicas train in parallel, scaling without redesigning model\n",
    "\n",
    ">* All replicas start from identical initial weights\n",
    ">* Each replica computes local gradients, then combines updates\n",
    "\n",
    ">* Full model copies need significant device memory\n",
    ">* Syncing updates yields one fast, consistent global model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c4f53d",
   "metadata": {},
   "source": [
    "### **1.2. Gradient Sync Mechanics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e70fce",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_08/Lecture_A/image_01_02.jpg?v=1769766650\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Each process computes gradients on its own data\n",
    ">* Collective ops average gradients, mimicking one big batch\n",
    "\n",
    ">* Gradients are all-reduced bucket by bucket\n",
    ">* Averaged gradients mimic one large batch update\n",
    "\n",
    ">* Autograd hooks trigger automatic, overlapping gradient all-reduce\n",
    ">* Training loop stays single-device like while scaling\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cfa332",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Gradient Sync Mechanics\n",
    "\n",
    "# This script illustrates gradient synchronization basics.\n",
    "# We simulate two workers averaging simple scalar gradients.\n",
    "# Focus on clear prints instead of heavy computation.\n",
    "# Required standard library imports for this demo.\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "\n",
    "# Set deterministic random seed for reproducibility.\n",
    "random.seed(42)\n",
    "# Define a tiny linear model with one weight.\n",
    "class TinyLinear:\n",
    "    # Initialize model with a single weight parameter.\n",
    "    def __init__(self, initial_weight: float = 0.0):\n",
    "        self.weight = float(initial_weight)\n",
    "\n",
    "    # Forward pass computing prediction from input value.\n",
    "    def forward(self, x_value: float) -> float:\n",
    "        return self.weight * float(x_value)\n",
    "\n",
    "    # Compute gradient of mean squared error loss.\n",
    "    def grad_mse(self, x_value: float, y_target: float) -> float:\n",
    "        prediction = self.forward(x_value)\n",
    "        error = prediction - float(y_target)\n",
    "        return 2.0 * error * float(x_value)\n",
    "\n",
    "\n",
    "# Simulate one worker computing gradient on local data.\n",
    "def worker_compute_grad(worker_id: int, model_weight: float) -> float:\n",
    "    # Create local model replica with shared initial weight.\n",
    "    model = TinyLinear(initial_weight=model_weight)\n",
    "    # Define tiny local dataset for this worker.\n",
    "    if worker_id == 0:\n",
    "        x_values = [1.0, 2.0]\n",
    "        y_values = [2.0, 4.0]\n",
    "    else:\n",
    "        x_values = [3.0, 4.0]\n",
    "        y_values = [6.0, 8.0]\n",
    "\n",
    "    # Validate dataset lengths before computing gradients.\n",
    "    assert len(x_values) == len(y_values)\n",
    "\n",
    "    # Accumulate gradient over local mini batch.\n",
    "    total_grad = 0.0\n",
    "    for x_value, y_target in zip(x_values, y_values):\n",
    "        total_grad += model.grad_mse(x_value, y_target)\n",
    "\n",
    "    # Return average gradient over local mini batch.\n",
    "    return total_grad / float(len(x_values))\n",
    "\n",
    "\n",
    "# Simulate all reduce averaging across two workers.\n",
    "def all_reduce_mean(gradients):\n",
    "    # Validate there is at least one gradient value.\n",
    "    assert len(gradients) > 0\n",
    "    # Compute summed gradients across all workers.\n",
    "    grad_sum = sum(gradients)\n",
    "    # Return averaged gradient representing synchronized value.\n",
    "    return grad_sum / float(len(gradients))\n",
    "\n",
    "\n",
    "# Main demonstration function for gradient synchronization.\n",
    "def main():\n",
    "    # Print simple header describing the demonstration purpose.\n",
    "    print(\"Gradient sync demo with two tiny workers.\")\n",
    "\n",
    "    # Initialize shared starting weight for both workers.\n",
    "    initial_weight = 0.5\n",
    "    print(\"Initial shared weight:\", initial_weight)\n",
    "\n",
    "    # Each worker computes its local gradient independently.\n",
    "    grad_worker0 = worker_compute_grad(0, initial_weight)\n",
    "    grad_worker1 = worker_compute_grad(1, initial_weight)\n",
    "\n",
    "    # Print local gradients before synchronization step.\n",
    "    print(\"Worker0 local gradient:\", round(grad_worker0, 3))\n",
    "    print(\"Worker1 local gradient:\", round(grad_worker1, 3))\n",
    "\n",
    "    # Perform all reduce mean to synchronize gradients.\n",
    "    synced_grad = all_reduce_mean([grad_worker0, grad_worker1])\n",
    "\n",
    "    # Print synchronized gradient shared by both workers.\n",
    "    print(\"Synchronized averaged gradient:\", round(synced_grad, 3))\n",
    "\n",
    "    # Apply one small gradient descent update step.\n",
    "    learning_rate = 0.1\n",
    "    updated_weight = initial_weight - learning_rate * synced_grad\n",
    "\n",
    "    # Show that both workers would apply same update.\n",
    "    print(\"Updated shared weight after sync:\", round(updated_weight, 3))\n",
    "\n",
    "\n",
    "# Execute main demonstration when script is run.\n",
    "main()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391d2794",
   "metadata": {},
   "source": [
    "### **1.3. Communication overhead**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64a8646",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_08/Lecture_A/image_01_03.jpg?v=1769766707\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* DDP adds coordination and gradient sharing costs\n",
    ">* Network transfers and synchronization can bottleneck training speed\n",
    "\n",
    ">* All-reduce gradient syncing dominates communication cost\n",
    ">* Cost grows with model size, processes, network speed\n",
    "\n",
    ">* Communication overhead limits scaling when adding GPUs\n",
    ">* Mitigated by tuning jobs and improving communication\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31c8761",
   "metadata": {},
   "source": [
    "## **2. Setting Up DDP**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47120465",
   "metadata": {},
   "source": [
    "### **2.1. Launching DDP with torchrun**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8637bdd0",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_08/Lecture_A/image_02_01.jpg?v=1769766725\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* torchrun launches and manages multi‑GPU training processes\n",
    ">* It coordinates process identities and shared environment state\n",
    "\n",
    ">* Torchrun defines process counts, ranks, and devices\n",
    ">* Scripts read env vars to init distributed backend\n",
    "\n",
    ">* Torchrun replaces manual multi‑GPU process management\n",
    ">* Same script scales easily from workstation to cluster\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe751de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Launching DDP with torchrun\n",
    "\n",
    "# This script introduces launching DDP with torchrun.\n",
    "# It runs safely on CPU only in Colab.\n",
    "# It simulates ranks instead of real multi GPU.\n",
    "\n",
    "# Required installs for real PyTorch distributed training.\n",
    "# !pip install torch torchvision torchaudio.\n",
    "\n",
    "# Import standard library modules for environment handling.\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "\n",
    "# Import typing tools for clearer function signatures.\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "# Set a deterministic random seed for reproducibility.\n",
    "random.seed(42)\n",
    "\n",
    "# Define a small helper to simulate environment variables.\n",
    "\n",
    "\n",
    "def build_fake_torchrun_env(world_size: int) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Build fake env dicts for each simulated process.\"\"\"\n",
    "    envs: List[Dict[str, Any]] = []\n",
    "    for rank in range(world_size):\n",
    "        env: Dict[str, Any] = {}\n",
    "        env[\"RANK\"] = str(rank)\n",
    "        env[\"WORLD_SIZE\"] = str(world_size)\n",
    "        env[\"LOCAL_RANK\"] = str(rank)\n",
    "        env[\"MASTER_ADDR\"] = \"127.0.0.1\"\n",
    "        env[\"MASTER_PORT\"] = \"29500\"\n",
    "        envs.append(env)\n",
    "    return envs\n",
    "\n",
    "# Define a function that prints key distributed settings.\n",
    "\n",
    "\n",
    "def describe_process_from_env(env: Dict[str, Any]) -> str:\n",
    "    \"\"\"Return a short description string for one process.\"\"\"\n",
    "    rank = int(env.get(\"RANK\", \"0\"))\n",
    "    world = int(env.get(\"WORLD_SIZE\", \"1\"))\n",
    "    local = int(env.get(\"LOCAL_RANK\", \"0\"))\n",
    "    addr = env.get(\"MASTER_ADDR\", \"unknown\")\n",
    "    port = env.get(\"MASTER_PORT\", \"0\")\n",
    "    desc = (\n",
    "        f\"Rank {rank} of {world}, local rank {local}, \"\n",
    "        f\"master {addr}:{port}\"\n",
    "    )\n",
    "    return desc\n",
    "\n",
    "# Define a tiny fake training step for demonstration.\n",
    "\n",
    "\n",
    "def fake_training_step(rank: int, world_size: int) -> str:\n",
    "    \"\"\"Simulate a tiny gradient sync description.\"\"\"\n",
    "    base_grad = 0.1 * (rank + 1)\n",
    "    avg_grad = sum(0.1 * (r + 1) for r in range(world_size))\n",
    "    avg_grad /= float(world_size)\n",
    "    msg = (\n",
    "        f\"Process {rank} computed grad {base_grad:.3f}, \"\n",
    "        f\"synced average {avg_grad:.3f}\"\n",
    "    )\n",
    "    return msg\n",
    "\n",
    "# Define a function that simulates one DDP process body.\n",
    "\n",
    "\n",
    "def simulated_ddp_process(env: Dict[str, Any]) -> List[str]:\n",
    "    \"\"\"Simulate what a DDP process might report.\"\"\"\n",
    "    rank = int(env[\"RANK\"])\n",
    "    world = int(env[\"WORLD_SIZE\"])\n",
    "    lines: List[str] = []\n",
    "    lines.append(describe_process_from_env(env))\n",
    "    lines.append(fake_training_step(rank, world))\n",
    "    return lines\n",
    "\n",
    "# Define a helper that prints a short header for the demo.\n",
    "\n",
    "\n",
    "def print_header() -> None:\n",
    "    \"\"\"Print a short explanation header.\"\"\"\n",
    "    print(\"Simulating how torchrun configures each process.\")\n",
    "    print(\"Each line represents one launched process.\")\n",
    "\n",
    "# Define the main function that orchestrates the simulation.\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    \"\"\"Main entry that mimics torchrun launching.\"\"\"\n",
    "    world_size = 4\n",
    "    envs = build_fake_torchrun_env(world_size)\n",
    "    print_header()\n",
    "    for env in envs:\n",
    "        lines = simulated_ddp_process(env)\n",
    "        for line in lines:\n",
    "            print(line)\n",
    "\n",
    "# Execute the main function when the script runs.\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5784024a",
   "metadata": {},
   "source": [
    "### **2.2. Initializing Process Groups**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5be3d0",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_08/Lecture_A/image_02_02.jpg?v=1769766787\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Process groups create a shared communication backbone\n",
    ">* World size and rank enable coordinated collective operations\n",
    "\n",
    ">* Choose backend and rendezvous to connect processes\n",
    ">* All processes must match settings or initialization fails\n",
    "\n",
    ">* Process groups can form flexible subgroups for communication\n",
    ">* Initialize early and shut down cleanly for reliability\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7364366c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Initializing Process Groups\n",
    "\n",
    "# This script introduces basic process group initialization.\n",
    "# It uses simple prints to explain distributed concepts.\n",
    "# Run it with torchrun to simulate multiple processes.\n",
    "\n",
    "# Install PyTorch if not already available in the environment.\n",
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu.\n",
    "\n",
    "# Import standard library modules for environment handling.\n",
    "import os\n",
    "import random\n",
    "import socket\n",
    "\n",
    "# Import torch and distributed utilities if available.\n",
    "try:\n",
    "    import torch\n",
    "    import torch.distributed as dist\n",
    "except ImportError:\n",
    "    torch = None\n",
    "    dist = None\n",
    "\n",
    "# Set deterministic seeds for reproducible behavior.\n",
    "random.seed(0)\n",
    "os.environ[\"PYTHONHASHSEED\"] = \"0\"\n",
    "\n",
    "\n",
    "# Define a helper function to detect single process mode.\n",
    "def is_single_process_mode() -> bool:\n",
    "    world_env = os.getenv(\"WORLD_SIZE\", \"1\")\n",
    "    return world_env == \"1\"\n",
    "\n",
    "\n",
    "# Define a helper to choose a default backend safely.\n",
    "def choose_backend() -> str:\n",
    "    if torch is None:\n",
    "        return \"gloo\"\n",
    "    if torch.cuda.is_available():\n",
    "        return \"nccl\"\n",
    "    return \"gloo\"\n",
    "\n",
    "\n",
    "# Define a function to print a short header line.\n",
    "def print_header(title: str) -> None:\n",
    "    line = \"=\" * len(title)\n",
    "    print(line)\n",
    "    print(title)\n",
    "    print(line)\n",
    "\n",
    "\n",
    "# Define the main demonstration function for process groups.\n",
    "def main() -> None:\n",
    "    print_header(\"Initializing a simple process group\")\n",
    "\n",
    "    # Read rank and world size from environment variables.\n",
    "    rank = int(os.getenv(\"RANK\", \"0\"))\n",
    "    world_size = int(os.getenv(\"WORLD_SIZE\", \"1\"))\n",
    "\n",
    "    # Show basic identity information for this process.\n",
    "    print(f\"Process rank: {rank}, world size: {world_size}\")\n",
    "\n",
    "    # Handle the case where torch or dist is not available.\n",
    "    if torch is None or dist is None:\n",
    "        print(\"PyTorch not available, showing conceptual explanation only.\")\n",
    "        return\n",
    "\n",
    "    # Choose backend based on hardware availability.\n",
    "    backend = choose_backend()\n",
    "    print(f\"Selected backend: {backend}\")\n",
    "\n",
    "    # Prepare rendezvous information for initialization.\n",
    "    master_addr = os.getenv(\"MASTER_ADDR\", \"127.0.0.1\")\n",
    "    master_port = os.getenv(\"MASTER_PORT\", \"29500\")\n",
    "\n",
    "    # Show rendezvous configuration for clarity.\n",
    "    print(f\"Rendezvous at {master_addr}:{master_port}\")\n",
    "\n",
    "    # Skip initialization when running in single process mode.\n",
    "    if is_single_process_mode():\n",
    "        print(\"Single process mode, process group not initialized.\")\n",
    "        return\n",
    "\n",
    "    # Initialize the default process group for all processes.\n",
    "    dist.init_process_group(\n",
    "        backend=backend,\n",
    "        init_method=f\"tcp://{master_addr}:{master_port}\",\n",
    "        rank=rank,\n",
    "        world_size=world_size,\n",
    "    )\n",
    "\n",
    "    # Verify that the process group is initialized correctly.\n",
    "    if dist.is_initialized():\n",
    "        print(\"Process group successfully initialized.\")\n",
    "    else:\n",
    "        print(\"Process group initialization failed.\")\n",
    "\n",
    "    # Cleanly destroy the process group before exiting.\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "\n",
    "# Execute the main function when the script runs.\n",
    "main()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f07819",
   "metadata": {},
   "source": [
    "### **2.3. DDP Model Wrapping**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f81ea38",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_08/Lecture_A/image_02_03.jpg?v=1769766842\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Move each model replica to its device\n",
    ">* Wrap model so gradients sync across processes automatically\n",
    "\n",
    ">* Wrapper syncs gradients via bucketed all-reduce\n",
    ">* All replicas share averaged updates, scaling training\n",
    "\n",
    ">* Register all trainable parameters before wrapping DDP\n",
    ">* Keep identical models per process to synchronize updates\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed833bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - DDP Model Wrapping\n",
    "\n",
    "# This script illustrates a simple DDP style concept.\n",
    "# We simulate wrapping a model and synchronizing gradients.\n",
    "# The focus is on beginner friendly conceptual clarity.\n",
    "\n",
    "# Required install for PyTorch if not already available.\n",
    "# !pip install torch torchvision torchaudio --quiet.\n",
    "\n",
    "# Import standard libraries for typing and randomness.\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "\n",
    "# Import torch modules for tensors and neural networks.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Set deterministic seeds for reproducible tiny experiment.\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Detect device but keep everything small and simple.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Print torch version and chosen device once only.\n",
    "print(\"Torch version:\", torch.__version__, \"Device:\", device)\n",
    "\n",
    "# Define a tiny linear model for demonstration purposes.\n",
    "class TinyModel(nn.Module):\n",
    "\n",
    "    # Initialize with a single linear layer only.\n",
    "    def __init__(self, in_features: int, out_features: int):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Linear(in_features, out_features)\n",
    "\n",
    "    # Forward pass applies the linear layer to inputs.\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.layer(x)\n",
    "\n",
    "# Create a helper to build the base model on cpu.\n",
    "def build_base_model() -> TinyModel:\n",
    "    # Use fixed input and output feature sizes.\n",
    "    model = TinyModel(in_features=4, out_features=2)\n",
    "    return model\n",
    "\n",
    "# Create two model replicas to mimic two DDP processes.\n",
    "base_model = build_base_model()\n",
    "replica_one = build_base_model()\n",
    "\n",
    "# Copy base weights so replicas start identically.\n",
    "replica_one.load_state_dict(base_model.state_dict())\n",
    "\n",
    "# Move both replicas to the selected device.\n",
    "base_model = base_model.to(device)\n",
    "replica_one = replica_one.to(device)\n",
    "\n",
    "# Define a simple mean squared error loss function.\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Create tiny fake batch data and targets.\n",
    "inputs = torch.randn(2, 4, device=device)\n",
    "targets = torch.randn(2, 2, device=device)\n",
    "\n",
    "# Validate shapes to avoid silent broadcasting mistakes.\n",
    "assert inputs.shape == (2, 4)\n",
    "assert targets.shape == (2, 2)\n",
    "\n",
    "# Perform forward pass on each replica separately.\n",
    "outputs_zero = base_model(inputs)\n",
    "outputs_one = replica_one(inputs)\n",
    "\n",
    "# Compute local losses for each simulated process.\n",
    "loss_zero = criterion(outputs_zero, targets)\n",
    "loss_one = criterion(outputs_one, targets)\n",
    "\n",
    "# Backward to compute local gradients on each replica.\n",
    "loss_zero.backward()\n",
    "loss_one.backward()\n",
    "\n",
    "# Collect gradients from both replicas into simple lists.\n",
    "grads_zero = [p.grad.detach().clone() for p in base_model.parameters()]\n",
    "grads_one = [p.grad.detach().clone() for p in replica_one.parameters()]\n",
    "\n",
    "# Simulate DDP all reduce by averaging gradients manually.\n",
    "averaged_grads = []\n",
    "for g0, g1 in zip(grads_zero, grads_one):\n",
    "    avg = (g0 + g1) / 2.0\n",
    "    averaged_grads.append(avg)\n",
    "\n",
    "# Apply averaged gradients back to both model replicas.\n",
    "with torch.no_grad():\n",
    "    for param, avg_grad in zip(base_model.parameters(), averaged_grads):\n",
    "        param.grad.copy_(avg_grad)\n",
    "    for param, avg_grad in zip(replica_one.parameters(), averaged_grads):\n",
    "        param.grad.copy_(avg_grad)\n",
    "\n",
    "# Define a tiny optimizer for each model replica.\n",
    "optimizer_zero = torch.optim.SGD(base_model.parameters(), lr=0.1)\n",
    "optimizer_one = torch.optim.SGD(replica_one.parameters(), lr=0.1)\n",
    "\n",
    "# Step both optimizers so parameters update identically.\n",
    "optimizer_zero.step()\n",
    "optimizer_one.step()\n",
    "\n",
    "# Compare parameters to confirm synchronized updates.\n",
    "max_difference = 0.0\n",
    "for p0, p1 in zip(base_model.parameters(), replica_one.parameters()):\n",
    "    diff = (p0.detach() - p1.detach()).abs().max().item()\n",
    "    max_difference = max(max_difference, diff)\n",
    "\n",
    "# Print a few key values to summarize the behavior.\n",
    "print(\"Loss process zero:\", float(loss_zero))\n",
    "print(\"Loss process one:\", float(loss_one))\n",
    "print(\"Max parameter difference after sync:\", max_difference)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0757b365",
   "metadata": {},
   "source": [
    "## **3. Distributed Data Samplers**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a585d2b2",
   "metadata": {},
   "source": [
    "### **3.1. Using DistributedSampler**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c06bcef",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_08/Lecture_A/image_03_01.jpg?v=1769766916\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* DistributedSampler splits dataset indices across processes\n",
    ">* Ensures full coverage without duplicate samples per epoch\n",
    "\n",
    ">* Sampler uses world size and rank to partition\n",
    ">* DataLoader then serves each process unique batches\n",
    "\n",
    ">* Sampler pads data to balance workloads\n",
    ">* Padding preserves fairness, uniqueness, and synchronization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1251a451",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Using DistributedSampler\n",
    "\n",
    "# This script demonstrates DistributedSampler usage conceptually.\n",
    "# It runs in a single process but mimics multiple ranks.\n",
    "# Focus on how indices are partitioned across fake processes.\n",
    "\n",
    "# No extra installs are required for this simple example.\n",
    "# All used modules are from the Python standard library.\n",
    "\n",
    "# Import random module for deterministic shuffling.\n",
    "import random\n",
    "\n",
    "# Set a deterministic seed for reproducible shuffling.\n",
    "random.seed(42)\n",
    "\n",
    "# Define a tiny toy dataset as a list of indices.\n",
    "full_dataset_indices = list(range(12))\n",
    "\n",
    "# Define world size representing total parallel processes.\n",
    "world_size = 3\n",
    "\n",
    "# Validate that world size is a positive integer.\n",
    "assert isinstance(world_size, int) and world_size > 0\n",
    "\n",
    "# Define a helper that mimics DistributedSampler behavior.\n",
    "def get_indices_for_rank(indices, world_size, rank):\n",
    "    # Validate rank is within the correct range.\n",
    "    assert 0 <= rank < world_size\n",
    "\n",
    "    # Shuffle a copy of indices deterministically.\n",
    "    shuffled = indices.copy()\n",
    "    random.shuffle(shuffled)\n",
    "\n",
    "    # Compute padded length divisible by world size.\n",
    "    remainder = len(shuffled) % world_size\n",
    "\n",
    "    # If needed, pad with initial indices to balance.\n",
    "    if remainder != 0:\n",
    "        pad_size = world_size - remainder\n",
    "\n",
    "        # Extend shuffled list with repeated indices.\n",
    "        shuffled.extend(shuffled[:pad_size])\n",
    "\n",
    "    # Compute per rank slice size after padding.\n",
    "    per_rank = len(shuffled) // world_size\n",
    "\n",
    "    # Compute start and end positions for this rank.\n",
    "    start = rank * per_rank\n",
    "    end = start + per_rank\n",
    "\n",
    "    # Return the slice assigned to this rank.\n",
    "    return shuffled[start:end]\n",
    "\n",
    "# Collect assigned indices for each fake rank.\n",
    "assigned = {}\n",
    "\n",
    "# Loop over ranks and compute their index subsets.\n",
    "for rank in range(world_size):\n",
    "    # Get indices for this rank using helper.\n",
    "    rank_indices = get_indices_for_rank(\n",
    "        full_dataset_indices,\n",
    "        world_size,\n",
    "        rank,\n",
    "    )\n",
    "\n",
    "    # Store the indices for later inspection.\n",
    "    assigned[rank] = rank_indices\n",
    "\n",
    "# Print framework style header for clarity.\n",
    "print(\"Simulated DistributedSampler index partitioning:\")\n",
    "\n",
    "# Show full dataset indices once for reference.\n",
    "print(\"Full dataset indices:\", full_dataset_indices)\n",
    "\n",
    "# Print how many samples each rank receives.\n",
    "print(\"World size:\", world_size, \"ranks in total\")\n",
    "\n",
    "# Loop again to display each rank assignment.\n",
    "for rank in range(world_size):\n",
    "    # Print rank specific subset of indices.\n",
    "    print(\"Rank\", rank, \"sees indices:\", assigned[rank])\n",
    "\n",
    "# Confirm that union of unique indices covers dataset.\n",
    "print(\"Unique indices covered:\", sorted(set(sum(assigned.values(), []))))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bbf81d",
   "metadata": {},
   "source": [
    "### **3.2. Epoch Shuffling Behavior**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb82239",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_08/Lecture_A/image_03_02.jpg?v=1769766970\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Sampler creates one global shuffled index order\n",
    ">* Each process gets a unique, deterministic slice\n",
    "\n",
    ">* Sampler seed depends on provided epoch number\n",
    ">* Updating epoch reshuffles data while keeping synchronization\n",
    "\n",
    ">* Sampler pads and shuffles to balance samples\n",
    ">* Careful reshuffling ensures fairness, stability, reproducibility\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc2ce2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Epoch Shuffling Behavior\n",
    "\n",
    "# This script illustrates distributed epoch shuffling behavior.\n",
    "# We simulate ranks and a sampler without external libraries.\n",
    "# Focus is on how epochs change sample order deterministically.\n",
    "\n",
    "# Example pip install for real distributed training environments.\n",
    "# !pip install torch.\n",
    "\n",
    "# Import standard random and math utilities.\n",
    "import random\n",
    "import math\n",
    "\n",
    "# Set a global seed for deterministic behavior.\n",
    "random.seed(42)\n",
    "\n",
    "# Define a tiny synthetic dataset of integer sample ids.\n",
    "dataset_indices = list(range(10))\n",
    "\n",
    "# Configure world size and ensure it divides padded length.\n",
    "world_size = 3\n",
    "\n",
    "# Validate dataset size is positive and world size reasonable.\n",
    "assert len(dataset_indices) > 0 and world_size > 0\n",
    "\n",
    "# Compute padded length so each rank gets equal samples.\n",
    "padded_len = int(math.ceil(len(dataset_indices) / world_size)) * world_size\n",
    "\n",
    "# Brief function to build a padded index list.\n",
    "def build_padded_indices(indices, padded_len):\n",
    "    # Repeat indices then truncate to padded length.\n",
    "    repeated = (indices * ((padded_len // len(indices)) + 1))[:padded_len]\n",
    "    return repeated\n",
    "\n",
    "# Build the padded index list once for all epochs.\n",
    "base_indices = build_padded_indices(dataset_indices, padded_len)\n",
    "\n",
    "# Function to simulate one epoch shuffling for all ranks.\n",
    "def simulate_epoch(epoch, world_size, base_indices):\n",
    "    # Create a local random generator seeded by epoch.\n",
    "    rng = random.Random(1000 + epoch)\n",
    "\n",
    "    # Copy and shuffle indices deterministically for this epoch.\n",
    "    shuffled = list(base_indices)\n",
    "    rng.shuffle(shuffled)\n",
    "\n",
    "    # Partition shuffled indices into equal rank chunks.\n",
    "    chunk_size = len(shuffled) // world_size\n",
    "    rank_chunks = []\n",
    "\n",
    "    # Slice shuffled list so each rank gets unique segment.\n",
    "    for rank in range(world_size):\n",
    "        start = rank * chunk_size\n",
    "        end = start + chunk_size\n",
    "        rank_chunks.append(shuffled[start:end])\n",
    "\n",
    "    # Return the per rank index assignment for this epoch.\n",
    "    return rank_chunks\n",
    "\n",
    "# Print a short header explaining the demonstration.\n",
    "print(\"Simulating epoch shuffling for\", world_size, \"ranks\")\n",
    "\n",
    "# Choose a few epochs to visualize behavior changes.\n",
    "epochs_to_show = [0, 1, 2]\n",
    "\n",
    "# Loop over epochs and display rank assignments.\n",
    "for epoch in epochs_to_show:\n",
    "    # Simulate shuffling and partitioning for this epoch.\n",
    "    rank_chunks = simulate_epoch(epoch, world_size, base_indices)\n",
    "\n",
    "    # Print epoch number and per rank index lists.\n",
    "    print(\"\\nEpoch\", epoch, \"rank index assignments:\")\n",
    "    for rank, chunk in enumerate(rank_chunks):\n",
    "        print(\"  Rank\", rank, \"sees indices:\", chunk)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88aa267",
   "metadata": {},
   "source": [
    "### **3.3. Debugging Rank Mismatches**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df5a4e5",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_08/Lecture_A/image_03_03.jpg?v=1769767010\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Rank mismatches cause inconsistent data slices per process\n",
    ">* Leads to hangs, wasted compute, and bad metrics\n",
    "\n",
    ">* Spot symptoms like uneven epochs or batches\n",
    ">* Log ranks, world size, dataset stats to compare\n",
    "\n",
    ">* Standardize configs, launches, environments, and data views\n",
    ">* Ensure sampler resets and each rank’s dataset slice\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0af60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Debugging Rank Mismatches\n",
    "\n",
    "# This script illustrates debugging rank mismatches simply.\n",
    "# We simulate two ranks and compare sampler index assignments.\n",
    "# Focus on understanding shapes sizes and mismatched expectations.\n",
    "\n",
    "# Required external install for torch in some environments.\n",
    "# !pip install torch torchvision torchaudio --quiet.\n",
    "\n",
    "# Import standard random and os modules.\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "\n",
    "# Import torch and set deterministic behavior.\n",
    "import torch\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "random.seed(0)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Define a tiny dummy dataset length.\n",
    "DATASET_LEN = 12\n",
    "\n",
    "# Define a helper to build per rank index slices.\n",
    "def build_indices(dataset_len, world_size, rank):\n",
    "    # Compute base samples per rank using ceiling division.\n",
    "    per_rank = math.ceil(dataset_len / world_size)\n",
    "    # Compute start and end positions for this rank.\n",
    "    start = rank * per_rank\n",
    "    end = min(start + per_rank, dataset_len)\n",
    "    # Return list of indices for this rank.\n",
    "    return list(range(start, end))\n",
    "\n",
    "# Simulate correct configuration with world size two.\n",
    "correct_world_size = 2\n",
    "\n",
    "# Build indices for rank zero with correct world size.\n",
    "correct_rank0 = build_indices(DATASET_LEN, correct_world_size, 0)\n",
    "\n",
    "# Build indices for rank one with correct world size.\n",
    "correct_rank1 = build_indices(DATASET_LEN, correct_world_size, 1)\n",
    "\n",
    "# Simulate a buggy configuration for rank one.\n",
    "buggy_world_size_rank1 = 3\n",
    "\n",
    "# Build buggy indices for rank one using wrong world size.\n",
    "buggy_rank1 = build_indices(DATASET_LEN, buggy_world_size_rank1, 1)\n",
    "\n",
    "# Print framework version in one concise line.\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "\n",
    "# Print correct per rank index assignments for reference.\n",
    "print(\"Correct rank0 indices:\", correct_rank0)\n",
    "\n",
    "# Print correct rank one indices for comparison.\n",
    "print(\"Correct rank1 indices:\", correct_rank1)\n",
    "\n",
    "# Print buggy rank one indices showing mismatch clearly.\n",
    "print(\"Buggy rank1 indices:\", buggy_rank1)\n",
    "\n",
    "# Compute union of indices for correct configuration.\n",
    "correct_union = sorted(set(correct_rank0 + correct_rank1))\n",
    "\n",
    "# Compute union of indices when rank one is buggy.\n",
    "buggy_union = sorted(set(correct_rank0 + buggy_rank1))\n",
    "\n",
    "# Print union sizes to highlight coverage differences.\n",
    "print(\"Correct union size:\", len(correct_union))\n",
    "\n",
    "# Print buggy union size and note potential missing samples.\n",
    "print(\"Buggy union size:\", len(buggy_union))\n",
    "\n",
    "# Compute intersection to show overlapping duplicated work.\n",
    "intersection = sorted(set(correct_rank0).intersection(buggy_rank1))\n",
    "\n",
    "# Final print summarizing overlap and mismatch situation.\n",
    "print(\"Overlapping indices between rank0 and buggy rank1:\", intersection)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12af48f",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**DDP Fundamentals**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636adf05",
   "metadata": {},
   "source": [
    "\n",
    "In this lecture, you learned to:\n",
    "- Describe the data‑parallel training paradigm and how DDP synchronizes gradients across processes. \n",
    "- Configure a basic DDP training script using torchrun, process groups, and DistributedDataParallel wrappers. \n",
    "- Use DistributedSampler with DataLoader to ensure each process sees a unique subset of data. \n",
    "\n",
    "In the next Lecture (Lecture B), we will go over 'Scaling Practices'"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
