{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2090b338",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Tensors Fundamentals**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc78566",
   "metadata": {},
   "source": [
    ">Last update: 20260129.\n",
    "    \n",
    "By the end of this Lecture, you will be able to:\n",
    "- Create and manipulate PyTorch tensors using common factory functions and indexing operations. \n",
    "- Explain how tensor shapes, dtypes, and devices affect performance and correctness in PyTorch 2.10.0. \n",
    "- Perform basic numerical operations on tensors and debug common shape and device mismatch errors. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8e05d1",
   "metadata": {},
   "source": [
    "## **1. Creating PyTorch Tensors**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cea37b",
   "metadata": {},
   "source": [
    "### **1.1. Tensor Creation Basics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2762de",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_01/Lecture_C/image_01_01.jpg?v=1769692672\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Tensors generalize scalars, vectors, and matrices\n",
    ">* Factory functions create tensors ready for computation\n",
    "\n",
    ">* Start from data or desired tensor shape\n",
    ">* Factory functions infer dimensions, layout, and dtype\n",
    "\n",
    ">* Plan tensor shapes and dimensions before coding\n",
    ">* Correct shapes simplify operations and prevent bugs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb638a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Tensor Creation Basics\n",
    "\n",
    "# This script introduces basic tensor creation concepts.\n",
    "# We use PyTorch to create and inspect tensors.\n",
    "# Focus on shapes, dtypes, and simple indexing.\n",
    "\n",
    "# !pip install torch torchvision torchaudio.\n",
    "\n",
    "# Import torch for tensor operations.\n",
    "import torch\n",
    "\n",
    "# Print PyTorch version for reproducibility.\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "\n",
    "# Create a 1D tensor from a Python list.\n",
    "monthly_sales = torch.tensor([10.0, 12.5, 9.0, 11.0])\n",
    "\n",
    "# Show the tensor values and basic properties.\n",
    "print(\"monthly_sales:\", monthly_sales)\n",
    "print(\"shape:\", monthly_sales.shape)\n",
    "print(\"dtype:\", monthly_sales.dtype)\n",
    "\n",
    "# Create a 2D tensor describing coordinates.\n",
    "coords = torch.tensor([[0.0, 1.0], [2.0, 3.0]])\n",
    "\n",
    "# Inspect the 2D tensor shape and contents.\n",
    "print(\"coords:\\n\", coords)\n",
    "print(\"coords shape:\", coords.shape)\n",
    "\n",
    "# Create a tensor of zeros with a chosen shape.\n",
    "zeros_grid = torch.zeros((2, 3), dtype=torch.float32)\n",
    "\n",
    "# Create a tensor of ones with matching shape.\n",
    "ones_grid = torch.ones_like(zeros_grid)\n",
    "\n",
    "# Show shapes to highlight factory function behavior.\n",
    "print(\"zeros_grid shape:\", zeros_grid.shape)\n",
    "print(\"ones_grid shape:\", ones_grid.shape)\n",
    "\n",
    "# Create a range tensor using arange factory.\n",
    "steps = torch.arange(0, 5, 1, dtype=torch.int64)\n",
    "\n",
    "# Demonstrate simple indexing on the range tensor.\n",
    "print(\"steps:\", steps)\n",
    "print(\"first element:\", steps[0].item())\n",
    "\n",
    "# Demonstrate slicing to get a subrange view.\n",
    "sub_steps = steps[1:4]\n",
    "\n",
    "# Print the sliced tensor and confirm its shape.\n",
    "print(\"sub_steps:\", sub_steps, \"shape:\", sub_steps.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f30178",
   "metadata": {},
   "source": [
    "### **1.2. Random And Constant Tensors**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028d4af6",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_01/Lecture_C/image_01_02.jpg?v=1769692698\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Random tensors sample values from probability distributions\n",
    ">* Constant tensors repeat one value, adding structure\n",
    "\n",
    ">* Match tensor shape and distribution to task\n",
    ">* Use distributions and seeds to control learning\n",
    "\n",
    ">* Constant tensors give structure, baselines, and masks\n",
    ">* Combine with random tensors for realistic, controllable setups\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b606d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Random And Constant Tensors\n",
    "\n",
    "# This script shows random and constant tensors.\n",
    "# It uses PyTorch to create simple examples.\n",
    "# Run cells step by step and observe outputs.\n",
    "\n",
    "# Install PyTorch in Colab if not already available.\n",
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu.\n",
    "\n",
    "# Import torch and check version information.\n",
    "import torch\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "\n",
    "# Set a manual seed for deterministic randomness.\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Choose device based on GPU availability.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Create a small random tensor with uniform distribution.\n",
    "rand_uniform = torch.rand((2, 3), device=device)\n",
    "print(\"Uniform random tensor shape:\", rand_uniform.shape)\n",
    "\n",
    "# Create a small random tensor with normal distribution.\n",
    "rand_normal = torch.randn((2, 3), device=device)\n",
    "print(\"Normal random tensor mean approx:\", float(rand_normal.mean()))\n",
    "\n",
    "# Create a constant tensor filled with zeros.\n",
    "zeros_tensor = torch.zeros((2, 3), device=device)\n",
    "print(\"Zeros tensor first row:\", zeros_tensor[0])\n",
    "\n",
    "# Create a constant tensor filled with ones.\n",
    "ones_tensor = torch.ones((2, 3), device=device)\n",
    "print(\"Ones tensor first row:\", ones_tensor[0])\n",
    "\n",
    "# Create a constant tensor filled with a custom value.\n",
    "custom_value = 0.5\n",
    "half_tensor = torch.full((2, 3), custom_value, device=device)\n",
    "print(\"Custom value tensor first row:\", half_tensor[0])\n",
    "\n",
    "# Demonstrate simple indexing on a random tensor.\n",
    "first_element = rand_uniform[0, 0]\n",
    "print(\"First element of uniform tensor:\", float(first_element))\n",
    "\n",
    "# Verify shapes match before adding tensors together.\n",
    "if rand_uniform.shape == ones_tensor.shape:\n",
    "    added = rand_uniform + ones_tensor\n",
    "    print(\"Added tensor sample value:\", float(added[0, 0]))\n",
    "else:\n",
    "    print(\"Shape mismatch, cannot safely add tensors.\")\n",
    "\n",
    "# Move one tensor to CPU for safe printing.\n",
    "print(\"Uniform tensor on cpu:\", rand_uniform.to(\"cpu\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8810966b",
   "metadata": {},
   "source": [
    "### **1.3. From NumPy arrays**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9fa63c",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_01/Lecture_C/image_01_03.jpg?v=1769692747\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Convert NumPy arrays into PyTorch tensors easily\n",
    ">* Reuse familiar shapes while gaining deep learning features\n",
    "\n",
    ">* Shape and data type are preserved exactly\n",
    ">* Arrays and tensors may share memory, so edits propagate\n",
    "\n",
    ">* NumPy handles early cleaning and preprocessing steps\n",
    ">* Tensors power training with acceleration and autograd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6df0e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - From NumPy arrays\n",
    "\n",
    "# This script shows NumPy to tensor conversion.\n",
    "# It focuses on shapes dtypes and devices.\n",
    "# Run cells step by step and observe outputs.\n",
    "\n",
    "# !pip install torch torchvision torchaudio.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Set deterministic random seeds.\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Try importing torch safely.\n",
    "try:\n",
    "    import torch\n",
    "except ImportError as e:\n",
    "    raise SystemExit(\"PyTorch is required for this demo.\")\n",
    "\n",
    "# Print PyTorch version in one short line.\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "\n",
    "# Create a small NumPy array of floats.\n",
    "np_array_float = np.array([[1.0, 2.0], [3.0, 4.0]], dtype=np.float32)\n",
    "\n",
    "# Show NumPy array shape and dtype.\n",
    "print(\"NumPy float array:\", np_array_float.shape, np_array_float.dtype)\n",
    "\n",
    "# Convert NumPy array to a PyTorch tensor.\n",
    "tensor_from_np = torch.from_numpy(np_array_float)\n",
    "\n",
    "# Show tensor shape dtype and device.\n",
    "print(\"Tensor from NumPy:\", tensor_from_np.shape,\n",
    "      tensor_from_np.dtype, tensor_from_np.device)\n",
    "\n",
    "# Modify tensor and observe shared memory.\n",
    "tensor_from_np[0, 0] = 10.0\n",
    "\n",
    "# Print both objects to see shared change.\n",
    "print(\"After tensor edit NumPy[0,0]:\", np_array_float[0, 0])\n",
    "\n",
    "# Create an independent tensor copy from NumPy.\n",
    "independent_tensor = torch.tensor(np_array_float.copy())\n",
    "\n",
    "# Change NumPy array and show tensor unchanged.\n",
    "np_array_float[0, 1] = 20.0\n",
    "\n",
    "# Print values to compare sharing behavior.\n",
    "print(\"NumPy[0,1] now:\", np_array_float[0, 1])\n",
    "print(\"Independent tensor[0,1]:\", independent_tensor[0, 1].item())\n",
    "\n",
    "# Check if CUDA GPU is available for device.\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "# Move tensor to GPU if available.\n",
    "if use_cuda:\n",
    "    tensor_device = independent_tensor.to(torch.device(\"cuda\"))\n",
    "else:\n",
    "    tensor_device = independent_tensor.to(torch.device(\"cpu\"))\n",
    "\n",
    "# Print final tensor device and shape summary.\n",
    "print(\"Final tensor device and shape:\", tensor_device.device,\n",
    "      tensor_device.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5292796d",
   "metadata": {},
   "source": [
    "## **2. Tensor Shapes and Dtypes**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f12abdb",
   "metadata": {},
   "source": [
    "### **2.1. Tensor Rank and Broadcasting**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e468b1",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_01/Lecture_C/image_02_01.jpg?v=1769692777\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Tensor rank is number of indexing dimensions\n",
    ">* Each rank dimension has specific semantic meaning\n",
    "\n",
    ">* Broadcasting lets different-shaped tensors interact efficiently\n",
    ">* Follow dimension rules carefully to avoid silent bugs\n",
    "\n",
    ">* Broadcasting boosts efficiency but can hide bugs\n",
    ">* Track ranks and shapes to ensure correct behavior\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342189d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Tensor Rank and Broadcasting\n",
    "\n",
    "# This script explains tensor rank and broadcasting.\n",
    "# It uses PyTorch tensors with small numeric examples.\n",
    "# Focus is on shapes dtypes and devices interactions.\n",
    "\n",
    "# !pip install torch torchvision torchaudio.\n",
    "\n",
    "# Import torch and check version for reproducibility.\n",
    "import torch\n",
    "\n",
    "# Set a deterministic seed for reproducible values.\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Create a scalar rank zero tensor example.\n",
    "scalar = torch.tensor(3.5)\n",
    "\n",
    "# Create a vector rank one tensor example.\n",
    "vector = torch.tensor([1.0, 2.0, 3.0])\n",
    "\n",
    "# Create a matrix rank two tensor example.\n",
    "matrix = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n",
    "\n",
    "# Print ranks and shapes for basic understanding.\n",
    "print(\"scalar rank and shape:\", scalar.dim(), scalar.shape)\n",
    "print(\"vector rank and shape:\", vector.dim(), vector.shape)\n",
    "print(\"matrix rank and shape:\", matrix.dim(), matrix.shape)\n",
    "\n",
    "# Create a batch of tiny RGB images rank four.\n",
    "images = torch.randn(2, 3, 4, 4)\n",
    "\n",
    "# Print rank and shape of the image batch.\n",
    "print(\"images rank and shape:\", images.dim(), images.shape)\n",
    "\n",
    "# Create a color correction vector for broadcasting.\n",
    "color_gain = torch.tensor([0.9, 1.1, 1.0])\n",
    "\n",
    "# Show shapes before broadcasting operation.\n",
    "print(\"images shape before:\", images.shape)\n",
    "print(\"color_gain shape before:\", color_gain.shape)\n",
    "\n",
    "# Apply broadcasting over channel dimension safely.\n",
    "corrected = images * color_gain.view(1, 3, 1, 1)\n",
    "\n",
    "# Confirm resulting shape matches original images.\n",
    "print(\"corrected images shape:\", corrected.shape)\n",
    "\n",
    "# Demonstrate dtype impact on computation precision.\n",
    "float32_tensor = torch.randn(2, 3, dtype=torch.float32)\n",
    "\n",
    "# Create a matching float16 tensor for comparison.\n",
    "float16_tensor = float32_tensor.to(torch.float16)\n",
    "\n",
    "# Compute mean with both dtypes for illustration.\n",
    "mean32 = float32_tensor.mean()\n",
    "mean16 = float16_tensor.mean()\n",
    "\n",
    "# Print dtypes and resulting means for comparison.\n",
    "print(\"float32 dtype and mean:\", float32_tensor.dtype, float32_tensor.mean())\n",
    "print(\"float16 dtype and mean:\", float16_tensor.dtype, float16_tensor.mean())\n",
    "\n",
    "# Safely check device and move tensors if possible.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move images and gains to the selected device.\n",
    "images_device = images.to(device)\n",
    "color_gain_device = color_gain.to(device)\n",
    "\n",
    "# Recompute corrected images on the chosen device.\n",
    "corrected_device = images_device * color_gain_device.view(1, 3, 1, 1)\n",
    "\n",
    "# Print final device information and shape summary.\n",
    "print(\"device used and final shape:\", device, corrected_device.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44760093",
   "metadata": {},
   "source": [
    "### **2.2. Precision and Performance**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a6c7f7",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_01/Lecture_C/image_02_02.jpg?v=1769692810\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Data type choice affects precision and speed\n",
    ">* Double is precise but slower; single is faster\n",
    "\n",
    ">* Low precision reduces memory use and speeds training\n",
    ">* Mixed precision balances speed with numerical stability risks\n",
    "\n",
    ">* Devices favor certain dtypes for speed\n",
    ">* Balance precision, memory, and stability using profiling\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0ec945",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Precision and Performance\n",
    "\n",
    "# This script explores tensor precision and performance.\n",
    "# We compare dtypes and simple operations in PyTorch.\n",
    "# Focus on shapes devices and numerical tradeoffs.\n",
    "\n",
    "# !pip install torch torchvision torchaudio.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "\n",
    "# Try importing torch and handle absence gracefully.\n",
    "try:\n",
    "    import torch\n",
    "except ImportError:\n",
    "    raise SystemExit(\"PyTorch is required for this lesson.\")\n",
    "\n",
    "# Set deterministic random seeds for reproducibility.\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Detect device preferring GPU when available.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Print PyTorch version and selected device.\n",
    "print(\"PyTorch version:\", torch.__version__, \"Device:\", device)\n",
    "\n",
    "# Define a helper to time simple tensor operations.\n",
    "\n",
    "def time_tensor_op(tensor, repeat=2000):\n",
    "    # Ensure tensor is on correct device.\n",
    "    t = tensor.to(device)\n",
    "    # Warmup few operations before timing.\n",
    "    for _ in range(10):\n",
    "        _ = t * 1.0001 + 0.0001\n",
    "    # Synchronize if using GPU device.\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    # Record start time using perf counter.\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(repeat):\n",
    "        t = t * 1.0001 + 0.0001\n",
    "    # Synchronize again before stopping.\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    end = time.perf_counter()\n",
    "    # Return elapsed time in milliseconds.\n",
    "    return (end - start) * 1000.0\n",
    "\n",
    "# Create a base tensor in double precision.\n",
    "size = 2000\n",
    "base_double = torch.linspace(0.0, 1.0, steps=size, dtype=torch.float64)\n",
    "\n",
    "# Create matching tensors in different floating dtypes.\n",
    "base_float = base_double.to(torch.float32)\n",
    "base_half = base_double.to(torch.float16)\n",
    "\n",
    "# Show basic information about each tensor.\n",
    "print(\"double shape\", base_double.shape, \"dtype\", base_double.dtype)\n",
    "print(\"float  shape\", base_float.shape, \"dtype\", base_float.dtype)\n",
    "print(\"half   shape\", base_half.shape, \"dtype\", base_half.dtype)\n",
    "\n",
    "# Time the same computation for each dtype.\n",
    "time_double = time_tensor_op(base_double)\n",
    "time_float = time_tensor_op(base_float)\n",
    "time_half = time_tensor_op(base_half)\n",
    "\n",
    "# Print timing results rounded for readability.\n",
    "print(\"double time ms\", round(time_double, 2))\n",
    "print(\"float  time ms\", round(time_float, 2))\n",
    "print(\"half   time ms\", round(time_half, 2))\n",
    "\n",
    "# Demonstrate precision differences with small increments.\n",
    "small_step = 1e-4\n",
    "start_value = 1.0\n",
    "\n",
    "# Accumulate many small steps in double precision.\n",
    "steps = 10000\n",
    "x_double = torch.tensor(start_value, dtype=torch.float64)\n",
    "for _ in range(steps):\n",
    "    x_double = x_double + small_step\n",
    "\n",
    "# Accumulate many small steps in half precision.\n",
    "x_half = torch.tensor(start_value, dtype=torch.float16)\n",
    "for _ in range(steps):\n",
    "    x_half = x_half + small_step\n",
    "\n",
    "# Compute theoretical exact result using Python float.\n",
    "exact_value = start_value + steps * small_step\n",
    "\n",
    "# Print final accumulated values for comparison.\n",
    "print(\"exact value\", exact_value)\n",
    "print(\"double value\", float(x_double))\n",
    "print(\"half   value\", float(x_half))\n",
    "\n",
    "# Show absolute errors for each dtype result.\n",
    "print(\"double abs error\", abs(float(x_double) - exact_value))\n",
    "print(\"half   abs error\", abs(float(x_half) - exact_value))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34a696b",
   "metadata": {},
   "source": [
    "### **2.3. Tensor Shape Transformations**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d5b9ef",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_01/Lecture_C/image_02_03.jpg?v=1769692888\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Shape changes help different data fit together\n",
    ">* Reshaping operations change layout, not numeric values\n",
    "\n",
    ">* Know when reshapes are views versus copies\n",
    ">* Keep element counts consistent and dimensions semantically meaningful\n",
    "\n",
    ">* Match tensor dimensions to each layer’s expectations\n",
    ">* Preserve data meaning to avoid subtle shape bugs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2a25bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Tensor Shape Transformations\n",
    "\n",
    "# This script explores basic tensor shape transformations.\n",
    "# It uses TensorFlow to mimic PyTorch style tensor operations.\n",
    "# Focus on shapes dtypes and simple reshaping operations.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import TensorFlow and NumPy for tensor operations.\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Print TensorFlow version for reproducibility.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Create a small 2D tensor representing grayscale images.\n",
    "images = tf.constant(\n",
    "    [[[1., 2.], [3., 4.]],\n",
    "     [[5., 6.], [7., 8.]]],\n",
    "    dtype=tf.float32,\n",
    ")\n",
    "\n",
    "# Print original tensor shape and dtype information.\n",
    "print(\"Original images shape:\", images.shape)\n",
    "print(\"Original images dtype:\", images.dtype)\n",
    "\n",
    "# Interpret shape as batch height width for images.\n",
    "batch_size, height, width = images.shape\n",
    "print(\"Batch size height width:\", batch_size, height, width)\n",
    "\n",
    "# Flatten each image into a single feature vector.\n",
    "flat_images = tf.reshape(images, (batch_size, height * width))\n",
    "print(\"Flattened images shape:\", flat_images.shape)\n",
    "\n",
    "# Check that element count is preserved after reshape.\n",
    "original_elements = tf.size(images)\n",
    "flat_elements = tf.size(flat_images)\n",
    "print(\"Elements preserved:\", int(original_elements == flat_elements))\n",
    "\n",
    "# Add a channel dimension to match common image layouts.\n",
    "images_with_channel = tf.reshape(images, (batch_size, height, width, 1))\n",
    "print(\"Images with channel shape:\", images_with_channel.shape)\n",
    "\n",
    "# Remove the singleton channel dimension using squeeze.\n",
    "squeezed_images = tf.squeeze(images_with_channel, axis=-1)\n",
    "print(\"Squeezed images shape:\", squeezed_images.shape)\n",
    "\n",
    "# Demonstrate a dtype change and its effect on memory.\n",
    "int_images = tf.cast(images, dtype=tf.int32)\n",
    "print(\"Int images dtype:\", int_images.dtype)\n",
    "\n",
    "# Show that shape stays same while dtype changes.\n",
    "print(\"Int images shape:\", int_images.shape)\n",
    "\n",
    "# Safely reshape back to original layout and verify equality.\n",
    "restored = tf.reshape(flat_images, (batch_size, height, width))\n",
    "print(\"Restored equals original:\", bool(tf.reduce_all(restored == images)) )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ba6bab",
   "metadata": {},
   "source": [
    "## **3. Tensor Devices and Moves**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f779fdfb",
   "metadata": {},
   "source": [
    "### **3.1. CPU and CUDA Tensors**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc9a1b6",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_01/Lecture_C/image_03_01.jpg?v=1769692953\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* CPU and CUDA tensors store similar data\n",
    ">* They run on different hardware, affecting speed\n",
    "\n",
    ">* Keep tensors on one device to maximize performance\n",
    ">* All operands must share device attribute to avoid errors\n",
    "\n",
    ">* Trace each tensor’s device when debugging issues\n",
    ">* Plan clear device moves to prevent mismatch errors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0effff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - CPU and CUDA Tensors\n",
    "\n",
    "# This script explores CPU and CUDA tensors.\n",
    "# It is designed for Google Colab use.\n",
    "# Focus on devices, moves, and errors.\n",
    "\n",
    "# Install PyTorch if not already available.\n",
    "# !pip install torch torchvision torchaudio.\n",
    "\n",
    "# Import torch for tensor operations.\n",
    "import torch\n",
    "\n",
    "# Print PyTorch version and CUDA availability.\n",
    "print(\"torch version:\", torch.__version__, \"cuda:\", torch.cuda.is_available())\n",
    "\n",
    "# Create a simple tensor on the default device.\n",
    "cpu_tensor = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n",
    "\n",
    "# Show the tensor and its device attribute.\n",
    "print(\"cpu_tensor:\", cpu_tensor, \"device:\", cpu_tensor.device)\n",
    "\n",
    "# Decide which device to use for CUDA examples.\n",
    "cuda_available = torch.cuda.is_available()\n",
    "\n",
    "# Safely create a CUDA tensor only if available.\n",
    "if cuda_available:\n",
    "    gpu_tensor = cpu_tensor.to(torch.device(\"cuda\"))\n",
    "else:\n",
    "    gpu_tensor = cpu_tensor.clone()\n",
    "\n",
    "# Show the tensor and its device after move.\n",
    "print(\"gpu_tensor:\", gpu_tensor, \"device:\", gpu_tensor.device)\n",
    "\n",
    "# Demonstrate a valid operation on matching devices.\n",
    "valid_sum = cpu_tensor + cpu_tensor\n",
    "\n",
    "# Print result and confirm device consistency.\n",
    "print(\"valid_sum device:\", valid_sum.device, \"value:\", valid_sum)\n",
    "\n",
    "# Try an unsafe operation mixing devices when CUDA exists.\n",
    "if cuda_available:\n",
    "    try:\n",
    "        bad_result = cpu_tensor + gpu_tensor\n",
    "    except RuntimeError as e:\n",
    "        short_message = str(e).split(\"\\n\")[0]\n",
    "        print(\"mismatch error:\", short_message)\n",
    "\n",
    "# Move gpu_tensor back to CPU for safe operations.\n",
    "back_to_cpu = gpu_tensor.to(torch.device(\"cpu\"))\n",
    "\n",
    "# Confirm both tensors now share the same device.\n",
    "print(\"back_to_cpu device:\", back_to_cpu.device)\n",
    "\n",
    "# Perform elementwise multiplication on CPU tensors.\n",
    "product = cpu_tensor * back_to_cpu\n",
    "\n",
    "# Print final result and its shape and device.\n",
    "print(\"product:\", product, \"shape:\", product.shape, \"device:\", product.device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd2e17a",
   "metadata": {},
   "source": [
    "### **3.2. Tensor Device Transfers**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee74994b",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_01/Lecture_C/image_03_02.jpg?v=1769693001\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Tensors live on specific devices like CPU, GPU\n",
    ">* All tensors must share a device before operations\n",
    "\n",
    ">* Move data between CPU and GPU deliberately\n",
    ">* Minimize transfers because CPU–GPU moves are slow\n",
    "\n",
    ">* Treat device like part of tensor identity\n",
    ">* Plan one-way transfers to avoid mismatches, overhead\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79470590",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Tensor Device Transfers\n",
    "\n",
    "# This script demonstrates tensor device transfers.\n",
    "# It focuses on basic numerical operations safely.\n",
    "# Run cells in order inside Google Colab.\n",
    "\n",
    "# Install PyTorch if not already available.\n",
    "# !pip install torch torchvision torchaudio.\n",
    "\n",
    "# Import torch and check version.\n",
    "import torch\n",
    "\n",
    "# Print the PyTorch version briefly.\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "\n",
    "# Check if a CUDA GPU is available.\n",
    "has_cuda = torch.cuda.is_available()\n",
    "\n",
    "# Select device string based on availability.\n",
    "device = \"cuda\" if has_cuda else \"cpu\"\n",
    "\n",
    "# Print which device will be used.\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Create a small tensor on the CPU.\n",
    "cpu_tensor = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n",
    "\n",
    "# Show its device and values.\n",
    "print(\"cpu_tensor device:\", cpu_tensor.device)\n",
    "\n",
    "# Move tensor to the selected device.\n",
    "model_tensor = cpu_tensor.to(device)\n",
    "\n",
    "# Confirm the new device location.\n",
    "print(\"model_tensor device:\", model_tensor.device)\n",
    "\n",
    "# Create another tensor directly on that device.\n",
    "other_tensor = torch.ones((2, 2), device=device)\n",
    "\n",
    "# Validate shapes before addition.\n",
    "assert model_tensor.shape == other_tensor.shape\n",
    "\n",
    "# Perform a safe addition on same device.\n",
    "result_tensor = model_tensor + other_tensor\n",
    "\n",
    "# Print a short description of the result.\n",
    "print(\"Result device:\", result_tensor.device)\n",
    "\n",
    "# Move result back to CPU for further use.\n",
    "result_cpu = result_tensor.to(\"cpu\")\n",
    "\n",
    "# Show final tensor values on CPU.\n",
    "print(\"Result on CPU:\\n\", result_cpu)\n",
    "\n",
    "# Demonstrate a common device mismatch error safely.\n",
    "try:\n",
    "    # Intentionally mix CPU and device tensors.\n",
    "    bad_sum = cpu_tensor + other_tensor\n",
    "except RuntimeError as e:\n",
    "    # Print a short trimmed error message.\n",
    "    msg = str(e).split(\"\\n\")[0]\n",
    "    print(\"Caught device error:\", msg)\n",
    "\n",
    "# Final confirmation that script finished correctly.\n",
    "print(\"Finished tensor device transfer demo.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d2b394",
   "metadata": {},
   "source": [
    "### **3.3. Common mismatch errors**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b4351b",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_01/Lecture_C/image_03_03.jpg?v=1769693052\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Mixing CPU and GPU tensors causes errors\n",
    ">* Mismatches often appear after partial GPU refactoring\n",
    "\n",
    ">* Incompatible tensor shapes cause operation failures despite broadcasting\n",
    ">* Know each dimension’s role to debug mismatches\n",
    "\n",
    ">* Subtle shape, device, and type mismatches propagate\n",
    ">* Routinely inspect shapes, devices, and conversions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f241c899",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Common mismatch errors\n",
    "\n",
    "# This script shows common tensor mismatch errors.\n",
    "# It focuses on shapes and devices in PyTorch.\n",
    "# Run cells to see errors and simple fixes.\n",
    "\n",
    "# Install PyTorch if not already available.\n",
    "# !pip install torch torchvision torchaudio.\n",
    "\n",
    "# Import torch and check version.\n",
    "import torch\n",
    "\n",
    "# Print the PyTorch version briefly.\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "\n",
    "# Check if a CUDA GPU is available.\n",
    "has_cuda = torch.cuda.is_available()\n",
    "\n",
    "# Create a small CPU tensor for demonstrations.\n",
    "cpu_tensor = torch.ones((2, 3), dtype=torch.float32)\n",
    "\n",
    "# If CUDA exists, create a similar GPU tensor.\n",
    "if has_cuda:\n",
    "    gpu_tensor = torch.ones((2, 3), device=torch.device(\"cuda\"))\n",
    "else:\n",
    "    gpu_tensor = None\n",
    "\n",
    "# Show basic information about the CPU tensor.\n",
    "print(\"CPU tensor device:\", cpu_tensor.device)\n",
    "\n",
    "# If GPU tensor exists, show its device too.\n",
    "if gpu_tensor is not None:\n",
    "    print(\"GPU tensor device:\", gpu_tensor.device)\n",
    "\n",
    "# Demonstrate a device mismatch error safely.\n",
    "if gpu_tensor is not None:\n",
    "    try:\n",
    "        bad_sum = cpu_tensor + gpu_tensor\n",
    "    except RuntimeError as e:\n",
    "        print(\"Device mismatch error caught.\")\n",
    "\n",
    "# Fix the device mismatch by moving tensors.\n",
    "if gpu_tensor is not None:\n",
    "    fixed_sum = cpu_tensor.to(gpu_tensor.device) + gpu_tensor\n",
    "    print(\"Fixed device sum shape:\", fixed_sum.shape)\n",
    "\n",
    "# Create tensors with incompatible shapes for addition.\n",
    "shape_a = torch.ones((2, 3))\n",
    "shape_b = torch.ones((4, 3))\n",
    "\n",
    "# Try an operation that will fail on shapes.\n",
    "try:\n",
    "    bad_shape_sum = shape_a + shape_b\n",
    "except RuntimeError as e:\n",
    "    print(\"Shape mismatch error caught.\")\n",
    "\n",
    "# Create a compatible tensor using broadcasting rules.\n",
    "shape_c = torch.ones((1, 3))\n",
    "\n",
    "# This addition works because of broadcasting.\n",
    "ok_shape_sum = shape_a + shape_c\n",
    "\n",
    "# Print the resulting shape to confirm success.\n",
    "print(\"Broadcasted sum shape:\", ok_shape_sum.shape)\n",
    "\n",
    "# Show a subtle extra dimension mismatch example.\n",
    "logits = torch.randn((2, 3))\n",
    "\n",
    "# Add an unwanted singleton dimension.\n",
    "logits_extra = logits.unsqueeze(2)\n",
    "\n",
    "# Validate shapes before combining tensors.\n",
    "print(\"Logits shape:\", logits.shape)\n",
    "\n",
    "# Print the shape with the extra dimension.\n",
    "print(\"Logits extra shape:\", logits_extra.shape)\n",
    "\n",
    "# Try an operation that fails due to extra dimension.\n",
    "try:\n",
    "    bad_extra = logits + logits_extra\n",
    "except RuntimeError as e:\n",
    "    print(\"Extra dimension mismatch error caught.\")\n",
    "\n",
    "# Fix by reshaping back to the intended shape.\n",
    "fixed_logits = logits_extra.squeeze(2)\n",
    "\n",
    "# Confirm the fixed shape matches the original.\n",
    "print(\"Fixed logits shape:\", fixed_logits.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fde8495",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Tensors Fundamentals**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff13fd2",
   "metadata": {},
   "source": [
    "\n",
    "In this lecture, you learned to:\n",
    "- Create and manipulate PyTorch tensors using common factory functions and indexing operations. \n",
    "- Explain how tensor shapes, dtypes, and devices affect performance and correctness in PyTorch 2.10.0. \n",
    "- Perform basic numerical operations on tensors and debug common shape and device mismatch errors. \n",
    "\n",
    "In the next Module (Module 2), we will go over 'Autograd and Basics'"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
