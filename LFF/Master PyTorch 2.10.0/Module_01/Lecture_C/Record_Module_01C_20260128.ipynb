{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b2bd08c",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Tensors Fundamentals**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3403fbeb",
   "metadata": {},
   "source": [
    ">Last update: 20260128.\n",
    "    \n",
    "By the end of this Lecture, you will be able to:\n",
    "- Create and manipulate PyTorch tensors using common factory functions and indexing operations. \n",
    "- Explain how tensor shapes, dtypes, and devices affect performance and correctness in PyTorch 2.10.0. \n",
    "- Perform basic numerical operations on tensors and debug common shape and device mismatch errors. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1bb4b4",
   "metadata": {},
   "source": [
    "## **1. Creating PyTorch Tensors**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a53f416",
   "metadata": {},
   "source": [
    "### **1.1. Tensor Creation Basics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b58164",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_01/Lecture_C/image_01_01.jpg?v=1769652247\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Tensors are multi-dimensional containers for numerical data\n",
    ">* Flexible tensor creation underpins efficient models and pipelines\n",
    "\n",
    ">* Build tensors from lists and nested lists\n",
    ">* Tensor shape controls how later operations behave\n",
    "\n",
    ">* Choose tensor dtypes based on data meaning\n",
    ">* These choices affect operations, memory, and workflows\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5706c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Tensor Creation Basics\n",
    "\n",
    "# This script introduces basic tensor creation concepts.\n",
    "# It focuses on simple examples and clear outputs.\n",
    "# Run each part to observe tensor behavior.\n",
    "\n",
    "# Uncomment and run this line if torch is missing.\n",
    "# !pip install torch torchvision torchaudio --quiet.\n",
    "\n",
    "# Import torch for tensor creation and manipulation.\n",
    "import torch\n",
    "\n",
    "# Set a manual seed for deterministic tensor values.\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Print the PyTorch version in a single short line.\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "\n",
    "# Create a scalar tensor representing a single temperature.\n",
    "scalar_temp = torch.tensor(23.5)\n",
    "\n",
    "# Create a 1D tensor from a Python list of sales.\n",
    "weekly_sales = torch.tensor([10.0, 12.5, 9.0, 11.0])\n",
    "\n",
    "# Create a 2D tensor from nested lists of exam scores.\n",
    "exam_scores = torch.tensor([[80, 90], [75, 88], [92, 85]])\n",
    "\n",
    "# Print tensors with their shapes and dtypes clearly.\n",
    "print(\"Scalar:\", scalar_temp, scalar_temp.shape, scalar_temp.dtype)\n",
    "\n",
    "# Show weekly sales tensor information in one concise line.\n",
    "print(\"Weekly sales:\", weekly_sales, weekly_sales.shape)\n",
    "\n",
    "# Show exam scores tensor information in one concise line.\n",
    "print(\"Exam scores:\\n\", exam_scores, exam_scores.shape)\n",
    "\n",
    "# Create a tensor of zeros with a specified shape.\n",
    "zeros_image = torch.zeros((2, 3))\n",
    "\n",
    "# Create a tensor of ones with a specified shape.\n",
    "ones_batch = torch.ones((3, 2))\n",
    "\n",
    "# Create a tensor with random values between zero and one.\n",
    "random_features = torch.rand((2, 4))\n",
    "\n",
    "# Print shapes of factory created tensors for comparison.\n",
    "print(\"Zeros shape:\", zeros_image.shape)\n",
    "\n",
    "# Show ones tensor shape and dtype in one short line.\n",
    "print(\"Ones shape and dtype:\", ones_batch.shape, ones_batch.dtype)\n",
    "\n",
    "# Show random tensor shape and a small slice of values.\n",
    "print(\"Random shape and slice:\", random_features.shape, random_features[0])\n",
    "\n",
    "# Demonstrate basic indexing on the weekly sales tensor.\n",
    "first_two_days = weekly_sales[:2]\n",
    "\n",
    "# Demonstrate row and column indexing on exam scores.\n",
    "first_student_scores = exam_scores[0]\n",
    "\n",
    "# Print indexed results to show how slicing works.\n",
    "print(\"First two days:\", first_two_days)\n",
    "\n",
    "# Show first student scores and confirm resulting shape.\n",
    "print(\"First student scores:\", first_student_scores, first_student_scores.shape)\n",
    "\n",
    "# Final line prints a short confirmation message.\n",
    "print(\"Finished basic tensor creation and indexing demo.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e86793",
   "metadata": {},
   "source": [
    "### **1.2. Random and Constant Tensors**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d698c39",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_01/Lecture_C/image_01_02.jpg?v=1769652277\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Random tensors generate data from probability distributions\n",
    ">* Constant tensors repeat one value for efficient setup\n",
    "\n",
    ">* Control tensor shape and random value distribution\n",
    ">* Use devices and seeds for efficiency, reproducibility\n",
    "\n",
    ">* Constant tensors encode reusable fixed values in models\n",
    ">* Choose shape and dtype to match data needs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e70a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Random and Constant Tensors\n",
    "\n",
    "# This script shows random and constant tensors.\n",
    "# It uses TensorFlow to mimic PyTorch behavior.\n",
    "# Focus on shapes dtypes and simple indexing.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import TensorFlow with a short alias.\n",
    "import tensorflow as tf\n",
    "\n",
    "# Print TensorFlow version for reference.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Set a global random seed for reproducibility.\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Choose a device string based on availability.\n",
    "physical_gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "\n",
    "# Select GPU if available otherwise use CPU.\n",
    "if physical_gpus:\n",
    "    device_name = \"/GPU:0\"\n",
    "else:\n",
    "    device_name = \"/CPU:0\"\n",
    "\n",
    "# Show which device will be used for tensors.\n",
    "print(\"Using device:\", device_name)\n",
    "\n",
    "# Use a device context to place tensors.\n",
    "with tf.device(device_name):\n",
    "    # Create a random normal tensor with shape.\n",
    "    random_normal = tf.random.normal(\n",
    "        shape=(2, 3), mean=0.0, stddev=1.0\n",
    "    )\n",
    "\n",
    "    # Create a random uniform tensor within range.\n",
    "    random_uniform = tf.random.uniform(\n",
    "        shape=(2, 3), minval=0.0, maxval=1.0\n",
    "    )\n",
    "\n",
    "    # Create a tensor filled with zeros constant.\n",
    "    zeros_tensor = tf.zeros(shape=(2, 3), dtype=tf.float32)\n",
    "\n",
    "    # Create a tensor filled with ones constant.\n",
    "    ones_tensor = tf.ones(shape=(2, 3), dtype=tf.float32)\n",
    "\n",
    "    # Create a tensor filled with a custom constant.\n",
    "    custom_tensor = tf.fill(dims=(2, 3), value=2.5)\n",
    "\n",
    "# Check that all tensors share the same shape.\n",
    "expected_shape = (2, 3)\n",
    "\n",
    "# Validate shapes defensively before operations.\n",
    "for name, tensor in [\n",
    "    (\"random_normal\", random_normal),\n",
    "    (\"random_uniform\", random_uniform),\n",
    "    (\"zeros_tensor\", zeros_tensor),\n",
    "    (\"ones_tensor\", ones_tensor),\n",
    "    (\"custom_tensor\", custom_tensor),\n",
    "]:\n",
    "\n",
    "    # Assert shape matches the expected shape.\n",
    "    assert tensor.shape == expected_shape, name + \" has wrong shape\"\n",
    "\n",
    "# Demonstrate simple indexing on a random tensor.\n",
    "first_row = random_normal[0]\n",
    "\n",
    "# Demonstrate elementwise addition of tensors.\n",
    "added = random_uniform + ones_tensor\n",
    "\n",
    "# Demonstrate broadcasting with a constant scalar.\n",
    "scaled = random_normal * 0.1\n",
    "\n",
    "# Print a few small results to inspect.\n",
    "print(\"random_normal shape and dtype:\", random_normal.shape, random_normal.dtype)\n",
    "print(\"first_row values:\", first_row.numpy())\n",
    "print(\"zeros_tensor values:\", zeros_tensor.numpy())\n",
    "print(\"ones_tensor values:\", ones_tensor.numpy())\n",
    "print(\"custom_tensor values:\", custom_tensor.numpy())\n",
    "print(\"added tensor values:\", added.numpy())\n",
    "print(\"scaled tensor values:\", scaled.numpy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d8b038",
   "metadata": {},
   "source": [
    "### **1.3. From NumPy arrays**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d9eb33",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_01/Lecture_C/image_01_03.jpg?v=1769652343\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Convert existing NumPy data into PyTorch tensors\n",
    ">* Keep data meaning while enabling GPU and autograd\n",
    "\n",
    ">* Tensor and NumPy array can share memory\n",
    ">* Choose between shared view or independent tensor copy\n",
    "\n",
    ">* Control tensor dtype and device after conversion\n",
    ">* Standardize types and devices for consistent workflows\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd341814",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - From NumPy arrays\n",
    "\n",
    "# This script shows tensors from NumPy arrays.\n",
    "# It focuses on safe conversions and devices.\n",
    "# Run each part and read printed outputs.\n",
    "\n",
    "# Install PyTorch if not already available.\n",
    "# !pip install torch torchvision torchaudio --quiet.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Print PyTorch version in one short line.\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "\n",
    "# Create a small NumPy array on CPU.\n",
    "np_array = np.array([[1.0, 2.0], [3.0, 4.0]], dtype=np.float32)\n",
    "\n",
    "# Show basic NumPy array information.\n",
    "print(\"NumPy array shape and dtype:\", np_array.shape, np_array.dtype)\n",
    "\n",
    "# Convert NumPy array to a PyTorch tensor.\n",
    "tensor_from_np = torch.from_numpy(np_array)\n",
    "\n",
    "# Show tensor details after conversion.\n",
    "print(\"Tensor shape, dtype, device:\", tensor_from_np.shape,\n",
    "      tensor_from_np.dtype, tensor_from_np.device)\n",
    "\n",
    "# Modify tensor and observe NumPy array change.\n",
    "tensor_from_np[0, 0] = 10.0\n",
    "\n",
    "# Print both to see shared memory effect.\n",
    "print(\"After tensor change, NumPy array:\", np_array)\n",
    "print(\"After tensor change, tensor:\", tensor_from_np)\n",
    "\n",
    "# Create an independent tensor copy from NumPy.\n",
    "tensor_copy = torch.tensor(np_array.copy(), dtype=torch.float32)\n",
    "\n",
    "# Modify copy and confirm original does not change.\n",
    "tensor_copy[0, 1] = 20.0\n",
    "\n",
    "# Print to compare original and copied tensor.\n",
    "print(\"Original NumPy after copy change:\", np_array)\n",
    "print(\"Independent tensor copy values:\", tensor_copy)\n",
    "\n",
    "# Detect GPU availability for optional move.\n",
    "has_cuda = torch.cuda.is_available()\n",
    "\n",
    "# Choose device string based on availability.\n",
    "device = torch.device(\"cuda\") if has_cuda else torch.device(\"cpu\")\n",
    "\n",
    "# Move tensor copy to the chosen device.\n",
    "tensor_on_device = tensor_copy.to(device=device)\n",
    "\n",
    "# Print final tensor device and shape.\n",
    "print(\"Final tensor device and shape:\", tensor_on_device.device,\n",
    "      tensor_on_device.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2443ad",
   "metadata": {},
   "source": [
    "## **2. Tensor Shapes and Dtypes**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c785808",
   "metadata": {},
   "source": [
    "### **2.1. Rank and Broadcasting**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f550342d",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_01/Lecture_C/image_02_01.jpg?v=1769652374\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Rank counts tensor dimensions, from scalar upward\n",
    ">* Broadcasting auto-expands smaller tensors, enabling operations\n",
    "\n",
    ">* Broadcasting compares shapes backward, stretching size-one dimensions\n",
    ">* Misaligned broadcasting can silently produce incorrect results\n",
    "\n",
    ">* Broadcasting is memory‑cheap but can backfire later\n",
    ">* Used well, broadcasting gives fast, scalable tensor operations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bea6802",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Rank and Broadcasting\n",
    "\n",
    "# This script explores tensor rank and broadcasting.\n",
    "# It uses TensorFlow tensors as PyTorch stand in.\n",
    "# Focus on shapes dtypes and safe broadcasting.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import TensorFlow and check version.\n",
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Create a scalar tensor rank zero example.\n",
    "scalar = tf.constant(3.0, dtype=tf.float32)\n",
    "print(\"Scalar shape:\", scalar.shape)\n",
    "\n",
    "# Create a vector tensor rank one example.\n",
    "vector = tf.constant([1.0, 2.0, 3.0], dtype=tf.float32)\n",
    "print(\"Vector shape:\", vector.shape)\n",
    "\n",
    "# Create a matrix tensor rank two example.\n",
    "matrix = tf.constant([[1.0, 2.0], [3.0, 4.0]], dtype=tf.float32)\n",
    "print(\"Matrix shape:\", matrix.shape)\n",
    "\n",
    "# Show broadcasting with scalar and matrix.\n",
    "result_scalar = matrix + scalar\n",
    "print(\"Matrix plus scalar shape:\", result_scalar.shape)\n",
    "\n",
    "# Create bias vector for column broadcasting.\n",
    "bias_col = tf.constant([10.0, 20.0], dtype=tf.float32)\n",
    "print(\"Bias column shape:\", bias_col.shape)\n",
    "\n",
    "# Correct broadcasting across columns.\n",
    "correct = matrix + bias_col\n",
    "print(\"Correct broadcast result:\", correct.numpy())\n",
    "\n",
    "# Reshape bias to broadcast across rows.\n",
    "bias_row = tf.reshape(bias_col, (2, 1))\n",
    "print(\"Bias row shape:\", bias_row.shape)\n",
    "\n",
    "# Different broadcasting pattern across rows.\n",
    "wrong = matrix + bias_row\n",
    "print(\"Different broadcast result:\", wrong.numpy())\n",
    "\n",
    "# Verify shapes before a risky operation.\n",
    "if matrix.shape == correct.shape:\n",
    "    safe_sum = matrix + correct\n",
    "else:\n",
    "    safe_sum = tf.zeros_like(matrix)\n",
    "\n",
    "# Print final tensor shape to confirm safety.\n",
    "print(\"Safe sum shape:\", safe_sum.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a416cf7",
   "metadata": {},
   "source": [
    "### **2.2. Precision Tradeoffs**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063f23dc",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_01/Lecture_C/image_02_02.jpg?v=1769652406\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Choosing dtypes balances precision, memory, and speed\n",
    ">* Higher precision improves correctness but costs performance\n",
    "\n",
    ">* Low-precision tensors speed up training and inference\n",
    ">* They risk numerical errors; mixed precision reduces problems\n",
    "\n",
    ">* Precision affects comparisons, thresholds, and rounding behavior\n",
    ">* Choose dtypes carefully to ensure reliable decisions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3dfb9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Precision Tradeoffs\n",
    "\n",
    "# This script explores tensor precision tradeoffs.\n",
    "# It uses TensorFlow tensors as PyTorch standins.\n",
    "# Focus on shapes dtypes and numerical differences.\n",
    "\n",
    "# Install TensorFlow only if missing in environment.\n",
    "# !pip install tensorflow==2.20.0 --quiet.\n",
    "\n",
    "# Import TensorFlow with a short alias.\n",
    "import tensorflow as tf\n",
    "\n",
    "# Print TensorFlow version for reproducibility.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Create a simple float64 tensor baseline.\n",
    "base_tensor = tf.constant([0.1, 0.2, 0.3], dtype=tf.float64)\n",
    "\n",
    "# Show baseline tensor dtype and values.\n",
    "print(\"Base tensor:\", base_tensor, \"dtype=\", base_tensor.dtype)\n",
    "\n",
    "# Create lower precision float32 version.\n",
    "low32_tensor = tf.cast(base_tensor, dtype=tf.float32)\n",
    "\n",
    "# Create even lower precision float16 version.\n",
    "low16_tensor = tf.cast(base_tensor, dtype=tf.float16)\n",
    "\n",
    "# Print dtypes to compare precision choices.\n",
    "print(\"float32 dtype:\", low32_tensor.dtype)\n",
    "print(\"float16 dtype:\", low16_tensor.dtype)\n",
    "\n",
    "# Perform many additions to amplify rounding.\n",
    "steps = 1000\n",
    "\n",
    "# Repeat addition in float64 for reference.\n",
    "ref64 = base_tensor\n",
    "for _ in range(steps):\n",
    "    ref64 = ref64 + base_tensor\n",
    "\n",
    "# Repeat addition in float32 for comparison.\n",
    "ref32 = tf.cast(base_tensor, tf.float32)\n",
    "for _ in range(steps):\n",
    "    ref32 = ref32 + tf.cast(base_tensor, tf.float32)\n",
    "\n",
    "# Repeat addition in float16 for comparison.\n",
    "ref16 = tf.cast(base_tensor, tf.float16)\n",
    "for _ in range(steps):\n",
    "    ref16 = ref16 + tf.cast(base_tensor, tf.float16)\n",
    "\n",
    "# Cast results to float64 for fair comparison.\n",
    "ref32_up = tf.cast(ref32, tf.float64)\n",
    "ref16_up = tf.cast(ref16, tf.float64)\n",
    "\n",
    "# Compute absolute errors versus float64 reference.\n",
    "err32 = tf.abs(ref64 - ref32_up)\n",
    "err16 = tf.abs(ref64 - ref16_up)\n",
    "\n",
    "# Print final values for each precision.\n",
    "print(\"Final float64:\", ref64.numpy())\n",
    "print(\"Final float32:\", ref32_up.numpy())\n",
    "print(\"Final float16:\", ref16_up.numpy())\n",
    "\n",
    "# Print absolute error for each lower precision.\n",
    "print(\"Abs error float32:\", err32.numpy())\n",
    "print(\"Abs error float16:\", err16.numpy())\n",
    "\n",
    "# Show how dtype affects equality comparisons.\n",
    "print(\"float64 == float32:\", tf.math.equal(ref64, ref32_up).numpy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17359f03",
   "metadata": {},
   "source": [
    "### **2.3. Tensor Shape Transformations**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcb9881",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_01/Lecture_C/image_02_03.jpg?v=1769652440\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Understand and control tensor shape changes safely\n",
    ">* Match element counts to avoid shape-related bugs\n",
    "\n",
    ">* Advanced shape changes ensure correct data interpretation\n",
    ">* Squeezing, permuting, reshaping affect broadcasting and gradients\n",
    "\n",
    ">* Cheap reshapes reuse memory; complex ones reorder data\n",
    ">* Plan shapes to reduce costly memory operations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24641559",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Tensor Shape Transformations\n",
    "\n",
    "# This script explores tensor shape transformations.\n",
    "# It focuses on reshaping and dimension operations.\n",
    "# All examples are small and beginner friendly.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import TensorFlow for tensor operations.\n",
    "import tensorflow as tf\n",
    "\n",
    "# Print TensorFlow version for reproducibility.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Create a simple 2D tensor with known shape.\n",
    "base_tensor = tf.constant([[1., 2.], [3., 4.]])\n",
    "\n",
    "# Show original tensor values and shape.\n",
    "print(\"Original tensor:\", base_tensor.numpy(), base_tensor.shape)\n",
    "\n",
    "# Reshape tensor to a flat vector with same elements.\n",
    "flat_tensor = tf.reshape(base_tensor, (4,))\n",
    "\n",
    "# Show flattened tensor and confirm element count.\n",
    "print(\"Flattened tensor:\", flat_tensor.numpy(), flat_tensor.shape)\n",
    "\n",
    "# Reshape back to original two by two shape.\n",
    "restored_tensor = tf.reshape(flat_tensor, (2, 2))\n",
    "\n",
    "# Confirm restored tensor matches original values.\n",
    "print(\"Restored tensor:\", restored_tensor.numpy(), restored_tensor.shape)\n",
    "\n",
    "# Demonstrate invalid reshape caught with element check.\n",
    "num_elements = tf.size(base_tensor).numpy()\n",
    "\n",
    "# Only reshape if element counts match exactly.\n",
    "if num_elements == 6:\n",
    "    bad_shape_tensor = tf.reshape(base_tensor, (3, 2))\n",
    "else:\n",
    "    print(\"Skip invalid reshape, element mismatch.\")\n",
    "\n",
    "# Add a batch dimension using expand_dims operation.\n",
    "batched_tensor = tf.expand_dims(base_tensor, axis=0)\n",
    "\n",
    "# Show new shape with leading batch dimension.\n",
    "print(\"Batched shape:\", batched_tensor.shape)\n",
    "\n",
    "# Remove the batch dimension using squeeze operation.\n",
    "unbatched_tensor = tf.squeeze(batched_tensor, axis=0)\n",
    "\n",
    "# Confirm squeeze restored original shape correctly.\n",
    "print(\"Unbatched shape:\", unbatched_tensor.shape)\n",
    "\n",
    "# Create a fake image batch with channels last layout.\n",
    "images_nhwc = tf.random.uniform((2, 3, 3, 1), seed=7)\n",
    "\n",
    "# Permute dimensions to channels first layout.\n",
    "images_nchw = tf.transpose(images_nhwc, perm=(0, 3, 1, 2))\n",
    "\n",
    "# Show both shapes to highlight dimension reordering.\n",
    "print(\"NHWC shape:\", images_nhwc.shape, \"NCHW shape:\", images_nchw.shape)\n",
    "\n",
    "# Flatten spatial dimensions while keeping batch and channels.\n",
    "flat_features = tf.reshape(images_nchw, (2, 1, 3 * 3))\n",
    "\n",
    "# Show final feature shape used for simple models.\n",
    "print(\"Flat feature shape:\", flat_features.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ee4029",
   "metadata": {},
   "source": [
    "## **3. Tensor Devices and Moves**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effb44f5",
   "metadata": {},
   "source": [
    "### **3.1. CPU and CUDA Tensors**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5eed034",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_01/Lecture_C/image_03_01.jpg?v=1769652483\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* CPU and CUDA tensors share mathematical meaning\n",
    ">* Different hardware and memory make GPUs much faster\n",
    "\n",
    ">* Operations require tensors on the same device\n",
    ">* Data often starts on CPU, then moves to GPU\n",
    "\n",
    ">* Mismatched CPU and CUDA tensors cause runtime errors\n",
    ">* Keep all related tensors on the same device\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b5de9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - CPU and CUDA Tensors\n",
    "\n",
    "# This script explores CPU and CUDA tensors.\n",
    "# It focuses on devices and common mistakes.\n",
    "# Run cells in order inside Google Colab.\n",
    "\n",
    "# Install PyTorch if not already available.\n",
    "# !pip install torch torchvision torchaudio.\n",
    "\n",
    "# Import torch and check its version.\n",
    "import torch\n",
    "\n",
    "# Print the PyTorch version briefly.\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "\n",
    "# Check whether a CUDA GPU is available.\n",
    "has_cuda = torch.cuda.is_available()\n",
    "\n",
    "# Print a short message about CUDA availability.\n",
    "print(\"CUDA available:\", has_cuda)\n",
    "\n",
    "# Create a simple CPU tensor for demonstration.\n",
    "cpu_tensor = torch.tensor([1.0, 2.0, 3.0])\n",
    "\n",
    "# Show the device and dtype of the CPU tensor.\n",
    "print(\"cpu_tensor device:\", cpu_tensor.device)\n",
    "\n",
    "# Safely create a CUDA tensor only if possible.\n",
    "if has_cuda:\n",
    "    # Move the CPU tensor to the CUDA device.\n",
    "    cuda_tensor = cpu_tensor.to(\"cuda\")\n",
    "\n",
    "# If CUDA exists, print its device information.\n",
    "if has_cuda:\n",
    "    print(\"cuda_tensor device:\", cuda_tensor.device)\n",
    "\n",
    "# Demonstrate a valid CPU plus CPU tensor addition.\n",
    "cpu_result = cpu_tensor + torch.tensor([10.0, 20.0, 30.0])\n",
    "\n",
    "# Print the result and its device for clarity.\n",
    "print(\"cpu_result:\", cpu_result, \"on\", cpu_result.device)\n",
    "\n",
    "# If CUDA exists, demonstrate CUDA plus CUDA addition.\n",
    "if has_cuda:\n",
    "    # Create another CUDA tensor with matching shape.\n",
    "    other_cuda = torch.tensor([5.0, 5.0, 5.0], device=\"cuda\")\n",
    "\n",
    "# If CUDA exists, safely add the CUDA tensors.\n",
    "if has_cuda:\n",
    "    cuda_result = cuda_tensor + other_cuda\n",
    "\n",
    "# If CUDA exists, print the CUDA result and device.\n",
    "if has_cuda:\n",
    "    print(\"cuda_result:\", cuda_result, \"on\", cuda_result.device)\n",
    "\n",
    "# Show a common device mismatch error using try block.\n",
    "try:\n",
    "    # Only attempt mismatch if CUDA is available.\n",
    "    if has_cuda:\n",
    "        bad_result = cpu_tensor + cuda_tensor\n",
    "\n",
    "# Catch the runtime error and explain briefly.\n",
    "except RuntimeError as e:\n",
    "    print(\"Caught device mismatch error:\")\n",
    "\n",
    "# If an error was caught, print a short message.\n",
    "if has_cuda:\n",
    "    print(\"Always move tensors to the same device.\")\n",
    "\n",
    "# Finally, confirm shapes before a safe operation.\n",
    "if cpu_tensor.shape == cpu_result.shape:\n",
    "    print(\"Shapes match, safe to add on CPU.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5cfe80",
   "metadata": {},
   "source": [
    "### **3.2. Device Transfer Methods**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1caf2c",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_01/Lecture_C/image_03_02.jpg?v=1769652516\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Data starts on CPU, heavy compute on GPU\n",
    ">* Move tensors CPU↔GPU for processing and results\n",
    "\n",
    ">* Control where tensors and computations run efficiently\n",
    ">* Move data between CPU and specific GPUs thoughtfully\n",
    "\n",
    ">* Plan and batch device transfers to reduce cost\n",
    ">* Trace tensor locations to debug slow or failing code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b824f047",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Device Transfer Methods\n",
    "\n",
    "# This script demonstrates tensor device transfers.\n",
    "# It focuses on basic moves and debugging mismatches.\n",
    "# Run in Colab to explore CPU and GPU devices.\n",
    "\n",
    "# Install PyTorch if not already available.\n",
    "# !pip install torch torchvision torchaudio --quiet.\n",
    "\n",
    "# Import torch and check version.\n",
    "import torch\n",
    "\n",
    "# Print the PyTorch version briefly.\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "\n",
    "# Check whether a CUDA GPU is available.\n",
    "has_cuda = torch.cuda.is_available()\n",
    "\n",
    "# Print which main device will be used.\n",
    "print(\"CUDA available:\", has_cuda)\n",
    "\n",
    "# Decide target device string safely.\n",
    "device = \"cuda\" if has_cuda else \"cpu\"\n",
    "\n",
    "# Show the chosen device for tensors.\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Create a small tensor on the CPU.\n",
    "cpu_tensor = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n",
    "\n",
    "# Confirm the tensor device and shape.\n",
    "print(\"cpu_tensor device:\", cpu_tensor.device)\n",
    "\n",
    "# Move tensor to the chosen device.\n",
    "device_tensor = cpu_tensor.to(device)\n",
    "\n",
    "# Confirm the new tensor device and shape.\n",
    "print(\"device_tensor device:\", device_tensor.device)\n",
    "\n",
    "# Perform a simple numerical operation.\n",
    "result_tensor = device_tensor * 2.0\n",
    "\n",
    "# Move result back to CPU for further use.\n",
    "result_cpu = result_tensor.to(\"cpu\")\n",
    "\n",
    "# Confirm the device of the returned tensor.\n",
    "print(\"result_cpu device:\", result_cpu.device)\n",
    "\n",
    "# Show the numerical values after round trip.\n",
    "print(\"result_cpu values:\\n\", result_cpu)\n",
    "\n",
    "# Create another tensor intentionally on CPU.\n",
    "other_cpu = torch.ones((2, 2), dtype=torch.float32)\n",
    "\n",
    "# Demonstrate a safe operation with matching devices.\n",
    "safe_sum = result_cpu + other_cpu\n",
    "\n",
    "# Print the safe operation result and device.\n",
    "print(\"safe_sum:\", safe_sum, \"on\", safe_sum.device)\n",
    "\n",
    "# Now create a tensor directly on the target device.\n",
    "other_device = torch.ones((2, 2), device=device)\n",
    "\n",
    "# Ensure shapes match before adding tensors.\n",
    "if other_device.shape == device_tensor.shape:\n",
    "    device_sum = device_tensor + other_device\n",
    "else:\n",
    "    device_sum = None\n",
    "\n",
    "# Move device_sum back to CPU if it exists.\n",
    "if device_sum is not None:\n",
    "    device_sum_cpu = device_sum.to(\"cpu\")\n",
    "else:\n",
    "    device_sum_cpu = None\n",
    "\n",
    "# Print final sum to confirm correct device handling.\n",
    "print(\"device_sum_cpu:\", device_sum_cpu)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7f4bb9",
   "metadata": {},
   "source": [
    "### **3.3. Common mismatch errors**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818dd8d2",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_01/Lecture_C/image_03_03.jpg?v=1769652549\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Operations require all tensors on same device\n",
    ">* Device mismatches cause errors; check error locations\n",
    "\n",
    ">* Shape mismatches break operations and broadcasting assumptions\n",
    ">* Compare tensor dimensions in errors to debug\n",
    "\n",
    ">* Mixed device and shape issues cause confusion\n",
    ">* Regularly check tensor devices and shapes during training\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc8fe58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Common mismatch errors\n",
    "\n",
    "# This script shows common tensor mismatch errors.\n",
    "# It focuses on shapes and devices in PyTorch.\n",
    "# Run cells stepwise and read printed explanations.\n",
    "\n",
    "# Install PyTorch if not already available in environment.\n",
    "# !pip install torch torchvision torchaudio --quiet.\n",
    "\n",
    "# Import torch and check version for reproducibility.\n",
    "import torch\n",
    "\n",
    "# Print the PyTorch version in one concise line.\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "\n",
    "# Check if a CUDA GPU is available for this runtime.\n",
    "device_gpu_available = torch.cuda.is_available()\n",
    "\n",
    "# Decide a default device string based on availability.\n",
    "default_device = \"cuda\" if device_gpu_available else \"cpu\"\n",
    "\n",
    "# Print which device will be used for demonstrations.\n",
    "print(\"Default device used in demo:\", default_device)\n",
    "\n",
    "# Create a simple CPU tensor for later comparisons.\n",
    "cpu_tensor = torch.tensor([1.0, 2.0, 3.0])\n",
    "\n",
    "# Move a copy of that tensor to the chosen device.\n",
    "device_tensor = cpu_tensor.to(default_device)\n",
    "\n",
    "# Show devices of both tensors to highlight differences.\n",
    "print(\"cpu_tensor device:\", cpu_tensor.device)\n",
    "\n",
    "# Print the device of the moved tensor for clarity.\n",
    "print(\"device_tensor device:\", device_tensor.device)\n",
    "\n",
    "# Demonstrate a safe operation with matching devices.\n",
    "safe_sum = cpu_tensor + cpu_tensor\n",
    "\n",
    "# Print result of safe addition to confirm success.\n",
    "print(\"Safe CPU addition result:\", safe_sum)\n",
    "\n",
    "# Try an unsafe device mix inside a protected block.\n",
    "try:\n",
    "    # Attempt to add CPU tensor with device tensor.\n",
    "    bad_sum = cpu_tensor + device_tensor\n",
    "except RuntimeError as error:\n",
    "    # Print a short message about the device mismatch.\n",
    "    print(\"Device mismatch error caught:\")\n",
    "\n",
    "    # Print only the first line of the error message.\n",
    "    print(str(error).split(\"\\n\")[0])\n",
    "\n",
    "# Create two tensors with clearly incompatible shapes.\n",
    "shape_a = torch.ones(2, 3)\n",
    "\n",
    "# Another tensor with a different incompatible shape.\n",
    "shape_b = torch.ones(4, 3)\n",
    "\n",
    "# Show shapes so learners can compare dimensions.\n",
    "print(\"shape_a size:\", tuple(shape_a.shape))\n",
    "\n",
    "# Print the second tensor shape for direct comparison.\n",
    "print(\"shape_b size:\", tuple(shape_b.shape))\n",
    "\n",
    "# Try an addition that will fail due to shapes.\n",
    "try:\n",
    "    # This operation cannot broadcast correctly.\n",
    "    bad_shape_sum = shape_a + shape_b\n",
    "except RuntimeError as error:\n",
    "    # Indicate that a shape mismatch was detected.\n",
    "    print(\"Shape mismatch error caught:\")\n",
    "\n",
    "    # Again print only the first line of the error text.\n",
    "    print(str(error).split(\"\\n\")[0])\n",
    "\n",
    "# Fix the shape mismatch using a compatible reshape.\n",
    "shape_b_fixed = shape_b.view(2, 2, 3)\n",
    "\n",
    "# Reduce one dimension so broadcasting becomes valid.\n",
    "shape_b_reduced = shape_b_fixed.sum(dim=1)\n",
    "\n",
    "# Confirm the new shape after reduction operation.\n",
    "print(\"shape_b_reduced size:\", tuple(shape_b_reduced.shape))\n",
    "\n",
    "# Perform a now valid addition with matching shapes.\n",
    "fixed_sum = shape_a + shape_b_reduced\n",
    "\n",
    "# Print final successful result shape to conclude.\n",
    "print(\"Fixed addition result size:\", tuple(fixed_sum.shape))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8811a30",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Tensors Fundamentals**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f447be5e",
   "metadata": {},
   "source": [
    "\n",
    "In this lecture, you learned to:\n",
    "- Create and manipulate PyTorch tensors using common factory functions and indexing operations. \n",
    "- Explain how tensor shapes, dtypes, and devices affect performance and correctness in PyTorch 2.10.0. \n",
    "- Perform basic numerical operations on tensors and debug common shape and device mismatch errors. \n",
    "\n",
    "In the next Module (Module 2), we will go over 'Autograd and Basics'"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
