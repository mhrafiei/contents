{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "054adc03",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Environment Setup**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db84d57b",
   "metadata": {},
   "source": [
    ">Last update: 20260129.\n",
    "    \n",
    "By the end of this Lecture, you will be able to:\n",
    "- Install PyTorch 2.10.0 with appropriate CUDA or CPU support on the target platform. \n",
    "- Configure a Python development environment suitable for PyTorch projects, including virtual environments and key extensions. \n",
    "- Organize a basic PyTorch project folder structure that supports experiments, logging, and version control. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac6a391",
   "metadata": {},
   "source": [
    "## **1. Install PyTorch Environment**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa5311c",
   "metadata": {},
   "source": [
    "### **1.1. pip and conda basics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52550fe",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_01/Lecture_B/image_01_01.jpg?v=1769691511\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Pip and conda are different package managers\n",
    ">* Conda also handles CUDA and system dependencies\n",
    "\n",
    ">* Conda suits shared clusters and complex GPU dependencies\n",
    ">* Pip fits lightweight, scriptable, cross-platform Python workflows\n",
    "\n",
    ">* Use pip or conda for isolated environments\n",
    ">* Isolated setups keep projects reproducible and conflict-free\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a922f9",
   "metadata": {},
   "source": [
    "### **1.2. CUDA Compatibility Basics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1d9783",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_01/Lecture_B/image_01_02.jpg?v=1769691534\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* CUDA version must match drivers and library\n",
    ">* Mismatched versions cause GPU errors after installation\n",
    "\n",
    ">* Library bundles CUDA runtime; toolkit often unnecessary\n",
    ">* Match driverâ€™s supported CUDA version to build\n",
    "\n",
    ">* Choose CPU-only if no compatible NVIDIA GPU\n",
    ">* Use CUDA builds where GPUs speed up training\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76af78d5",
   "metadata": {},
   "source": [
    "### **1.3. Verifying Installation Script**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27e3f4a",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_01/Lecture_B/image_01_03.jpg?v=1769691553\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Run quick checks right after installing PyTorch\n",
    ">* Confirm imports, basic tensors, and GPU visibility\n",
    "\n",
    ">* Create a simple, repeatable environment check routine\n",
    ">* Run basic imports and tensor operations to detect issues\n",
    "\n",
    ">* GPU verification prevents hidden configuration mistakes\n",
    ">* Reusable scripts test GPUs, tensors, and reliability\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1093ad44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Verifying Installation Script\n",
    "\n",
    "# This script verifies a basic PyTorch style environment.\n",
    "# It focuses on import checks and simple tensor operations.\n",
    "# Use it after installing deep learning libraries locally.\n",
    "\n",
    "# Install torch if available in your environment manager.\n",
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121.\n",
    "\n",
    "# Import standard library modules for environment checks.\n",
    "import sys\n",
    "import platform\n",
    "import random\n",
    "\n",
    "# Import numpy for simple numeric comparisons.\n",
    "import numpy as np\n",
    "\n",
    "# Try importing torch inside a safe protective block.\n",
    "try:\n",
    "    import torch\n",
    "    TORCH_AVAILABLE = True\n",
    "except Exception as import_error:\n",
    "    TORCH_AVAILABLE = False\n",
    "    torch = None\n",
    "\n",
    "\n",
    "# Define a helper function for compact status printing.\n",
    "def print_status(label, value):\n",
    "    print(f\"{label:<28}: {value}\")\n",
    "\n",
    "# Set deterministic seeds for reproducible behavior.\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Print basic Python and system information summary.\n",
    "print_status(\"Python version\", sys.version.split()[0])\n",
    "print_status(\"Platform\", platform.platform())\n",
    "\n",
    "# Report whether torch import succeeded or failed.\n",
    "print_status(\"Torch import success\", TORCH_AVAILABLE)\n",
    "\n",
    "# Exit early if torch is not available at all.\n",
    "if not TORCH_AVAILABLE:\n",
    "    print(\"Torch is not available, please install it.\")\n",
    "\n",
    "# Only run further checks when torch import succeeded.\n",
    "if TORCH_AVAILABLE:\n",
    "\n",
    "    # Print the detected torch version string.\n",
    "    print_status(\"Torch version\", torch.__version__)\n",
    "\n",
    "    # Check whether CUDA support is compiled and available.\n",
    "    cuda_available = torch.cuda.is_available()\n",
    "    print_status(\"CUDA available\", cuda_available)\n",
    "\n",
    "    # Determine the device string based on CUDA availability.\n",
    "    device = \"cuda\" if cuda_available else \"cpu\"\n",
    "    print_status(\"Selected device\", device)\n",
    "\n",
    "    # Create a small tensor on the selected device.\n",
    "    tensor = torch.tensor([1.0, 2.0, 3.0], device=device)\n",
    "\n",
    "    # Verify tensor shape and device properties explicitly.\n",
    "    shape_ok = tensor.shape == (3,)\n",
    "    device_ok = str(tensor.device) == device or device == \"cpu\"\n",
    "\n",
    "    # Perform a simple arithmetic operation on the tensor.\n",
    "    result = tensor * 2.0\n",
    "\n",
    "    # Move result back to cpu for safe numpy conversion.\n",
    "    result_cpu = result.to(\"cpu\")\n",
    "\n",
    "    # Convert to numpy and check numeric correctness.\n",
    "    result_np = result_cpu.numpy()\n",
    "    expected_np = np.array([2.0, 4.0, 6.0], dtype=np.float32)\n",
    "    values_ok = np.allclose(result_np, expected_np)\n",
    "\n",
    "    # Summarize tensor checks in a compact status line.\n",
    "    all_ok = bool(shape_ok and device_ok and values_ok)\n",
    "    print_status(\"Tensor verification\", all_ok)\n",
    "\n",
    "    # When CUDA is available, print basic device information.\n",
    "    if cuda_available:\n",
    "\n",
    "        # Get number of visible CUDA devices from torch.\n",
    "        device_count = torch.cuda.device_count()\n",
    "\n",
    "        # Read the name of the first CUDA device safely.\n",
    "        first_name = torch.cuda.get_device_name(0) if device_count > 0 else \"None\"\n",
    "\n",
    "        # Print compact CUDA device summary information.\n",
    "        print_status(\"CUDA device count\", device_count)\n",
    "        print_status(\"First CUDA device\", first_name)\n",
    "\n",
    "    # Final message summarizing overall environment health.\n",
    "    print(\"Environment verification script finished.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e05ff3a",
   "metadata": {},
   "source": [
    "## **2. PyTorch Dev Environment**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8727cf3f",
   "metadata": {},
   "source": [
    "### **2.1. Choosing Python Environments**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5122422",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_01/Lecture_B/image_02_01.jpg?v=1769691619\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Use isolated Python environments per project setup\n",
    ">* Improves reproducibility, avoids conflicts, eases collaboration\n",
    "\n",
    ">* Choose env tools based on system and needs\n",
    ">* Stick to one method and learn core commands\n",
    "\n",
    ">* Plan environments to work across local and cloud\n",
    ">* Standardize versions to enable collaboration and reproducibility\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31f2508",
   "metadata": {},
   "source": [
    "### **2.2. VS Code and Jupyter**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c81d0c2",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_01/Lecture_B/image_02_02.jpg?v=1769691635\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* VS Code centralizes coding, experiments, and results\n",
    ">* Correct interpreter enables smart PyTorch hints and help\n",
    "\n",
    ">* VS Code extensions run notebooks inside the editor\n",
    ">* Prototype in notebooks, then refactor into scripts\n",
    "\n",
    ">* Jupyter in VS Code supports exploratory experiments\n",
    ">* Per notebook kernels keep environments isolated and reproducible\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b562665a",
   "metadata": {},
   "source": [
    "### **2.3. Dependency Management Files**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238e01f4",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_01/Lecture_B/image_02_03.jpg?v=1769691655\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Dependency files list required packages and versions\n",
    ">* They enable reproducible, shareable PyTorch project environments\n",
    "\n",
    ">* Start with flexible, high-level dependency lists\n",
    ">* Later pin exact versions for team reproducibility\n",
    "\n",
    ">* Committed dependency files capture environment history precisely\n",
    ">* They enable reproducible runs and reliable automation pipelines\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a71c5a1",
   "metadata": {},
   "source": [
    "## **3. PyTorch Project Structure**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033d0113",
   "metadata": {},
   "source": [
    "### **3.1. Source and Tests Layout**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02521d61",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_01/Lecture_B/image_03_01.jpg?v=1769691677\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Separate source and tests for clear structure\n",
    ">* Use src and tests folders for maintainability\n",
    "\n",
    ">* Split source into data, model, training modules\n",
    ">* Modular design enables easy swapping and reuse\n",
    "\n",
    ">* Match test files to each source module\n",
    ">* Use tests as safety net and CI\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51e2970",
   "metadata": {},
   "source": [
    "### **3.2. Configs and Logs**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9e7b2b",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_01/Lecture_B/image_03_02.jpg?v=1769691694\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Keep configs and logs in separate folders\n",
    ">* This makes experiments reproducible, clear, and traceable\n",
    "\n",
    ">* Group configs by models, datasets, training modes\n",
    ">* Use versioned config files as experiment truth\n",
    "\n",
    ">* Use per-run log folders with configs, artifacts\n",
    ">* Keep logs separate to compare, backup, integrate\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1a3c4d",
   "metadata": {},
   "source": [
    "### **3.3. Git and Ignore Files**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eff126f",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master PyTorch 2.10.0/Module_01/Lecture_B/image_03_03.jpg?v=1769691711\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Track shareable code, configs, docs, small data\n",
    ">* Ignore large, generated, machine-specific, sensitive artifacts\n",
    "\n",
    ">* Use ignore files to skip unwanted paths\n",
    ">* Prevent experiment outputs from cluttering git history\n",
    "\n",
    ">* Ignore rules separate core code from artifacts\n",
    ">* Helps protect sensitive data and keep projects scalable\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc884e4",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Environment Setup**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f082bc",
   "metadata": {},
   "source": [
    "\n",
    "In this lecture, you learned to:\n",
    "- Install PyTorch 2.10.0 with appropriate CUDA or CPU support on the target platform. \n",
    "- Configure a Python development environment suitable for PyTorch projects, including virtual environments and key extensions. \n",
    "- Organize a basic PyTorch project folder structure that supports experiments, logging, and version control. \n",
    "\n",
    "In the next Lecture (Lecture C), we will go over 'Tensors Fundamentals'"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
