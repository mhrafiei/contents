{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dff4c0b6",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Optimizing Pipelines**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d6d7db",
   "metadata": {},
   "source": [
    ">Last update: 20251228.\n",
    "    \n",
    "By the end of this Lecture, you will be able to:\n",
    "- Optimize Polars IO operations by choosing appropriate file formats and scan options. \n",
    "- Reduce memory usage in Polars pipelines through column pruning, type choices, and lazy evaluation. \n",
    "- Profile and benchmark Polars pipelines against existing Pandas implementations to quantify performance improvements. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07550ac3",
   "metadata": {},
   "source": [
    "## **1. Efficient Polars IO**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934ba0b0",
   "metadata": {},
   "source": [
    "### **1.1. Choosing CSV vs Parquet**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f8eab4",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Pandas to Polars Migration/Module_04/Lecture_B/image_01_01.jpg?v=1766900451\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* CSV is simple and shareable but slow\n",
    ">* Parquet is compressed, typed, and much faster\n",
    "\n",
    ">* CSV needs heavy parsing, slowing large scans\n",
    ">* Parquet skips parsing, enabling faster repeated queries\n",
    "\n",
    ">* Use CSV at system edges for interoperability\n",
    ">* Convert cleaned data to Parquet for efficient analytics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad086bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Choosing CSV vs Parquet\n",
    "\n",
    "# Compare reading CSV versus Parquet with Polars for simple analytics demonstration.\n",
    "# Show how file formats affect read speed and file size clearly here.\n",
    "# Help choose CSV or Parquet for faster repeated analytical queries overall.\n",
    "\n",
    "import time\n",
    "import os\n",
    "import polars as pl\n",
    "\n",
    "# Create a small synthetic dataset with several numeric columns for demonstration.\n",
    "num_rows = 500000\n",
    "sales_df = pl.DataFrame({\"store_id\": pl.arange(0, num_rows), \"day_index\": pl.arange(0, num_rows), \"units_sold\": pl.arange(0, num_rows) % 50, \"unit_price\": pl.repeat(19.99, num_rows)})\n",
    "\n",
    "# Define file paths inside current working directory for CSV and Parquet outputs.\n",
    "csv_path = \"demo_sales.csv\"\n",
    "parquet_path = \"demo_sales.parquet\"\n",
    "\n",
    "# Write dataset to CSV format without index column, using default comma delimiter.\n",
    "start_write_csv = time.time()\n",
    "sales_df.write_csv(csv_path)\n",
    "end_write_csv = time.time()\n",
    "\n",
    "# Write dataset to Parquet format using default compression settings for efficiency.\n",
    "start_write_parquet = time.time()\n",
    "sales_df.write_parquet(parquet_path)\n",
    "end_write_parquet = time.time()\n",
    "\n",
    "# Measure file sizes in bytes for both formats using operating system utilities.\n",
    "csv_size_bytes = os.path.getsize(csv_path)\n",
    "parquet_size_bytes = os.path.getsize(parquet_path)\n",
    "\n",
    "# Time reading CSV using lazy scan, then collect to execute the query fully.\n",
    "start_read_csv = time.time()\n",
    "result_csv = pl.scan_csv(csv_path).select([pl.col(\"units_sold\").sum()]).collect()\n",
    "end_read_csv = time.time()\n",
    "\n",
    "# Time reading Parquet using lazy scan, then collect to execute the query fully.\n",
    "start_read_parquet = time.time()\n",
    "result_parquet = pl.scan_parquet(parquet_path).select([pl.col(\"units_sold\").sum()]).collect()\n",
    "end_read_parquet = time.time()\n",
    "\n",
    "# Helper function to format bytes as kilobytes with two decimal places for readability.\n",
    "def format_kilobytes(byte_count):\n",
    "    return f\"{byte_count / 1024:.2f} KB\"\n",
    "\n",
    "# Print summary comparing file sizes and read times for CSV versus Parquet formats.\n",
    "print(\"CSV size:\", format_kilobytes(csv_size_bytes), \"Read seconds:\", round(end_read_csv - start_read_csv, 4))\n",
    "print(\"Parquet size:\", format_kilobytes(parquet_size_bytes), \"Read seconds:\", round(end_read_parquet - start_read_parquet, 4))\n",
    "\n",
    "# Print sums to confirm both formats contain identical numeric information after reading.\n",
    "print(\"CSV units_sold sum:\", int(result_csv[0, 0]))\n",
    "print(\"Parquet units_sold sum:\", int(result_parquet[0, 0]))\n",
    "\n",
    "# Print simple recommendation based on observed read times and file sizes here.\n",
    "print(\"Parquet usually wins for repeated analytical reads on large datasets overall.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399d0d5a",
   "metadata": {},
   "source": [
    "### **1.2. Streaming And Chunked Reads**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f2bb59",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Pandas to Polars Migration/Module_04/Lecture_B/image_01_02.jpg?v=1766900469\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Process large datasets by streaming small chunks\n",
    ">* Compute while reading to keep memory usage low\n",
    "\n",
    ">* Polars stores and processes data in chunks\n",
    ">* Chunk-wise processing cuts memory use and runtime\n",
    "\n",
    ">* Best for single-pass filters and aggregations\n",
    ">* Enables scalable, memory-efficient analytics on huge datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccb52b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Streaming And Chunked Reads\n",
    "\n",
    "# Demonstrate Polars streaming and chunked reads with a simple aggregation example.\n",
    "# Compare normal eager reading with lazy streaming over a synthetic CSV dataset.\n",
    "# Show memory friendly processing by scanning and aggregating data in manageable chunks.\n",
    "\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Create a synthetic CSV file representing large daily sales records.\n",
    "num_rows = 200000\n",
    "rng = np.random.default_rng(seed=42)\n",
    "\n",
    "cities = [\"New York\", \"Chicago\", \"Dallas\", \"Seattle\"]\n",
    "city_data = rng.choice(cities, size=num_rows)\n",
    "\n",
    "\n",
    "amount_data = rng.integers(low=5, high=500, size=num_rows)\n",
    "\n",
    "\n",
    "csv_path = \"sales_data.csv\"\n",
    "\n",
    "\n",
    "with open(csv_path, \"w\") as f:\n",
    "    f.write(\"city,amount\\n\")\n",
    "    for city, amount in zip(city_data, amount_data):\n",
    "        f.write(f\"{city},{amount}\\n\")\n",
    "\n",
    "\n",
    "file_size_mb = os.path.getsize(csv_path) / (1024 * 1024)\n",
    "print(f\"CSV file size megabytes approximately: {file_size_mb:.2f}\")\n",
    "\n",
    "\n",
    "# Eager read loads entire file into memory before computing aggregations.\n",
    "\n",
    "\n",
    "sales_eager = pl.read_csv(csv_path)\n",
    "\n",
    "\n",
    "result_eager = sales_eager.group_by(\"city\").agg(pl.col(\"amount\").sum().alias(\"total_amount\"))\n",
    "\n",
    "\n",
    "print(\"Eager mode totals by city dollars:\")\n",
    "print(result_eager)\n",
    "\n",
    "\n",
    "# Lazy streaming read processes data in chunks and avoids full materialization.\n",
    "\n",
    "\n",
    "sales_lazy = pl.scan_csv(csv_path)\n",
    "\n",
    "\n",
    "result_streaming = (\n",
    "    sales_lazy.group_by(\"city\")\n",
    "    .agg(pl.col(\"amount\").sum().alias(\"total_amount\"))\n",
    "    .collect(streaming=True)\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Streaming mode totals by city dollars:\")\n",
    "print(result_streaming)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056e4f4a",
   "metadata": {},
   "source": [
    "### **1.3. Large File Scan Tuning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cf98da",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Pandas to Polars Migration/Module_04/Lecture_B/image_01_03.jpg?v=1766900485\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Configure scans carefully for huge datasets\n",
    ">* Select needed columns and time ranges only\n",
    "\n",
    ">* Balance parallel reads with available memory limits\n",
    ">* Tune batch size, threads, and IO per environment\n",
    "\n",
    ">* Align scans with data partitions and layout\n",
    ">* Refine filters and layout to minimize IO\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d53ea63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Large File Scan Tuning\n",
    "\n",
    "# Demonstrate Polars large file scan tuning with column and row selection.\n",
    "# Compare full scan versus tuned scan using a synthetic large dataset.\n",
    "# Show timing differences when pruning columns and pushing down filters.\n",
    "\n",
    "import time\n",
    "import polars as pl\n",
    "\n",
    "# Create a synthetic dataset representing large log records.\n",
    "# We keep row count moderate for Colab memory safety.\n",
    "row_count = 2_000_000\n",
    "\n",
    "# Build a lazy frame with several columns including timestamps and error codes.\n",
    "lf = pl.DataFrame(\n",
    "    {\n",
    "        \"timestamp\": pl.datetime_range(\n",
    "            low=pl.datetime(2024, 1, 1),\n",
    "            high=pl.datetime(2024, 12, 31),\n",
    "            interval=\"1m\",\n",
    "            eager=True,\n",
    "        )[:row_count],\n",
    "        \"error_code\": pl.randint(100, 600, row_count),\n",
    "        \"user_id\": pl.randint(1, 50_000, row_count),\n",
    "        \"payload\": pl.repeat(\"some long text payload\", row_count),\n",
    "    }\n",
    ").lazy()\n",
    "\n",
    "# Write the dataset to Parquet to simulate a large columnar log file.\n",
    "parquet_path = \"large_logs.parquet\"\n",
    "lf.collect().write_parquet(parquet_path)\n",
    "\n",
    "# Define a helper function for timing lazy queries with clear labels.\n",
    "def run_and_time(label, lazy_frame):\n",
    "    start = time.time()\n",
    "    result = lazy_frame.collect()\n",
    "    duration = time.time() - start\n",
    "    print(f\"{label} took {duration:.3f} seconds, rows {result.height}.\")\n",
    "\n",
    "\n",
    "# Scenario one: naive scan reading all columns and all rows.\n",
    "lf_naive = pl.scan_parquet(parquet_path)\n",
    "\n",
    "# Scenario two: tuned scan selecting needed columns and recent rows only.\n",
    "lf_tuned = (\n",
    "    pl.scan_parquet(parquet_path)\n",
    "    .select([\"timestamp\", \"error_code\"])\n",
    "    .filter(pl.col(\"timestamp\") >= pl.datetime(2024, 12, 1))\n",
    ")\n",
    "\n",
    "# Scenario three: tuned scan with smaller row groups for memory friendliness.\n",
    "# We simulate by reading then rechunking into smaller groups.\n",
    "lf_small_groups = (\n",
    "    pl.scan_parquet(parquet_path)\n",
    "    .with_row_count(\"row_index\")\n",
    "    .filter(pl.col(\"row_index\") % 10 == 0)\n",
    "    .select([\"timestamp\", \"error_code\"])\n",
    ")\n",
    "\n",
    "# Run and time each scenario to observe performance differences.\n",
    "run_and_time(\"Naive full scan\", lf_naive)\n",
    "run_and_time(\"Tuned column and date scan\", lf_tuned)\n",
    "run_and_time(\"Sampled small group scan\", lf_small_groups)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9fc23b",
   "metadata": {},
   "source": [
    "## **2. Memory Efficient Pipelines**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4c985d",
   "metadata": {},
   "source": [
    "### **2.1. Column Pruning Basics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a47641",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Pandas to Polars Migration/Module_04/Lecture_B/image_02_01.jpg?v=1766900511\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Select only needed columns to save memory\n",
    ">* Lazy engines skip reading unused columns entirely\n",
    "\n",
    ">* Select only columns needed for current task\n",
    ">* Fewer columns reduce disk reads and memory use\n",
    "\n",
    ">* Lazy evaluation identifies only truly needed columns\n",
    ">* Pruned columns speed up joins, aggregations, scalability\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b03400d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Column Pruning Basics\n",
    "\n",
    "# Demonstrate column pruning using Polars lazy scanning basics.\n",
    "# Compare scanning all columns versus selecting only needed columns.\n",
    "# Show memory friendly behavior using simple timing and column counts.\n",
    "\n",
    "import polars as pl\n",
    "import time\n",
    "\n",
    "# Create a wide DataFrame with many unnecessary columns.\n",
    "num_rows = 1_000_000\n",
    "wide_df = pl.DataFrame({\n",
    "    \"customer_id\": pl.arange(0, num_rows),\n",
    "    \"order_amount\": pl.arange(0, num_rows) * 1.5,\n",
    "    \"order_date\": pl.date_range(\n",
    "        low=pl.datetime(2020, 1, 1),\n",
    "        high=pl.datetime(2020, 1, 10),\n",
    "        interval=\"1m\",\n",
    "        eager=True,\n",
    "    )[:num_rows],\n",
    "    \"channel\": [\"email\"] * num_rows,\n",
    "    \"big_text\": [\"unused description\"] * num_rows,\n",
    "})\n",
    "\n",
    "# Write the DataFrame to Parquet to simulate warehouse storage.\n",
    "file_path = \"wide_orders.parquet\"\n",
    "wide_df.write_parquet(file_path)\n",
    "\n",
    "# Define a lazy scan that reads all columns without pruning.\n",
    "scan_all = pl.scan_parquet(file_path)\n",
    "\n",
    "# Define a lazy scan that selects only needed columns for aggregation.\n",
    "scan_pruned = pl.scan_parquet(file_path).select([\n",
    "    \"customer_id\",\n",
    "    \"order_amount\",\n",
    "    \"channel\",\n",
    "])\n",
    "\n",
    "# Define a simple aggregation that uses only selected columns.\n",
    "agg_all = scan_all.groupby(\"channel\").agg([\n",
    "    pl.col(\"order_amount\").mean().alias(\"avg_order_amount\"),\n",
    "])\n",
    "\n",
    "# Define the same aggregation using the pruned lazy scan.\n",
    "agg_pruned = scan_pruned.groupby(\"channel\").agg([\n",
    "    pl.col(\"order_amount\").mean().alias(\"avg_order_amount\"),\n",
    "])\n",
    "\n",
    "# Helper function to time lazy query execution with minimal overhead.\n",
    "def run_and_time_lazy(query, label):\n",
    "    start = time.time()\n",
    "    result = query.collect()\n",
    "    duration = time.time() - start\n",
    "    print(f\"{label} took {duration:.4f} seconds, columns: {result.width}.\")\n",
    "    return result\n",
    "\n",
    "# Run both queries and compare timings and column counts.\n",
    "result_all = run_and_time_lazy(agg_all, \"Aggregation with all columns\")\n",
    "\n",
    "result_pruned = run_and_time_lazy(agg_pruned, \"Aggregation with pruned columns\")\n",
    "\n",
    "# Show that both results are logically identical despite different column usage.\n",
    "print(\"Results equal:\", result_all.frame_equal(result_pruned))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fdc763",
   "metadata": {},
   "source": [
    "### **2.2. Compact Data Types**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b54f85",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Pandas to Polars Migration/Module_04/Lecture_B/image_02_02.jpg?v=1766900526\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Smaller data types cut per-column memory use\n",
    ">* Match type to value range to save gigabytes\n",
    "\n",
    ">* Match data types to real value ranges\n",
    ">* Use categorical codes instead of repeated strings\n",
    "\n",
    ">* Use dates and booleans to save memory\n",
    ">* Right-sized types compound into faster, larger pipelines\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46133b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Compact Data Types\n",
    "\n",
    "# Show how compact data types reduce memory usage in Polars pipelines.\n",
    "# Compare wide default types with smaller integer and categorical types.\n",
    "# Print memory usage for both schemas using a simple retail style example.\n",
    "\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "\n",
    "# Create a small example dataset representing daily store sales counts.\n",
    "num_rows = 1_000_000\n",
    "store_ids = np.random.randint(1, 51, size=num_rows)\n",
    "items_sold = np.random.randint(0, 500, size=num_rows)\n",
    "category_names = np.random.choice([\"Grocery\", \"Clothing\", \"Electronics\", \"Toys\"], size=num_rows)\n",
    "\n",
    "# Build a lazy frame using default wide integer and string types.\n",
    "lf_wide = pl.LazyFrame(\n",
    "    {\n",
    "        \"store_id\": store_ids,\n",
    "        \"items_sold\": items_sold,\n",
    "        \"category\": category_names,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Build a lazy frame using compact integer and categorical types.\n",
    "lf_compact = pl.LazyFrame(\n",
    "    {\n",
    "        \"store_id\": store_ids.astype(\"int16\"),\n",
    "        \"items_sold\": items_sold.astype(\"int16\"),\n",
    "        \"category\": category_names,\n",
    "    }\n",
    ").with_columns(pl.col(\"category\").cast(pl.Categorical))\n",
    "\n",
    "# Collect both frames into memory so we can inspect their schemas and sizes.\n",
    "df_wide = lf_wide.collect()\n",
    "df_compact = lf_compact.collect()\n",
    "\n",
    "# Helper function to estimate memory usage using Polars estimated_size method.\n",
    "def estimate_megabytes(df: pl.DataFrame) -> float:\n",
    "    return df.estimated_size() / (1024 * 1024)\n",
    "\n",
    "# Print schemas and memory usage for wide and compact representations.\n",
    "print(\"Wide schema types:\", df_wide.schema)\n",
    "print(\"Compact schema types:\", df_compact.schema)\n",
    "print(\"Wide frame megabytes:\", round(estimate_megabytes(df_wide), 2))\n",
    "print(\"Compact frame megabytes:\", round(estimate_megabytes(df_compact), 2))\n",
    "print(\"Memory reduction megabytes:\", round(estimate_megabytes(df_wide) - estimate_megabytes(df_compact), 2))\n",
    "print(\"Memory reduction percent:\", round(100 * (estimate_megabytes(df_wide) - estimate_megabytes(df_compact)) / estimate_megabytes(df_wide), 2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b503e09",
   "metadata": {},
   "source": [
    "### **2.3. Minimizing Data Copies**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fef35a2",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Pandas to Polars Migration/Module_04/Lecture_B/image_02_03.jpg?v=1766900540\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Limit full dataset copies between transformations\n",
    ">* Use views and delay materialization to save memory\n",
    "\n",
    ">* Delay materializing intermediates; avoid unnecessary full copies\n",
    ">* Let optimizer combine steps and reuse buffers\n",
    "\n",
    ">* Avoid hidden copy operations and unnecessary conversions\n",
    ">* Share base data, reuse heavy work across branches\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f05543",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Minimizing Data Copies\n",
    "\n",
    "# Demonstrate minimizing data copies using Polars lazy evaluation pipeline.\n",
    "# Compare eager materialization versus single lazy pipeline execution memory behavior.\n",
    "# Show that delaying materialization reduces temporary memory usage for transformations.\n",
    "\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "\n",
    "n_rows = 1_000_000\n",
    "np.random.seed(42)\n",
    "\n",
    "base_df = pl.DataFrame({\"user_id\": np.random.randint(0, 100_000, n_rows)})\n",
    "base_df = base_df.with_columns(pl.col(\"user_id\").cast(pl.Int32))\n",
    "\n",
    "base_df = base_df.with_columns(pl.col(\"user_id\").alias(\"user_id_copy\"))\n",
    "base_df = base_df.with_columns((pl.col(\"user_id_copy\") % 100).alias(\"bucket\"))\n",
    "\n",
    "print(\"Eager pipeline shape and columns:\", base_df.shape, list(base_df.columns))\n",
    "\n",
    "lazy_df = pl.scan_ipc(pl.BytesIO(base_df.write_ipc(compression_level=0)))\n",
    "\n",
    "lazy_pipeline = (\n",
    "    lazy_df\n",
    "    .with_columns(pl.col(\"user_id\").alias(\"user_id_copy\"))\n",
    "    .with_columns((pl.col(\"user_id_copy\") % 100).alias(\"bucket\"))\n",
    ")\n",
    "\n",
    "result_df = lazy_pipeline.collect()\n",
    "\n",
    "print(\"Lazy pipeline shape and columns:\", result_df.shape, list(result_df.columns))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e224fa75",
   "metadata": {},
   "source": [
    "## **3. Profiling Polars Pipelines**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453f26a2",
   "metadata": {},
   "source": [
    "### **3.1. Fair Timing Methods**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f77608",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Pandas to Polars Migration/Module_04/Lecture_B/image_03_01.jpg?v=1766900556\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Measure realistic, end-to-end pipelines, not fragments\n",
    ">* Keep data, steps, and logic identical across libraries\n",
    "\n",
    ">* Separate setup from runs; repeat measurements carefully\n",
    ">* Consider cold versus warm runs and caching\n",
    "\n",
    ">* Include full lazy execution, not just planning\n",
    ">* Define consistent timing window, including IO costs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6886a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Fair Timing Methods\n",
    "\n",
    "# Demonstrate fair timing for Pandas and Polars pipelines end to end.\n",
    "# Separate setup from execution and discard slow warmup runs fairly.\n",
    "# Ensure equivalent logic and clear timing windows for both libraries.\n",
    "\n",
    "import timeit, textwrap, statistics as stats\n",
    "\n",
    "setup_code = textwrap.dedent(\n",
    "    \"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "n_rows = 500_000\n",
    "np.random.seed(0)\n",
    "\n",
    "values = np.random.rand(n_rows)\n",
    "keys = np.random.randint(0, 1000, size=n_rows)\n",
    "\n",
    "pdf = pd.DataFrame({\"key\": keys, \"value\": values})\n",
    "\n",
    "pldf = pl.DataFrame({\"key\": keys, \"value\": values})\n",
    "\n",
    "lazy_pldf = pldf.lazy()\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "pandas_stmt = textwrap.dedent(\n",
    "    \"\"\"\n",
    "result = (\n",
    "    pdf[pdf[\"value\"] > 0.5]\n",
    "    .assign(value_squared=lambda df: df[\"value\"] ** 2)\n",
    "    .groupby(\"key\", as_index=False)[\"value_squared\"]\n",
    "    .mean()\n",
    ")\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "polars_stmt = textwrap.dedent(\n",
    "    \"\"\"\n",
    "result = (\n",
    "    lazy_pldf\n",
    "    .filter(pl.col(\"value\") > 0.5)\n",
    "    .with_columns((pl.col(\"value\") ** 2).alias(\"value_squared\"))\n",
    "    .group_by(\"key\")\n",
    "    .agg(pl.col(\"value_squared\").mean())\n",
    "    .collect()\n",
    ")\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "warmup_runs = 1\n",
    "\n",
    "repeat_runs = 5\n",
    "\n",
    "pandas_times = timeit.repeat(\n",
    "    stmt=pandas_stmt,\n",
    "    setup=setup_code,\n",
    "    repeat=repeat_runs + warmup_runs,\n",
    "    number=1,\n",
    ")\n",
    "\n",
    "polars_times = timeit.repeat(\n",
    "    stmt=polars_stmt,\n",
    "    setup=setup_code,\n",
    "    repeat=repeat_runs + warmup_runs,\n",
    "    number=1,\n",
    ")\n",
    "\n",
    "pandas_steady = sorted(pandas_times[warmup_runs:])\n",
    "\n",
    "polars_steady = sorted(polars_times[warmup_runs:])\n",
    "\n",
    "pandas_mean = stats.mean(pandas_steady)\n",
    "\n",
    "polars_mean = stats.mean(polars_steady)\n",
    "\n",
    "print(\"Pandas steady runs seconds:\", [round(t, 4) for t in pandas_steady])\n",
    "\n",
    "print(\"Polars steady runs seconds:\", [round(t, 4) for t in polars_steady])\n",
    "\n",
    "print(\"Pandas mean seconds:\", round(pandas_mean, 4))\n",
    "\n",
    "print(\"Polars mean seconds:\", round(polars_mean, 4))\n",
    "\n",
    "print(\"Speedup factor Polars versus Pandas:\", round(pandas_mean / polars_mean, 2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a957c2c4",
   "metadata": {},
   "source": [
    "### **3.2. Reading Profiling Results**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3425e49",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Pandas to Polars Migration/Module_04/Lecture_B/image_03_02.jpg?v=1766900658\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Compare time spent in each pipeline stage\n",
    ">* Identify consistent savings and operations benefiting most\n",
    "\n",
    ">* Check memory and CPU, not time only\n",
    ">* Compare efficiency, scalability, and multicore usage patterns\n",
    "\n",
    ">* Use profiling insights to refine pipeline design\n",
    ">* Iterate to build intuition for performance gains\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e13532",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Reading Profiling Results\n",
    "\n",
    "# Compare simple Pandas and Polars timings and memory usage side by side.\n",
    "# Show how to read profiling style results beyond single wall clock numbers.\n",
    "# Keep output short, clear, and beginner friendly for quick Colab experimentation.\n",
    "\n",
    "import time\n",
    "import psutil\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "process = psutil.Process()\n",
    "np.random.seed(42)\n",
    "\n",
    "n_rows = 500000\n",
    "n_groups = 50\n",
    "\n",
    "sizes = np.random.randint(1, 500, size=n_rows)\n",
    "weights = np.random.rand(n_rows) * 10.0\n",
    "\n",
    "groups = np.random.randint(0, n_groups, size=n_rows)\n",
    "\n",
    "pdf = pd.DataFrame({\"group\": groups, \"size\": sizes, \"weight\": weights})\n",
    "\n",
    "pldf = pl.from_pandas(pdf)\n",
    "\n",
    "start_mem = process.memory_info().rss\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "pd_result = pdf.groupby(\"group\").agg({\"size\": \"sum\", \"weight\": \"mean\"})\n",
    "\n",
    "pd_time = time.perf_counter() - start_time\n",
    "pd_mem = process.memory_info().rss - start_mem\n",
    "\n",
    "start_mem = process.memory_info().rss\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "pl_result = (\n",
    "    pldf.lazy()\n",
    "    .groupby(\"group\")\n",
    "    .agg([pl.col(\"size\").sum(), pl.col(\"weight\").mean()])\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "pl_time = time.perf_counter() - start_time\n",
    "pl_mem = process.memory_info().rss - start_mem\n",
    "\n",
    "print(\"Pandas time seconds:\", round(pd_time, 4))\n",
    "print(\"Polars time seconds:\", round(pl_time, 4))\n",
    "print(\"Pandas memory bytes:\", pd_mem)\n",
    "print(\"Polars memory bytes:\", pl_mem)\n",
    "\n",
    "print(\"Faster library overall:\", \"Polars\" if pl_time < pd_time else \"Pandas\")\n",
    "print(\"Lower memory library:\", \"Polars\" if pl_mem < pd_mem else \"Pandas\")\n",
    "\n",
    "print(\"Pandas result sample:\")\n",
    "print(pd_result.head(3))\n",
    "\n",
    "print(\"Polars result sample:\")\n",
    "print(pl_result.head(3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6c2772",
   "metadata": {},
   "source": [
    "### **3.3. Practical Performance Targets**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a53caad",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Pandas to Polars Migration/Module_04/Lecture_B/image_03_03.jpg?v=1766900672\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Turn raw timings into context-specific performance goals\n",
    ">* Tie benchmarks to real business and user needs\n",
    "\n",
    ">* Set relative targets like speedup and memory cuts\n",
    ">* Tie targets to real workflows and user expectations\n",
    "\n",
    ">* Set tiered targets from baseline to ambitious\n",
    ">* Link tiers to profiling metrics and stop tuning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f1c4bd",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Optimizing Pipelines**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edeac4ca",
   "metadata": {},
   "source": [
    "\n",
    "In this lecture, you learned to:\n",
    "- Optimize Polars IO operations by choosing appropriate file formats and scan options. \n",
    "- Reduce memory usage in Polars pipelines through column pruning, type choices, and lazy evaluation. \n",
    "- Profile and benchmark Polars pipelines against existing Pandas implementations to quantify performance improvements. \n",
    "\n",
    "In the next Module (Module 5), we will go over 'Migration Patterns and Tools'"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
