{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d684235b",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Pandas vs Polars**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4981effa",
   "metadata": {},
   "source": [
    ">Last update: 20260101.\n",
    "    \n",
    "By the end of this Lecture, you will be able to:\n",
    "- Explain the key architectural differences between pandas and Polars. \n",
    "- Map common pandas objects and operations to their closest Polars equivalents. \n",
    "- Evaluate when pandas remains sufficient and when Polars is likely to provide clear benefits. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac563b3d",
   "metadata": {},
   "source": [
    "## **1. Pandas vs Polars Architecture**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8160dcf4",
   "metadata": {},
   "source": [
    "### **1.1. Memory Models Compared**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e2e954",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Pandas to Polars Migration/Module_01/Lecture_B/image_01_01.jpg?v=1767307182\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Older library uses flexible, loosely organized memory\n",
    ">* Fragmented layout increases overhead, hurting scalability\n",
    "\n",
    ">* Uses compact, typed column buffers for storage\n",
    ">* Improves cache use, speed, and memory efficiency\n",
    "\n",
    ">* Traditional model risks memory bloat and errors\n",
    ">* Columnar model stays compact and scales better\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa0a3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Memory Models Compared\n",
    "\n",
    "# Show memory usage differences between pandas and Polars tables.\n",
    "# Compare object heavy columns with compact typed columnar storage.\n",
    "# Connect memory model ideas with simple printed size measurements.\n",
    "\n",
    "# !pip install pandas polars pyarrow.\n",
    "\n",
    "# Import required libraries for data handling and measurement.\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "\n",
    "# Create a small example size representing millions of rows.\n",
    "n_rows = 1_000_000\n",
    "\n",
    "# Build pandas DataFrame using flexible object string column.\n",
    "pdf_object = pd.DataFrame({\"city\": [\"New York\"] * n_rows})\n",
    "\n",
    "# Build pandas DataFrame using efficient categorical encoded column.\n",
    "pdf_categorical = pd.DataFrame({\"city\": pd.Categorical([\"New York\"] * n_rows)})\n",
    "\n",
    "# Build Polars DataFrame using compact UTF8 typed column.\n",
    "pldf = pl.DataFrame({\"city\": [\"New York\"] * n_rows})\n",
    "\n",
    "# Define helper function printing memory usage in megabytes.\n",
    "def show_size(label, size_bytes):\n",
    "    size_mb = size_bytes / (1024 * 1024)\n",
    "    print(f\"{label}: {size_mb:.2f} MB\")\n",
    "\n",
    "# Measure pandas object column memory including index overhead.\n",
    "size_pdf_object = pdf_object.memory_usage(deep=True).sum()\n",
    "\n",
    "# Measure pandas categorical column memory including index overhead.\n",
    "size_pdf_categorical = pdf_categorical.memory_usage(deep=True).sum()\n",
    "\n",
    "# Measure Polars DataFrame memory using estimated size method.\n",
    "size_pldf = pldf.estimated_size()\n",
    "\n",
    "# Print clear header describing upcoming memory comparison results.\n",
    "print(\"Memory usage for one million city rows:\")\n",
    "\n",
    "# Print memory usage for pandas object based column layout.\n",
    "show_size(\"pandas object column\", size_pdf_object)\n",
    "\n",
    "# Print memory usage for pandas categorical encoded column layout.\n",
    "show_size(\"pandas categorical column\", size_pdf_categorical)\n",
    "\n",
    "# Print memory usage for Polars compact columnar layout.\n",
    "show_size(\"polars UTF8 column\", size_pldf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f546e5a",
   "metadata": {},
   "source": [
    "### **1.2. Row vs Columnar**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de333e4",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Pandas to Polars Migration/Module_01/Lecture_B/image_01_02.jpg?v=1767307204\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Pandas stores data by rows, record-focused\n",
    ">* Row layout slows column-only operations across many rows\n",
    "\n",
    ">* Polars stores each column together in memory\n",
    ">* This speeds up analytics and reduces memory use\n",
    "\n",
    ">* Row-based pandas suits transactional, record-focused workflows\n",
    ">* Columnar Polars excels at large, analytical workloads\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6754d1f8",
   "metadata": {},
   "source": [
    "### **1.3. Parallel Execution Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe20f3c",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Pandas to Polars Migration/Module_01/Lecture_B/image_01_03.jpg?v=1767307232\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Pandas runs steps eagerly on one core\n",
    ">* Polars plans, optimizes, and runs steps in parallel\n",
    "\n",
    ">* Single-threaded processing underuses available CPU cores\n",
    ">* Parallel model splits data, runs tasks simultaneously\n",
    "\n",
    ">* Engine optimizes whole query plan before execution\n",
    ">* Parallelism reduces work, boosts speed and predictability\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff629de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Parallel Execution Model\n",
    "\n",
    "# Show simple timing difference between pandas and Polars operations.\n",
    "# Illustrate single threaded versus parallel execution behavior conceptually.\n",
    "# Use a medium sized dataset to keep runtime friendly.\n",
    "\n",
    "# !pip install pandas polars pyarrow.\n",
    "\n",
    "# Import required libraries for data handling and timing.\n",
    "import time as time_module\n",
    "import pandas as pandas_module\n",
    "import polars as polars_module\n",
    "\n",
    "# Create a reasonably large row count for demonstration.\n",
    "row_count = 2_000_000\n",
    "\n",
    "# Build a pandas DataFrame with simple numeric columns.\n",
    "pandas_df = pandas_module.DataFrame({\"a\": range(row_count), \"b\": range(row_count)})\n",
    "\n",
    "# Define a function performing chained operations using pandas.\n",
    "def run_pandas_operations(input_df):\n",
    "    filtered = input_df[input_df[\"a\"] % 2 == 0]\n",
    "    enriched = filtered.assign(c=filtered[\"a\"] * 1.5 + filtered[\"b\"])\n",
    "    grouped = enriched.groupby(enriched[\"a\"] % 10).agg({\"c\": \"mean\"})\n",
    "    return grouped\n",
    "\n",
    "# Time the pandas operations using a simple wall clock measurement.\n",
    "start_pandas = time_module.time()\n",
    "result_pandas = run_pandas_operations(pandas_df)\n",
    "elapsed_pandas = time_module.time() - start_pandas\n",
    "\n",
    "# Build an equivalent Polars DataFrame from the pandas DataFrame.\n",
    "polars_df = polars_module.from_pandas(pandas_df)\n",
    "\n",
    "# Define a function performing similar operations using Polars expressions.\n",
    "def run_polars_operations(input_df):\n",
    "    lazy_frame = input_df.lazy()\n",
    "    lazy_filtered = lazy_frame.filter(polars_module.col(\"a\") % 2 == 0)\n",
    "    lazy_enriched = lazy_filtered.with_columns((polars_module.col(\"a\") * 1.5 + polars_module.col(\"b\")).alias(\"c\"))\n",
    "    lazy_grouped = lazy_enriched.group_by(polars_module.col(\"a\") % 10).agg(polars_module.col(\"c\").mean())\n",
    "    return lazy_grouped.collect()\n",
    "\n",
    "# Time the Polars operations which may use multiple cores internally.\n",
    "start_polars = time_module.time()\n",
    "result_polars = run_polars_operations(polars_df)\n",
    "elapsed_polars = time_module.time() - start_polars\n",
    "\n",
    "# Print concise timing comparison and small result samples.\n",
    "print(\"Pandas elapsed seconds:\", round(elapsed_pandas, 3))\n",
    "print(\"Polars elapsed seconds:\", round(elapsed_polars, 3))\n",
    "print(\"Pandas result rows count:\", len(result_pandas))\n",
    "print(\"Polars result rows count:\", result_polars.height)\n",
    "print(\"Pandas head preview:\\n\", result_pandas.head())\n",
    "print(\"Polars head preview:\\n\", result_polars.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0a27bb",
   "metadata": {},
   "source": [
    "## **2. Pandas to Polars Mapping**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445393ec",
   "metadata": {},
   "source": [
    "### **2.1. DataFrame Concepts Compared**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760b4ffc",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Pandas to Polars Migration/Module_01/Lecture_B/image_02_01.jpg?v=1767307257\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Data frames are shared table-like core objects\n",
    ">* Same workflow, different internal design and optimization\n",
    "\n",
    ">* Common table operations feel similar in both\n",
    ">* Syntax differs, but high-level transformations match\n",
    "\n",
    ">* Joins, concatenation, reshaping work similarly in both\n",
    ">* Reuse existing workflow habits across both libraries\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b4c9f8",
   "metadata": {},
   "source": [
    "### **2.2. Series Index and Columns**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb71e9d",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Pandas to Polars Migration/Module_01/Lecture_B/image_02_02.jpg?v=1767307267\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Traditional library relies heavily on implicit indexes\n",
    ">* New library uses explicit columns as row keys\n",
    "\n",
    ">* Indexes become explicit date and category columns\n",
    ">* You manually specify join keys and filter conditions\n",
    "\n",
    ">* Pandas series act independently, relying on indexes\n",
    ">* Polars treats series as columns with explicit keys\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bf1d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Series Index and Columns\n",
    "\n",
    "# Show how pandas index differs from Polars columns focus.\n",
    "# Compare row labeling, selection, and alignment using both libraries.\n",
    "# Emphasize explicit column keys instead of hidden index behavior.\n",
    "\n",
    "# !pip install pandas polars.\n",
    "\n",
    "# Import pandas and polars libraries for comparison.\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "# Create simple sales data with dates as index in pandas.\n",
    "pd_sales = pd.DataFrame({\"date\": [\"2024-01-01\", \"2024-01-02\"], \"store\": [\"A\", \"A\"], \"sales\": [100, 150]}).set_index(\"date\")\n",
    "\n",
    "# Show pandas DataFrame where date acts as hidden index.\n",
    "print(\"Pandas DataFrame with date index:\")\n",
    "print(pd_sales)\n",
    "\n",
    "# Select one row using index label in pandas.\n",
    "print(\"\\nPandas select by index label:\")\n",
    "print(pd_sales.loc[\"2024-01-02\"])\n",
    "\n",
    "# Create equivalent Polars DataFrame keeping date as normal column.\n",
    "pl_sales = pl.DataFrame({\"date\": [\"2024-01-01\", \"2024-01-02\"], \"store\": [\"A\", \"A\"], \"sales\": [100, 150]})\n",
    "\n",
    "# Show Polars DataFrame where date remains explicit column.\n",
    "print(\"\\nPolars DataFrame with date column:\")\n",
    "print(pl_sales)\n",
    "\n",
    "# Select one row using explicit column filter in Polars.\n",
    "row_pl = pl_sales.filter(pl.col(\"date\") == \"2024-01-02\")\n",
    "\n",
    "# Display Polars selection emphasizing explicit column based filtering.\n",
    "print(\"\\nPolars select by date column:\")\n",
    "print(row_pl)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6dc3f5",
   "metadata": {},
   "source": [
    "### **2.3. LazyFrame Query Plans**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3dc6e7",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Pandas to Polars Migration/Module_01/Lecture_B/image_02_03.jpg?v=1767307288\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Pandas runs each step immediately, creating intermediates\n",
    ">* Polars LazyFrame stores and optimizes the whole plan\n",
    "\n",
    ">* Pandas runs each step immediately, creating intermediates\n",
    ">* Polars builds one optimized lazy query, then executes\n",
    "\n",
    ">* LazyFrame is a fully optimized pandas-like pipeline\n",
    ">* Express whole workflows as one lazy integrated query\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03492e26",
   "metadata": {},
   "source": [
    "## **3. Choosing Pandas or Polars**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfcff7c",
   "metadata": {},
   "source": [
    "### **3.1. Quick Exploratory Workflows**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd277ea6",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Pandas to Polars Migration/Module_01/Lecture_B/image_03_01.jpg?v=1767307315\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Pandas suits quick exploration on smaller datasets\n",
    ">* Familiar tools and ecosystem outweigh Polarsâ€™ speed\n",
    "\n",
    ">* Polars keeps exploration fast on very large data\n",
    ">* Optimized, lazy engine supports many quick experiments\n",
    "\n",
    ">* Match tool choice to project tempo, scale\n",
    ">* Start in pandas, switch to Polars when constrained\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c51998",
   "metadata": {},
   "source": [
    "### **3.2. Scaling Complex Pipelines**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e5b547",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Pandas to Polars Migration/Module_01/Lecture_B/image_03_02.jpg?v=1767307327\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Large, repeatable pipelines expose tool performance limits\n",
    ">* Polars optimizes complex workflows; pandas can bottleneck\n",
    "\n",
    ">* Growing pandas pipelines hit memory and speed limits\n",
    ">* Polars optimizes, parallelizes, and scales complex workflows\n",
    "\n",
    ">* Small, infrequent pipelines can stay on pandas\n",
    ">* Choose Polars when growth, speed, reliability matter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21be16e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Scaling Complex Pipelines\n",
    "\n",
    "# Demonstrate scaling complex pipelines using pandas and Polars side by side.\n",
    "# Show eager versus lazy execution on a repeatable transformation pipeline.\n",
    "# Highlight runtime differences when pipeline complexity and data volume increase.\n",
    "\n",
    "# !pip install polars pandas.\n",
    "\n",
    "# Import required libraries for data handling and timing.\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import time\n",
    "\n",
    "# Define a helper function creating synthetic daily sales data.\n",
    "def create_sales_data(num_days, rows_per_day):\n",
    "    dates = pd.date_range(\"2024-01-01\", periods=num_days, freq=\"D\")\n",
    "    stores = [f\"store_{i}\" for i in range(10)]\n",
    "    products = [f\"product_{j}\" for j in range(20)]\n",
    "    data = {\n",
    "        \"date\": pd.Series(pd.NA, index=range(num_days * rows_per_day)),\n",
    "        \"store\": pd.Series(pd.NA, index=range(num_days * rows_per_day)),\n",
    "        \"product\": pd.Series(pd.NA, index=range(num_days * rows_per_day)),\n",
    "        \"units_sold\": pd.Series(0, index=range(num_days * rows_per_day)),\n",
    "        \"unit_price_usd\": pd.Series(0.0, index=range(num_days * rows_per_day)),\n",
    "    }\n",
    "\n",
    "    idx = 0\n",
    "    for d in dates:\n",
    "        for _ in range(rows_per_day):\n",
    "            data[\"date\"][idx] = d\n",
    "            data[\"store\"][idx] = stores[idx % len(stores)]\n",
    "            data[\"product\"][idx] = products[idx % len(products)]\n",
    "            data[\"units_sold\"][idx] = (idx % 50) + 1\n",
    "            data[\"unit_price_usd\"][idx] = float((idx % 30) + 5)\n",
    "            idx += 1\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Create a moderately sized dataset representing daily sales logs.\n",
    "pandas_df = create_sales_data(num_days=60, rows_per_day=2000)\n",
    "\n",
    "# Cast columns to simple numpy-backed dtypes so Polars conversion does not require pyarrow.\n",
    "pandas_df = pandas_df.astype({\n",
    "    \"date\": \"datetime64[ns]\",\n",
    "    \"store\": \"object\",\n",
    "    \"product\": \"object\",\n",
    "    \"units_sold\": \"int64\",\n",
    "    \"unit_price_usd\": \"float64\",\n",
    "})\n",
    "\n",
    "# Convert pandas DataFrame into Polars DataFrame for comparison.\n",
    "polars_df = pl.from_pandas(pandas_df)\n",
    "\n",
    "# Define a complex pandas pipeline executed eagerly step by step.\n",
    "def run_pandas_pipeline(df):\n",
    "    step1 = df[df[\"units_sold\"] > 10]\n",
    "    step2 = step1.assign(revenue_usd=step1[\"units_sold\"] * step1[\"unit_price_usd\"])\n",
    "    step3 = step2.assign(revenue_usd=step2[\"revenue_usd\"] * 1.05)\n",
    "    step4 = step3.assign(revenue_usd=step3[\"revenue_usd\"] * 0.97)\n",
    "    step5 = step4.assign(revenue_usd=step4[\"revenue_usd\"] * 1.02)\n",
    "\n",
    "    grouped = step5.groupby([\"date\", \"store\"], as_index=False)[\"revenue_usd\"].sum()\n",
    "    result = grouped.sort_values([\"date\", \"store\"]).head(5)\n",
    "    return result\n",
    "\n",
    "# Define a comparable Polars lazy pipeline optimized before execution.\n",
    "def run_polars_pipeline(df):\n",
    "    lazy = df.lazy()\n",
    "    lazy = lazy.filter(pl.col(\"units_sold\") > 10)\n",
    "    lazy = lazy.with_columns((pl.col(\"units_sold\") * pl.col(\"unit_price_usd\")).alias(\"revenue_usd\"))\n",
    "    lazy = lazy.with_columns((pl.col(\"revenue_usd\") * 1.05).alias(\"revenue_usd\"))\n",
    "    lazy = lazy.with_columns((pl.col(\"revenue_usd\") * 0.97).alias(\"revenue_usd\"))\n",
    "\n",
    "    lazy = lazy.with_columns((pl.col(\"revenue_usd\") * 1.02).alias(\"revenue_usd\"))\n",
    "    lazy = lazy.group_by([\"date\", \"store\"]).agg(pl.col(\"revenue_usd\").sum())\n",
    "    lazy = lazy.sort([\"date\", \"store\"]).limit(5)\n",
    "    result = lazy.collect()\n",
    "    return result\n",
    "\n",
    "# Time the pandas pipeline to observe eager execution cost.\n",
    "start_pandas = time.perf_counter()\n",
    "pandas_result = run_pandas_pipeline(pandas_df)\n",
    "pandas_time = time.perf_counter() - start_pandas\n",
    "\n",
    "# Time the Polars pipeline to observe lazy optimized execution.\n",
    "start_polars = time.perf_counter()\n",
    "polars_result = run_polars_pipeline(polars_df)\n",
    "polars_time = time.perf_counter() - start_polars\n",
    "\n",
    "# Print concise comparison of runtimes and sample outputs.\n",
    "print(\"Pandas pipeline runtime seconds:\", round(pandas_time, 4))\n",
    "print(\"Polars pipeline runtime seconds:\", round(polars_time, 4))\n",
    "print(\"\\nPandas aggregated sample rows:\")\n",
    "print(pandas_result)\n",
    "print(\"\\nPolars aggregated sample rows:\")\n",
    "print(polars_result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35bae5a",
   "metadata": {},
   "source": [
    "### **3.3. Hybrid Pandas Polars Workflows**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10cf141",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Pandas to Polars Migration/Module_01/Lecture_B/image_03_03.jpg?v=1767307367\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Combine pandas and Polars to exploit strengths\n",
    ">* Offload heavy transformations to Polars, keep pandas\n",
    "\n",
    ">* Use Polars mid-pipeline for heavy transformations\n",
    ">* Convert between pandas and Polars to balance performance\n",
    "\n",
    ">* Use hybrid workflows when Polars removes bottlenecks\n",
    ">* Keep light tasks in pandas, heavy in Polars\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36800746",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Pandas vs Polars**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180ed555",
   "metadata": {},
   "source": [
    "\n",
    "In this lecture, you learned to:\n",
    "- Explain the key architectural differences between pandas and Polars. \n",
    "- Map common pandas objects and operations to their closest Polars equivalents. \n",
    "- Evaluate when pandas remains sufficient and when Polars is likely to provide clear benefits. \n",
    "\n",
    "In the next Lecture (Lecture C), we will go over 'Setting Up Polars'"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
