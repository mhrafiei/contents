{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccfff201",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Pandas vs Polars**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044b65b0",
   "metadata": {},
   "source": [
    ">Last update: 20251227.\n",
    "    \n",
    "By the end of this Lecture, you will be able to:\n",
    "- Compare the core data structures and execution models of Pandas and Polars. \n",
    "- Explain how Polars’ lazy and eager APIs relate to typical Pandas workflows. \n",
    "- Identify common areas where Pandas habits may not map directly to Polars. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ae84af",
   "metadata": {},
   "source": [
    "## **1. Pandas and Polars Structures**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c04a73",
   "metadata": {},
   "source": [
    "### **1.1. Pandas Core Structures**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2ee272",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Pandas to Polars Migration/Module_01/Lecture_B/image_01_01.jpg?v=1766890070\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Series are labeled one-dimensional data arrays\n",
    ">* DataFrames are indexed, spreadsheet-like tables of Series\n",
    "\n",
    ">* DataFrames use NumPy columns plus rich metadata\n",
    ">* Work in memory with immediate, stepwise transformations\n",
    "\n",
    ">* Pandas runs each DataFrame step immediately in memory\n",
    ">* Easy to inspect steps, but can waste resources\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0d5c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Pandas Core Structures\n",
    "\n",
    "# Demonstrate Pandas Series and DataFrame core structures clearly.\n",
    "# Show labeled data with indexes and column names using small examples.\n",
    "# Highlight eager execution by creating and transforming DataFrames stepwise.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Create a simple Series representing daily high temperatures in Fahrenheit.\n",
    "temps_series = pd.Series([72, 75, 70], index=[\"Mon\", \"Tue\", \"Wed\"])\n",
    "\n",
    "# Print the Series to show values and index labels together clearly.\n",
    "print(\"Series: daily high temperatures (Fahrenheit)\")\n",
    "print(temps_series)\n",
    "\n",
    "# Create a DataFrame representing small store sales with labeled columns.\n",
    "data = {\"item\": [\"Soda\", \"Chips\", \"Candy\"], \"units_sold\": [30, 45, 25]}\n",
    "\n",
    "sales_df = pd.DataFrame(data)\n",
    "\n",
    "# Print the DataFrame to show table structure with rows and columns.\n",
    "print(\"\\nDataFrame: small store daily sales table\")\n",
    "print(sales_df)\n",
    "\n",
    "# Perform an eager operation adding a new revenue column immediately.\n",
    "sales_df[\"revenue_dollars\"] = sales_df[\"units_sold\"] * 2.5\n",
    "\n",
    "# Print updated DataFrame to show new column created by eager execution.\n",
    "print(\"\\nUpdated DataFrame with revenue_dollars column\")\n",
    "print(sales_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbd727f",
   "metadata": {},
   "source": [
    "### **1.2. Polars Core Structures**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f36db4",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Pandas to Polars Migration/Module_01/Lecture_B/image_01_02.jpg?v=1766890086\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Polars DataFrame is column-based using Arrow arrays\n",
    ">* Strictly typed Series enable fast, vectorized operations\n",
    "\n",
    ">* Same structures support eager and lazy execution\n",
    ">* Lazy mode builds optimizable graphs from operations\n",
    "\n",
    ">* Immutable DataFrames make each transformation state clear\n",
    ">* Structural clarity enables parallelism and efficient data processing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf22032",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Polars Core Structures\n",
    "\n",
    "# Demonstrate Polars DataFrame and Series core structures simply.\n",
    "# Show eager versus lazy execution using the same table structures.\n",
    "# Highlight column types and immutability using small customer purchase data.\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "# Create a small Polars DataFrame with typed columns.\n",
    "customers_df = pl.DataFrame({\"customer_id\": [1, 2, 3], \"state\": [\"CA\", \"NY\", \"TX\"], \"dollars_spent\": [120.5, 89.0, 150.0]})\n",
    "\n",
    "# Show the DataFrame and its schema to highlight columnar typed structure.\n",
    "print(\"Eager DataFrame view and schema:\")\n",
    "print(customers_df)\n",
    "print(customers_df.schema)\n",
    "\n",
    "# Demonstrate that a Series is a single typed column from the DataFrame.\n",
    "spent_series = customers_df[\"dollars_spent\"]\n",
    "print(\"\\nSingle Series column and its data type:\")\n",
    "print(spent_series)\n",
    "print(spent_series.dtype)\n",
    "\n",
    "# Show that transformations create new DataFrames instead of mutating originals.\n",
    "with_discount_df = customers_df.with_columns((pl.col(\"dollars_spent\") * 0.9).alias(\"discounted_dollars\"))\n",
    "print(\"\\nOriginal dollars_spent column remains unchanged:\")\n",
    "print(customers_df[\"dollars_spent\"])\n",
    "print(\"Discounted dollars column in new DataFrame:\")\n",
    "print(with_discount_df[\"discounted_dollars\"])\n",
    "\n",
    "# Build a lazy query using the same core structures and then collect results.\n",
    "lazy_query = customers_df.lazy().select([pl.col(\"state\"), pl.col(\"dollars_spent\").mean().alias(\"average_dollars\")])\n",
    "result_df = lazy_query.collect()\n",
    "print(\"\\nLazy query result DataFrame:\")\n",
    "print(result_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4877e7a0",
   "metadata": {},
   "source": [
    "### **1.3. Columnar Arrow Foundations**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d641a19d",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Pandas to Polars Migration/Module_01/Lecture_B/image_01_03.jpg?v=1766890101\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Columnar layout stores each column in contiguous memory\n",
    ">* Arrow standardizes columns, enabling fast, shareable analytics\n",
    "\n",
    ">* Polars stores columns as Arrow-style arrays\n",
    ">* Enables fast analytics and zero-copy interoperability\n",
    "\n",
    ">* Non-Arrow tools add indirection and memory overhead\n",
    ">* Arrow columns encourage whole-column, vectorized thinking\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13d674c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Columnar Arrow Foundations\n",
    "\n",
    "# Demonstrate columnar thinking using Polars and Arrow style arrays.\n",
    "# Compare whole column operations with row style mental models.\n",
    "# Show efficient aggregations on contiguous numeric transaction columns.\n",
    "\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "\n",
    "# Create simple transaction data with amounts and categories.\n",
    "amounts_dollars = [10.0, 25.5, 7.0, 40.0, 15.5]\n",
    "merchant_categories = [\"grocery\", \"gas\", \"grocery\", \"electronics\", \"gas\"]\n",
    "\n",
    "# Build a Polars DataFrame that stores columns contiguously.\n",
    "transactions = pl.DataFrame({\"amount_usd\": amounts_dollars, \"category\": merchant_categories})\n",
    "\n",
    "# Show the DataFrame structure, emphasizing column based layout.\n",
    "print(\"Polars transactions DataFrame with columnar layout:\")\n",
    "print(transactions)\n",
    "\n",
    "# Compute total amount per category using whole column aggregation.\n",
    "category_totals = transactions.group_by(\"category\").agg(pl.col(\"amount_usd\").sum().alias(\"total_usd\"))\n",
    "\n",
    "# Display aggregated results that use contiguous numeric column processing.\n",
    "print(\"\\nTotal amount per category using columnar aggregation:\")\n",
    "print(category_totals)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da1d9bd",
   "metadata": {},
   "source": [
    "## **2. Polars Execution Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858a777c",
   "metadata": {},
   "source": [
    "### **2.1. Rowwise to Columnar Mindset**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae63e210",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Pandas to Polars Migration/Module_01/Lecture_B/image_02_01.jpg?v=1766890117\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Shift from row-by-row thinking to columns\n",
    ">* Column focus changes transformations, performance, and reasoning\n",
    "\n",
    ">* Think in whole columns, not individual rows\n",
    ">* Use vectorized filters and aggregations for optimization\n",
    "\n",
    ">* Describe relationships between whole columns, not rows\n",
    ">* Engine fuses expressions, enabling optimization and parallelism\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642982f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Rowwise to Columnar Mindset\n",
    "\n",
    "# Show rowwise thinking versus columnar thinking using simple customer transactions.\n",
    "# Compare manual per row loops with vectorized column operations using Polars expressions.\n",
    "# Highlight how columnar mindset matches Polars execution and improves clarity and performance.\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "# Create a small transactions DataFrame with simple customer purchase information.\n",
    "# Columns include customer identifier, purchase price dollars, and purchase month name.\n",
    "transactions = pl.DataFrame({\n",
    "    \"customer_id\": [1, 2, 3, 4],\n",
    "    \"price_usd\": [20.0, 35.5, 12.0, 50.0],\n",
    "    \"month\": [\"November\", \"December\", \"December\", \"October\"],\n",
    "})\n",
    "\n",
    "# Simulate rowwise mindset using a Python loop over individual transaction rows.\n",
    "# We manually check each row month and accumulate December revenue using imperative style.\n",
    "loop_total = 0.0\n",
    "for row in transactions.iter_rows():\n",
    "    customer, price, month = row\n",
    "    if month == \"December\":\n",
    "        loop_total += price\n",
    "\n",
    "# Now use columnar mindset with a single expression over the entire price column.\n",
    "# We filter December rows using a boolean mask and aggregate prices in one vectorized step.\n",
    "columnar_total = (\n",
    "    transactions\n",
    "    .filter(pl.col(\"month\") == \"December\")\n",
    "    .select(pl.col(\"price_usd\").sum())\n",
    "    .item()\n",
    ")\n",
    "\n",
    "# Print both results to show they match while using very different mental models.\n",
    "# Columnar approach describes transformations on columns instead of iterating individual rows.\n",
    "print(\"Loop based December revenue total:\", loop_total)\n",
    "print(\"Columnar December revenue total:\", columnar_total)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b990979",
   "metadata": {},
   "source": [
    "### **2.2. Lazy Query Planning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c41ec12",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Pandas to Polars Migration/Module_01/Lecture_B/image_02_02.jpg?v=1766890133\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Describe transformations first; delay actual computation\n",
    ">* Deferred pipeline enables global optimization and speed\n",
    "\n",
    ">* Pandas runs each step immediately, creating intermediates\n",
    ">* Polars builds one lazy plan, then optimizes execution\n",
    "\n",
    ">* Plan the whole query like a roadmap\n",
    ">* Engine reorders, pushes filters, shares work efficiently\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49d80f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Lazy Query Planning\n",
    "\n",
    "# Demonstrate Polars lazy query planning versus eager Pandas style execution.\n",
    "# Show that Polars builds a plan and executes only when collecting results.\n",
    "# Compare printed outputs to highlight when work actually happens in each library.\n",
    "\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "print(\"Creating small sales dataset for demonstration only.\")\n",
    "\n",
    "sales_data = {\n",
    "    \"store\": [\"A\", \"A\", \"B\", \"B\", \"C\", \"C\"],\n",
    "    \"day\": [\"Mon\", \"Tue\", \"Mon\", \"Tue\", \"Mon\", \"Tue\"],\n",
    "    \"revenue_usd\": [120, 130, 200, 210, 90, 95],\n",
    "}\n",
    "\n",
    "pdf = pd.DataFrame(sales_data)\n",
    "\n",
    "print(\"Pandas eager style executes each step immediately.\")\n",
    "\n",
    "pdf_filtered = pdf[pdf[\"revenue_usd\"] > 100]\n",
    "\n",
    "pdf_grouped = pdf_filtered.groupby(\"store\")[\"revenue_usd\"].mean()\n",
    "\n",
    "print(\"Pandas result average revenue by store:\")\n",
    "\n",
    "print(pdf_grouped)\n",
    "\n",
    "print(\"Now build equivalent Polars lazy query without immediate execution.\")\n",
    "\n",
    "pldf_lazy = pl.DataFrame(sales_data).lazy()\n",
    "\n",
    "lazy_plan = (\n",
    "    pldf_lazy\n",
    "    .filter(pl.col(\"revenue_usd\") > 100)\n",
    "    .group_by(\"store\")\n",
    "    .agg(pl.col(\"revenue_usd\").mean().alias(\"avg_revenue_usd\"))\n",
    ")\n",
    "\n",
    "print(\"Polars has built a lazy plan but not executed yet.\")\n",
    "\n",
    "print(\"Trigger execution now by collecting final lazy result.\")\n",
    "\n",
    "result = lazy_plan.collect()\n",
    "\n",
    "print(\"Polars lazy result average revenue by store:\")\n",
    "\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8bab0a",
   "metadata": {},
   "source": [
    "### **2.3. Pandas Style Eager Use**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b474c212",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Pandas to Polars Migration/Module_01/Lecture_B/image_02_03.jpg?v=1766890148\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Eager Polars feels like interactive Pandas workflows\n",
    ">* Immediate feedback builds intuition for columnar operations\n",
    "\n",
    ">* Eager Polars uses the same fast engine\n",
    ">* Each step runs immediately with efficient performance\n",
    "\n",
    ">* Start with eager prototypes on small samples\n",
    ">* Then convert to lazy pipelines for optimization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fc2a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Pandas Style Eager Use\n",
    "\n",
    "# Demonstrate Polars eager mode with familiar stepwise data exploration.\n",
    "# Show immediate results after each transformation using a small toy dataset.\n",
    "# Compare original and transformed data to highlight conversational analysis style.\n",
    "\n",
    "import sys\n",
    "import subprocess\n",
    "import importlib\n",
    "\n",
    "# Ensure Polars is installed in the current environment.\n",
    "# This works in Google Colab and standard notebook environments.\n",
    "# Installation happens only when Polars is missing.\n",
    "# Users do not need to run any extra commands.\n",
    "if importlib.util.find_spec(\"polars\") is None:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"polars\", \"--quiet\"])\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "# Create a tiny survey style dataset using a Polars DataFrame.\n",
    "# Distances are in miles, and durations are in minutes.\n",
    "# This small dataset keeps printed output short and readable.\n",
    "# Column names are simple and descriptive for beginners.\n",
    "df = pl.DataFrame({\"name\": [\"Ann\", \"Bob\", \"Cara\", \"Dan\"], \"distance_miles\": [1.2, 3.5, 0.8, 2.0], \"duration_min\": [15, 40, 10, 25]})\n",
    "\n",
    "# Show the original data to emphasize immediate eager evaluation behavior.\n",
    "# This print call displays the full DataFrame in a compact table.\n",
    "# Learners can see raw values before any transformations.\n",
    "# Output remains under the fifteen line requirement.\n",
    "print(\"Original data frame (eager mode):\")\n",
    "print(df)\n",
    "\n",
    "# Add a new derived column using whole column operations, not row loops.\n",
    "# Here we compute average walking speed in miles per hour units.\n",
    "# The with_columns method executes immediately in eager mode.\n",
    "# The result is another Polars DataFrame object.\n",
    "df_speed = df.with_columns((pl.col(\"distance_miles\") / (pl.col(\"duration_min\") / 60)).alias(\"speed_mph\"))\n",
    "\n",
    "# Filter rows interactively to keep only relatively fast walkers.\n",
    "# This chained operation still runs eagerly, step by step.\n",
    "# The filter condition uses the new derived speed column.\n",
    "# The result is a smaller, immediately available DataFrame.\n",
    "fast_walkers = df_speed.filter(pl.col(\"speed_mph\") > 3.0)\n",
    "\n",
    "# Show the transformed data to highlight conversational analysis style.\n",
    "# Each transformation produced visible results without delayed execution.\n",
    "# Learners can imagine iterating with more questions and filters.\n",
    "# This demonstrates Polars eager mode as notebook friendly.\n",
    "print(\"\\nFast walkers with derived speed column:\")\n",
    "print(fast_walkers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2a3336",
   "metadata": {},
   "source": [
    "## **3. Polars Ecosystem Integration**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2382e219",
   "metadata": {},
   "source": [
    "### **3.1. NumPy and Arrow Bridges**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a180d4d",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Pandas to Polars Migration/Module_01/Lecture_B/image_03_01.jpg?v=1766890170\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Pandas-style dataframe-to-NumPy conversion no longer automatic\n",
    ">* Polars uses columnar Arrow model, requiring deliberate conversions\n",
    "\n",
    ">* Whole-table array conversion is often inefficient\n",
    ">* Select needed numeric columns; preserve rich types\n",
    "\n",
    ">* Arrow tables become the shared data language\n",
    ">* Minimize array conversions to keep pipelines efficient\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ce1d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - NumPy and Arrow Bridges\n",
    "\n",
    "# Demonstrate converting Polars data to NumPy arrays carefully.\n",
    "# Show Arrow as an efficient bridge between tools.\n",
    "# Highlight selecting numeric columns before numerical conversion.\n",
    "\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "\n",
    "# Create a small Polars DataFrame with mixed column types.\n",
    "# Include numeric, string, and datetime columns for illustration.\n",
    "# This mimics a realistic customer behavior style dataset.\n",
    "\n",
    "df = pl.DataFrame({\"customer_id\": [1, 2, 3, 4], \"state\": [\"CA\", \"NY\", \"TX\", \"WA\"], \"spend_usd\": [120.5, 80.0, 150.25, 60.75], \"signup_date\": pl.date_range(low=pl.datetime(2024, 1, 1), high=pl.datetime(2024, 1, 4), interval=\"1d\", eager=True)})\n",
    "\n",
    "# Show the original Polars DataFrame structure and column types.\n",
    "# This highlights the rich typed, columnar representation.\n",
    "# We keep the printed output compact and readable.\n",
    "\n",
    "print(\"Original Polars DataFrame structure:\")\n",
    "print(df)\n",
    "\n",
    "# Convert the entire DataFrame directly to a NumPy array.\n",
    "# This forces mixed types into a single object dtype array.\n",
    "# Important information like nullability or categories can be lost.\n",
    "\n",
    "full_numpy = df.to_numpy()\n",
    "print(\"\\nNumPy array from full DataFrame:\")\n",
    "print(full_numpy)\n",
    "\n",
    "# Select only numeric columns before conversion to NumPy.\n",
    "# This matches better with numerical modeling expectations.\n",
    "# The resulting array has a clean floating point dtype.\n",
    "\n",
    "numeric_df = df.select([\"spend_usd\"])\n",
    "numeric_numpy = numeric_df.to_numpy()\n",
    "print(\"\\nNumPy array from numeric column only:\")\n",
    "print(numeric_numpy)\n",
    "\n",
    "# Use Arrow as an intermediate bridge representation.\n",
    "# Convert the Polars DataFrame to an Arrow table efficiently.\n",
    "# Then convert selected columns to NumPy when truly necessary.\n",
    "\n",
    "arrow_table = df.to_arrow()\n",
    "arrow_spend_array = arrow_table[\"spend_usd\"].to_numpy()\n",
    "print(\"\\nArrow to NumPy for spend_usd column:\")\n",
    "print(narrow_spend_array)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd63ea9c",
   "metadata": {},
   "source": [
    "### **3.2. Data Formats and IO**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d1afd7",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Pandas to Polars Migration/Module_01/Lecture_B/image_03_02.jpg?v=1766890185\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Pandas-style CSV-first thinking can mislead you\n",
    ">* Polars favors columnar formats for real performance\n",
    "\n",
    ">* Polars prefers partitioned columnar datasets over single CSVs\n",
    ">* This enables pushdown, less IO, and memory savings\n",
    "\n",
    ">* On-disk columnar files participate directly in computation\n",
    ">* Lazy Parquet workflows avoid materializing full intermediates\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597ee580",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Data Formats and IO\n",
    "\n",
    "# Demonstrate Polars reading CSV and Parquet formats efficiently.\n",
    "# Show lazy scanning of partitioned Parquet files on local storage.\n",
    "# Compare row counts and highlight minimal memory materialization.\n",
    "\n",
    "import polars as pl\n",
    "import os as operating_system\n",
    "\n",
    "# Create small example dataframe representing daily sales records.\n",
    "example_df = pl.DataFrame({\"day\": [\"2025-01-01\", \"2025-01-02\"], \"sales_dollars\": [120.0, 150.0]})\n",
    "\n",
    "# Save dataframe as CSV text file and Parquet columnar file formats.\n",
    "example_df.write_csv(\"sales_example.csv\")\n",
    "example_df.write_parquet(\"sales_example.parquet\")\n",
    "\n",
    "# Read entire CSV eagerly into memory and show resulting row count.\n",
    "csv_df = pl.read_csv(\"sales_example.csv\")\n",
    "print(\"CSV eager rows:\", csv_df.height)\n",
    "\n",
    "# Read Parquet lazily using scan_parquet without immediate materialization.\n",
    "parquet_lazy = pl.scan_parquet(\"sales_example.parquet\")\n",
    "filtered_lazy = parquet_lazy.filter(pl.col(\"sales_dollars\") > 130.0)\n",
    "\n",
    "# Collect filtered lazy result and show resulting row count.\n",
    "filtered_parquet_df = filtered_lazy.collect()\n",
    "print(\"Parquet lazy filtered rows:\", filtered_parquet_df.height)\n",
    "\n",
    "# Create directory for partitioned Parquet files representing separate days.\n",
    "partition_directory = \"sales_partitions\"\n",
    "operating_system.makedirs(partition_directory, exist_ok=True)\n",
    "\n",
    "# Write separate Parquet files for each day partition inside directory.\n",
    "for index, row in enumerate(example_df.iter_rows()):\n",
    "    day_value, sales_value = row\n",
    "    partition_path = operating_system.path.join(partition_directory, f\"day_{index}.parquet\")\n",
    "    pl.DataFrame({\"day\": [day_value], \"sales_dollars\": [sales_value]}).write_parquet(partition_path)\n",
    "\n",
    "# Lazily scan all partitioned Parquet files using wildcard pattern.\n",
    "partition_lazy = pl.scan_parquet(operating_system.path.join(partition_directory, \"*.parquet\"))\n",
    "month_lazy = partition_lazy.filter(pl.col(\"sales_dollars\") >= 130.0).select([\"day\", \"sales_dollars\"])\n",
    "\n",
    "# Collect final result and print concise summary of filtered partitions.\n",
    "month_df = month_lazy.collect()\n",
    "print(\"Partitioned Parquet filtered rows:\", month_df.height, \"rows with strong sales.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e9d9a6",
   "metadata": {},
   "source": [
    "### **3.3. Tooling and Library Integration**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8872484",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Pandas to Polars Migration/Module_01/Lecture_B/image_03_03.jpg?v=1766890202\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Many tools assume Pandas DataFrame inputs\n",
    ">* Polars often needs translation layers or conversions\n",
    "\n",
    ">* Notebook previews and debugging feel different in Polars\n",
    ">* You must explicitly materialize and inspect intermediate results\n",
    "\n",
    ">* Production systems often assume Pandas-like DataFrames\n",
    ">* Plan where Polars fits and needs bridges\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22036b3e",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Pandas vs Polars**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387946c6",
   "metadata": {},
   "source": [
    "\n",
    "In this lecture, you learned to:\n",
    "- Compare the core data structures and execution models of Pandas and Polars. \n",
    "- Explain how Polars’ lazy and eager APIs relate to typical Pandas workflows. \n",
    "- Identify common areas where Pandas habits may not map directly to Polars. \n",
    "\n",
    "In the next Lecture (Lecture C), we will go over 'Setting Up Polars'"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
