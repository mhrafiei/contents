{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd1dbf63",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Joins and Reshaping**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43527c2f",
   "metadata": {},
   "source": [
    ">Last update: 20260101.\n",
    "    \n",
    "By the end of this Lecture, you will be able to:\n",
    "- Implement Polars joins that replicate common pandas merge and join patterns across multiple tables. \n",
    "- Apply Polars reshaping operations such as pivot and melt to reproduce pandas pivot_table and stack or unstack behavior. \n",
    "- Design an end-to-end Polars pipeline that combines joins and reshaping to match the output of an existing pandas workflow. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01aaadfb",
   "metadata": {},
   "source": [
    "## **1. Core Join Techniques**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95336a4e",
   "metadata": {},
   "source": [
    "### **1.1. Core Join Types**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4fda30",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Pandas to Polars Migration/Module_03/Lecture_B/image_01_01.jpg?v=1767316373\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Inner, left, right joins mirror familiar pandas patterns\n",
    ">* They differ in which rows each join keeps\n",
    "\n",
    ">* Full outer joins keep all rows from both\n",
    ">* Great for complete views, accepting missing values\n",
    "\n",
    ">* Semi joins keep matching rows without extra columns\n",
    ">* Anti joins keep only rows with no matches\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14feb173",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Core Join Types\n",
    "\n",
    "# Demonstrate core Polars join types with tiny example tables.\n",
    "# Show inner, left, outer, semi, and anti joins clearly.\n",
    "# Keep output small, readable, and beginner friendly.\n",
    "\n",
    "# pip install polars if running outside Colab environment.\n",
    "\n",
    "# Import polars library for DataFrame operations.\n",
    "import polars as pl\n",
    "\n",
    "# Create left customers table with simple customer identifiers.\n",
    "customers = pl.DataFrame({\"customer_id\": [1, 2, 3], \"name\": [\"Alice\", \"Bob\", \"Carol\"]})\n",
    "\n",
    "# Create right orders table with matching and non matching customers.\n",
    "orders = pl.DataFrame({\"customer_id\": [2, 3, 4], \"order_total_usd\": [50, 75, 20]})\n",
    "\n",
    "# Show original customers table for reference context.\n",
    "print(\"Customers table:\\n\", customers)\n",
    "\n",
    "# Show original orders table for reference context.\n",
    "print(\"\\nOrders table:\\n\", orders)\n",
    "\n",
    "# Perform inner join keeping only matching customer identifiers.\n",
    "inner_join = customers.join(orders, on=\"customer_id\", how=\"inner\")\n",
    "\n",
    "# Perform left join keeping all customers from left table.\n",
    "left_join = customers.join(orders, on=\"customer_id\", how=\"left\")\n",
    "\n",
    "# Perform outer join keeping all customers and orders rows.\n",
    "outer_join = customers.join(orders, on=\"customer_id\", how=\"outer\")\n",
    "\n",
    "# Perform semi join filtering customers that have at least one order.\n",
    "semi_join = customers.join(orders, on=\"customer_id\", how=\"semi\")\n",
    "\n",
    "# Perform anti join finding customers without any matching orders.\n",
    "anti_join = customers.join(orders, on=\"customer_id\", how=\"anti\")\n",
    "\n",
    "# Print all join results in compact labeled form.\n",
    "print(\"\\nInner join:\\n\", inner_join, \"\\n\\nLeft join:\\n\", left_join, \"\\n\\nOuter join:\\n\", outer_join, \"\\n\\nSemi join:\\n\", semi_join, \"\\n\\nAnti join:\\n\", anti_join)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040ed9ef",
   "metadata": {},
   "source": [
    "### **1.2. Key Based Joins**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd5dc88",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Pandas to Polars Migration/Module_03/Lecture_B/image_01_02.jpg?v=1767316394\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Carefully choose columns that uniquely identify records\n",
    ">* Join keys must clearly map related rows across tables\n",
    "\n",
    ">* Handle joins with renamed or multi-column keys\n",
    ">* Preserve original key meaning when migrating workflows\n",
    "\n",
    ">* Join keys must reflect data quality, meaning\n",
    ">* Clear key semantics prevent subtle, hard-to-find errors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ece2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Key Based Joins\n",
    "\n",
    "# Demonstrate Polars key based joins with renamed and composite key columns.\n",
    "# Show joining sales facts to products and calendar using clear join keys.\n",
    "# Compare pandas style thinking with Polars syntax for multiple join keys.\n",
    "\n",
    "# pip install polars.\n",
    "\n",
    "# Import Polars for DataFrame creation and joins.\n",
    "import polars as pl\n",
    "\n",
    "# Create a sales fact table with product and date identifiers.\n",
    "sales_df = pl.DataFrame({\"store_id\": [1, 1, 2], \"product_id\": [101, 102, 101], \"sale_date\": [\"2024-01-01\", \"2024-01-01\", \"2024-01-02\"], \"units_sold\": [5, 3, 7]})\n",
    "\n",
    "# Create a product dimension table with a differently named key column.\n",
    "products_df = pl.DataFrame({\"client_key\": [101, 102], \"product_name\": [\"Widget\", \"Gadget\"], \"category\": [\"Tools\", \"Electronics\"]})\n",
    "\n",
    "# Create a calendar table keyed by date for additional attributes.\n",
    "calendar_df = pl.DataFrame({\"calendar_date\": [\"2024-01-01\", \"2024-01-02\"], \"day_name\": [\"Monday\", \"Tuesday\"]})\n",
    "\n",
    "# Join sales to products using differently named key columns.\n",
    "sales_products = sales_df.join(products_df, left_on=\"product_id\", right_on=\"client_key\", how=\"left\")\n",
    "\n",
    "# Join the result to calendar using a composite key of store and date.\n",
    "final_joined = sales_products.join(calendar_df, left_on=[\"sale_date\"], right_on=[\"calendar_date\"], how=\"left\")\n",
    "\n",
    "# Select and print a few columns to show joined result.\n",
    "print(final_joined.select([\"store_id\", \"product_id\", \"product_name\", \"sale_date\", \"day_name\", \"units_sold\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa3a5cd",
   "metadata": {},
   "source": [
    "### **1.3. Handling duplicate and missing keys**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e03fdf",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Pandas to Polars Migration/Module_03/Lecture_B/image_01_03.jpg?v=1767316416\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Real data has duplicate and missing join keys\n",
    ">* These keys change join results and need preprocessing\n",
    "\n",
    ">* Duplicate keys cause many-to-many join expansion\n",
    ">* Polars preserves all combinations; aggregate or validate keys\n",
    "\n",
    ">* Join type controls dropping or keeping missing keys\n",
    ">* Polars treats missing keys as non-matching values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5829af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Handling duplicate and missing keys\n",
    "\n",
    "# Demonstrate Polars joins with duplicate and missing keys clearly.\n",
    "# Show how many-to-many joins expand row counts significantly.\n",
    "# Show how missing keys behave differently across join types.\n",
    "\n",
    "# !pip install polars pyarrow --quiet.\n",
    "\n",
    "# Import required Polars library for DataFrame operations.\n",
    "import polars as pl\n",
    "\n",
    "# Create customers DataFrame with unique customer identifiers.\n",
    "customers = pl.DataFrame({\"customer_id\": [1, 2, 3], \"state\": [\"NY\", \"CA\", \"TX\"]})\n",
    "\n",
    "# Create orders DataFrame with duplicate and missing customer identifiers.\n",
    "orders = pl.DataFrame({\"order_id\": [101, 102, 103, 104], \"customer_id\": [1, 1, 2, None]})\n",
    "\n",
    "# Perform inner join to show duplicate key expansion clearly.\n",
    "inner_join = customers.join(orders, on=\"customer_id\", how=\"inner\")\n",
    "\n",
    "# Perform left join to keep all customers including unmatched ones.\n",
    "left_join = customers.join(orders, on=\"customer_id\", how=\"left\")\n",
    "\n",
    "# Perform right join to keep all orders including missing customer identifiers.\n",
    "right_join = customers.join(orders, on=\"customer_id\", how=\"right\")\n",
    "\n",
    "# Print inner join result showing duplicated customer one rows.\n",
    "print(\"Inner join result with duplicate customer one rows:\")\n",
    "print(inner_join)\n",
    "\n",
    "# Print left join result showing all customers preserved.\n",
    "print(\"\\nLeft join result preserving all customers including unmatched:\")\n",
    "print(left_join)\n",
    "\n",
    "# Print right join result showing order with missing customer identifier.\n",
    "print(\"\\nRight join result preserving all orders including missing customer:\")\n",
    "print(right_join)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ad47b0",
   "metadata": {},
   "source": [
    "## **2. Polars Data Reshaping**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb27b3f",
   "metadata": {},
   "source": [
    "### **2.1. Wide To Long Pivoting**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a62393a",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Pandas to Polars Migration/Module_03/Lecture_B/image_02_01.jpg?v=1767316436\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Convert many measurement columns into stacked rows\n",
    ">* Use variable and value columns for analysis\n",
    "\n",
    ">* Pivoting creates one value column plus indicator\n",
    ">* Long format simplifies comparisons, analysis, and visualization\n",
    "\n",
    ">* Choose identifier columns and stack measurement columns\n",
    ">* Long format enables flexible, reusable Polars pipelines\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7debca99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Wide To Long Pivoting\n",
    "\n",
    "# Demonstrate wide to long pivoting using Polars DataFrame example.\n",
    "# Show simple store revenue data reshaped from wide to long format.\n",
    "# Compare original wide table and long tidy table for clarity.\n",
    "\n",
    "# !pip install polars pyarrow --quiet.\n",
    "\n",
    "# Import polars library for DataFrame operations.\n",
    "import polars as pl\n",
    "\n",
    "# Create a simple wide DataFrame with quarterly revenue columns.\n",
    "stores_wide = pl.DataFrame({\n",
    "    \"store\": [\"North\", \"South\"],\n",
    "    \"revenue_q1_usd\": [1200, 900],\n",
    "    \"revenue_q2_usd\": [1500, 1100],\n",
    "})\n",
    "\n",
    "# Print the original wide DataFrame for reference.\n",
    "print(\"Wide format DataFrame:\\n\", stores_wide)\n",
    "\n",
    "# Use melt to pivot quarterly columns into long format rows.\n",
    "stores_long = stores_wide.melt(\n",
    "    id_vars=[\"store\"],\n",
    "    value_vars=[\"revenue_q1_usd\", \"revenue_q2_usd\"],\n",
    "    variable_name=\"quarter\",\n",
    "    value_name=\"revenue_usd\",\n",
    ")\n",
    "\n",
    "# Print the resulting long DataFrame showing one row per store quarter.\n",
    "print(\"\\nLong format DataFrame:\\n\", stores_long)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05d6785",
   "metadata": {},
   "source": [
    "### **2.2. Column Melting Basics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a7f4be",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Pandas to Polars Migration/Module_03/Lecture_B/image_02_02.jpg?v=1767316458\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Melting converts wide tables into long format\n",
    ">* Stacks measurement columns, repeats identifiers per row\n",
    "\n",
    ">* Identifier columns stay fixed and repeat per row\n",
    ">* Value columns melt into variable and value pairs\n",
    "\n",
    ">* Melting converts wide, human-friendly tables to long\n",
    ">* Long format supports trends, comparisons, and reshaping\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04357e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Column Melting Basics\n",
    "\n",
    "# Demonstrate basic column melting from wide to long format in Polars.\n",
    "# Show difference between identifier columns and value columns clearly.\n",
    "# Compare original wide table and melted long table visually.\n",
    "\n",
    "# !pip install polars pyarrow --quiet.\n",
    "\n",
    "# Import Polars library for DataFrame operations.\n",
    "import polars as pl\n",
    "\n",
    "# Create a simple wide DataFrame with student scores.\n",
    "students_df = pl.DataFrame({\n",
    "    \"student_id\": [1, 2],\n",
    "    \"school\": [\"North High\", \"South High\"],\n",
    "    \"math_score\": [88, 92],\n",
    "    \"science_score\": [91, 85],\n",
    "    \"history_score\": [79, 87],\n",
    "})\n",
    "\n",
    "# Print the original wide DataFrame for reference.\n",
    "print(\"Original wide DataFrame with separate subject columns:\")\n",
    "print(students_df)\n",
    "\n",
    "# Melt subject score columns into long tidy format.\n",
    "long_df = students_df.melt(\n",
    "    id_vars=[\"student_id\", \"school\"],\n",
    "    value_vars=[\"math_score\", \"science_score\", \"history_score\"],\n",
    "    variable_name=\"subject\",\n",
    "    value_name=\"score\",\n",
    ")\n",
    "\n",
    "# Print the melted long DataFrame showing subject and score columns.\n",
    "print(\"\\nMelted long DataFrame with subject and score columns:\")\n",
    "print(long_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cae3d2",
   "metadata": {},
   "source": [
    "### **2.3. Pivot Table Equivalents**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c2053a",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Pandas to Polars Migration/Module_03/Lecture_B/image_02_03.jpg?v=1767316483\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Pivot tables summarize long data into matrices\n",
    ">* Group, aggregate, then reshape summaries into rows-columns\n",
    "\n",
    ">* Group long-form data and aggregate key values\n",
    ">* Reshape grouped results into matrix-style comparison table\n",
    "\n",
    ">* Support multiple measures, custom aggregations, hierarchies\n",
    ">* Group, summarize, then reshape categories into columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41122115",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Pivot Table Equivalents\n",
    "\n",
    "# Demonstrate Polars pivot table style reshaping from long transactional data.\n",
    "# Show grouping, aggregation, and pivoting to create a summary matrix.\n",
    "# Compare original long data with pivoted wide summary output.\n",
    "\n",
    "# !pip install polars pyarrow.\n",
    "\n",
    "# Import required Polars library for DataFrame operations.\n",
    "import polars as pl\n",
    "\n",
    "# Create simple long format sales data with regions and product categories.\n",
    "data = {\n",
    "    \"region\": [\"North\", \"North\", \"South\", \"South\", \"West\", \"West\"],\n",
    "    \"product\": [\"Gadget\", \"Widget\", \"Gadget\", \"Widget\", \"Gadget\", \"Widget\"],\n",
    "    \"revenue_dollars\": [1200, 800, 1500, 600, 900, 700],\n",
    "}\n",
    "\n",
    "# Build Polars DataFrame from the dictionary data structure.\n",
    "df_sales = pl.DataFrame(data)\n",
    "\n",
    "# Print original long format sales data for reference understanding.\n",
    "print(\"Original long sales data:\")\n",
    "print(df_sales)\n",
    "\n",
    "# Group by region and product then aggregate total revenue dollars.\n",
    "df_grouped = df_sales.group_by([\"region\", \"product\"]).agg(\n",
    "    pl.col(\"revenue_dollars\").sum().alias(\"total_revenue\"),\n",
    ")\n",
    "\n",
    "# Pivot grouped data so products become columns like spreadsheet pivot tables.\n",
    "df_pivot = df_grouped.pivot(\n",
    "    values=\"total_revenue\",\n",
    "    index=\"region\",\n",
    "    columns=\"product\",\n",
    ")\n",
    "\n",
    "# Print pivoted wide summary showing revenue by region and product.\n",
    "print(\"\\nPivot style revenue summary:\")\n",
    "print(df_pivot)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a43bde2",
   "metadata": {},
   "source": [
    "## **3. Polars Pipeline Design**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4ef3ba",
   "metadata": {},
   "source": [
    "### **3.1. Combining Joins And Transforms**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723831d2",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Pandas to Polars Migration/Module_03/Lecture_B/image_03_01.jpg?v=1767316506\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Plan joins and reshaping as one pipeline\n",
    ">* Anticipate final shape to avoid awkward intermediates\n",
    "\n",
    ">* Plan from final table and work backward\n",
    ">* Use joins for context, reshaping for layout\n",
    "\n",
    ">* Join type and keys shape later reshaping\n",
    ">* Coordinate joins and pivots for robust pipelines\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb48f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Combining Joins And Transforms\n",
    "\n",
    "# Demonstrate combining joins and reshaping using Polars pipeline style.\n",
    "# Show how multiple tables join then pivot into final summary.\n",
    "# Keep example small, clear, and beginner friendly.\n",
    "\n",
    "# !pip install polars.\n",
    "\n",
    "# Import polars for DataFrame operations.\n",
    "import polars as pl\n",
    "\n",
    "# Create small customers table with simple attributes.\n",
    "customers = pl.DataFrame({\"customer_id\": [1, 2], \"state\": [\"CA\", \"NY\"]})\n",
    "\n",
    "# Create products table with categories and prices.\n",
    "products = pl.DataFrame({\"product_id\": [10, 20, 30], \"category\": [\"Tools\", \"Toys\", \"Tools\"], \"price_usd\": [15.0, 8.0, 22.0]})\n",
    "\n",
    "# Create transactions table linking customers and products.\n",
    "transactions = pl.DataFrame({\"customer_id\": [1, 1, 2, 2], \"product_id\": [10, 20, 20, 30], \"quantity\": [2, 1, 3, 1]})\n",
    "\n",
    "# Join transactions with products to attach categories and prices.\n",
    "trx_with_products = transactions.join(products, on=\"product_id\", how=\"left\")\n",
    "\n",
    "# Compute revenue dollars per transaction line.\n",
    "trx_with_revenue = trx_with_products.with_columns((pl.col(\"quantity\") * pl.col(\"price_usd\")).alias(\"revenue_usd\"))\n",
    "\n",
    "# Join enriched transactions with customers for state information.\n",
    "full_trx = trx_with_revenue.join(customers, on=\"customer_id\", how=\"left\")\n",
    "\n",
    "# Aggregate revenue by customer and category before reshaping.\n",
    "agg = full_trx.group_by([\"customer_id\", \"category\"]).agg(pl.col(\"revenue_usd\").sum().alias(\"total_revenue_usd\"))\n",
    "\n",
    "# Pivot tall aggregated table into wide customer by category matrix.\n",
    "customer_category_matrix = agg.pivot(values=\"total_revenue_usd\", index=\"customer_id\", columns=\"category\").sort(\"customer_id\")\n",
    "\n",
    "# Print final matrix showing combined joins and transforms.\n",
    "print(customer_category_matrix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5208b881",
   "metadata": {},
   "source": [
    "### **3.2. LazyFrame Pipeline Patterns**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92968076",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Pandas to Polars Migration/Module_03/Lecture_B/image_03_02.jpg?v=1767316524\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Treat LazyFrames as blueprints for data pipelines\n",
    ">* Deferred execution lets Polars optimize all transformations\n",
    "\n",
    ">* Break pipelines into ingestion, joins, reshaping stages\n",
    ">* Chain LazyFrame steps for clarity and optimization\n",
    "\n",
    ">* Keep all intermediate steps inside one LazyFrame\n",
    ">* This improves performance, memory use, and reproducibility\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab76b44",
   "metadata": {},
   "source": [
    "### **3.3. Validating Pipeline Equivalence**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16154fb",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Pandas to Polars Migration/Module_03/Lecture_B/image_03_03.jpg?v=1767316550\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Define what “same result” means clearly\n",
    ">* Set precise criteria to guide validation strategy\n",
    "\n",
    ">* Test equivalence in layers using small datasets\n",
    ">* Compare pandas and Polars outputs on key metrics\n",
    "\n",
    ">* Validate with real data using summary metrics\n",
    ">* Monitor and recheck pipelines continuously as things change\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61825fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Validating Pipeline Equivalence\n",
    "\n",
    "# Demonstrate validating pandas and Polars pipeline equivalence with simple metrics.\n",
    "# Show how to define equivalence criteria and compare aggregated outputs.\n",
    "# Use small customer order data to validate joins and reshaping equivalence.\n",
    "\n",
    "# !pip install polars pandas.\n",
    "\n",
    "# Import required libraries for pandas and Polars usage.\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "# Create small pandas DataFrames representing customers and their orders.\n",
    "customers_pd = pd.DataFrame({\"customer_id\": [1, 2, 3], \"region\": [\"East\", \"West\", \"East\"]})\n",
    "orders_pd = pd.DataFrame({\"order_id\": [10, 11, 12, 13], \"customer_id\": [1, 1, 2, 3], \"revenue_usd\": [100.0, 50.0, 80.0, 120.0]})\n",
    "\n",
    "# Build original pandas pipeline joining tables and aggregating revenue by region.\n",
    "pandas_result = (\n",
    "    orders_pd.merge(customers_pd, on=\"customer_id\", how=\"left\")\n",
    "    .groupby(\"region\", as_index=False)[\"revenue_usd\"].sum()\n",
    "    .sort_values(\"region\")\n",
    ")\n",
    "\n",
    "# Convert pandas DataFrames into Polars DataFrames for equivalent processing.\n",
    "customers_pl = pl.from_pandas(customers_pd)\n",
    "orders_pl = pl.from_pandas(orders_pd)\n",
    "\n",
    "# Build Polars pipeline using join and groupby to mirror pandas behavior.\n",
    "polars_result = (\n",
    "    orders_pl.join(customers_pl, on=\"customer_id\", how=\"left\")\n",
    ")\n",
    "polars_result = (\n",
    "    polars_result\n",
    "    .group_by(\"region\")\n",
    "    .agg(pl.col(\"revenue_usd\").sum())\n",
    "    .sort(\"region\")\n",
    ")\n",
    "\n",
    "# Define equivalence criteria focusing on region totals rather than row ordering.\n",
    "print(\"Pandas region revenue totals:\")\n",
    "print(pandas_result.to_string(index=False))\n",
    "\n",
    "# Print Polars result converted to pandas for easier side by side comparison.\n",
    "print(\"Polars region revenue totals:\")\n",
    "print(polars_result.to_pandas().to_string(index=False))\n",
    "\n",
    "# Check numeric equivalence by comparing sorted region and revenue columns.\n",
    "match = pandas_result.reset_index(drop=True).equals(polars_result.to_pandas().reset_index(drop=True))\n",
    "print(\"Pipelines produce equivalent regional revenue:\", match)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6195af",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Joins and Reshaping**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590e3e2e",
   "metadata": {},
   "source": [
    "\n",
    "In this lecture, you learned to:\n",
    "- Implement Polars joins that replicate common pandas merge and join patterns across multiple tables. \n",
    "- Apply Polars reshaping operations such as pivot and melt to reproduce pandas pivot_table and stack or unstack behavior. \n",
    "- Design an end-to-end Polars pipeline that combines joins and reshaping to match the output of an existing pandas workflow. \n",
    "\n",
    "<font color='yellow'>Congratulations on completing this course!</font>"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
