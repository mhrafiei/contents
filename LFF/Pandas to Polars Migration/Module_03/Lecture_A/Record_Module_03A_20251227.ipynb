{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f4bbef2",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Grouping and Aggregation**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036d4ecb",
   "metadata": {},
   "source": [
    ">Last update: 20251227.\n",
    "    \n",
    "By the end of this Lecture, you will be able to:\n",
    "- Perform groupby operations in Polars that mirror common Pandas groupby use cases. \n",
    "- Define multiple aggregations and custom expressions within a single Polars groupby call. \n",
    "- Compare performance and readability of Polars groupby pipelines to their Pandas counterparts on sample datasets. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a8b738",
   "metadata": {},
   "source": [
    "## **1. Polars Groupby Essentials**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2cad3d",
   "metadata": {},
   "source": [
    "### **1.1. Multi Key Grouping**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d365d0",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Pandas to Polars Migration/Module_03/Lecture_A/image_01_01.jpg?v=1766895377\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Multi key grouping uses combinations of columns\n",
    ">* Each unique value combination forms a detailed group\n",
    "\n",
    ">* Group by multiple columns to reveal patterns\n",
    ">* Polars handles multi key groups naturally and efficiently\n",
    "\n",
    ">* Multi key grouping slices data into detailed segments\n",
    ">* Polars efficiently summarizes statistics across these segments\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ede178",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Multi Key Grouping\n",
    "\n",
    "# Demonstrate multi key grouping using Polars DataFrame operations.\n",
    "# Show grouping by store and product category together.\n",
    "# Compare single key and multi key aggregation outputs clearly.\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "# Create a small retail style dataset with multiple grouping keys.\n",
    "data = {\n",
    "    \"store\": [\"North\", \"North\", \"South\", \"South\", \"West\", \"West\"],\n",
    "    \"category\": [\"Tools\", \"Garden\", \"Tools\", \"Garden\", \"Tools\", \"Garden\"],\n",
    "    \"day\": [\"Mon\", \"Mon\", \"Mon\", \"Tue\", \"Tue\", \"Tue\"],\n",
    "    \"revenue_dollars\": [120, 80, 150, 60, 200, 90],\n",
    "}\n",
    "\n",
    "\n",
    "# Build a Polars DataFrame from the dictionary data structure.\n",
    "df = pl.DataFrame(data)\n",
    "\n",
    "# Show the original data to understand available grouping keys.\n",
    "print(\"Original data with store, category, day, revenue:\")\n",
    "print(df)\n",
    "\n",
    "\n",
    "# Group by a single key, here only by store column.\n",
    "by_store = df.group_by(\"store\").agg(pl.col(\"revenue_dollars\").sum().alias(\"total_revenue\"))\n",
    "\n",
    "# Display total revenue per store using single key grouping.\n",
    "print(\"\\nTotal revenue grouped only by store:\")\n",
    "print(by_store)\n",
    "\n",
    "\n",
    "# Group by two keys, store and category together as composite key.\n",
    "by_store_category = df.group_by([\"store\", \"category\"]).agg(\n",
    "    pl.col(\"revenue_dollars\").sum().alias(\"total_revenue\"))\n",
    "\n",
    "# Display revenue per store and category pair using multi key grouping.\n",
    "print(\"\\nTotal revenue grouped by store and category:\")\n",
    "print(by_store_category)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef3208f",
   "metadata": {},
   "source": [
    "### **1.2. Basic Group Aggregations**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2410ebf",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Pandas to Polars Migration/Module_03/Lecture_A/image_01_02.jpg?v=1766895393\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Use groupby to compute basic summary statistics\n",
    ">* Summaries collapse many rows into per-group totals\n",
    "\n",
    ">* Grouped aggregations answer many real analysis questions\n",
    ">* They turn raw rows into comparable summary metrics\n",
    "\n",
    ">* Shift mindset from rows to aggregated views\n",
    ">* Use grouped summaries to spot patterns and outliers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8822b145",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Basic Group Aggregations\n",
    "\n",
    "# Demonstrate basic group aggregations using Polars on a tiny sales dataset.\n",
    "# Show how to group by region and compute simple summary statistics clearly.\n",
    "# Compare raw data and aggregated results to reinforce grouped aggregation concepts.\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "# Create a small sales DataFrame with region, units, and revenue columns.\n",
    "data = {\n",
    "    \"region\": [\"North\", \"North\", \"South\", \"South\", \"West\", \"West\"],\n",
    "    \"units_sold\": [10, 5, 8, 12, 7, 3],\n",
    "    \"revenue_usd\": [200, 120, 160, 300, 140, 60],\n",
    "}\n",
    "\n",
    "# Build the Polars DataFrame from the dictionary data structure.\n",
    "df = pl.DataFrame(data)\n",
    "\n",
    "# Show the original row level data for context and understanding.\n",
    "print(\"Original sales data by order row:\")\n",
    "print(df)\n",
    "\n",
    "# Group by region and compute count, sum, and average aggregations.\n",
    "agg_df = (\n",
    "    df.groupby(\"region\")\n",
    "    .agg([\n",
    "        pl.count().alias(\"order_count\"),\n",
    "        pl.col(\"units_sold\").sum().alias(\"total_units\"),\n",
    "        pl.col(\"revenue_usd\").sum().alias(\"total_revenue\"),\n",
    "        pl.col(\"revenue_usd\").mean().alias(\"average_revenue\"),\n",
    "    ])\n",
    ")\n",
    "\n",
    "# Display the aggregated results, one summary row per region group.\n",
    "print(\"\\nAggregated sales metrics per region:\")\n",
    "print(agg_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b9c744",
   "metadata": {},
   "source": [
    "### **1.3. Missing Groups and Categories**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c6cb0f",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Pandas to Polars Migration/Module_03/Lecture_A/image_01_03.jpg?v=1766895426\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Groupby usually returns only observed categories\n",
    ">* Know difference between missing groups and zero values\n",
    "\n",
    ">* Groupby only returns categories present in data\n",
    ">* Add missing categories later using reference tables\n",
    "\n",
    ">* Distinguish missing groups from true zero values\n",
    ">* Build full group grid to reveal data gaps\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469a00ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Missing Groups and Categories\n",
    "\n",
    "# Show how missing categories disappear after grouping results.\n",
    "# Demonstrate reintroducing all categories using a reference table.\n",
    "# Compare grouped output before and after filling missing categories.\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "# Create simple sales data with missing product categories.\n",
    "data = pl.DataFrame({\"month\": [\"Jan\", \"Jan\", \"Feb\", \"Feb\"], \"category\": [\"A\", \"B\", \"A\", \"A\"], \"sales_dollars\": [100, 150, 80, 40]})\n",
    "\n",
    "# Define all possible categories, including one never appearing in data.\n",
    "all_categories = pl.DataFrame({\"category\": [\"A\", \"B\", \"C\"]})\n",
    "\n",
    "# Group by month and category, summing sales dollars for each combination.\n",
    "grouped = data.group_by([\"month\", \"category\"]).agg(pl.col(\"sales_dollars\").sum().alias(\"total_sales\"))\n",
    "\n",
    "# Print grouped result, note category C never appears anywhere.\n",
    "print(\"Grouped result without missing categories filled:\")\n",
    "print(grouped)\n",
    "\n",
    "# Build full grid of months and all categories using cross join operation.\n",
    "months = data.select(\"month\").unique().sort()\n",
    "full_grid = months.join(all_categories, how=\"cross\")\n",
    "\n",
    "# Join grouped data onto full grid, filling missing totals with zero values.\n",
    "completed = full_grid.join(grouped, on=[\"month\", \"category\"], how=\"left\").with_columns(pl.col(\"total_sales\").fill_null(0))\n",
    "\n",
    "# Print completed result, now every month shows every category explicitly.\n",
    "print(\"\\nCompleted result with explicit zero sales categories:\")\n",
    "print(completed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53272271",
   "metadata": {},
   "source": [
    "## **2. Multiple Polars Aggregations**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3dcab6",
   "metadata": {},
   "source": [
    "### **2.1. Multi Column Aggregations**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d49f327",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Pandas to Polars Migration/Module_03/Lecture_A/image_02_01.jpg?v=1766895442\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Compute many summaries across multiple columns together\n",
    ">* Get a wide table of metrics per group\n",
    "\n",
    ">* Combine many group metrics in one step\n",
    ">* Reduces mental overhead and improves engine optimization\n",
    "\n",
    ">* Group many stakeholder metrics in one summary\n",
    ">* Single table improves consistency and repeatable analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e632ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Multi Column Aggregations\n",
    "\n",
    "# Demonstrate grouped multi column aggregations using Polars DataFrame operations.\n",
    "# Show several summary metrics computed together for multiple numeric columns.\n",
    "# Compare results for grouped stores and months in a compact summary table.\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "# Create a small example dataset with store, month, and transaction details.\n",
    "data = {\n",
    "    \"store\": [\"A\", \"A\", \"A\", \"B\", \"B\", \"B\"],\n",
    "    \"month\": [\"Jan\", \"Jan\", \"Feb\", \"Jan\", \"Feb\", \"Feb\"],\n",
    "    \"revenue_usd\": [120.0, 80.0, 150.0, 200.0, 90.0, 110.0],\n",
    "    \"discount_percent\": [5.0, 10.0, 0.0, 15.0, 5.0, 0.0],\n",
    "    \"checkout_seconds\": [45, 60, 50, 55, 65, 70],\n",
    "}\n",
    "\n",
    "# Build the Polars DataFrame from the dictionary data structure.\n",
    "df = pl.DataFrame(data)\n",
    "\n",
    "# Show the original data to understand the grouped transaction records.\n",
    "print(\"Original transactions DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "# Group by store and month, then compute multiple column aggregations together.\n",
    "summary = (\n",
    "    df.group_by([\"store\", \"month\"]).agg(\n",
    "        [\n",
    "            pl.col(\"revenue_usd\").sum().alias(\"total_revenue_usd\"),\n",
    "            pl.col(\"revenue_usd\").max().alias(\"max_transaction_usd\"),\n",
    "            pl.col(\"discount_percent\").mean().alias(\"avg_discount_percent\"),\n",
    "            pl.col(\"checkout_seconds\").mean().alias(\"avg_checkout_seconds\"),\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "# Display the compact summary table with multiple metrics per group.\n",
    "print(\"\\nGrouped multi column aggregation summary:\")\n",
    "print(summary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad0f61c",
   "metadata": {},
   "source": [
    "### **2.2. Readable Aggregation Aliases**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86e0a4d",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Pandas to Polars Migration/Module_03/Lecture_A/image_02_02.jpg?v=1766895456\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Use clear aliases when aggregating many metrics\n",
    ">* Descriptive names make grouped results readable and shareable\n",
    "\n",
    ">* Use specific aliases to distinguish similar metrics\n",
    ">* Consistent names reduce confusion and aid collaboration\n",
    "\n",
    ">* Name custom metrics for meaning and transparency\n",
    ">* Good aliases aid debugging, extension, and collaboration\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584de690",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Readable Aggregation Aliases\n",
    "\n",
    "# Demonstrate Polars groupby aggregations with readable aliases for clarity.\n",
    "# Show difference between automatic names and explicit alias names clearly.\n",
    "# Help beginners understand why descriptive aggregation aliases improve readability.\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "# Create a small sales DataFrame with store, orders, and revenue columns.\n",
    "data = {\"store\": [\"North\", \"North\", \"South\", \"South\"], \"orders\": [10, 15, 8, 12], \"revenue_usd\": [200, 350, 160, 300]}\n",
    "\n",
    "# Build the Polars DataFrame from the dictionary data structure.\n",
    "df = pl.DataFrame(data)\n",
    "\n",
    "# Group by store and aggregate without aliases, using default generated column names.\n",
    "no_alias_result = df.group_by(\"store\").agg([pl.col(\"orders\").sum(), pl.col(\"revenue_usd\").mean()])\n",
    "\n",
    "# Group by store and aggregate with clear aliases for each computed metric.\n",
    "with_alias_result = df.group_by(\"store\").agg([pl.col(\"orders\").sum().alias(\"total_orders\"), pl.col(\"revenue_usd\").mean().alias(\"avg_revenue_usd\")])\n",
    "\n",
    "# Print both results to compare column names and overall readability.\n",
    "print(\"Without readable aliases result:\")\n",
    "print(no_alias_result)\n",
    "\n",
    "print(\"\\nWith readable aliases result:\")\n",
    "print(with_alias_result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9aa77e7",
   "metadata": {},
   "source": [
    "### **2.3. Custom Aggregation Expressions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331faafa",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Pandas to Polars Migration/Module_03/Lecture_A/image_02_03.jpg?v=1766895469\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Use custom groupby expressions for rich logic\n",
    ">* Combine filters, sorting, and math per group\n",
    "\n",
    ">* Chain filters and transformations inside each group\n",
    ">* Compute tailored metrics like medians, ranges, extremes\n",
    "\n",
    ">* Inline group logic encodes complex business rules\n",
    ">* Reduces intermediate steps and errors in summaries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557a2938",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Custom Aggregation Expressions\n",
    "\n",
    "# Demonstrate custom aggregation expressions within Polars groupby operations.\n",
    "# Show filtering, conditional logic, and arithmetic inside grouped aggregations.\n",
    "# Compare multiple tailored metrics computed per customer group.\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "# Create a small transactions DataFrame with simple customer purchases.\n",
    "data = {\n",
    "    \"customer_id\": [1, 1, 1, 2, 2, 3],\n",
    "    \"month\": [\"Jan\", \"Feb\", \"Feb\", \"Jan\", \"Mar\", \"Jan\"],\n",
    "    \"amount_usd\": [50, 80, 40, 100, 60, 30],\n",
    "    \"used_coupon\": [True, False, True, False, True, False],\n",
    "}\n",
    "\n",
    "\n",
    "df = pl.DataFrame(data)\n",
    "\n",
    "# Show the original data for quick reference.\n",
    "print(\"Original transactions DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "# Group by customer and define several custom aggregation expressions.\n",
    "result = (\n",
    "    df.groupby(\"customer_id\")\n",
    "    .agg(\n",
    "        [\n",
    "            pl.col(\"amount_usd\").sum().alias(\"total_spend_usd\"),\n",
    "            pl.col(\"amount_usd\").filter(pl.col(\"used_coupon\")).sum().alias(\"coupon_spend_usd\"),\n",
    "            pl.col(\"amount_usd\").max().alias(\"max_single_purchase_usd\"),\n",
    "            (pl.col(\"amount_usd\").max() - pl.col(\"amount_usd\").min()).alias(\"spend_range_usd\"),\n",
    "            pl.when(pl.col(\"used_coupon\").any())\n",
    "            .then(True)\n",
    "            .otherwise(False)\n",
    "            .alias(\"ever_used_coupon\"),\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "print(\"\\nCustom aggregated metrics per customer:\")\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8faca00f",
   "metadata": {},
   "source": [
    "## **3. Polars Groupby Performance**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1956c1f",
   "metadata": {},
   "source": [
    "### **3.1. Runtime Comparison**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e2a40d",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Pandas to Polars Migration/Module_03/Lecture_A/image_03_01.jpg?v=1766895490\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Runtime measures speed of grouping large datasets\n",
    ">* Polars groups faster than row-based tools, especially repeatedly\n",
    "\n",
    ">* Polars scales better as datasets grow larger\n",
    ">* Parallelism and less data movement control runtimes\n",
    "\n",
    ">* Real analyses chain many groupby transformations together\n",
    ">* Polars optimizes whole pipelines for faster runtimes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28078d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Runtime Comparison\n",
    "\n",
    "# Compare runtime of Pandas and Polars groupby operations simply.\n",
    "# Generate a synthetic dataset with many rows for timing tests.\n",
    "# Show which library finishes the groupby aggregation faster overall.\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "!pip install polars --quiet\n",
    "import polars as pl\n",
    "\n",
    "np.random.seed(42)\n",
    "num_rows = 2_000_000\n",
    "\n",
    "stores = np.random.randint(1, 101, size=num_rows)\n",
    "\n",
    "\n",
    "sales = np.random.uniform(5.0, 500.0, size=num_rows)\n",
    "\n",
    "pandas_df = pd.DataFrame({\"store_id\": stores, \"sales_amount\": sales})\n",
    "\n",
    "polars_df = pl.DataFrame({\"store_id\": stores, \"sales_amount\": sales})\n",
    "\n",
    "start_pandas = time.time()\n",
    "\n",
    "pandas_result = pandas_df.groupby(\"store_id\")[\"sales_amount\"].agg([\"sum\", \"mean\"])\n",
    "\n",
    "end_pandas = time.time()\n",
    "\n",
    "pandas_time = end_pandas - start_pandas\n",
    "\n",
    "start_polars = time.time()\n",
    "\n",
    "polars_result = polars_df.groupby(\"store_id\").agg([\n",
    "    pl.col(\"sales_amount\").sum().alias(\"sum\"),\n",
    "    pl.col(\"sales_amount\").mean().alias(\"mean\"),\n",
    "])\n",
    "\n",
    "end_polars = time.time()\n",
    "\n",
    "polars_time = end_polars - start_polars\n",
    "\n",
    "print(\"Pandas groupby runtime seconds:\", round(pandas_time, 4))\n",
    "\n",
    "print(\"Polars groupby runtime seconds:\", round(polars_time, 4))\n",
    "\n",
    "print(\"Faster library for this run:\", \"Polars\" if polars_time < pandas_time else \"Pandas\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad399d49",
   "metadata": {},
   "source": [
    "### **3.2. Memory Efficient Grouping**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf9eac8",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Pandas to Polars Migration/Module_03/Lecture_A/image_03_02.jpg?v=1766895516\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Grouping can use lots of memory quickly\n",
    ">* Polars reuses columnar data to reduce memory\n",
    "\n",
    ">* Naive groupby pipelines create many copies, wasting memory\n",
    ">* Polars fuses operations, streaming aggregations with minimal buffers\n",
    "\n",
    ">* Lower memory use improves speed and stability\n",
    ">* Enables larger, clearer groupby analyses on one machine\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f031e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Memory Efficient Grouping\n",
    "\n",
    "# Demonstrate memory efficient grouping using Polars and Pandas side by side.\n",
    "# Create synthetic sales data and perform similar groupby aggregations.\n",
    "# Compare peak memory usage and runtime for both libraries briefly.\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import psutil\n",
    "import polars as pl\n",
    "\n",
    "process = psutil.Process()\n",
    "start_memory_mb = process.memory_info().rss / (1024 * 1024)\n",
    "\n",
    "n_rows = 1_000_000\n",
    "n_customers = 5_000\n",
    "\n",
    "np.random.seed(42)\n",
    "customer_ids = np.random.randint(0, n_customers, size=n_rows)\n",
    "\n",
    "categories = np.random.choice([\"tools\", \"sports\", \"clothes\", \"toys\"], size=n_rows)\n",
    "\n",
    "amounts = np.random.exponential(scale=50.0, size=n_rows)\n",
    "\n",
    "pandas_start_time = time.time()\n",
    "\n",
    "pdf = pd.DataFrame({\"customer_id\": customer_ids, \"category\": categories, \"amount\": amounts})\n",
    "\n",
    "pandas_grouped = pdf.groupby([\"customer_id\", \"category\"], as_index=False).agg({\"amount\": [\"sum\", \"mean\"]})\n",
    "\n",
    "pandas_runtime = time.time() - pandas_start_time\n",
    "\n",
    "pandas_memory_mb = process.memory_info().rss / (1024 * 1024)\n",
    "\n",
    "polars_start_time = time.time()\n",
    "\n",
    "pldf = pl.DataFrame({\"customer_id\": customer_ids, \"category\": categories, \"amount\": amounts})\n",
    "\n",
    "polars_grouped = pldf.group_by([\"customer_id\", \"category\"]).agg([pl.col(\"amount\").sum().alias(\"total_amount\"), pl.col(\"amount\").mean().alias(\"avg_amount\")])\n",
    "\n",
    "polars_runtime = time.time() - polars_start_time\n",
    "\n",
    "end_memory_mb = process.memory_info().rss / (1024 * 1024)\n",
    "\n",
    "pandas_memory_increase = pandas_memory_mb - start_memory_mb\n",
    "\n",
    "polars_memory_increase = end_memory_mb - pandas_memory_mb\n",
    "\n",
    "print(\"Pandas groupby runtime seconds:\", round(pandas_runtime, 4))\n",
    "\n",
    "print(\"Polars groupby runtime seconds:\", round(polars_runtime, 4))\n",
    "\n",
    "print(\"Pandas approximate memory increase MB:\", round(pandas_memory_increase, 2))\n",
    "\n",
    "print(\"Polars approximate memory increase MB:\", round(polars_memory_increase, 2))\n",
    "\n",
    "print(\"Pandas grouped rows example head:\")\n",
    "\n",
    "print(pandas_grouped.head(3))\n",
    "\n",
    "print(\"Polars grouped rows example head:\")\n",
    "\n",
    "print(polars_grouped.head(3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff55cadc",
   "metadata": {},
   "source": [
    "### **3.3. Lazy Groupby Optimization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646ce284",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Pandas to Polars Migration/Module_03/Lecture_A/image_03_03.jpg?v=1766895537\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Describe full pipelines; Polars optimizes before running\n",
    ">* Engine reorders work, reducing data and computations\n",
    "\n",
    ">* Lazy queries filter and select columns early\n",
    ">* Grouping runs on smaller data, saving work\n",
    "\n",
    ">* Polars reuses work across many groupby aggregations\n",
    ">* Whole pipeline optimized as one clear lazy query\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab687f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Lazy Groupby Optimization\n",
    "\n",
    "# Demonstrate lazy groupby optimization using Polars expressions.\n",
    "# Compare eager and lazy pipelines on a simple sales dataset.\n",
    "# Show how filters pushdown reduces grouped data size.\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "# Create a small example sales DataFrame with simple columns.\n",
    "data = {\n",
    "    \"store\": [\"A\", \"A\", \"B\", \"B\", \"B\", \"C\"],\n",
    "    \"month\": [\"Jan\", \"Feb\", \"Jan\", \"Feb\", \"Mar\", \"Jan\"],\n",
    "    \"revenue_usd\": [100, 120, 90, 130, 80, 70],\n",
    "}\n",
    "\n",
    "sales_eager = pl.DataFrame(data)\n",
    "\n",
    "# Eager style groups everything before filtering recent months.\n",
    "eager_group = (\n",
    "    sales_eager.groupby(\"store\")\n",
    "    .agg(pl.col(\"revenue_usd\").sum().alias(\"total_revenue\"))\n",
    ")\n",
    "\n",
    "recent_eager = eager_group.filter(pl.col(\"total_revenue\") > 150)\n",
    "\n",
    "print(\"Eager result after grouping everything:\")\n",
    "print(recent_eager)\n",
    "\n",
    "# Lazy style filters months before grouping, reducing grouped rows.\n",
    "sales_lazy = sales_eager.lazy()\n",
    "\n",
    "lazy_pipeline = (\n",
    "    sales_lazy\n",
    "    .filter(pl.col(\"month\").is_in([\"Feb\", \"Mar\"]))\n",
    "    .groupby(\"store\")\n",
    "    .agg(pl.col(\"revenue_usd\").sum().alias(\"total_revenue\"))\n",
    "    .filter(pl.col(\"total_revenue\") > 100)\n",
    ")\n",
    "\n",
    "lazy_result = lazy_pipeline.collect()\n",
    "\n",
    "print(\"\\nLazy result with pushed down filter:\")\n",
    "print(lazy_result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090f8daf",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Grouping and Aggregation**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35e5751",
   "metadata": {},
   "source": [
    "\n",
    "In this lecture, you learned to:\n",
    "- Perform groupby operations in Polars that mirror common Pandas groupby use cases. \n",
    "- Define multiple aggregations and custom expressions within a single Polars groupby call. \n",
    "- Compare performance and readability of Polars groupby pipelines to their Pandas counterparts on sample datasets. \n",
    "\n",
    "In the next Lecture (Lecture B), we will go over 'Joins and Reshaping'"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
