{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bff4cf6",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Groupby and Aggregation**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f70a62",
   "metadata": {},
   "source": [
    ">Last update: 20260101.\n",
    "    \n",
    "By the end of this Lecture, you will be able to:\n",
    "- Translate typical pandas groupby and aggregation code into equivalent Polars groupby expressions. \n",
    "- Use Polars window functions to replace more complex pandas groupby and rolling patterns where appropriate. \n",
    "- Validate that migrated Polars aggregations match pandas results across multiple groups and metrics. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbaa4958",
   "metadata": {},
   "source": [
    "## **1. Core Groupby Patterns**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9d8127",
   "metadata": {},
   "source": [
    "### **1.1. Single Key Groupby Basics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3d6685",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Pandas to Polars Migration/Module_03/Lecture_A/image_01_01.jpg?v=1767314626\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Single column groupby splits data into groups\n",
    ">* Same concept, new syntax when switching libraries\n",
    "\n",
    ">* Pick a group column and target columns\n",
    ">* Apply chosen aggregations; syntax differs across tools\n",
    "\n",
    ">* Recognize recurring single-key groupby usage patterns\n",
    ">* Separate analytical intent from tool-specific syntax\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9291911",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Single Key Groupby Basics\n",
    "\n",
    "# Demonstrate single key groupby using pandas and polars side by side.\n",
    "# Show how to group by region and summarize sales amounts clearly.\n",
    "# Help beginners see conceptual similarity despite different library syntax.\n",
    "\n",
    "# !pip install pandas polars pyarrow.\n",
    "\n",
    "# Import required libraries for data handling and grouping.\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "# Create a small sales dataset with regions and revenue values.\n",
    "sales_data = {\n",
    "    \"region\": [\"North\", \"South\", \"North\", \"West\", \"South\", \"West\"],\n",
    "    \"revenue_dollars\": [1200, 800, 600, 1500, 700, 900],\n",
    "}\n",
    "\n",
    "# Build a pandas DataFrame from the sales dictionary.\n",
    "pdf = pd.DataFrame(sales_data)\n",
    "\n",
    "# Build a polars DataFrame from the same sales dictionary.\n",
    "pldf = pl.DataFrame(sales_data)\n",
    "\n",
    "# Perform single key groupby in pandas using region column.\n",
    "pandas_grouped = pdf.groupby(\"region\", as_index=False)[\"revenue_dollars\"].sum()\n",
    "\n",
    "# Perform single key groupby in polars using region column.\n",
    "polars_grouped = pldf.group_by(\"region\").agg(pl.col(\"revenue_dollars\").sum())\n",
    "\n",
    "# Print original pandas DataFrame to show raw rows.\n",
    "print(\"Original pandas sales data:\")\n",
    "print(pdf)\n",
    "\n",
    "# Print pandas groupby result showing total revenue per region.\n",
    "print(\"\\nPandas revenue by region:\")\n",
    "print(pandas_grouped)\n",
    "\n",
    "# Print polars groupby result showing total revenue per region.\n",
    "print(\"\\nPolars revenue by region:\")\n",
    "print(polars_grouped)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7574b9ed",
   "metadata": {},
   "source": [
    "### **1.2. Multi Column Groupby**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f04cdae",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Pandas to Polars Migration/Module_03/Lecture_A/image_01_02.jpg?v=1767314692\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Multi column groupby creates groups from key combinations\n",
    ">* Polars uses expressions evaluated within these groups\n",
    "\n",
    ">* Group by multiple columns, then define metrics\n",
    ">* Plan keys and metrics, write aggregation expressions\n",
    "\n",
    ">* Handle complex multi key groupby with many metrics\n",
    ">* Use one Polars groupby plus clear aggregations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb01e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Multi Column Groupby\n",
    "\n",
    "# Demonstrate multi column groupby in pandas and Polars side by side.\n",
    "# Show how to group by region and sales channel together.\n",
    "# Compare total and average sales metrics across both libraries.\n",
    "\n",
    "# !pip install polars pandas.\n",
    "\n",
    "# Import required libraries for data handling and analysis.\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "# Create a small pandas DataFrame with sales example data.\n",
    "data = {\n",
    "    \"region\": [\"North\", \"North\", \"South\", \"South\", \"West\", \"West\"],\n",
    "    \"channel\": [\"Online\", \"Store\", \"Online\", \"Store\", \"Online\", \"Store\"],\n",
    "    \"revenue_usd\": [1200, 800, 600, 400, 500, 700],\n",
    "    \"discount_percent\": [10, 5, 0, 15, 5, 10],\n",
    "}\n",
    "\n",
    "# Build the pandas DataFrame using the example dictionary.\n",
    "df_pd = pd.DataFrame(data)\n",
    "\n",
    "# Perform pandas groupby using two keys region and channel.\n",
    "result_pd = (\n",
    "    df_pd.groupby([\"region\", \"channel\"], as_index=False)\n",
    "    .agg({\"revenue_usd\": \"sum\", \"discount_percent\": \"mean\"})\n",
    ")\n",
    "\n",
    "# Convert the pandas DataFrame into a Polars DataFrame.\n",
    "df_pl = pl.from_pandas(df_pd)\n",
    "\n",
    "# Perform Polars groupby using two keys with expression aggregations.\n",
    "result_pl = (\n",
    "    df_pl.group_by([\"region\", \"channel\"])\n",
    "    .agg([\n",
    "        pl.col(\"revenue_usd\").sum().alias(\"revenue_usd_sum\"),\n",
    "        pl.col(\"discount_percent\").mean().alias(\"discount_percent_mean\"),\n",
    "    ])\n",
    ")\n",
    "\n",
    "# Print pandas multi column groupby aggregation result clearly.\n",
    "print(\"Pandas multi column groupby result:\\n\", result_pd)\n",
    "\n",
    "# Print Polars multi column groupby aggregation result clearly.\n",
    "print(\"\\nPolars multi column groupby result:\\n\", result_pl)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c739d991",
   "metadata": {},
   "source": [
    "### **1.3. Aggregation Basics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f220a4",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Pandas to Polars Migration/Module_03/Lecture_A/image_01_03.jpg?v=1767314716\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Grouping stays the same; aggregations change style\n",
    ">* Define clear expressions for each group metric\n",
    "\n",
    ">* Use explicit aggregation expressions instead of dictionaries\n",
    ">* Name metrics clearly and adjust them easily\n",
    "\n",
    ">* Explicitly define each metric and involved columns\n",
    ">* Clear expressions prevent mistakes and aid maintenance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29dff1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Aggregation Basics\n",
    "\n",
    "# Demonstrate basic groupby aggregations in pandas and equivalent expressions in polars.\n",
    "# Show multiple metrics per group with clear output column names.\n",
    "# Help beginners see explicit aggregation expressions replacing pandas aggregation dictionaries.\n",
    "\n",
    "# !pip install pandas polars.\n",
    "\n",
    "# Import required libraries for pandas and polars examples.\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "# Create simple sales data with store, transactions, and revenue columns.\n",
    "data = {\n",
    "    \"store\": [\"North\", \"North\", \"South\", \"South\", \"West\", \"West\"],\n",
    "    \"transactions\": [10, 15, 8, 12, 9, 11],\n",
    "    \"revenue_usd\": [200, 330, 160, 250, 180, 220],\n",
    "}\n",
    "\n",
    "# Build pandas DataFrame from the sales data dictionary.\n",
    "df_pd = pd.DataFrame(data)\n",
    "\n",
    "# Perform pandas groupby with dictionary based aggregations.\n",
    "summary_pd = df_pd.groupby(\"store\").agg({\n",
    "    \"transactions\": [\"sum\", \"mean\"],\n",
    "    \"revenue_usd\": [\"sum\", \"mean\"],\n",
    "})\n",
    "\n",
    "# Print pandas grouped summary to compare with polars results.\n",
    "print(\"Pandas groupby summary:\\n\", summary_pd)\n",
    "\n",
    "# Build polars DataFrame from the same sales data dictionary.\n",
    "df_pl = pl.DataFrame(data)\n",
    "\n",
    "# Perform polars groupby using explicit aggregation expressions list.\n",
    "summary_pl = (\n",
    "    df_pl\n",
    "    .group_by(\"store\")\n",
    "    .agg([\n",
    "        pl.col(\"transactions\").sum().alias(\"total_transactions\"),\n",
    "        pl.col(\"transactions\").mean().alias(\"avg_transactions\"),\n",
    "        pl.col(\"revenue_usd\").sum().alias(\"total_revenue_usd\"),\n",
    "        pl.col(\"revenue_usd\").mean().alias(\"avg_revenue_usd\"),\n",
    "    ])\n",
    ")\n",
    "\n",
    "# Print polars grouped summary showing clear metric names per store.\n",
    "print(\"\\nPolars groupby summary:\\n\", summary_pl)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c662410",
   "metadata": {},
   "source": [
    "## **2. Advanced Polars Aggregations**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9665fb",
   "metadata": {},
   "source": [
    "### **2.1. Custom Aggregation Expressions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67220a8b",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Pandas to Polars Migration/Module_03/Lecture_A/image_02_01.jpg?v=1767314743\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Define custom aggregations using declarative Polars expressions\n",
    ">* Chain filters, conditionals, arithmetic for optimized metrics\n",
    "\n",
    ">* Combine custom aggregations with window functions efficiently\n",
    ">* Express filters and rolling logic in one plan\n",
    "\n",
    ">* Compose vectorized pieces instead of custom functions\n",
    ">* Combine filters, weights, windows for domain metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b91c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Custom Aggregation Expressions\n",
    "\n",
    "# Demonstrate custom aggregation expressions using Polars groupby and window functions.\n",
    "# Show conditional filtering and arithmetic inside a single aggregation expression.\n",
    "# Compare simple average with custom conditional average per customer group.\n",
    "\n",
    "# !pip install polars pyarrow.\n",
    "\n",
    "# Import required Polars library for DataFrame operations.\n",
    "import polars as pl\n",
    "\n",
    "# Create a small example dataset with customers and purchases.\n",
    "data = {\n",
    "    \"customer_id\": [\"A\", \"A\", \"A\", \"B\", \"B\", \"C\"],\n",
    "    \"order_number\": [1, 2, 3, 1, 2, 1],\n",
    "    \"amount_usd\": [20.0, 55.0, 80.0, 15.0, 120.0, 40.0],\n",
    "}\n",
    "\n",
    "# Build a Polars DataFrame from the example dictionary.\n",
    "df = pl.DataFrame(data)\n",
    "\n",
    "# Define a threshold for high value purchases in US dollars.\n",
    "threshold = 50.0\n",
    "\n",
    "# Compute simple and custom aggregations per customer using expressions.\n",
    "agg_df = df.group_by(\"customer_id\").agg(\n",
    "    [\n",
    "        pl.col(\"amount_usd\").mean().alias(\"avg_amount\"),\n",
    "        pl.col(\"amount_usd\")\n",
    "        .filter(pl.col(\"amount_usd\") > threshold)\n",
    "        .mean()\n",
    "        .alias(\"avg_high_value\"),\n",
    "        (pl.col(\"amount_usd\").max() - pl.col(\"amount_usd\").min()).alias(\"range_amount\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Add a window expression showing rolling high value average per customer.\n",
    "window_df = df.with_columns(\n",
    "    [\n",
    "        pl.when(pl.col(\"amount_usd\") > threshold)\n",
    "        .then(pl.col(\"amount_usd\"))\n",
    "        .otherwise(None)\n",
    "        .rolling_mean(window_size=2)\n",
    "        .over(\"customer_id\")\n",
    "        .alias(\"rolling_high_avg\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Print the original data, aggregated results, and windowed custom metric.\n",
    "print(\"Original purchases DataFrame:\\n\", df)\n",
    "print(\"\\nCustom aggregations per customer:\\n\", agg_df)\n",
    "print(\"\\nWindowed custom rolling high value average:\\n\", window_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38a22a7",
   "metadata": {},
   "source": [
    "### **2.2. Multi Column Aggregations**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fe48e5",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Pandas to Polars Migration/Module_03/Lecture_A/image_02_02.jpg?v=1767314772\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Window functions add multi-column metrics per row\n",
    ">* Shared grouping keys enable compact, efficient complex logic\n",
    "\n",
    ">* Window functions capture relationships between columns\n",
    ">* Add many rolling metrics in one window\n",
    "\n",
    ">* Align individual metrics with group-level benchmarks\n",
    ">* Layer multi column windows into one coherent pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09352d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Multi Column Aggregations\n",
    "\n",
    "# Demonstrate multi column window aggregations using Polars expressions.\n",
    "# Show customer level metrics computed per transaction row context.\n",
    "# Compare multiple windowed aggregations sharing identical grouping keys.\n",
    "\n",
    "# !pip install polars pyarrow.\n",
    "\n",
    "# Import required libraries for dataframe creation and manipulation.\n",
    "import polars as pl\n",
    "\n",
    "# Create a small retail style dataset with multiple numeric columns.\n",
    "data = {\n",
    "    \"customer_id\": [\"A\", \"A\", \"A\", \"B\", \"B\", \"C\"],\n",
    "    \"transaction_id\": [1, 2, 3, 1, 2, 1],\n",
    "    \"amount_usd\": [40.0, 25.0, 35.0, 60.0, 20.0, 15.0],\n",
    "    \"discount_percent\": [5.0, 10.0, 0.0, 15.0, 5.0, 0.0],\n",
    "    \"category\": [\"Books\", \"Games\", \"Books\", \"Electronics\", \"Books\", \"Games\"],\n",
    "}\n",
    "\n",
    "# Build a Polars DataFrame from the dictionary data structure.\n",
    "df = pl.DataFrame(data)\n",
    "\n",
    "# Define a window expression grouped by customer identifier column.\n",
    "customer_window = pl.col(\"customer_id\").over(\"customer_id\")\n",
    "\n",
    "# Compute multi column aggregations within the same customer window.\n",
    "result = df.with_columns([\n",
    "    pl.col(\"amount_usd\").sum().over(\"customer_id\").alias(\"customer_total_spend\"),\n",
    "    pl.col(\"category\").n_unique().over(\"customer_id\").alias(\"customer_unique_categories\"),\n",
    "    pl.col(\"discount_percent\").mean().over(\"customer_id\").alias(\"customer_avg_discount\"),\n",
    "    (pl.col(\"amount_usd\") * (1 - pl.col(\"discount_percent\") / 100)).alias(\"net_amount_usd\"),\n",
    "])\n",
    "\n",
    "# Select and print a compact view showing original and derived metrics.\n",
    "print(result.select([\n",
    "    \"customer_id\",\n",
    "    \"transaction_id\",\n",
    "    \"amount_usd\",\n",
    "    \"discount_percent\",\n",
    "    \"category\",\n",
    "    \"customer_total_spend\",\n",
    "    \"customer_unique_categories\",\n",
    "    \"customer_avg_discount\",\n",
    "    \"net_amount_usd\",\n",
    "]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9629714b",
   "metadata": {},
   "source": [
    "### **2.3. Missing Data Strategies**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2feaa9",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Pandas to Polars Migration/Module_03/Lecture_A/image_02_03.jpg?v=1767314792\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Window functions must handle unavoidable data gaps\n",
    ">* Different gap treatments change meaning of aggregations\n",
    "\n",
    ">* Separate missing-data handling from aggregation logic\n",
    ">* Control window construction to match analytical intent\n",
    "\n",
    ">* Use windows to handle informative missing values\n",
    ">* Compare parallel metrics to detect gaps versus change\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b90c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Missing Data Strategies\n",
    "\n",
    "# Demonstrate Polars window functions handling missing values in rolling aggregations.\n",
    "# Compare filling missing values with zeros versus ignoring missing values in windows.\n",
    "# Show how missing data strategies change rolling average temperature interpretations.\n",
    "\n",
    "# !pip install polars pyarrow matplotlib.\n",
    "\n",
    "# Import required libraries for data handling and plotting.\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create simple temperature data with intentional missing values.\n",
    "data = pl.DataFrame({\"day\":[1,2,3,4,5,6,7],\"temp_f\":[70,None,75,80,None,85,90]})\n",
    "\n",
    "# Define a rolling window size for three day averages.\n",
    "window_size = 3\n",
    "\n",
    "# Compute rolling average treating missing values as zeros explicitly.\n",
    "filled_avg = data.with_columns([\n",
    "    pl.col(\"temp_f\").fill_null(0).rolling_mean(window_size).alias(\"avg_fill_zero\")\n",
    "])\n",
    "\n",
    "# Compute rolling average ignoring missing values within each window.\n",
    "ignore_avg = data.with_columns([\n",
    "    pl.col(\"temp_f\").rolling_mean(window_size).alias(\"avg_ignore_null\")\n",
    "])\n",
    "\n",
    "# Join both strategies into one comparison DataFrame.\n",
    "comparison = data.join(filled_avg.select([\"day\",\"avg_fill_zero\"]),on=\"day\").join(ignore_avg.select([\"day\",\"avg_ignore_null\"]),on=\"day\")\n",
    "\n",
    "# Print concise comparison to observe different missing data strategies.\n",
    "print(comparison)\n",
    "\n",
    "# Plot both rolling averages to visualize strategy differences clearly.\n",
    "plt.plot(comparison[\"day\"],comparison[\"avg_fill_zero\"],marker=\"o\",label=\"fill_missing_zero\")\n",
    "\n",
    "# Add second line for ignoring missing values in rolling averages.\n",
    "plt.plot(comparison[\"day\"],comparison[\"avg_ignore_null\"],marker=\"x\",label=\"ignore_missing\")\n",
    "\n",
    "# Label axes and add legend for clarity.\n",
    "plt.xlabel(\"Day index\")\n",
    "plt.ylabel(\"Temperature Fahrenheit\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34112da7",
   "metadata": {},
   "source": [
    "## **3. Validating window aggregations**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a08874",
   "metadata": {},
   "source": [
    "### **3.1. Group Based Window Setup**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37e896e",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Pandas to Polars Migration/Module_03/Lecture_A/image_03_01.jpg?v=1767314817\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Match groups, sort columns, and window ranges\n",
    ">* Keep filters and preprocessing identical between systems\n",
    "\n",
    ">* Plan how windows handle irregular group edges\n",
    ">* Create varied test groups and compare window outputs\n",
    "\n",
    ">* Ensure stable, deterministic sorting within each group\n",
    ">* Align grouping, ordering, and tie-breaking across systems\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799aacf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Group Based Window Setup\n",
    "\n",
    "# Demonstrate group based window setup using pandas and polars side by side.\n",
    "# Show grouping keys, ordering columns, and stable tie breaking within groups.\n",
    "# Compare cumulative sums to validate identical group window definitions.\n",
    "\n",
    "# !pip install pandas polars pyarrow quietly in Colab environment.\n",
    "\n",
    "# Import required libraries for pandas and polars usage.\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "# Create small sales dataset with store, date, and sales columns.\n",
    "data = {\n",
    "    \"store_id\": [\"A\", \"A\", \"A\", \"B\", \"B\", \"B\"],\n",
    "    \"date\": [\"2024-01-01\", \"2024-01-01\", \"2024-01-02\", \"2024-01-01\", \"2024-01-02\", \"2024-01-02\"],\n",
    "    \"event_id\": [1, 2, 3, 1, 2, 3],\n",
    "    \"sales_dollars\": [100, 50, 80, 60, 40, 30],\n",
    "}\n",
    "\n",
    "# Build pandas DataFrame and sort by grouping and ordering columns.\n",
    "df_pd = pd.DataFrame(data).sort_values([\"store_id\", \"date\", \"event_id\"])\n",
    "\n",
    "# Compute pandas cumulative sales within each store ordered by date and event.\n",
    "df_pd[\"cum_sales_store\"] = df_pd.groupby(\"store_id\")[\"sales_dollars\"].cumsum()\n",
    "\n",
    "# Convert pandas DataFrame into polars DataFrame for comparison.\n",
    "df_pl = pl.from_pandas(df_pd)\n",
    "\n",
    "# Define polars expression for cumulative sum over group based window.\n",
    "expr_cum = pl.col(\"sales_dollars\").cum_sum().over(\"store_id\")\n",
    "\n",
    "# Apply expression and select relevant columns for clear comparison.\n",
    "df_pl_result = df_pl.select([\n",
    "    pl.col(\"store_id\"),\n",
    "    pl.col(\"date\"),\n",
    "    pl.col(\"event_id\"),\n",
    "    pl.col(\"sales_dollars\"),\n",
    "    expr_cum.alias(\"cum_sales_store_polars\"),\n",
    "])\n",
    "\n",
    "# Merge pandas and polars results on keys to validate identical windows.\n",
    "merged = df_pd.merge(\n",
    "    df_pl_result.to_pandas(),\n",
    "    on=[\"store_id\", \"date\", \"event_id\", \"sales_dollars\"],\n",
    ")\n",
    "\n",
    "# Print merged comparison showing both cumulative columns side by side.\n",
    "print(merged[[\"store_id\", \"date\", \"event_id\", \"cum_sales_store\", \"cum_sales_store_polars\"]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb5b618",
   "metadata": {},
   "source": [
    "### **3.2. Cumulative Metrics Validation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b39f73",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Pandas to Polars Migration/Module_03/Lecture_A/image_03_02.jpg?v=1767314843\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Treat cumulative metrics as full time-based sequences\n",
    ">* Match every step; ordering and grouping must align\n",
    "\n",
    ">* Design small, focused test cases for cumulatives\n",
    ">* Compare rowwise across systems, stressing tricky edge cases\n",
    "\n",
    ">* Derived cumulative metrics magnify small counting mismatches\n",
    ">* Validate components and final rates under messy data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa19c027",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Cumulative Metrics Validation\n",
    "\n",
    "# Demonstrate validating cumulative metrics between pandas and Polars stepwise.\n",
    "# Show identical cumulative sales trajectories for each store and date ordering.\n",
    "# Highlight how mismatches appear when ordering or grouping definitions differ.\n",
    "\n",
    "# !pip install pandas polars matplotlib seaborn.\n",
    "\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "\n",
    "# Create simple transaction data for two example stores.\n",
    "data = {\n",
    "    \"store_id\": [\"A\", \"A\", \"A\", \"B\", \"B\", \"B\"],\n",
    "    \"date\": [\"2024-01-01\", \"2024-01-02\", \"2024-01-03\", \"2024-01-01\", \"2024-01-02\", \"2024-01-03\"],\n",
    "    \"sales_dollars\": [100, 50, 150, 80, 120, 200],\n",
    "}\n",
    "\n",
    "# Build pandas DataFrame with parsed dates and sorted rows.\n",
    "df_pd = pd.DataFrame(data)\n",
    "df_pd[\"date\"] = pd.to_datetime(df_pd[\"date\"])\n",
    "df_pd = df_pd.sort_values([\"store_id\", \"date\"])\n",
    "\n",
    "# Compute pandas cumulative sales per store ordered by date.\n",
    "df_pd[\"cum_sales_pd\"] = df_pd.groupby(\"store_id\")[\"sales_dollars\"].cumsum()\n",
    "\n",
    "# Build Polars DataFrame with parsed dates and sorted rows.\n",
    "df_pl = pl.DataFrame(data).with_columns(\n",
    "    pl.col(\"date\").str.strptime(pl.Date, strict=False).alias(\"date\")\n",
    ").sort([\"store_id\", \"date\"])\n",
    "\n",
    "# Compute Polars cumulative sales using groupby and cumulative sum.\n",
    "df_pl = df_pl.with_columns(\n",
    "    pl.col(\"sales_dollars\").cum_sum().over(\"store_id\").alias(\"cum_sales_pl\")\n",
    ")\n",
    "\n",
    "# Convert Polars result to pandas for aligned comparison.\n",
    "df_pl_pd = df_pl.to_pandas()\n",
    "\n",
    "# Merge pandas and Polars cumulative sequences for rowwise validation.\n",
    "merged = df_pd.merge(\n",
    "    df_pl_pd[[\"store_id\", \"date\", \"cum_sales_pl\"]],\n",
    "    on=[\"store_id\", \"date\"],\n",
    "    how=\"inner\",\n",
    ")\n",
    "\n",
    "# Add boolean flag showing whether cumulative values match exactly.\n",
    "merged[\"match_flag\"] = np.isclose(merged[\"cum_sales_pd\"], merged[\"cum_sales_pl\"])\n",
    "\n",
    "# Select and print key columns to inspect cumulative trajectories.\n",
    "print(merged[[\"store_id\", \"date\", \"sales_dollars\", \"cum_sales_pd\", \"cum_sales_pl\", \"match_flag\"]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718f2222",
   "metadata": {},
   "source": [
    "### **3.3. Replacing pandas rolling patterns**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b935911",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Pandas to Polars Migration/Module_03/Lecture_A/image_03_03.jpg?v=1767314867\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Carefully replicate original rolling window definitions\n",
    ">* Validate new windows across groups and edge cases\n",
    "\n",
    ">* Compare pandas and Polars rolling results rowwise\n",
    ">* Use edge-case users to reveal subtle mismatches\n",
    "\n",
    ">* Scale validation to full data and complexity\n",
    ">* Use stats and deep dives to confirm equivalence\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d623f825",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Groupby and Aggregation**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd5fc50",
   "metadata": {},
   "source": [
    "\n",
    "In this lecture, you learned to:\n",
    "- Translate typical pandas groupby and aggregation code into equivalent Polars groupby expressions. \n",
    "- Use Polars window functions to replace more complex pandas groupby and rolling patterns where appropriate. \n",
    "- Validate that migrated Polars aggregations match pandas results across multiple groups and metrics. \n",
    "\n",
    "<font color='yellow'>Congratulations on completing this course!</font>"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
