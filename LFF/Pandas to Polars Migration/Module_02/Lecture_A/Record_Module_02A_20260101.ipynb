{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74d7f86f",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**DataFrames and Schema**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ac8ea6",
   "metadata": {},
   "source": [
    ">Last update: 20260101.\n",
    "    \n",
    "By the end of this Lecture, you will be able to:\n",
    "- Create Polars DataFrames and LazyFrames from in-memory data and external files that are commonly used with pandas. \n",
    "- Inspect and adjust Polars schemas to ensure column names and data types match expectations from existing pandas workflows. \n",
    "- Compare pandas and Polars data loading patterns to identify where direct one-to-one translations are possible. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc2481a",
   "metadata": {},
   "source": [
    "## **1. Building Polars DataFrames**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81a0dcb",
   "metadata": {},
   "source": [
    "### **1.1. Dictionaries and Lists**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a436760f",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Pandas to Polars Migration/Module_02/Lecture_A/image_01_01.jpg?v=1767311272\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Use dictionaries and lists to build DataFrames\n",
    ">* Polars turns them into columns for quick analysis\n",
    "\n",
    ">* Lists represent row-wise records like customers\n",
    ">* Polars turns nested lists and dicts into DataFrames\n",
    "\n",
    ">* Prototype with small in-memory data in Polars\n",
    ">* Reuse same patterns when scaling to larger datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796e6d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Dictionaries and Lists\n",
    "\n",
    "# Demonstrate creating Polars DataFrames from dictionaries and lists.\n",
    "# Show how column names map from dictionary keys and record fields.\n",
    "# Print small DataFrames to compare structures and understand behavior.\n",
    "\n",
    "# !pip install polars --quiet.\n",
    "\n",
    "# Import polars library for DataFrame creation.\n",
    "import polars as pl\n",
    "\n",
    "# Create a dictionary where keys represent column names.\n",
    "sales_dict = {\"product\": [\"Book\", \"Pen\"], \"quantity\": [3, 10]}\n",
    "\n",
    "# Build a DataFrame directly from the dictionary structure.\n",
    "df_from_dict = pl.DataFrame(sales_dict)\n",
    "\n",
    "# Print the DataFrame created from the dictionary.\n",
    "print(\"DataFrame from dictionary:\\n\", df_from_dict)\n",
    "\n",
    "# Create a list of dictionaries representing individual sales records.\n",
    "sales_list_dicts = [{\"product\": \"Book\", \"quantity\": 3}, {\"product\": \"Pen\", \"quantity\": 10}]\n",
    "\n",
    "# Build a DataFrame from the list of dictionaries records.\n",
    "df_from_list_dicts = pl.DataFrame(sales_list_dicts)\n",
    "\n",
    "# Print the DataFrame created from the list of dictionaries.\n",
    "print(\"\\nDataFrame from list of dictionaries:\\n\", df_from_list_dicts)\n",
    "\n",
    "# Create a list of lists representing rows with consistent ordering.\n",
    "sales_list_rows = [[\"Book\", 3], [\"Pen\", 10]]\n",
    "\n",
    "# Build a DataFrame from list rows with explicit column names.\n",
    "df_from_list_rows = pl.DataFrame(sales_list_rows, schema=[\"product\", \"quantity\"], orient=\"row\")\n",
    "\n",
    "# Print the DataFrame created from the list of lists rows.\n",
    "print(\"\\nDataFrame from list of lists:\\n\", df_from_list_rows)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01239c66",
   "metadata": {},
   "source": [
    "### **1.2. Loading CSV and Parquet**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36968dd5",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Pandas to Polars Migration/Module_02/Lecture_A/image_01_02.jpg?v=1767311307\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Polars loads CSV and Parquet efficiently\n",
    ">* Creates accurate DataFrames ready for further analysis\n",
    "\n",
    ">* Control headers, missing values, and type inference\n",
    ">* Avoid mis-typed columns that break downstream work\n",
    "\n",
    ">* Parquet stores typed, columnar data Polars optimizes\n",
    ">* Enables fast, selective reads across large datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d966cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Loading CSV and Parquet\n",
    "\n",
    "# Demonstrate loading CSV and Parquet files with Polars DataFrames.\n",
    "# Show basic options for headers and data type inference with CSV files.\n",
    "# Compare loading the same data from CSV and Parquet formats efficiently.\n",
    "\n",
    "# !pip install polars pyarrow.\n",
    "\n",
    "# Import required libraries for data handling and file formats.\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Create a small pandas DataFrame representing simple sales records.\n",
    "data_dict = {\"order_id\": [\"0001\", \"0002\"], \"amount_usd\": [19.99, 5.50]}\n",
    "pdf = pd.DataFrame(data_dict)\n",
    "\n",
    "# Save the pandas DataFrame as a CSV file for Polars loading.\n",
    "csv_path = \"sales_small.csv\"\n",
    "pdf.to_csv(csv_path, index=False)\n",
    "\n",
    "# Save the same pandas DataFrame as a Parquet file for comparison.\n",
    "parquet_path = \"sales_small.parquet\"\n",
    "table = pa.Table.from_pandas(pdf)\n",
    "pq.write_table(table, parquet_path)\n",
    "\n",
    "# Load the CSV file into a Polars DataFrame with header handling.\n",
    "df_csv = pl.read_csv(csv_path, has_header=True)\n",
    "print(\"CSV DataFrame loaded with Polars:\")\n",
    "print(df_csv)\n",
    "\n",
    "# Load the Parquet file into a Polars DataFrame efficiently.\n",
    "df_parquet = pl.read_parquet(parquet_path)\n",
    "print(\"Parquet DataFrame loaded with Polars:\")\n",
    "print(df_parquet)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32416ee6",
   "metadata": {},
   "source": [
    "### **1.3. Migrating from pandas**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06834e58",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Pandas to Polars Migration/Module_02/Lecture_A/image_01_03.jpg?v=1767311336\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Reuse existing workflows when moving to Polars\n",
    ">* Match outputs to build confidence before optimizations\n",
    "\n",
    ">* Replace initial tabular steps with Polars creation\n",
    ">* Validate schema and sample values before migrating\n",
    "\n",
    ">* Use lazy queries instead of eager loading\n",
    ">* Gain speed, scalability, and easier workflow maintenance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b829e6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Migrating from pandas\n",
    "\n",
    "# Show simple migration from pandas DataFrame into Polars DataFrame and LazyFrame.\n",
    "# Compare basic properties to confirm both DataFrames contain matching rows and columns.\n",
    "# Demonstrate starting with pandas habits while gradually adopting Polars loading patterns.\n",
    "\n",
    "# pip install polars and pandas if running outside Colab environment.\n",
    "# !pip install polars pandas --quiet.\n",
    "\n",
    "# Import pandas and polars libraries for DataFrame creation and comparison.\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "# Create small pandas DataFrame that mimics daily sales report structure.\n",
    "pd_df = pd.DataFrame({\"day\":[\"Mon\",\"Tue\",\"Wed\"],\"region\":[\"East\",\"West\",\"East\"],\"sales_dollars\":[120.0,150.5,99.0]})\n",
    "\n",
    "# Convert pandas DataFrame into Polars DataFrame using from_pandas constructor.\n",
    "pl_from_pd = pl.from_pandas(pd_df)\n",
    "\n",
    "# Load same data directly with Polars to mirror existing pandas file workflow.\n",
    "pl_direct = pl.DataFrame({\"day\":[\"Mon\",\"Tue\",\"Wed\"],\"region\":[\"East\",\"West\",\"East\"],\"sales_dollars\":[120.0,150.5,99.0]})\n",
    "\n",
    "# Create Polars LazyFrame to prepare for larger future datasets and lazy execution.\n",
    "lazy_sales = pl_direct.lazy()\n",
    "\n",
    "# Print basic shape comparison to confirm matching row and column counts.\n",
    "print(\"pandas shape:\", pd_df.shape, \"polars shape:\", pl_from_pd.shape)\n",
    "\n",
    "# Print column names from both libraries to verify consistent schema alignment.\n",
    "print(\"pandas columns:\", list(pd_df.columns))\n",
    "print(\"polars columns:\", pl_from_pd.columns)\n",
    "\n",
    "# Collect lazy result and show first rows to mirror familiar head style checks.\n",
    "print(\"lazy head:\")\n",
    "print(lazy_sales.head(2).collect())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e165fd5e",
   "metadata": {},
   "source": [
    "## **2. LazyFrame Schema Essentials**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2589128d",
   "metadata": {},
   "source": [
    "### **2.1. Building LazyFrames From Scans**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d915bba3",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Pandas to Polars Migration/Module_02/Lecture_A/image_02_01.jpg?v=1767311403\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Use scans to build lazy data blueprints\n",
    ">* Infer, review, and fix schemas before analysis\n",
    "\n",
    ">* Schema comes from metadata and sampled rows\n",
    ">* Immediately inspect and fix column names, types\n",
    "\n",
    ">* Review and adjust scanned schemas before transformations\n",
    ">* Rename columns, fix types, ensure workflow alignment\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305466d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Building LazyFrames From Scans\n",
    "\n",
    "# Demonstrate building LazyFrames from scan operations with schema inspection.\n",
    "# Show how Polars infers column names and data types from external files.\n",
    "# Adjust schema to match expectations from previous pandas style workflows.\n",
    "\n",
    "# !pip install polars pyarrow fsspec.\n",
    "\n",
    "# Import required libraries for data handling and filesystem operations.\n",
    "import polars as pl\n",
    "import os\n",
    "\n",
    "# Create a small directory for our example CSV files.\n",
    "os.makedirs(\"sales_data\", exist_ok=True)\n",
    "\n",
    "# Define CSV content representing daily sales with mixed data types.\n",
    "csv_content = \"transaction_id,date,amount_usd\\n1,2024-01-01,10.5\\n2,2024-01-02,20.0\\n3,2024-01-03,15.75\\n\"\n",
    "\n",
    "# Write the CSV content into a file inside the directory.\n",
    "with open(\"sales_data/day1.csv\", \"w\", encoding=\"utf-8\") as file_handle:\n",
    "    file_handle.write(csv_content)\n",
    "\n",
    "# Build a LazyFrame by scanning the CSV directory lazily.\n",
    "lazy_sales = pl.scan_csv(\"sales_data/*.csv\")\n",
    "\n",
    "# Print the inferred schema to inspect column names and data types.\n",
    "print(\"Inferred schema from scan:\")\n",
    "print(lazy_sales.schema)\n",
    "\n",
    "# Rename columns to match expected pandas style naming conventions.\n",
    "lazy_renamed = lazy_sales.rename({\"amount_usd\": \"revenue_usd\"})\n",
    "\n",
    "# Cast transaction_id to string to match identifier expectations.\n",
    "lazy_casted = lazy_renamed.with_columns(pl.col(\"transaction_id\").cast(pl.Utf8))\n",
    "\n",
    "# Print the adjusted schema after renaming and casting operations.\n",
    "print(\"\\nAdjusted schema after alignment:\")\n",
    "print(lazy_casted.schema)\n",
    "\n",
    "# Collect a small sample to confirm that lazy operations worked correctly.\n",
    "print(\"\\nSample rows after lazy alignment:\")\n",
    "print(lazy_casted.limit(3).collect())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3763c79d",
   "metadata": {},
   "source": [
    "### **2.2. Deferred Execution Basics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d83a011",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Pandas to Polars Migration/Module_02/Lecture_A/image_02_02.jpg?v=1767311420\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Lazy queries build a plan, not data\n",
    ">* Schema preview shows future column names and types\n",
    "\n",
    ">* Design-time schema checks speed up migration tweaks\n",
    ">* Early validation prevents costly, real-world data errors\n",
    "\n",
    ">* Deferred execution lets Polars globally optimize transformations\n",
    ">* Optimizations keep final schema consistent with pandas\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9def70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Deferred Execution Basics\n",
    "\n",
    "# Demonstrate lazy deferred execution with Polars LazyFrame schemas.\n",
    "# Show schema prediction before any actual data loading happens.\n",
    "# Compare lazy schema inspection with final executed DataFrame result.\n",
    "\n",
    "# !pip install polars pyarrow.\n",
    "\n",
    "# Import required Polars library for lazy DataFrame operations.\n",
    "import polars as pl\n",
    "\n",
    "# Create a small in memory dataset representing simple web log rows.\n",
    "data_rows = [\n",
    "    {\"user_id\": \"u1\", \"clicks\": \"3\", \"miles\": \"12.5\"},\n",
    "    {\"user_id\": \"u2\", \"clicks\": \"5\", \"miles\": \"7.0\"},\n",
    "]\n",
    "\n",
    "# Build a LazyFrame from the in memory dataset using scan like behavior.\n",
    "lf = pl.LazyFrame(data_rows)\n",
    "\n",
    "# Define a lazy transformation pipeline with renaming and type casting operations.\n",
    "lf_transformed = (\n",
    "    lf.rename({\"miles\": \"distance_miles\"})\n",
    "      .with_columns(pl.col(\"clicks\").cast(pl.Int64))\n",
    "      .with_columns(pl.col(\"distance_miles\").cast(pl.Float64))\n",
    ")\n",
    "\n",
    "# Inspect the lazy schema prediction before any data execution happens.\n",
    "print(\"Lazy schema prediction before execution:\")\n",
    "print(lf_transformed.schema)\n",
    "\n",
    "# Execute the lazy pipeline to materialize an actual DataFrame result.\n",
    "df_result = lf_transformed.collect()\n",
    "\n",
    "# Show the final DataFrame and its concrete schema after execution.\n",
    "print(\"\\nExecuted DataFrame and its schema:\")\n",
    "print(df_result)\n",
    "print(df_result.schema)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facdbe41",
   "metadata": {},
   "source": [
    "### **2.3. DataFrame LazyFrame Conversion**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6494953",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Pandas to Polars Migration/Module_02/Lecture_A/image_02_03.jpg?v=1767311438\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Convert DataFrame to LazyFrame to preserve schema\n",
    ">* LazyFrame keeps original column names and types\n",
    "\n",
    ">* Use LazyFrame as a workspace for schema cleanup\n",
    ">* Rename columns, fix types, and defer execution\n",
    "\n",
    ">* Convert DataFrames to LazyFrames for schema cleanup\n",
    ">* Standardize types and names in one execution\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eeeae93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - DataFrame LazyFrame Conversion\n",
    "\n",
    "# Demonstrate converting Polars DataFrame into LazyFrame safely.\n",
    "# Show that schema stays consistent during lazy transformations.\n",
    "# Compare DataFrame and LazyFrame schemas before and after changes.\n",
    "\n",
    "# pip install polars.\n",
    "\n",
    "# Import polars library for DataFrame and LazyFrame usage.\n",
    "import polars as pl\n",
    "\n",
    "# Create example data using a simple Python dictionary.\n",
    "data = {\n",
    "    \"customer_id\": [101, 102, 103],\n",
    "    \"signup_date\": [\"2024-01-01\", \"2024-01-05\", \"2024-01-10\"],\n",
    "    \"revenue_usd\": [\"$10.50\", \"$20.00\", \"$7.25\"],\n",
    "}\n",
    "\n",
    "# Build a Polars DataFrame from the in memory dictionary.\n",
    "df = pl.DataFrame(data)\n",
    "\n",
    "# Print original DataFrame schema to inspect column types.\n",
    "print(\"Original DataFrame schema:\")\n",
    "print(df.schema)\n",
    "\n",
    "# Convert the existing DataFrame into a LazyFrame object.\n",
    "lf = df.lazy()\n",
    "\n",
    "# Print LazyFrame schema which should match DataFrame schema.\n",
    "print(\"\\nLazyFrame schema after conversion:\")\n",
    "print(lf.schema)\n",
    "\n",
    "# Plan lazy schema refinements without executing immediately.\n",
    "lf_fixed = (\n",
    "    lf.with_columns([\n",
    "        pl.col(\"revenue_usd\").str.replace_all(\"$\", \"\").cast(pl.Float64, strict=False),\n",
    "        pl.col(\"signup_date\").str.strptime(pl.Date, strict=False),\n",
    "    ])\n",
    "    .rename({\"revenue_usd\": \"revenue_dollars\"})\n",
    ")\n",
    "\n",
    "# Print refined LazyFrame schema showing updated column types and names.\n",
    "print(\"\\nLazyFrame schema after lazy refinements:\")\n",
    "print(lf_fixed.schema)\n",
    "\n",
    "# Collect final DataFrame to execute lazy plan and view results.\n",
    "result_df = lf_fixed.collect()\n",
    "\n",
    "# Print final DataFrame to confirm schema and values after execution.\n",
    "print(\"\\nFinal DataFrame after lazy execution:\")\n",
    "print(result_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122c9652",
   "metadata": {},
   "source": [
    "## **3. Schema alignment basics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e4bb60",
   "metadata": {},
   "source": [
    "### **3.1. Column Types Overview**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da18731",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Pandas to Polars Migration/Module_02/Lecture_A/image_03_01.jpg?v=1767311465\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Pandas and Polars describe column types differently\n",
    ">* Type differences affect one‑to‑one code translation\n",
    "\n",
    ">* Group columns into numeric, text, temporal, boolean, categorical\n",
    ">* Compare these families to spot semantic differences\n",
    "\n",
    ">* Mismatched column types can break existing workflows\n",
    ">* Understand each library’s types to safely translate\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78a7a7d",
   "metadata": {},
   "source": [
    "### **3.2. Column casting and renaming**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e507d9",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Pandas to Polars Migration/Module_02/Lecture_A/image_03_02.jpg?v=1767311475\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Use casting and renaming to match expectations\n",
    ">* Align schemas to avoid pipeline bugs\n",
    "\n",
    ">* Casting fixes wrong column data types\n",
    ">* Prevents lost leading zeros and broken date logic\n",
    "\n",
    ">* Rename columns to match shared naming contracts\n",
    ">* Keeps pipelines unchanged while swapping dataframe libraries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edd8bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Column casting and renaming\n",
    "\n",
    "# Demonstrate simple column casting between pandas and Polars dataframes.\n",
    "# Show how identifier columns can be safely converted to string types.\n",
    "# Show how columns can be renamed to match existing pipeline expectations.\n",
    "# !pip install polars pandas.\n",
    "\n",
    "# Import required libraries for pandas and Polars usage.\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "# Create a small pandas DataFrame with numeric looking identifiers.\n",
    "pd_df = pd.DataFrame({\"customer_id\": [101, 102, 103], \"net_revenue\": [25.5, 40.0, 32.5]})\n",
    "\n",
    "# Display original pandas DataFrame and its dtypes for comparison.\n",
    "print(\"Original pandas DataFrame and dtypes:\")\n",
    "print(pd_df)\n",
    "print(pd_df.dtypes)\n",
    "\n",
    "# Convert pandas DataFrame into a Polars DataFrame for migration demonstration.\n",
    "pl_df = pl.from_pandas(pd_df)\n",
    "\n",
    "# Show Polars schema before any casting or renaming operations.\n",
    "print(\"\\nOriginal Polars schema and data:\")\n",
    "print(pl_df)\n",
    "print(pl_df.schema)\n",
    "\n",
    "# Cast customer_id column to Utf8 string type to preserve identifier semantics.\n",
    "pl_casted = pl_df.with_columns(pl.col(\"customer_id\").cast(pl.Utf8).alias(\"customer_id\"))\n",
    "\n",
    "# Rename columns to match a different expected schema from existing pipelines.\n",
    "pl_renamed = pl_casted.rename({\"customer_id\": \"customerId\", \"net_revenue\": \"netRevenue\"})\n",
    "\n",
    "# Show final Polars schema and data after casting and renaming operations.\n",
    "print(\"\\nPolars after casting and renaming:\")\n",
    "print(pl_renamed)\n",
    "print(pl_renamed.schema)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9e411f",
   "metadata": {},
   "source": [
    "### **3.3. Aligning schemas with pandas expectations**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ce3993",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Pandas to Polars Migration/Module_02/Lecture_A/image_03_03.jpg?v=1767311494\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Check schemas match what analyses expect downstream\n",
    ">* Adjust column names and types to avoid errors\n",
    "\n",
    ">* Start from existing workflows and their expectations\n",
    ">* Match column names and types to prevent mismatches\n",
    "\n",
    ">* Schema alignment supports collaboration, regulation, and documentation\n",
    ">* It ensures reproducibility, validation, and cross-tool consistency\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a66a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Aligning schemas with pandas expectations\n",
    "\n",
    "# Demonstrate aligning Polars schema with existing pandas expectations.\n",
    "# Show mismatched types and names between two similar DataFrames.\n",
    "# Fix Polars schema so it matches the trusted pandas based workflow.\n",
    "\n",
    "# !pip install polars pandas.\n",
    "\n",
    "# Import required libraries for pandas and Polars usage.\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "# Create a pandas DataFrame representing trusted workflow expectations.\n",
    "pd_df = pd.DataFrame({\"campaign_id\": [\"A101\", \"B202\"], \"revenue_cents\": [1500, 2300]})\n",
    "\n",
    "# Create a Polars DataFrame with slightly different schema assumptions.\n",
    "pl_df = pl.DataFrame({\"campaignId\": [101, 202], \"revenue_dollars\": [15.0, 23.0]})\n",
    "\n",
    "# Show original pandas schema and first rows for comparison clarity.\n",
    "print(\"PANDAS SCHEMA AND DATA:\")\n",
    "print(pd_df.dtypes)\n",
    "print(pd_df.head())\n",
    "\n",
    "# Show original Polars schema and first rows before any alignment.\n",
    "print(\"\\nPOLARS ORIGINAL SCHEMA AND DATA:\")\n",
    "print(pl_df.schema)\n",
    "print(pl_df.head())\n",
    "\n",
    "# Align Polars column names and types with pandas expectations.\n",
    "pl_aligned = pl_df.rename({\"campaignId\": \"campaign_id\", \"revenue_dollars\": \"revenue_cents\"}).with_columns(\n",
    "    pl.col(\"campaign_id\").cast(pl.Utf8),\n",
    "    (pl.col(\"revenue_cents\") * 100).cast(pl.Int64),\n",
    ")\n",
    "\n",
    "# Show aligned Polars schema and first rows after transformations.\n",
    "print(\"\\nPOLARS ALIGNED SCHEMA AND DATA:\")\n",
    "print(pl_aligned.schema)\n",
    "print(pl_aligned.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efefe1bb",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**DataFrames and Schema**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc546f1",
   "metadata": {},
   "source": [
    "\n",
    "In this lecture, you learned to:\n",
    "- Create Polars DataFrames and LazyFrames from in-memory data and external files that are commonly used with pandas. \n",
    "- Inspect and adjust Polars schemas to ensure column names and data types match expectations from existing pandas workflows. \n",
    "- Compare pandas and Polars data loading patterns to identify where direct one-to-one translations are possible. \n",
    "\n",
    "In the next Lecture (Lecture B), we will go over 'Selecting and Filtering'"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
