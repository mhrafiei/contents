{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a19637af",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Selecting and Filtering**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b59578",
   "metadata": {},
   "source": [
    ">Last update: 20251227.\n",
    "    \n",
    "By the end of this Lecture, you will be able to:\n",
    "- Select and rename columns in Polars using expressions that parallel common Pandas patterns. \n",
    "- Filter rows in Polars based on boolean conditions and combined predicates. \n",
    "- Apply simple column expressions such as arithmetic, string operations, and conditional logic in Polars. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9898349",
   "metadata": {},
   "source": [
    "## **1. Selecting Polars Columns**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b81c1b2",
   "metadata": {},
   "source": [
    "### **1.1. Column Name Patterns**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b49b2c",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Pandas to Polars Migration/Module_02/Lecture_B/image_01_01.jpg?v=1766893954\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Use name patterns instead of listing columns\n",
    ">* Select column groups by shared prefixes, suffixes, keywords\n",
    "\n",
    ">* Pattern-based selection adapts as schemas change\n",
    ">* Automatically includes new related columns, reducing errors\n",
    "\n",
    ">* Patterned names promote clear, consistent data schemas\n",
    ">* They simplify selecting, documenting, and reusing related columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014c4d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Column Name Patterns\n",
    "\n",
    "# Demonstrate selecting Polars columns using simple name patterns.\n",
    "# Show prefix, suffix, and substring based column selections clearly.\n",
    "# Compare pattern selections with manual column listing briefly.\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "# Create a small example DataFrame with patterned column names.\n",
    "data = {\n",
    "    \"demo_age_years\": [25, 40, 32],\n",
    "    \"demo_income_usd\": [50000, 82000, 61000],\n",
    "    \"temp_F_morning\": [68.0, 70.5, 69.2],\n",
    "    \"temp_F_evening\": [72.3, 73.1, 71.8],\n",
    "}\n",
    "\n",
    "# Build the DataFrame from the dictionary using Polars constructor.\n",
    "df = pl.DataFrame(data)\n",
    "\n",
    "# Show the full DataFrame so patterns in column names are visible.\n",
    "print(\"Full DataFrame with patterned column names:\")\n",
    "print(df)\n",
    "\n",
    "# Select all columns starting with prefix \"demo_\" using Polars selector.\n",
    "demo_cols = df.select(pl.col(\"demo_*\"))\n",
    "\n",
    "# Select all columns ending with suffix \"_evening\" using Polars selector.\n",
    "evening_cols = df.select(pl.col(\"*_evening\"))\n",
    "\n",
    "# Select all columns containing substring \"temp\" anywhere in their names.\n",
    "temp_cols = df.select(pl.col(\"*temp*\"))\n",
    "\n",
    "# Print the three pattern based selections to compare results clearly.\n",
    "print(\"\\nColumns starting with 'demo_':\")\n",
    "print(demo_cols)\n",
    "\n",
    "print(\"\\nColumns ending with '_evening':\")\n",
    "print(evening_cols)\n",
    "\n",
    "print(\"\\nColumns containing 'temp' substring:\")\n",
    "print(temp_cols)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be725c1a",
   "metadata": {},
   "source": [
    "### **1.2. Managing Columns**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db16470a",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Pandas to Polars Migration/Module_02/Lecture_B/image_01_02.jpg?v=1766893969\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Choose, drop, and reorder columns intentionally\n",
    ">* Use expressions to combine selection, exclusion, reordering\n",
    "\n",
    ">* Combine keep and drop in one selection\n",
    ">* Reorder key columns first for easier inspection\n",
    "\n",
    ">* Rename columns during selection for cleaner schemas\n",
    ">* Standardized names simplify multi-source data analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5355cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Managing Columns\n",
    "\n",
    "# Demonstrate managing Polars columns with selection and exclusion.\n",
    "# Show keeping important columns and dropping temporary helper columns.\n",
    "# Also show renaming and reordering columns in a single expression.\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "# Create a simple transactions DataFrame with extra temporary columns.\n",
    "transactions = pl.DataFrame({\n",
    "    \"transaction_id\": [1, 2, 3],\n",
    "    \"customer_id\": [101, 102, 103],\n",
    "    \"amount_usd\": [25.0, 40.5, 12.0],\n",
    "    \"temp_check\": [0.1, 0.2, 0.3],\n",
    "    \"debug_flag\": [True, False, True],\n",
    "})\n",
    "\n",
    "# Show the original DataFrame with all columns included.\n",
    "print(\"Original transactions DataFrame with all columns:\")\n",
    "print(transactions)\n",
    "\n",
    "# Select important business columns and drop temporary helper columns.\n",
    "cleaned = transactions.select([\n",
    "    pl.col(\"transaction_id\"),\n",
    "    pl.col(\"customer_id\"),\n",
    "    pl.col(\"amount_usd\"),\n",
    "])\n",
    "\n",
    "# Show the cleaned DataFrame after dropping temporary helper columns.\n",
    "print(\"\\nCleaned DataFrame without temporary helper columns:\")\n",
    "print(cleaned)\n",
    "\n",
    "# Reorder columns and rename amount_usd to total_amount_usd for clarity.\n",
    "reordered = transactions.select([\n",
    "    pl.col(\"customer_id\").alias(\"customer_id\"),\n",
    "    pl.col(\"transaction_id\").alias(\"transaction_id\"),\n",
    "    pl.col(\"amount_usd\").alias(\"total_amount_usd\"),\n",
    "])\n",
    "\n",
    "# Show the reordered and renamed columns in the final DataFrame.\n",
    "print(\"\\nReordered and renamed columns in final DataFrame:\")\n",
    "print(reordered)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247e26b7",
   "metadata": {},
   "source": [
    "### **1.3. Expression Based Selection**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cbfc3e",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Pandas to Polars Migration/Module_02/Lecture_B/image_01_03.jpg?v=1766893983\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Columns become dynamic transformations, not fixed labels\n",
    ">* One expression defines, transforms, and renames output columns\n",
    "\n",
    ">* Combine select, transform, and rename in one step\n",
    ">* Describe final columns; Polars optimizes execution automatically\n",
    "\n",
    ">* Think declaratively; Polars handles execution details\n",
    ">* Define, transform, and label columns in one expression\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa332c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Expression Based Selection\n",
    "\n",
    "# Demonstrate Polars expression based column selection and transformation.\n",
    "# Show selecting, creating, and renaming columns in one expression.\n",
    "# Compare original DataFrame with transformed selection result clearly.\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "# Create a simple retail transactions DataFrame with three columns.\n",
    "df = pl.DataFrame({\"customer_id\": [1, 2, 3], \"items\": [2, 1, 4], \"price_usd\": [5.0, 10.0, 3.5]})\n",
    "\n",
    "# Show the original DataFrame for clear comparison later.\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "# Use expression based selection to define the final schema.\n",
    "result = df.select([\n",
    "    pl.col(\"customer_id\").alias(\"customer\"),\n",
    "    (pl.col(\"items\") * pl.col(\"price_usd\")).alias(\"total_revenue_usd\"),\n",
    "    (pl.col(\"price_usd\") * 1.1).round(2).alias(\"price_usd_with_tax\"),\n",
    "])\n",
    "\n",
    "# Print the transformed DataFrame showing selected and computed columns.\n",
    "print(\"\\nTransformed selection with expressions:\")\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231a2887",
   "metadata": {},
   "source": [
    "## **2. Filtering Rows in Polars**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93404950",
   "metadata": {},
   "source": [
    "### **2.1. Boolean Masks Basics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521a9154",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Pandas to Polars Migration/Module_02/Lecture_B/image_02_01.jpg?v=1766893998\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Filtering uses a boolean mask per row\n",
    ">* True rows are kept, false rows removed\n",
    "\n",
    ">* Boolean masks are first-class Polars expression results\n",
    ">* Each mask value aligns with a specific row\n",
    "\n",
    ">* Boolean masks are reusable, composable filter expressions\n",
    ">* They map human conditions to concrete filtered rows\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082b23d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Boolean Masks Basics\n",
    "\n",
    "# Demonstrate basic boolean masks with simple Polars DataFrame filtering.\n",
    "# Show how conditions create true or false values per DataFrame row.\n",
    "# Apply boolean masks to keep only rows that satisfy chosen conditions.\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "# Create a small DataFrame representing simple customer orders in dollars.\n",
    "orders = pl.DataFrame({\"customer\": [\"Ann\", \"Bob\", \"Cara\", \"Dan\"], \"total_dollars\": [15, 45, 8, 60]})\n",
    "\n",
    "# Build a boolean mask expression for orders above a chosen dollar threshold.\n",
    "mask_expr = pl.col(\"total_dollars\") > 20\n",
    "\n",
    "# Show the DataFrame and the evaluated boolean mask side by side.\n",
    "print(\"Original orders DataFrame and boolean mask values:\")\n",
    "print(orders.with_columns(mask_expr.alias(\"above_20\")))\n",
    "\n",
    "# Use the same boolean mask expression to filter and keep only expensive orders.\n",
    "filtered_orders = orders.filter(mask_expr)\n",
    "\n",
    "# Display the filtered DataFrame that only includes rows where mask was true.\n",
    "print(\"\\nFiltered orders where total_dollars is greater than twenty:\")\n",
    "print(filtered_orders)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db2d0a6",
   "metadata": {},
   "source": [
    "### **2.2. Logical Conditions in Polars**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964ef227",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Pandas to Polars Migration/Module_02/Lecture_B/image_02_02.jpg?v=1766894013\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Combine conditions with and, or, and not\n",
    ">* Use logical building blocks for precise filters\n",
    "\n",
    ">* Each comparison creates a boolean column per row\n",
    ">* Combine booleans with AND or OR for filtering\n",
    "\n",
    ">* Use NOT to exclude unwanted rows precisely\n",
    ">* Combine AND, OR, NOT and verify logic carefully\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11bca26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Logical Conditions in Polars\n",
    "\n",
    "# Demonstrate logical conditions with Polars filters.\n",
    "# Show and, or, and not combinations clearly.\n",
    "# Print filtered tables with few example rows.\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "# Create a small orders DataFrame with simple columns.\n",
    "orders = pl.DataFrame({\"order_id\": [1, 2, 3, 4, 5], \"amount_usd\": [40, 120, 75, 200, 15], \"country\": [\"USA\", \"USA\", \"Canada\", \"Mexico\", \"USA\"], \"is_expedited\": [True, False, True, False, True]}).\n",
    "\n",
    "# Define a condition for high value orders above one hundred dollars.\n",
    "high_value = pl.col(\"amount_usd\") > 100\n",
    "\n",
    "# Define a condition for domestic orders shipped inside the United States.\n",
    "domestic_usa = pl.col(\"country\") == \"USA\"\n",
    "\n",
    "# Filter orders that are both high value and domestic using logical and.\n",
    "filtered_and = orders.filter(high_value & domestic_usa)\n",
    "\n",
    "# Filter orders that are high value or expedited using logical or.\n",
    "filtered_or = orders.filter(high_value | pl.col(\"is_expedited\"))\n",
    "\n",
    "# Filter orders that are not domestic using logical not negation.\n",
    "filtered_not = orders.filter(~domestic_usa)\n",
    "\n",
    "# Print results for each logical combination with clear labels.\n",
    "print(\"High value and domestic orders:\")\n",
    "print(filtered_and)\n",
    "\n",
    "print(\"\\nHigh value or expedited orders:\")\n",
    "print(filtered_or)\n",
    "\n",
    "print(\"\\nOrders not from USA:\")\n",
    "print(filtered_not)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2ee1f4",
   "metadata": {},
   "source": [
    "### **2.3. Null Aware Filtering**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e39766",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Pandas to Polars Migration/Module_02/Lecture_B/image_02_03.jpg?v=1766894026\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Nulls mean missing or unknown, not regular values\n",
    ">* Comparisons with null become null and exclude rows\n",
    "\n",
    ">* Combined conditions can drop rows with nulls\n",
    ">* Explicitly test or recode nulls to control filtering\n",
    "\n",
    ">* Decide how missing values affect your analysis\n",
    ">* Build filters that explicitly include or reinterpret nulls\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da1b300",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Null Aware Filtering\n",
    "\n",
    "# Demonstrate null aware filtering behavior using simple Polars DataFrame.\n",
    "# Show how comparisons with null values behave during filtering operations.\n",
    "# Compare default filtering with explicit null handling using is_not_null conditions.\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "# Create a small DataFrame with ages and incomes including null values.\n",
    "df = pl.DataFrame({\"name\": [\"Ann\", \"Bob\", \"Cara\", \"Dan\"], \"age\": [28, None, 42, None], \"income_dollars\": [50000, 62000, None, 45000]})\n",
    "\n",
    "# Show the original DataFrame so we can see null positions clearly.\n",
    "print(\"Original DataFrame with possible null values:\\n\", df)\n",
    "\n",
    "# Filter rows where age is greater than thirty, note null ages are automatically excluded.\n",
    "filtered_age = df.filter(pl.col(\"age\") > 30)\n",
    "\n",
    "# Display filtered result, notice rows with null age are missing from this output.\n",
    "print(\"\\nFiltered where age greater than thirty, null ages excluded:\\n\", filtered_age)\n",
    "\n",
    "# Now filter using explicit null handling, require age not null before comparison.\n",
    "filtered_explicit = df.filter(pl.col(\"age\").is_not_null() & (pl.col(\"age\") > 30))\n",
    "\n",
    "# Display explicitly handled result, which matches previous but shows intentional null treatment.\n",
    "print(\"\\nFiltered with explicit null handling on age column:\\n\", filtered_explicit)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35d3b52",
   "metadata": {},
   "source": [
    "## **3. Core Column Expressions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a5a5a9",
   "metadata": {},
   "source": [
    "### **3.1. Numeric Column Arithmetic**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b91f5d",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Pandas to Polars Migration/Module_02/Lecture_B/image_03_01.jpg?v=1766894042\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Use arithmetic on columns to create insights\n",
    ">* Operations run columnwise with lazy, fast execution\n",
    "\n",
    ">* Combine columns, constants, and math functions flexibly\n",
    ">* Chain multiple arithmetic steps without temporary columns\n",
    "\n",
    ">* Watch dtypes, nulls, and precision in arithmetic\n",
    ">* Handle zeros, null denominators, and extreme outliers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a38143f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Numeric Column Arithmetic\n",
    "\n",
    "# Demonstrate basic numeric column arithmetic using Polars DataFrame operations.\n",
    "# Show how to compute revenue and discounts from price and quantity columns.\n",
    "# Highlight how expressions operate on entire columns without manual loops.\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "# Create a simple DataFrame with item prices and quantities.\n",
    "data = {\"item\": [\"apple\", \"banana\", \"orange\"], \"price_usd\": [1.5, 0.75, 1.25], \"quantity\": [4, 6, 3]}\n",
    "\n",
    "df = pl.DataFrame(data)\n",
    "\n",
    "# Compute line revenue and discounted revenue using numeric column arithmetic.\n",
    "result = df.with_columns([\n",
    "    (pl.col(\"price_usd\") * pl.col(\"quantity\")).alias(\"line_revenue_usd\"),\n",
    "    (pl.col(\"price_usd\") * 0.9).alias(\"discount_price_usd\"),\n",
    "])\n",
    "\n",
    "# Print the original and transformed DataFrames to observe numeric arithmetic effects.\n",
    "print(\"Original DataFrame with prices and quantities:\")\n",
    "print(df)\n",
    "\n",
    "print(\"\\nDataFrame with computed revenue and discounted prices:\")\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e2c14a",
   "metadata": {},
   "source": [
    "### **3.2. String Operations and Regex**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216112a9",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Pandas to Polars Migration/Module_02/Lecture_B/image_03_02.jpg?v=1766894057\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Use vectorized string transforms on text columns\n",
    ">* Define column-wide operations instead of looping rows\n",
    "\n",
    ">* Parse semi-structured text into meaningful columns\n",
    ">* Chain column-level string steps into clean pipelines\n",
    "\n",
    ">* Use regex patterns to match and extract text\n",
    ">* Apply patterns columnwise to clean and validate data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0a7240",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - String Operations and Regex\n",
    "\n",
    "# Demonstrate basic Polars string operations on text columns.\n",
    "# Show regex usage for extracting structured information from text.\n",
    "# Compare original and transformed columns with concise printed output.\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "# Create a simple DataFrame with messy text and product codes.\n",
    "df = pl.DataFrame({\"name\": [\"  alice smith  \", \"BOB JONES\", \"cArOl king\"], \"email\": [\"alice@example.com\", \"bob_sales@shop.co.uk\", \"carol-2025@data.io\"], \"product_code\": [\"ELEC-123-US\", \"TOY-77-CA\", \"BOOK-999-UK\"]})\n",
    "\n",
    "# Build expressions that clean names and extract email domains.\n",
    "clean_name_expr = pl.col(\"name\").str.strip_chars().str.to_titlecase()\n",
    "email_domain_expr = pl.col(\"email\").str.extract(r\"@(.+)$\", group_index=1)\n",
    "\n",
    "# Use regex to split product codes into category and numeric identifier parts.\n",
    "category_expr = pl.col(\"product_code\").str.extract(r\"^([A-Z]+)-\", group_index=1)\n",
    "number_expr = pl.col(\"product_code\").str.extract(r\"-([0-9]+)-\", group_index=1)\n",
    "\n",
    "# Select original columns plus new transformed columns for comparison.\n",
    "result = df.select([pl.col(\"name\"), clean_name_expr.alias(\"clean_name\"), pl.col(\"email\"), email_domain_expr.alias(\"email_domain\"), pl.col(\"product_code\"), category_expr.alias(\"category\"), number_expr.alias(\"item_number\")])\n",
    "\n",
    "# Print the final DataFrame to show string and regex results.\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c8d83f",
   "metadata": {},
   "source": [
    "### **3.3. Conditional Column Logic**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded94003",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Pandas to Polars Migration/Module_02/Lecture_B/image_03_03.jpg?v=1766894070\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Use if-then logic to build new columns\n",
    ">* Encode business rules and classifications directly in transformations\n",
    "\n",
    ">* Define clear true-or-false predicates for rows\n",
    ">* Map predicates to outcomes for readable classifications\n",
    "\n",
    ">* Combine conditional logic with other column expressions\n",
    ">* Scales to large data using declarative rules\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cf823a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Conditional Column Logic\n",
    "\n",
    "# Demonstrate simple conditional column logic using Polars expressions.\n",
    "# Classify orders based on total amount and shipping distance thresholds.\n",
    "# Show how conditions create readable business rule driven columns.\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "# Create a small example DataFrame with order information.\n",
    "orders = pl.DataFrame({\"order_id\": [1, 2, 3, 4], \"amount_usd\": [20, 120, 260, 80], \"distance_miles\": [5, 40, 15, 70]})\n",
    "\n",
    "# Build a conditional expression that classifies order value tiers.\n",
    "value_tier_expr = (\n",
    "    pl.when(pl.col(\"amount_usd\") < 50)\n",
    "    .then(\"small\")\n",
    "    .when(pl.col(\"amount_usd\") < 200)\n",
    "    .then(\"medium\")\n",
    "    .otherwise(\"large\")\n",
    ")\n",
    "\n",
    "# Build another conditional expression for shipping urgency categories.\n",
    "shipping_status_expr = (\n",
    "    pl.when(pl.col(\"distance_miles\") <= 10)\n",
    "    .then(\"local\")\n",
    "    .when(pl.col(\"distance_miles\") <= 50)\n",
    "    .then(\"regional\")\n",
    "    .otherwise(\"long_haul\")\n",
    ")\n",
    "\n",
    "# Apply both conditional expressions to create new descriptive columns.\n",
    "result = orders.with_columns([\n",
    "    value_tier_expr.alias(\"value_tier\"),\n",
    "    shipping_status_expr.alias(\"shipping_status\"),\n",
    "])\n",
    "\n",
    "# Print the final DataFrame to inspect conditional logic results.\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5eff693",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Selecting and Filtering**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f347da",
   "metadata": {},
   "source": [
    "\n",
    "In this lecture, you learned to:\n",
    "- Select and rename columns in Polars using expressions that parallel common Pandas patterns. \n",
    "- Filter rows in Polars based on boolean conditions and combined predicates. \n",
    "- Apply simple column expressions such as arithmetic, string operations, and conditional logic in Polars. \n",
    "\n",
    "In the next Module (Module 3), we will go over 'Transformations and Joins'"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
