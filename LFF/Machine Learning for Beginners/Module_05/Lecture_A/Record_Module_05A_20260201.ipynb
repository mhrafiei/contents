{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b3372c0",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Loss And Error**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3180cf4c",
   "metadata": {},
   "source": [
    ">Last update: 20260201.\n",
    "    \n",
    "By the end of this Lecture, you will be able to:\n",
    "- Define a loss function as a numerical measure of prediction error. \n",
    "- Compute simple loss values for individual predictions in regression and classification examples. \n",
    "- Explain why training focuses on reducing average loss across a dataset. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1908ae14",
   "metadata": {},
   "source": [
    "## **1. Understanding Loss Functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029626cd",
   "metadata": {},
   "source": [
    "### **1.1. Quantifying Prediction Error**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5681c891",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_05/Lecture_A/image_01_01.jpg?v=1769959794\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Loss is a number measuring prediction wrongness\n",
    ">* Small loss means accurate predictions, large loss inaccurate\n",
    "\n",
    ">* Different tasks assign different sizes to errors\n",
    ">* Numeric loss guides models toward smaller, better errors\n",
    "\n",
    ">* Loss is a penalty for bad predictions\n",
    ">* Penalties guide training to reduce costly mistakes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0b815b",
   "metadata": {},
   "source": [
    "### **1.2. Task Specific Loss**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c314c2",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_05/Lecture_A/image_01_02.jpg?v=1769959806\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Loss depends on the specific prediction task\n",
    ">* Loss formula matches what counts as serious mistakes\n",
    "\n",
    ">* Different tasks value errors very differently\n",
    ">* Loss functions weight costly mistakes more heavily\n",
    "\n",
    ">* Loss is shaped to match task success\n",
    ">* Design loss to mirror real-world evaluation criteria\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f8b019",
   "metadata": {},
   "source": [
    "### **1.3. Sample Loss Calculations**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0296bdcf",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_05/Lecture_A/image_01_03.jpg?v=1769959823\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Use examples to turn prediction errors into numbers\n",
    ">* Loss is the size of prediction-actual difference\n",
    "\n",
    ">* Loss equals difference between predicted and actual price\n",
    ">* Loss is one number showing mistake severity\n",
    "\n",
    ">* Classification loss scores correct versus incorrect labels\n",
    ">* Loss numbers make model performance comparable and visible\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4eb6f48",
   "metadata": {},
   "source": [
    "## **2. Understanding Regression Loss**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7148be72",
   "metadata": {},
   "source": [
    "### **2.1. Comparing Predictions and Targets**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262935b5",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_05/Lecture_A/image_02_01.jpg?v=1769959839\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Loss compares model prediction with actual outcome\n",
    ">* Smaller gaps mean small loss, larger gaps larger\n",
    "\n",
    ">* Car mileage examples show prediction versus reality\n",
    ">* Loss turns prediction gaps into single numeric scores\n",
    "\n",
    ">* Compare prediction and target for every example\n",
    ">* Loss gives shared scale for errors across domains\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051c2ebc",
   "metadata": {},
   "source": [
    "### **2.2. Why Squared Errors**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9e1b38",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_05/Lecture_A/image_02_02.jpg?v=1769959858\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Squared error turns prediction differences into positives\n",
    ">* It penalizes large mistakes much more than small\n",
    "\n",
    ">* Squared error changes smoothly as predictions change\n",
    ">* Smooth loss gives clear gradients for training\n",
    "\n",
    ">* Large prediction errors can be especially harmful\n",
    ">* Squaring errors penalizes big mistakes much more\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e951aa",
   "metadata": {},
   "source": [
    "### **2.3. Impact of Large Errors**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31e6153",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_05/Lecture_A/image_02_03.jpg?v=1769959871\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Squaring errors makes big mistakes count much more\n",
    ">* Training updates focus strongly on fixing huge errors\n",
    "\n",
    ">* Squared loss highlights rare, very large mistakes\n",
    ">* Helps avoid dangerous errors in medicine and finance\n",
    "\n",
    ">* Outliers and bad data can dominate training\n",
    ">* Choose losses, cleaning, preprocessing for robustness\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c74018",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Impact of Large Errors\n",
    "\n",
    "# This script shows squared error impact visually.\n",
    "# We compare small and large regression prediction errors.\n",
    "# Notice how large errors dominate the total loss.\n",
    "\n",
    "# Import required numerical and plotting libraries.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create true house prices for a tiny toy dataset.\n",
    "true_prices = np.array([200_000, 220_000, 250_000], dtype=float)\n",
    "\n",
    "# Create mostly good predictions with one huge mistake.\n",
    "pred_prices = np.array([205_000, 218_000, 400_000], dtype=float)\n",
    "\n",
    "# Compute prediction errors as predicted minus true values.\n",
    "errors = pred_prices - true_prices\n",
    "\n",
    "# Compute squared errors to highlight large mistakes.\n",
    "squared_errors = errors ** 2\n",
    "\n",
    "# Print values to compare errors and squared errors.\n",
    "print(\"True prices:\", true_prices)\n",
    "print(\"Predicted prices:\", pred_prices)\n",
    "print(\"Errors (pred - true):\", errors)\n",
    "print(\"Squared errors:\", squared_errors)\n",
    "print(\"Total squared loss:\", squared_errors.sum())\n",
    "\n",
    "# Prepare labels for each house example on x axis.\n",
    "example_labels = [\"House 1\", \"House 2\", \"House 3\"]\n",
    "\n",
    "# Create a bar chart comparing squared errors per house.\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar(example_labels, squared_errors, color=[\"green\", \"green\", \"red\"])\n",
    "\n",
    "# Add title and axis labels explaining large error impact.\n",
    "plt.title(\"Squared error makes one large mistake dominate loss\")\n",
    "plt.ylabel(\"Squared error value\")\n",
    "\n",
    "# Adjust layout and display the single plot.\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8387ce3",
   "metadata": {},
   "source": [
    "## **3. Understanding Classification Loss**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3863d6c",
   "metadata": {},
   "source": [
    "### **3.1. Correct vs Incorrect Labels**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a3d5de",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_05/Lecture_A/image_03_01.jpg?v=1769959916\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Loss depends on matching predicted and true labels\n",
    ">* This simple right-or-wrong view underlies classification loss\n",
    "\n",
    ">* We judge models over many examples together\n",
    ">* Average loss guides training to reduce overall mistakes\n",
    "\n",
    ">* Average loss links training to real performance\n",
    ">* Rewards consistent accuracy across many real situations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00f5249",
   "metadata": {},
   "source": [
    "### **3.2. Counting Classification Errors**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aeee1df",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_05/Lecture_A/image_03_02.jpg?v=1769959932\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Count how many predictions are classified incorrectly\n",
    ">* Divide total errors by examples to get error rate\n",
    "\n",
    ">* Average error links model performance to real impact\n",
    ">* Training aims to lower this overall error rate\n",
    "\n",
    ">* Optimize average error, not single mistakes\n",
    ">* Global error signal improves overall, balanced accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adefcae5",
   "metadata": {},
   "source": [
    "### **3.3. Confidence Weighted Penalties**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967d975d",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_05/Lecture_A/image_03_03.jpg?v=1769959944\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Loss considers prediction confidence, not just correctness\n",
    ">* Confident mistakes get larger penalties than uncertain ones\n",
    "\n",
    ">* High-confidence wrong predictions get large loss penalties\n",
    ">* Average loss reflects how often and how dangerously wrong\n",
    "\n",
    ">* Average confidence loss improves probabilities and calibration\n",
    ">* This leads to safer, more reliable predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23df963",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Confidence Weighted Penalties\n",
    "\n",
    "# This script illustrates confidence weighted classification penalties.\n",
    "# We compare simple accuracy with confidence sensitive loss values.\n",
    "# Focus on average loss across several small prediction examples.\n",
    "\n",
    "# import numpy for numerical arrays and calculations.\n",
    "import numpy as np\n",
    "\n",
    "# define true labels and predicted probabilities for three classes.\n",
    "true_labels = np.array([0, 1, 2, 1, 0])\n",
    "\n",
    "# define model predicted probabilities for each example row.\n",
    "pred_probs = np.array([\n",
    "    [0.7, 0.2, 0.1],\n",
    "    [0.2, 0.6, 0.2],\n",
    "    [0.1, 0.1, 0.8],\n",
    "    [0.6, 0.3, 0.1],\n",
    "    [0.4, 0.3, 0.3],\n",
    "])\n",
    "\n",
    "# check shapes to ensure labels and probabilities align correctly.\n",
    "assert pred_probs.shape[0] == true_labels.shape[0]\n",
    "\n",
    "# compute predicted labels using highest probability for each example.\n",
    "pred_labels = np.argmax(pred_probs, axis=1)\n",
    "\n",
    "# compute simple accuracy ignoring confidence information entirely.\n",
    "accuracy = np.mean(pred_labels == true_labels)\n",
    "\n",
    "# define a small epsilon to avoid logarithm of zero values.\n",
    "epsilon = 1e-9\n",
    "\n",
    "# gather probabilities assigned to the correct class for each example.\n",
    "correct_class_probs = pred_probs[np.arange(true_labels.size), true_labels]\n",
    "\n",
    "# clip probabilities to avoid taking log of zero values.\n",
    "correct_class_probs = np.clip(correct_class_probs, epsilon, 1.0)\n",
    "\n",
    "# compute negative log likelihood as confidence weighted loss.\n",
    "loss_values = -np.log(correct_class_probs)\n",
    "\n",
    "# compute average loss across all examples in this dataset.\n",
    "average_loss = float(np.mean(loss_values))\n",
    "\n",
    "# print accuracy and average loss to compare training objectives.\n",
    "print(\"Simple accuracy ignoring confidence:\", float(accuracy))\n",
    "\n",
    "# print individual loss values to show confidence weighted penalties.\n",
    "print(\"Confidence weighted loss values:\", loss_values.tolist())\n",
    "\n",
    "# print average loss summarizing overall confidence quality.\n",
    "print(\"Average confidence weighted loss:\", average_loss)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c104c42",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Loss And Error**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7379ebe9",
   "metadata": {},
   "source": [
    "\n",
    "In this lecture, you learned to:\n",
    "- Define a loss function as a numerical measure of prediction error. \n",
    "- Compute simple loss values for individual predictions in regression and classification examples. \n",
    "- Explain why training focuses on reducing average loss across a dataset. \n",
    "\n",
    "In the next Lecture (Lecture B), we will go over 'Fitting And Overfitting'"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
