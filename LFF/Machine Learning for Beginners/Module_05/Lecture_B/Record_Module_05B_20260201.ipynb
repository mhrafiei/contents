{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fac6a5f",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Fitting And Overfitting**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bc5cb0",
   "metadata": {},
   "source": [
    ">Last update: 20260201.\n",
    "    \n",
    "By the end of this Lecture, you will be able to:\n",
    "- Describe in intuitive terms how training adjusts model parameters to reduce loss. \n",
    "- Differentiate underfitting, appropriate fitting, and overfitting using examples. \n",
    "- Explain why evaluating models only on training data can be misleading. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d4dd87",
   "metadata": {},
   "source": [
    "## **1. How Models Adjust**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978486ee",
   "metadata": {},
   "source": [
    "### **1.1. From Guess to Better Fit**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0553f92",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_05/Lecture_B/image_01_01.jpg?v=1769960586\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Training starts with a rough, usually poor guess\n",
    ">* Loss measures errors and guides gradual parameter adjustments\n",
    "\n",
    ">* Bread recipe tuning mirrors model training feedback\n",
    ">* Loss guides parameter tweaks to improve predictions\n",
    "\n",
    ">* Model follows downhill slope from high loss\n",
    ">* Many small steps gradually reach better predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679534ce",
   "metadata": {},
   "source": [
    "### **1.2. Stepwise Model Improvement**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef2ab03",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_05/Lecture_B/image_01_02.jpg?v=1769960598\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Model checks prediction errors and tweaks parameters slightly\n",
    ">* Repeated small tweaks gradually lower loss and improve fit\n",
    "\n",
    ">* Training is like walking downhill using local slope\n",
    ">* Many small, local steps gradually reach lower loss\n",
    "\n",
    ">* Like practicing piano, learning happens through refinements\n",
    ">* Many small parameter tweaks gradually reduce prediction errors\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8a1565",
   "metadata": {},
   "source": [
    "### **1.3. Seeing Parameters Move**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede981d2",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_05/Lecture_B/image_01_03.jpg?v=1769960611\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Model parameters act like adjustable control knobs\n",
    ">* Training nudges knobs to lower loss over time\n",
    "\n",
    ">* Simple house-price example shows parameter knobs\n",
    ">* Training adjusts size and age weights to reduce loss\n",
    "\n",
    ">* Basketball practice mirrors gradual parameter adjustments\n",
    ">* Refined parameters give accurate responses without retraining\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1019adfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Seeing Parameters Move\n",
    "\n",
    "# This script shows parameters moving during simple training.\n",
    "# We use a tiny linear model with manual updates.\n",
    "# Watch how loss shrinks as weights slowly adjust.\n",
    "\n",
    "# import required built in and numerical libraries.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# set deterministic random seed for reproducible behavior.\n",
    "np.random.seed(0)\n",
    "\n",
    "# create small synthetic dataset for a simple line.\n",
    "true_w, true_b = 2.0, 1.0\n",
    "x = np.linspace(0, 5, 20)\n",
    "\n",
    "# generate targets with small noise for realism.\n",
    "noise = np.random.normal(loc=0.0, scale=0.5, size=x.shape)\n",
    "y = true_w * x + true_b + noise\n",
    "\n",
    "# initialize model parameters with poor starting guess.\n",
    "w, b = -1.0, 5.0\n",
    "\n",
    "# choose learning rate and number of training steps.\n",
    "learning_rate = 0.05\n",
    "steps = 20\n",
    "\n",
    "# prepare lists to store parameter and loss history.\n",
    "history_w, history_b, history_loss = [], [], []\n",
    "\n",
    "# define function to compute predictions from parameters.\n",
    "def predict(x_values, weight, bias):\n",
    "    return weight * x_values + bias\n",
    "\n",
    "# define function to compute mean squared error loss.\n",
    "def mse_loss(y_true, y_pred):\n",
    "    diff = y_true - y_pred\n",
    "    return float(np.mean(diff ** 2))\n",
    "\n",
    "# training loop that nudges parameters to reduce loss.\n",
    "for step in range(steps):\n",
    "    y_pred = predict(x, w, b)\n",
    "    loss = mse_loss(y, y_pred)\n",
    "\n",
    "    # compute gradients for weight and bias analytically.\n",
    "    error = y_pred - y\n",
    "    grad_w = float(np.mean(2 * error * x))\n",
    "    grad_b = float(np.mean(2 * error))\n",
    "\n",
    "    # update parameters by stepping opposite gradient direction.\n",
    "    w = w - learning_rate * grad_w\n",
    "    b = b - learning_rate * grad_b\n",
    "\n",
    "    # store history for later inspection and plotting.\n",
    "    history_w.append(w)\n",
    "    history_b.append(b)\n",
    "    history_loss.append(loss)\n",
    "\n",
    "# print a few snapshots to see parameters move.\n",
    "print(\"Initial guess weight and bias were -1.0 and 5.0.\")\n",
    "print(\"Final learned weight and bias are\", round(w, 2), round(b, 2))\n",
    "print(\"True underlying weight and bias are\", true_w, true_b)\n",
    "print(\"First recorded loss value was\", round(history_loss[0], 3))\n",
    "print(\"Last recorded loss value was\", round(history_loss[-1], 3))\n",
    "\n",
    "# create a simple plot showing parameter and loss changes.\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(8, 3))\n",
    "\n",
    "# plot loss over training steps to show improvement.\n",
    "axes[0].plot(range(1, steps + 1), history_loss, marker=\"o\")\n",
    "axes[0].set_title(\"Loss shrinking as parameters move\")\n",
    "axes[0].set_xlabel(\"Training step index\")\n",
    "axes[0].set_ylabel(\"Mean squared error loss\")\n",
    "\n",
    "# plot weight and bias trajectories together for comparison.\n",
    "axes[1].plot(range(1, steps + 1), history_w, label=\"weight w\")\n",
    "axes[1].plot(range(1, steps + 1), history_b, label=\"bias b\")\n",
    "axes[1].axhline(true_w, color=\"gray\", linestyle=\"--\", label=\"true w\")\n",
    "axes[1].axhline(true_b, color=\"black\", linestyle=\":\", label=\"true b\")\n",
    "axes[1].set_title(\"Parameters nudged toward better values\")\n",
    "axes[1].set_xlabel(\"Training step index\")\n",
    "axes[1].set_ylabel(\"Parameter value magnitude\")\n",
    "axes[1].legend(loc=\"best\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3502dadc",
   "metadata": {},
   "source": [
    "## **2. Model Complexity Balance**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c9127e",
   "metadata": {},
   "source": [
    "### **2.1. Underfitting Simple Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbf8af1",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_05/Lecture_B/image_02_01.jpg?v=1769960664\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Underfitting means the model is too simple\n",
    ">* It makes systematic errors and misses key patterns\n",
    "\n",
    ">* Straight-line models can’t follow curved data patterns\n",
    ">* Oversimplified features cause systematic errors across domains\n",
    "\n",
    ">* Overly simple models stay biased and inaccurate\n",
    ">* Spot underfitting and choose richer models instead\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93331958",
   "metadata": {},
   "source": [
    "### **2.2. Overly Complex Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceda3319",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_05/Lecture_B/image_02_02.jpg?v=1769960676\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Too-flexible models fit noise and quirks\n",
    ">* They memorize training data, failing to generalize\n",
    "\n",
    ">* Moderate models learn general links between features\n",
    ">* Overly complex models memorize quirks, fail on new\n",
    "\n",
    ">* Complex models memorize quirks from specific datasets\n",
    ">* They seem accurate but fail on new situations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e72789",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Overly Complex Models\n",
    "\n",
    "# This script shows overly complex models intuitively.\n",
    "# We compare simple and complex curves on noisy data.\n",
    "# Focus on how complexity affects generalization performance.\n",
    "\n",
    "# import required numerical and plotting libraries.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# set deterministic random seed for reproducible results.\n",
    "np.random.seed(42)\n",
    "\n",
    "# create one dimensional input data for training examples.\n",
    "x_train = np.linspace(0, 1, 10)\n",
    "\n",
    "# define true underlying relationship as a simple line.\n",
    "true_y_train = 2 * x_train + 1\n",
    "\n",
    "# add small noise to create realistic training targets.\n",
    "noise_train = np.random.normal(loc=0.0, scale=0.1, size=x_train.shape)\n",
    "\n",
    "# compute noisy training targets from true line plus noise.\n",
    "y_train = true_y_train + noise_train\n",
    "\n",
    "# create separate test inputs to check generalization behavior.\n",
    "x_test = np.linspace(0, 1, 100)\n",
    "\n",
    "# compute true outputs for test inputs without noise.\n",
    "true_y_test = 2 * x_test + 1\n",
    "\n",
    "# fit simple linear polynomial model to training data.\n",
    "coeffs_simple = np.polyfit(x_train, y_train, deg=1)\n",
    "\n",
    "# fit overly complex polynomial model with high degree.\n",
    "coeffs_complex = np.polyfit(x_train, y_train, deg=9)\n",
    "\n",
    "# evaluate simple model predictions on training and test inputs.\n",
    "y_pred_simple_train = np.polyval(coeffs_simple, x_train)\n",
    "\n",
    "# evaluate complex model predictions on training and test inputs.\n",
    "y_pred_complex_train = np.polyval(coeffs_complex, x_train)\n",
    "\n",
    "# compute simple model predictions on dense test grid.\n",
    "y_pred_simple_test = np.polyval(coeffs_simple, x_test)\n",
    "\n",
    "# compute complex model predictions on dense test grid.\n",
    "y_pred_complex_test = np.polyval(coeffs_complex, x_test)\n",
    "\n",
    "# define helper function for mean squared error calculation.\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    # validate shapes before computing mean squared error.\n",
    "    assert y_true.shape == y_pred.shape\n",
    "\n",
    "    return float(np.mean((y_true - y_pred) ** 2))\n",
    "\n",
    "# compute training error for simple linear model.\n",
    "mse_simple_train = mean_squared_error(y_train, y_pred_simple_train)\n",
    "\n",
    "# compute training error for complex polynomial model.\n",
    "mse_complex_train = mean_squared_error(y_train, y_pred_complex_train)\n",
    "\n",
    "# compute test error for simple linear model.\n",
    "mse_simple_test = mean_squared_error(true_y_test, y_pred_simple_test)\n",
    "\n",
    "# compute test error for complex polynomial model.\n",
    "mse_complex_test = mean_squared_error(true_y_test, y_pred_complex_test)\n",
    "\n",
    "# print concise summary comparing training and test errors.\n",
    "print(\"Simple model training MSE:\", round(mse_simple_train, 4))\n",
    "print(\"Complex model training MSE:\", round(mse_complex_train, 4))\n",
    "print(\"Simple model test MSE:\", round(mse_simple_test, 4))\n",
    "print(\"Complex model test MSE:\", round(mse_complex_test, 4))\n",
    "\n",
    "# create figure to visualize true line and both models.\n",
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "# plot noisy training points as scattered markers.\n",
    "plt.scatter(x_train, y_train, color=\"black\", label=\"Training data\")\n",
    "\n",
    "# plot true underlying simple relationship line.\n",
    "plt.plot(x_test, true_y_test, color=\"green\", label=\"True relationship\")\n",
    "\n",
    "# plot simple model line which should generalize well.\n",
    "plt.plot(x_test, y_pred_simple_test, color=\"blue\", label=\"Simple model\")\n",
    "\n",
    "# plot complex model curve showing overfitting wiggles.\n",
    "plt.plot(x_test, y_pred_complex_test, color=\"red\", label=\"Complex model\")\n",
    "\n",
    "# add axis labels and legend for clarity.\n",
    "plt.xlabel(\"Input feature value\")\n",
    "plt.ylabel(\"Target value\")\n",
    "plt.legend()\n",
    "\n",
    "# display the final plot to visually compare models.\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806642ec",
   "metadata": {},
   "source": [
    "### **2.3. Balanced Model Fit**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011b0540",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_05/Lecture_B/image_02_03.jpg?v=1769960731\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Captures main data patterns without chasing noise\n",
    ">* Performs similarly on training and new data\n",
    "\n",
    ">* Balanced model uses meaningful medical patterns, not trivia\n",
    ">* It generalizes reliably across new patients and hospitals\n",
    "\n",
    ">* Choose model complexity that matches real patterns\n",
    ">* Align model behavior with domain knowledge, common sense\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbe6d45",
   "metadata": {},
   "source": [
    "## **3. Generalization Beyond Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730d9e8b",
   "metadata": {},
   "source": [
    "### **3.1. Training vs Unseen Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e08030f",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_05/Lecture_B/image_03_01.jpg?v=1769960745\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Training adjusts model parameters using labeled examples\n",
    ">* Real goal is accurate predictions on unseen cases\n",
    "\n",
    ">* Memorizing practice questions hides lack of understanding\n",
    ">* Models must generalize beyond training examples to work\n",
    "\n",
    ">* Generalization means handling new, slightly different data\n",
    ">* Training-only success can hide failures on real cases\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19025c8",
   "metadata": {},
   "source": [
    "### **3.2. Limits of Memorization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e8f053",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_05/Lecture_B/image_03_02.jpg?v=1769960756\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Overfitting stores examples instead of learning patterns\n",
    ">* Training accuracy can hide poor generalization performance\n",
    "\n",
    ">* Memorized spam filters fail on new tricks\n",
    ">* We must test if models learn general patterns\n",
    "\n",
    ">* Memorizing one hospital’s data fails elsewhere\n",
    ">* Training accuracy hides poor real‑world generalization\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2f5a3d",
   "metadata": {},
   "source": [
    "### **3.3. Why Separate Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618ee6c3",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_05/Lecture_B/image_03_03.jpg?v=1769960768\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Training data shows performance on familiar examples only\n",
    ">* Separate evaluation checks generalization to new situations\n",
    "\n",
    ">* Complex models can memorize noise and quirks\n",
    ">* Separate evaluation reveals poor performance on new data\n",
    "\n",
    ">* Separate evaluation checks if changes truly generalize\n",
    ">* Prevents complex models that impress training but fail\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77e93c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Why Separate Evaluation\n",
    "\n",
    "# This script shows why separate evaluation matters.\n",
    "# We compare training and test errors for memorization.\n",
    "# Focus is on generalization beyond seen training examples.\n",
    "\n",
    "# Required libraries are available in Colab by default.\n",
    "# No extra installations are necessary for this script.\n",
    "# Uncomment below only if environment lacks numpy or matplotlib.\n",
    "# pip install numpy matplotlib seaborn.\n",
    "\n",
    "# Import required standard and numerical libraries.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set deterministic random seed for reproducibility.\n",
    "np.random.seed(0)\n",
    "\n",
    "# Create simple one dimensional input features.\n",
    "x_all = np.linspace(0.0, 1.0, 20)\n",
    "\n",
    "# Create true underlying linear relationship with noise.\n",
    "true_y_all = 2.0 * x_all + 1.0 + np.random.normal(0.0, 0.05, 20)\n",
    "\n",
    "# Split into small training and test sets.\n",
    "train_x, test_x = x_all[:10], x_all[10:]\n",
    "\n",
    "# Split corresponding target values for training and test.\n",
    "train_y, test_y = true_y_all[:10], true_y_all[10:]\n",
    "\n",
    "# Validate shapes before further computations.\n",
    "assert train_x.shape == train_y.shape\n",
    "\n",
    "# Define helper function to compute mean squared error.\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    # Ensure shapes match before computing error.\n",
    "    assert y_true.shape == y_pred.shape\n",
    "\n",
    "    return float(np.mean((y_true - y_pred) ** 2))\n",
    "\n",
    "\n",
    "# Train simple linear model using closed form solution.\n",
    "X_design = np.vstack((np.ones_like(train_x), train_x)).T\n",
    "\n",
    "# Compute parameters using normal equation solution.\n",
    "theta = np.linalg.pinv(X_design.T @ X_design) @ (X_design.T @ train_y)\n",
    "\n",
    "# Extract intercept and slope from parameter vector.\n",
    "intercept, slope = float(theta[0]), float(theta[1])\n",
    "\n",
    "# Compute predictions for training and test sets.\n",
    "train_pred_linear = intercept + slope * train_x\n",
    "\n",
    "# Compute linear model predictions on test inputs.\n",
    "test_pred_linear = intercept + slope * test_x\n",
    "\n",
    "# Compute training and test errors for linear model.\n",
    "train_mse_linear = mean_squared_error(train_y, train_pred_linear)\n",
    "\n",
    "# Compute test mean squared error for linear model.\n",
    "test_mse_linear = mean_squared_error(test_y, test_pred_linear)\n",
    "\n",
    "# Build memorization model that stores training targets.\n",
    "memorized_train_y = train_y.copy()\n",
    "\n",
    "# Define function that predicts by nearest training neighbor.\n",
    "def memorize_predict(train_inputs, train_targets, new_inputs):\n",
    "    # Ensure training and target shapes are compatible.\n",
    "    assert train_inputs.shape == train_targets.shape\n",
    "\n",
    "    # Allocate prediction array for new inputs.\n",
    "    preds = np.zeros_like(new_inputs)\n",
    "\n",
    "    # Loop over each new input and find nearest neighbor.\n",
    "    for i, value in enumerate(new_inputs):\n",
    "        distances = np.abs(train_inputs - value)\n",
    "        nearest_index = int(np.argmin(distances))\n",
    "        preds[i] = train_targets[nearest_index]\n",
    "\n",
    "    return preds\n",
    "\n",
    "\n",
    "# Compute memorization predictions on training data.\n",
    "train_pred_mem = memorize_predict(train_x, memorized_train_y, train_x)\n",
    "\n",
    "# Compute memorization predictions on test data.\n",
    "test_pred_mem = memorize_predict(train_x, memorized_train_y, test_x)\n",
    "\n",
    "# Compute training and test errors for memorization model.\n",
    "train_mse_mem = mean_squared_error(train_y, train_pred_mem)\n",
    "\n",
    "# Compute test mean squared error for memorization model.\n",
    "test_mse_mem = mean_squared_error(test_y, test_pred_mem)\n",
    "\n",
    "# Print concise comparison of both models and datasets.\n",
    "print(\"Linear model train MSE:\", round(train_mse_linear, 4))\n",
    "\n",
    "# Print linear model test error for comparison.\n",
    "print(\"Linear model test MSE:\", round(test_mse_linear, 4))\n",
    "\n",
    "# Print memorization model training error showing perfect fit.\n",
    "print(\"Memorization model train MSE:\", round(train_mse_mem, 4))\n",
    "\n",
    "# Print memorization model test error showing poor generalization.\n",
    "print(\"Memorization model test MSE:\", round(test_mse_mem, 4))\n",
    "\n",
    "# Create dense grid for plotting model predictions.\n",
    "plot_x = np.linspace(0.0, 1.0, 100)\n",
    "\n",
    "# Compute linear predictions on dense grid.\n",
    "plot_y_linear = intercept + slope * plot_x\n",
    "\n",
    "# Compute memorization predictions on dense grid.\n",
    "plot_y_mem = memorize_predict(train_x, memorized_train_y, plot_x)\n",
    "\n",
    "# Start a new figure for visualization.\n",
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "# Plot training points as blue circles.\n",
    "plt.scatter(train_x, train_y, color=\"blue\", label=\"Train points\")\n",
    "\n",
    "# Plot test points as orange triangles.\n",
    "plt.scatter(test_x, test_y, color=\"orange\", label=\"Test points\")\n",
    "\n",
    "# Plot linear model prediction line.\n",
    "plt.plot(plot_x, plot_y_linear, color=\"green\", label=\"Linear model\")\n",
    "\n",
    "# Plot memorization model step like curve.\n",
    "plt.step(plot_x, plot_y_mem, where=\"mid\", color=\"red\", label=\"Memorization model\")\n",
    "\n",
    "# Add axis labels and title for clarity.\n",
    "plt.xlabel(\"Input feature x value\")\n",
    "\n",
    "# Label y axis to show target variable.\n",
    "plt.ylabel(\"Target y value\")\n",
    "\n",
    "# Add legend explaining plotted elements.\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "# Add title emphasizing generalization beyond training.\n",
    "plt.title(\"Training fit versus generalization on separate test data\")\n",
    "\n",
    "# Display the final plot to the user.\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16dcdf6",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Fitting And Overfitting**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa0d06c",
   "metadata": {},
   "source": [
    "\n",
    "In this lecture, you learned to:\n",
    "- Describe in intuitive terms how training adjusts model parameters to reduce loss. \n",
    "- Differentiate underfitting, appropriate fitting, and overfitting using examples. \n",
    "- Explain why evaluating models only on training data can be misleading. \n",
    "\n",
    "In the next Module (Module 6), we will go over 'Evaluation Metrics'"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
