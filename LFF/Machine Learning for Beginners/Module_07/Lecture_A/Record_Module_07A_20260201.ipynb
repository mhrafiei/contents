{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ea8a60e",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Splits And Validation**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0af1f96",
   "metadata": {},
   "source": [
    ">Last update: 20260201.\n",
    "    \n",
    "By the end of this Lecture, you will be able to:\n",
    "- Describe the roles of training, validation, and test splits in model development. \n",
    "- Propose simple ways to split a small dataset into separate parts. \n",
    "- Explain how improper splitting can lead to overly optimistic performance estimates. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a561641",
   "metadata": {},
   "source": [
    "## **1. Why We Split Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f75588",
   "metadata": {},
   "source": [
    "### **1.1. Separating learning and testing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a461c8a2",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_07/Lecture_A/image_01_01.jpg?v=1769963625\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Keep training and testing data strictly separate\n",
    ">* Use unseen test data to check generalization\n",
    "\n",
    ">* Models must be tested on unseen future-like data\n",
    ">* Held-out test sets give realistic performance estimates\n",
    "\n",
    ">* Using test data during tuning biases decisions\n",
    ">* Keeping test data untouched gives honest generalization estimate\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f640d3",
   "metadata": {},
   "source": [
    "### **1.2. Preventing Data Leakage**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4f546d",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_07/Lecture_A/image_01_02.jpg?v=1769963637\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Data leakage lets models secretly use future information\n",
    ">* Separate train, validation, test sets block these leaks\n",
    "\n",
    ">* Leakage often happens during preprocessing and feature engineering\n",
    ">* Fit all transformations only on training data\n",
    "\n",
    ">* Make all data decisions using training only\n",
    ">* Keep validation, test untouched to avoid leakage\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b5e623",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Preventing Data Leakage\n",
    "\n",
    "# This script shows simple data leakage prevention.\n",
    "# We use tiny synthetic data for illustration.\n",
    "# Focus on splitting before preprocessing operations.\n",
    "\n",
    "# import required built in and numeric libraries.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# set deterministic random seed for reproducibility.\n",
    "rng = np.random.default_rng(seed=42)\n",
    "\n",
    "# create tiny dataset with target and one feature.\n",
    "size = 20\n",
    "feature = rng.normal(loc=50.0, scale=10.0, size=size)\n",
    "\n",
    "# create target correlated with feature plus noise.\n",
    "target = (feature * 0.5) + rng.normal(loc=0.0, scale=5.0, size=size)\n",
    "\n",
    "# build pandas dataframe from arrays.\n",
    "data = pd.DataFrame({\"feature\": feature, \"target\": target})\n",
    "\n",
    "# show first few rows to understand structure.\n",
    "print(\"Head of full dataset with feature and target:\")\n",
    "print(data.head(5))\n",
    "\n",
    "# define train size and compute split index.\n",
    "train_size = 14\n",
    "split_index = train_size\n",
    "\n",
    "# perform incorrect scaling using full dataset.\n",
    "full_mean = data[\"feature\"].mean()\n",
    "full_std = data[\"feature\"].std(ddof=0)\n",
    "\n",
    "# scale feature using information from all rows.\n",
    "data[\"feature_scaled_leaky\"] = (data[\"feature\"] - full_mean) / full_std\n",
    "\n",
    "# perform correct split before computing statistics.\n",
    "train_data = data.iloc[:split_index].copy()\n",
    "test_data = data.iloc[split_index:].copy()\n",
    "\n",
    "# compute scaling parameters using only training data.\n",
    "train_mean = train_data[\"feature\"].mean()\n",
    "train_std = train_data[\"feature\"].std(ddof=0)\n",
    "\n",
    "# scale train and test using training statistics only.\n",
    "train_data[\"feature_scaled_safe\"] = (\n",
    "    (train_data[\"feature\"] - train_mean) / train_std\n",
    ")\n",
    "\n",
    "# apply same transformation to test data safely.\n",
    "test_data[\"feature_scaled_safe\"] = (\n",
    "    (test_data[\"feature\"] - train_mean) / train_std\n",
    ")\n",
    "\n",
    "# print comparison of means for scaled features.\n",
    "print(\"\\nLeaky scaled feature mean using all data:\")\n",
    "print(round(data[\"feature_scaled_leaky\"].mean(), 4))\n",
    "\n",
    "# show safe scaled feature mean on training data.\n",
    "print(\"Safe scaled training feature mean using train only:\")\n",
    "print(round(train_data[\"feature_scaled_safe\"].mean(), 4))\n",
    "\n",
    "# show safe scaled feature mean on test data.\n",
    "print(\"Safe scaled test feature mean using train only:\")\n",
    "print(round(test_data[\"feature_scaled_safe\"].mean(), 4))\n",
    "\n",
    "# final print summarizing why leakage is problematic.\n",
    "print(\"\\nLeaky scaling secretly used test information during preprocessing.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fe6f84",
   "metadata": {},
   "source": [
    "### **1.3. Fair Model Assessment**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e902747",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_07/Lecture_A/image_01_03.jpg?v=1769963662\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Splits simulate how models face real data\n",
    ">* Held-out test set estimates future model performance\n",
    "\n",
    ">* Reusing training data for testing inflates performance\n",
    ">* A separate untouched test set checks real-world accuracy\n",
    "\n",
    ">* Use one shared, untouched test set\n",
    ">* Ensures fair, unbiased comparison of model performance\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30cd0a9",
   "metadata": {},
   "source": [
    "## **2. Basic Data Splits**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc39c89d",
   "metadata": {},
   "source": [
    "### **2.1. Simple Holdout Split**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ea196a",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_07/Lecture_A/image_02_01.jpg?v=1769963677\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Split data into training and holdout groups\n",
    ">* Holdout set tests real-world performance, prevents overfitting\n",
    "\n",
    ">* Randomly hold out a small shuffled portion\n",
    ">* Evaluate model on held-out data for honesty\n",
    "\n",
    ">* Simple holdout splits are quick, easy, practical\n",
    ">* Single splits can be unstable on limited data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeee4c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Simple Holdout Split\n",
    "\n",
    "# This script demonstrates a simple holdout split.\n",
    "# We use tiny synthetic data for clear illustration.\n",
    "# Focus on separating training and holdout evaluation sets.\n",
    "\n",
    "# Import required standard libraries for randomness and math.\n",
    "import random\n",
    "import math\n",
    "\n",
    "# Set a deterministic random seed for reproducible splitting.\n",
    "random.seed(42)\n",
    "\n",
    "# Create a tiny synthetic dataset of labeled examples.\n",
    "examples = []\n",
    "for i in range(1, 21):\n",
    "    label = \"positive\" if i % 2 == 0 else \"negative\"\n",
    "    examples.append({\"id\": i, \"text\": f\"review_{i}\", \"label\": label})\n",
    "\n",
    "# Verify dataset size is safely large enough for splitting.\n",
    "if len(examples) < 4:\n",
    "    raise ValueError(\"Dataset too small for meaningful holdout split.\")\n",
    "\n",
    "# Decide the holdout fraction for evaluation set size.\n",
    "holdout_fraction = 0.25\n",
    "\n",
    "# Compute holdout size using floor to avoid overshooting.\n",
    "raw_holdout_size = len(examples) * holdout_fraction\n",
    "holdout_size = max(1, math.floor(raw_holdout_size))\n",
    "\n",
    "# Shuffle dataset copy so original order remains unchanged.\n",
    "shuffled_examples = examples.copy()\n",
    "random.shuffle(shuffled_examples)\n",
    "\n",
    "# Split shuffled data into training and holdout subsets.\n",
    "holdout_set = shuffled_examples[:holdout_size]\n",
    "training_set = shuffled_examples[holdout_size:]\n",
    "\n",
    "# Validate that splits cover all examples without overlap.\n",
    "all_ids = {e[\"id\"] for e in examples}\n",
    "train_ids = {e[\"id\"] for e in training_set}\n",
    "holdout_ids = {e[\"id\"] for e in holdout_set}\n",
    "\n",
    "# Ensure no example appears in both training and holdout sets.\n",
    "if train_ids.intersection(holdout_ids):\n",
    "    raise RuntimeError(\"Overlap detected between training and holdout sets.\")\n",
    "\n",
    "# Ensure combined split sizes exactly match original dataset size.\n",
    "if len(train_ids) + len(holdout_ids) != len(all_ids):\n",
    "    raise RuntimeError(\"Split sizes do not match original dataset size.\")\n",
    "\n",
    "# Print short summary describing the simple holdout split.\n",
    "print(\"Total examples in full dataset:\", len(examples))\n",
    "print(\"Training examples after split:\", len(training_set))\n",
    "print(\"Holdout examples after split:\", len(holdout_set))\n",
    "\n",
    "# Show a few training examples to illustrate their structure.\n",
    "print(\"\\nSample training examples (id, text, label):\")\n",
    "for example in training_set[:3]:\n",
    "    print(example[\"id\"], example[\"text\"], example[\"label\"])\n",
    "\n",
    "# Show all holdout examples since dataset is intentionally tiny.\n",
    "print(\"\\nAll holdout examples (id, text, label):\")\n",
    "for example in holdout_set:\n",
    "    print(example[\"id\"], example[\"text\"], example[\"label\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e925e5ba",
   "metadata": {},
   "source": [
    "### **2.2. Choosing Split Ratios**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e0d53b",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_07/Lecture_A/image_02_02.jpg?v=1769963719\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Balance training size with evaluation reliability\n",
    ">* Too little training or testing harms model learning\n",
    "\n",
    ">* Start with simple ratios like 80-20 splits\n",
    ">* Adjust splits based on size, diversity, purpose\n",
    "\n",
    ">* Plan split sizes around validation needs\n",
    ">* Balance dataset size, diversity, and tuning cycles\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131ab173",
   "metadata": {},
   "source": [
    "### **2.3. Random Versus Ordered Splits**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c90712",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_07/Lecture_A/image_02_03.jpg?v=1769963731\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Random splits mix examples for balanced representation\n",
    ">* Ordered splits follow dataset sequence but risk bias\n",
    "\n",
    ">* Use ordered splits when data have sequence\n",
    ">* Preserving order better reflects real future performance\n",
    "\n",
    ">* Random splits mix different subgroups more fairly\n",
    ">* Choose split method that matches real data use\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b433b9",
   "metadata": {},
   "source": [
    "## **3. Splitting Mistakes**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceee8de6",
   "metadata": {},
   "source": [
    "### **3.1. Overusing Test Sets**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065f90a1",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_07/Lecture_A/image_03_01.jpg?v=1769963748\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Test set is a one-time final exam\n",
    ">* Repeated checks overfit to its noise, inflating performance\n",
    "\n",
    ">* Team keeps tweaking models using same test set\n",
    ">* Test score inflates, real-world performance drops\n",
    "\n",
    ">* Overusing test sets happens across many fields\n",
    ">* Turns test data into training, inflating performance\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6346cab2",
   "metadata": {},
   "source": [
    "### **3.2. Peeking at evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61f456c",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_07/Lecture_A/image_03_02.jpg?v=1769963762\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Repeatedly tweaking after evaluation quietly uses test data\n",
    ">* Model overfits quirks, inflating expected real-world performance\n",
    "\n",
    ">* Repeated tuning on one split encodes quirks\n",
    ">* Model looks great there, fails on new data\n",
    "\n",
    ">* Repeated validation checks secretly guide model design\n",
    ">* This captures noise, inflating performance and trust\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33524290",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Peeking at evaluation\n",
    "\n",
    "# This script shows peeking at evaluation effects.\n",
    "# It uses a tiny synthetic regression dataset.\n",
    "# We will overfit by reusing validation feedback.\n",
    "\n",
    "# import required built in and numerical libraries.\n",
    "import numpy as np\n",
    "\n",
    "# set deterministic random seed for reproducibility.\n",
    "np.random.seed(42)\n",
    "\n",
    "# generate simple one dimensional input feature values.\n",
    "X = np.linspace(0.0, 1.0, 60).reshape(-1, 1)\n",
    "\n",
    "# generate target values from a noisy linear relationship.\n",
    "y = 2.0 * X[:, 0] + 0.3 + np.random.normal(0.0, 0.05, 60)\n",
    "\n",
    "# verify shapes are as expected before splitting.\n",
    "assert X.shape == (60, 1) and y.shape == (60,)\n",
    "\n",
    "# create indices for training validation and test splits.\n",
    "train_idx = np.arange(0, 30)\n",
    "\n",
    "# define validation indices using a fixed slice.\n",
    "val_idx = np.arange(30, 45)\n",
    "\n",
    "# define test indices using the remaining slice.\n",
    "test_idx = np.arange(45, 60)\n",
    "\n",
    "# create split arrays using the index selections.\n",
    "X_train, y_train = X[train_idx], y[train_idx]\n",
    "\n",
    "# create validation arrays using the index selections.\n",
    "X_val, y_val = X[val_idx], y[val_idx]\n",
    "\n",
    "# create test arrays using the index selections.\n",
    "X_test, y_test = X[test_idx], y[test_idx]\n",
    "\n",
    "# define a function computing mean squared error safely.\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    # ensure shapes match before computing error.\n",
    "    assert y_true.shape == y_pred.shape\n",
    "\n",
    "    # compute average squared difference between arrays.\n",
    "    return float(np.mean((y_true - y_pred) ** 2))\n",
    "\n",
    "# define a function fitting polynomial regression models.\n",
    "def fit_polynomial_model(x_values, y_values, degree):\n",
    "    # build Vandermonde matrix for polynomial features.\n",
    "    X_poly = np.vander(x_values[:, 0], degree + 1, increasing=True)\n",
    "\n",
    "    # solve least squares problem for polynomial coefficients.\n",
    "    coeffs, *_ = np.linalg.lstsq(X_poly, y_values, rcond=None)\n",
    "\n",
    "    # return learned coefficient vector for later predictions.\n",
    "    return coeffs\n",
    "\n",
    "# define a function predicting using polynomial coefficients.\n",
    "def predict_polynomial(x_values, coeffs):\n",
    "    # build Vandermonde matrix for polynomial features.\n",
    "    X_poly = np.vander(x_values[:, 0], len(coeffs), increasing=True)\n",
    "\n",
    "    # compute predictions using matrix multiplication.\n",
    "    return X_poly @ coeffs\n",
    "\n",
    "# define candidate polynomial degrees to explore greedily.\n",
    "candidate_degrees = list(range(1, 13))\n",
    "\n",
    "# prepare containers for tracking validation and test errors.\n",
    "val_errors = []\n",
    "\n",
    "# prepare containers for tracking test errors for each degree.\n",
    "test_errors = []\n",
    "\n",
    "# loop over candidate degrees simulating repeated peeking.\n",
    "for degree in candidate_degrees:\n",
    "    # fit model on training data using current degree.\n",
    "    coeffs = fit_polynomial_model(X_train, y_train, degree)\n",
    "\n",
    "    # compute validation predictions using current model.\n",
    "    val_pred = predict_polynomial(X_val, coeffs)\n",
    "\n",
    "    # compute test predictions using current model.\n",
    "    test_pred = predict_polynomial(X_test, coeffs)\n",
    "\n",
    "    # compute and store validation mean squared error.\n",
    "    val_mse = mean_squared_error(y_val, val_pred)\n",
    "\n",
    "    # compute and store test mean squared error.\n",
    "    test_mse = mean_squared_error(y_test, test_pred)\n",
    "\n",
    "    # append errors to tracking lists for later inspection.\n",
    "    val_errors.append(val_mse)\n",
    "\n",
    "    # append test errors to tracking lists for later inspection.\n",
    "    test_errors.append(test_mse)\n",
    "\n",
    "# convert error lists to numpy arrays for easier indexing.\n",
    "val_errors = np.array(val_errors)\n",
    "\n",
    "# convert test error list to numpy array for easier indexing.\n",
    "test_errors = np.array(test_errors)\n",
    "\n",
    "# find degree with smallest validation error after peeking.\n",
    "best_index = int(np.argmin(val_errors))\n",
    "\n",
    "# extract best degree and corresponding validation error.\n",
    "best_degree = candidate_degrees[best_index]\n",
    "\n",
    "# extract corresponding test error for the chosen degree.\n",
    "best_val_error = float(val_errors[best_index])\n",
    "\n",
    "# compute test error for a simple baseline degree.\n",
    "baseline_degree = 1\n",
    "\n",
    "# find index of baseline degree within candidate list.\n",
    "baseline_index = candidate_degrees.index(baseline_degree)\n",
    "\n",
    "# extract baseline validation and test errors.\n",
    "baseline_val_error = float(val_errors[baseline_index])\n",
    "\n",
    "# extract baseline test error from stored array.\n",
    "baseline_test_error = float(test_errors[baseline_index])\n",
    "\n",
    "# extract test error for the best validation degree.\n",
    "best_test_error = float(test_errors[best_index])\n",
    "\n",
    "# print short header explaining the experiment scenario.\n",
    "print(\"Simulating peeking at validation to choose model complexity.\")\n",
    "\n",
    "# print baseline model degree and its validation error.\n",
    "print(\"Baseline degree\", baseline_degree, \"validation MSE\", round(baseline_val_error, 4))\n",
    "\n",
    "# print baseline model degree and its test error.\n",
    "print(\"Baseline degree\", baseline_degree, \"test MSE\", round(baseline_test_error, 4))\n",
    "\n",
    "# print chosen degree after repeated validation peeking.\n",
    "print(\"Chosen degree after peeking\", best_degree, \"validation MSE\", round(best_val_error, 4))\n",
    "\n",
    "# print test error for the chosen degree to show optimism.\n",
    "print(\"Chosen degree after peeking\", best_degree, \"test MSE\", round(best_test_error, 4))\n",
    "\n",
    "# print short interpretation highlighting optimistic validation performance.\n",
    "print(\"Notice validation looks best for\", best_degree, \"but test error is worse.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9751e610",
   "metadata": {},
   "source": [
    "### **3.3. Common Leakage Scenarios**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c95afa",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_07/Lecture_A/image_03_03.jpg?v=1769963789\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Data leakage happens when future information leaks\n",
    ">* Same customer in train and test inflates performance\n",
    "\n",
    ">* Preprocessing must be learned only from training data\n",
    ">* Using full data for scaling or selection inflates performance\n",
    "\n",
    ">* Using future-only data secretly boosts evaluation scores\n",
    ">* Match training information to real prediction-time availability\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9372d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Common Leakage Scenarios\n",
    "\n",
    "# This script shows simple data leakage examples.\n",
    "# It compares correct and incorrect dataset splitting.\n",
    "# Focus on how leakage inflates model performance.\n",
    "\n",
    "# import required built in modules only.\n",
    "import random\n",
    "import statistics\n",
    "\n",
    "# set deterministic random seed for reproducibility.\n",
    "random.seed(42)\n",
    "\n",
    "# generate tiny synthetic customer dataset with labels.\n",
    "customers = [\n",
    "    {\"id\": cid, \"income\": 30000 + cid * 1000,\n",
    "     \"debt\": 1000 + (cid % 3) * 500,\n",
    "     \"default\": 1 if cid % 4 == 0 else 0}\n",
    "    for cid in range(1, 13)\n",
    "]\n",
    "\n",
    "# verify dataset size is small and safe.\n",
    "assert len(customers) == 12\n",
    "\n",
    "# define simple accuracy function for predictions.\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    correct = sum(1 for t, p in zip(y_true, y_pred) if t == p)\n",
    "\n",
    "    return correct / len(y_true) if y_true else 0.0\n",
    "\n",
    "\n",
    "# build a naive rule based model from training data.\n",
    "\n",
    "def train_rule_model(train_rows):\n",
    "    incomes_default = [r[\"income\"] for r in train_rows if r[\"default\"] == 1]\n",
    "    incomes_ok = [r[\"income\"] for r in train_rows if r[\"default\"] == 0]\n",
    "\n",
    "    if not incomes_default or not incomes_ok:\n",
    "        threshold = statistics.mean(r[\"income\"] for r in train_rows)\n",
    "    else:\n",
    "        threshold = (statistics.mean(incomes_default) + statistics.mean(incomes_ok)) / 2\n",
    "\n",
    "    return threshold\n",
    "\n",
    "\n",
    "# use the learned threshold to make predictions.\n",
    "\n",
    "def predict_with_threshold(rows, threshold):\n",
    "\n",
    "    return [1 if r[\"income\"] >= threshold else 0 for r in rows]\n",
    "\n",
    "\n",
    "# create a leaky split where same customers appear twice.\n",
    "leaky_train = []\n",
    "leaky_test = []\n",
    "for row in customers:\n",
    "    duplicate = dict(row)\n",
    "    leaky_train.append(row)\n",
    "    leaky_test.append(duplicate)\n",
    "\n",
    "# train model on leaky training data.\n",
    "leaky_threshold = train_rule_model(leaky_train)\n",
    "\n",
    "# evaluate model on leaky test data.\n",
    "leaky_preds = predict_with_threshold(leaky_test, leaky_threshold)\n",
    "leaky_true = [r[\"default\"] for r in leaky_test]\n",
    "leaky_acc = accuracy(leaky_true, leaky_preds)\n",
    "\n",
    "# create a proper split by separating customer identities.\n",
    "proper_train = customers[:8]\n",
    "proper_test = customers[8:]\n",
    "\n",
    "# ensure no overlapping customer identifiers.\n",
    "assert not {r[\"id\"] for r in proper_train}.intersection({r[\"id\"] for r in proper_test})\n",
    "\n",
    "# train model on proper training data only.\n",
    "proper_threshold = train_rule_model(proper_train)\n",
    "\n",
    "# evaluate model on truly unseen customers.\n",
    "proper_preds = predict_with_threshold(proper_test, proper_threshold)\n",
    "proper_true = [r[\"default\"] for r in proper_test]\n",
    "proper_acc = accuracy(proper_true, proper_preds)\n",
    "\n",
    "# print concise comparison of both scenarios.\n",
    "print(\"Leaky split accuracy looks unrealistically high:\", round(leaky_acc, 2))\n",
    "print(\"Proper split accuracy is more realistic:\", round(proper_acc, 2))\n",
    "print(\"Same rule, different splitting, different trust level.\")\n",
    "print(\"Leakage happens when test customers influence training.\")\n",
    "print(\"Always design splits that mimic real deployment.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03417998",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Splits And Validation**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec20ab1",
   "metadata": {},
   "source": [
    "\n",
    "In this lecture, you learned to:\n",
    "- Describe the roles of training, validation, and test splits in model development. \n",
    "- Propose simple ways to split a small dataset into separate parts. \n",
    "- Explain how improper splitting can lead to overly optimistic performance estimates. \n",
    "\n",
    "In the next Lecture (Lecture B), we will go over 'Cleaning And Scaling'"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
