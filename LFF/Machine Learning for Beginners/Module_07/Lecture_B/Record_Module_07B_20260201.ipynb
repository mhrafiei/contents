{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e758bcbc",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Cleaning And Scaling**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794a9b66",
   "metadata": {},
   "source": [
    ">Last update: 20260201.\n",
    "    \n",
    "By the end of this Lecture, you will be able to:\n",
    "- Identify common data issues such as missing values and inconsistent formats. \n",
    "- Describe simple strategies for handling missing values in features. \n",
    "- Explain why scaling numeric features can help some models behave more sensibly. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8aec499",
   "metadata": {},
   "source": [
    "## **1. Common Data Issues**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504c3aa0",
   "metadata": {},
   "source": [
    "### **1.1. Handling Missing Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddee253",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_07/Lecture_B/image_01_01.jpg?v=1769965126\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Missing data are absent, unknown, or unusable values\n",
    ">* They appear as blanks, codes, or impossible numbers\n",
    "\n",
    ">* Missing data can be random or systematic\n",
    ">* Check where gaps cluster and what they correlate with\n",
    "\n",
    ">* Use checks and domain knowledge to spot missingness\n",
    ">* Question odd values, gaps, and placeholder codes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f851f3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Handling Missing Data\n",
    "\n",
    "# This script shows basic missing data handling.\n",
    "# It uses a tiny table with simple issues.\n",
    "# Focus on detecting and counting missing entries.\n",
    "\n",
    "# import pandas for simple table handling.\n",
    "import pandas as pd\n",
    "\n",
    "# create a tiny dataset with obvious missing values.\n",
    "data = {\n",
    "    \"age\": [25, None, 40, -1],\n",
    "    \"city\": [\"Paris\", \"\", None, \"Berlin\"],\n",
    "}\n",
    "\n",
    "# build a DataFrame from the dictionary.\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# replace impossible age values with proper missing markers.\n",
    "df.loc[df[\"age\"] < 0, \"age\"] = pd.NA\n",
    "\n",
    "# treat empty city strings as missing markers also.\n",
    "df.loc[df[\"city\"] == \"\", \"city\"] = pd.NA\n",
    "\n",
    "# show the small cleaned table with missing markers.\n",
    "print(\"Cleaned tiny dataset with missing markers:\")\n",
    "print(df)\n",
    "\n",
    "# count missing values in each column for inspection.\n",
    "missing_counts = df.isna().sum()\n",
    "\n",
    "# print a short summary of missing values per column.\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(missing_counts)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08d9208",
   "metadata": {},
   "source": [
    "### **1.2. Unit Mismatches**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c334c4",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_07/Lecture_B/image_01_02.jpg?v=1769965158\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Same feature recorded in different units misleads\n",
    ">* Model learns unit errors instead of real patterns\n",
    "\n",
    ">* Unit confusion appears across domains and sources\n",
    ">* Plausible numbers hide issues, causing bad models\n",
    "\n",
    ">* Inspect distributions and clusters to spot inconsistent units\n",
    ">* Use metadata and experts to standardize measurements\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6654d9",
   "metadata": {},
   "source": [
    "### **1.3. Detecting Outliers**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad78e566",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_07/Lecture_B/image_01_03.jpg?v=1769965172\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Outliers are values far from most data\n",
    ">* They may be real events or errors\n",
    "\n",
    ">* Use plots and summaries to spot extremes\n",
    ">* Unusual values may be real events or errors\n",
    "\n",
    ">* Use judgment and context when evaluating extreme values\n",
    ">* Distinguish meaningful rare events from errors or noise\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae58458",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Detecting Outliers\n",
    "\n",
    "# This script shows simple numeric outlier detection.\n",
    "# It uses pandas and matplotlib for tiny examples.\n",
    "# Focus on visualizing unusual values in small data.\n",
    "\n",
    "# !pip install pandas matplotlib seaborn.\n",
    "\n",
    "# Import required libraries for data handling.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set deterministic random seed for reproducibility.\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create mostly normal purchase amounts with noise.\n",
    "normal_purchases = np.random.normal(loc=50.0, scale=10.0, size=40)\n",
    "\n",
    "# Manually add a few extreme purchase amounts.\n",
    "outlier_purchases = np.array([5.0, 200.0, 300.0, 400.0])\n",
    "\n",
    "# Combine normal and outlier purchases into one array.\n",
    "all_purchases = np.concatenate((normal_purchases, outlier_purchases))\n",
    "\n",
    "# Build a DataFrame to hold purchase amounts.\n",
    "data = pd.DataFrame({\"purchase_amount\": all_purchases})\n",
    "\n",
    "# Compute simple summary statistics for quick overview.\n",
    "summary = data[\"purchase_amount\"].describe()\n",
    "\n",
    "# Print summary to see min max and quartiles.\n",
    "print(\"Purchase amount summary statistics:\\n\", summary)\n",
    "\n",
    "# Compute interquartile range for rule based detection.\n",
    "q1 = summary[\"25%\"]\n",
    "q3 = summary[\"75%\"]\n",
    "\n",
    "# Calculate IQR and lower and upper detection bounds.\n",
    "iqr = q3 - q1\n",
    "lower_bound = q1 - 1.5 * iqr\n",
    "upper_bound = q3 + 1.5 * iqr\n",
    "\n",
    "# Create boolean mask for suspected outlier rows.\n",
    "outlier_mask = (\n",
    "    (data[\"purchase_amount\"] < lower_bound)\n",
    "    | (data[\"purchase_amount\"] > upper_bound)\n",
    ")\n",
    "\n",
    "# Select only rows flagged as potential outliers.\n",
    "suspected_outliers = data.loc[outlier_mask, \"purchase_amount\"]\n",
    "\n",
    "# Print detected outliers with their index positions.\n",
    "print(\"\\nSuspected outliers using IQR rule:\")\n",
    "print(suspected_outliers)\n",
    "\n",
    "# Create a simple boxplot to visualize outliers.\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.boxplot(data[\"purchase_amount\"], vert=True)\n",
    "\n",
    "# Label axes and title for clear interpretation.\n",
    "plt.ylabel(\"Purchase amount in dollars\")\n",
    "plt.title(\"Boxplot highlighting potential purchase outliers\")\n",
    "\n",
    "# Display the plot to visually inspect unusual values.\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee4979a",
   "metadata": {},
   "source": [
    "## **2. Handling Missing Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afa2d22",
   "metadata": {},
   "source": [
    "### **2.1. Removing Incomplete Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e617f8bf",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_07/Lecture_B/image_02_01.jpg?v=1769965219\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Drop rows or features with missing values\n",
    ">* Method is simple but can remove important information\n",
    "\n",
    ">* Use missing proportion and feature importance together\n",
    ">* Drop rare incomplete rows or mostly empty features\n",
    "\n",
    ">* Dropping missing data can bias your results\n",
    ">* Check missingness patterns and trade-offs before deleting\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8587bf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Removing Incomplete Data\n",
    "\n",
    "# This script shows removing incomplete data safely.\n",
    "# We use pandas to drop missing rows simply.\n",
    "# Focus on understanding what gets removed carefully.\n",
    "\n",
    "# import pandas for simple table handling.\n",
    "import pandas as pd\n",
    "\n",
    "# create a tiny survey style dataset.\n",
    "data = {\n",
    "    \"age\": [25, 30, None, 40],\n",
    "    \"satisfaction\": [4, None, 5, 3],\n",
    "    \"comment\": [\"good\", \"ok\", \"great\", None],\n",
    "}\n",
    "\n",
    "# build a DataFrame from the dictionary.\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# show the original data with missing values.\n",
    "print(\"Original data with possible missing values:\")\n",
    "print(df)\n",
    "\n",
    "# count missing values per column for inspection.\n",
    "missing_counts = df.isna().sum()\n",
    "\n",
    "# print a short summary of missing values.\n",
    "print(\"\\nMissing values per column before dropping:\")\n",
    "print(missing_counts)\n",
    "\n",
    "# drop rows where any value is missing completely.\n",
    "df_dropped_any = df.dropna(how=\"any\")\n",
    "\n",
    "# show the remaining rows after dropping any missing.\n",
    "print(\"\\nData after dropping rows with any missing:\")\n",
    "print(df_dropped_any)\n",
    "\n",
    "# drop rows only if satisfaction is missing specifically.\n",
    "df_dropped_target = df.dropna(subset=[\"satisfaction\"])\n",
    "\n",
    "# show data where satisfaction is always present.\n",
    "print(\"\\nData after keeping rows with satisfaction present:\")\n",
    "print(df_dropped_target)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c88749",
   "metadata": {},
   "source": [
    "### **2.2. Basic Imputation Methods**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0e127d",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_07/Lecture_B/image_02_02.jpg?v=1769965253\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Fill missing values using simple imputation guesses\n",
    ">* Use mean or median to keep data\n",
    "\n",
    ">* Fill missing categories with the most common value\n",
    ">* Method is simple but can add bias\n",
    "\n",
    ">* Use nearby time or space values to fill\n",
    ">* Assumes gradual change; imputed values add uncertainty\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c497ea16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Basic Imputation Methods\n",
    "\n",
    "# This script shows basic imputation methods.\n",
    "# It uses a tiny housing style dataset.\n",
    "# Focus is on simple missing value strategies.\n",
    "\n",
    "# import required libraries for data handling.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# set a deterministic random seed for reproducibility.\n",
    "np.random.seed(42)\n",
    "\n",
    "# create a tiny dataset with some missing values.\n",
    "data = {\n",
    "    \"size_sqm\": [50, 60, None, 80, 90, None],\n",
    "    \"bedrooms\": [1, 2, 2, None, 3, None],\n",
    "    \"city\": [\n",
    "        \"Springfield\",\n",
    "        \"Springfield\",\n",
    "        None,\n",
    "        \"Shelbyville\",\n",
    "        \"Springfield\",\n",
    "        None,\n",
    "    ],\n",
    "}\n",
    "\n",
    "# build a pandas dataframe from the dictionary.\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# show the original data with missing values.\n",
    "print(\"Original data with missing values:\")\n",
    "print(df)\n",
    "\n",
    "# compute mean and median for numeric size feature.\n",
    "size_mean = df[\"size_sqm\"].mean()\n",
    "size_median = df[\"size_sqm\"].median()\n",
    "\n",
    "# compute mode for the categorical city feature.\n",
    "city_mode_series = df[\"city\"].mode()\n",
    "city_mode = city_mode_series.iloc[0] if not city_mode_series.empty else None\n",
    "\n",
    "# create copies for different imputation strategies.\n",
    "df_mean_imputed = df.copy()\n",
    "df_median_imputed = df.copy()\n",
    "df_mode_imputed = df.copy()\n",
    "\n",
    "# fill numeric size using mean imputation.\n",
    "df_mean_imputed[\"size_sqm\"] = df_mean_imputed[\"size_sqm\"].fillna(size_mean)\n",
    "\n",
    "# fill numeric size using median imputation.\n",
    "df_median_imputed[\"size_sqm\"] = df_median_imputed[\"size_sqm\"].fillna(\n",
    "    size_median\n",
    ")\n",
    "\n",
    "# fill categorical city using mode imputation.\n",
    "if city_mode is not None:\n",
    "    df_mode_imputed[\"city\"] = df_mode_imputed[\"city\"].fillna(city_mode)\n",
    "\n",
    "# demonstrate forward fill on bedrooms as a time like feature.\n",
    "df_ffill = df.copy()\n",
    "df_ffill[\"bedrooms\"] = df_ffill[\"bedrooms\"].ffill()\n",
    "\n",
    "# print summary of each imputation result.\n",
    "print(\"\\nMean imputed size_sqm column:\")\n",
    "print(df_mean_imputed[\"size_sqm\"])\n",
    "\n",
    "print(\"\\nMedian imputed size_sqm column:\")\n",
    "print(df_median_imputed[\"size_sqm\"])\n",
    "\n",
    "print(\"\\nMode imputed city column:\")\n",
    "print(df_mode_imputed[\"city\"])\n",
    "\n",
    "print(\"\\nForward filled bedrooms column:\")\n",
    "print(df_ffill[\"bedrooms\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4e884d",
   "metadata": {},
   "source": [
    "### **2.3. Recording Data Assumptions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9810c2ca",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_07/Lecture_B/image_02_03.jpg?v=1769965316\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Handling missing data always involves hidden assumptions\n",
    ">* Write assumptions clearly to ensure transparency and critique\n",
    "\n",
    ">* Link each missing-data choice to its reason\n",
    ">* Document assumptions and impacts in shared project notes\n",
    "\n",
    ">* Note uncertainty and bias from missing-data choices\n",
    ">* Document impacts to compare methods and ensure fairness\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fdaa6c",
   "metadata": {},
   "source": [
    "## **3. Why Scale Features**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508068b5",
   "metadata": {},
   "source": [
    "### **3.1. Uneven Feature Scales**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f5e9b4",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_07/Lecture_B/image_03_01.jpg?v=1769965330\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Different features can have very different ranges\n",
    ">* Algorithms treat large numbers as more important features\n",
    "\n",
    ">* Uneven scales let large-magnitude features dominate learning\n",
    ">* Small-range but important features may be ignored\n",
    "\n",
    ">* Uneven scales hide true feature importance in models\n",
    ">* They distort similarity, leading to biased decisions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b9b076",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Uneven Feature Scales\n",
    "\n",
    "# This script shows uneven feature scales clearly.\n",
    "# We compare raw and scaled numeric house features.\n",
    "# Focus on how large scales dominate distances.\n",
    "\n",
    "# import required numeric and plotting libraries.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# set deterministic random seed for reproducibility.\n",
    "np.random.seed(42)\n",
    "\n",
    "# create tiny synthetic house dataset with uneven scales.\n",
    "sizes = np.array([60, 80, 120, 200, 250], dtype=float)\n",
    "\n",
    "# create prices with much larger numeric magnitudes.\n",
    "prices = np.array([120000, 150000, 220000, 400000, 520000], dtype=float)\n",
    "\n",
    "# create bedroom counts with very small magnitudes.\n",
    "bedrooms = np.array([2, 2, 3, 4, 5], dtype=float)\n",
    "\n",
    "# stack features into matrix with shape validation.\n",
    "features = np.column_stack((sizes, prices, bedrooms))\n",
    "\n",
    "# confirm feature matrix shape is as expected.\n",
    "print(\"Feature matrix shape:\", features.shape)\n",
    "\n",
    "# choose reference house index for distance comparisons.\n",
    "ref_index = 0\n",
    "\n",
    "# compute raw Euclidean distances to reference house.\n",
    "raw_diffs = features - features[ref_index]\n",
    "raw_distances = np.sqrt(np.sum(raw_diffs ** 2, axis=1))\n",
    "\n",
    "# print raw distances to show price domination.\n",
    "print(\"Raw distances from first house:\")\n",
    "print(np.round(raw_distances, 2))\n",
    "\n",
    "# manually scale each feature to zero mean unit variance.\n",
    "means = features.mean(axis=0)\n",
    "stds = features.std(axis=0)\n",
    "\n",
    "# avoid division by zero using safe replacement.\n",
    "stds_safe = np.where(stds == 0, 1.0, stds)\n",
    "\n",
    "# compute standardized feature matrix using safe denominators.\n",
    "features_scaled = (features - means) / stds_safe\n",
    "\n",
    "# compute scaled distances to same reference house.\n",
    "scaled_diffs = features_scaled - features_scaled[ref_index]\n",
    "scaled_distances = np.sqrt(np.sum(scaled_diffs ** 2, axis=1))\n",
    "\n",
    "# print scaled distances where features contribute comparably.\n",
    "print(\"Scaled distances from first house:\")\n",
    "print(np.round(scaled_distances, 2))\n",
    "\n",
    "# prepare x positions for plotting raw and scaled distances.\n",
    "indices = np.arange(features.shape[0])\n",
    "\n",
    "# create bar plot comparing raw and scaled distances.\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar(indices - 0.15, raw_distances, width=0.3, label=\"Raw distances\")\n",
    "\n",
    "# add bars for scaled distances beside raw distances.\n",
    "plt.bar(indices + 0.15, scaled_distances, width=0.3, label=\"Scaled distances\")\n",
    "\n",
    "# label axes and add legend for clarity.\n",
    "plt.xlabel(\"House index\")\n",
    "plt.ylabel(\"Distance from first house\")\n",
    "plt.title(\"Effect of uneven feature scales on distances\")\n",
    "plt.legend()\n",
    "\n",
    "# display the plot to visually compare distances.\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09fe62a",
   "metadata": {},
   "source": [
    "### **3.2. Model Sensitivity to Scale**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b59245f",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_07/Lecture_B/image_03_02.jpg?v=1769965391\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Large-number features can unfairly dominate learning\n",
    ">* Distance-based models ignore small-scale feature differences\n",
    "\n",
    ">* Large-scale features dominate gradients and slow learning\n",
    ">* Distance-based models overemphasize big-number features\n",
    "\n",
    ">* Unscaled features distort coefficient penalties and interpretation\n",
    ">* Scaling makes regularization fair and reflects true importance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8511be50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Model Sensitivity to Scale\n",
    "\n",
    "# This script shows why feature scaling matters.\n",
    "# We compare distances before and after scaling.\n",
    "# Focus on how one large feature dominates distances.\n",
    "\n",
    "# Required libraries are already available in Colab.\n",
    "# Uncomment next line only if numpy is missing.\n",
    "# pip install numpy.\n",
    "\n",
    "# Import numpy for simple numeric operations.\n",
    "import numpy as np\n",
    "\n",
    "# Set a deterministic random seed for reproducibility.\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create tiny dataset with two numeric features.\n",
    "# Feature one is small, feature two is very large.\n",
    "small_feature = np.array([1.0, 2.0, 3.0, 4.0])\n",
    "large_feature = np.array([1000.0, 2000.0, 3000.0, 4000.0])\n",
    "\n",
    "# Stack features into a matrix with shape checks.\n",
    "X = np.column_stack((small_feature, large_feature))\n",
    "assert X.shape == (4, 2)\n",
    "\n",
    "# Choose a reference point similar to first row.\n",
    "reference_point = np.array([1.5, 1500.0])\n",
    "assert reference_point.shape == (2,)\n",
    "\n",
    "# Define a simple Euclidean distance function.\n",
    "def euclidean_distance(a, b):\n",
    "    diff = a - b\n",
    "    return float(np.sqrt(np.sum(diff ** 2)))\n",
    "\n",
    "# Compute distances before scaling to show dominance.\n",
    "distances_raw = []\n",
    "for row in X:\n",
    "    distances_raw.append(euclidean_distance(row, reference_point))\n",
    "\n",
    "# Manually standardize each feature using mean and std.\n",
    "means = X.mean(axis=0)\n",
    "stds = X.std(axis=0)\n",
    "\n",
    "# Avoid division by zero using safe replacement.\n",
    "stds_safe = np.where(stds == 0, 1.0, stds)\n",
    "\n",
    "# Scale dataset and reference point using statistics.\n",
    "X_scaled = (X - means) / stds_safe\n",
    "reference_scaled = (reference_point - means) / stds_safe\n",
    "\n",
    "# Validate scaled shapes before computing distances.\n",
    "assert X_scaled.shape == X.shape\n",
    "assert reference_scaled.shape == reference_point.shape\n",
    "\n",
    "# Compute distances after scaling to compare effects.\n",
    "distances_scaled = []\n",
    "for row in X_scaled:\n",
    "    distances_scaled.append(euclidean_distance(row, reference_scaled))\n",
    "\n",
    "# Print raw and scaled distances side by side.\n",
    "print(\"Index  Raw_Distance  Scaled_Distance\")\n",
    "for i, (d_raw, d_scaled) in enumerate(zip(distances_raw, distances_scaled)):\n",
    "    print(i, round(d_raw, 2), round(d_scaled, 2))\n",
    "\n",
    "# Print short explanation about model sensitivity.\n",
    "print(\"\\nNotice how scaling balances feature influence on distance.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6075a723",
   "metadata": {},
   "source": [
    "### **3.3. Normalization Basics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e10851c",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_07/Lecture_B/image_03_03.jpg?v=1769965414\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Normalization puts numeric features on similar scales\n",
    ">* Keeps information while preventing one feature dominating\n",
    "\n",
    ">* Turn different units into comparable standardized scores\n",
    ">* Keep order of values while adjusting their scale\n",
    "\n",
    ">* Normalization stabilizes training and speeds model learning\n",
    ">* It prevents large-scale features from overpowering others\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc7b4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Normalization Basics\n",
    "\n",
    "# This script shows basic feature normalization concepts.\n",
    "# We compare raw and normalized numeric feature scales.\n",
    "# Focus is on simple min max and zscore.\n",
    "\n",
    "# No extra installs are required for this script.\n",
    "# All used libraries are available by default.\n",
    "\n",
    "# Import numpy for numeric array handling.\n",
    "import numpy as np\n",
    "\n",
    "# Create a tiny dataset with different feature scales.\n",
    "raw_data = np.array([\n",
    "    [20.0, 50000.0],\n",
    "    [35.0, 80000.0],\n",
    "    [50.0, 120000.0],\n",
    "    [65.0, 200000.0],\n",
    "])\n",
    "\n",
    "# Print the raw data to inspect scales.\n",
    "print(\"Raw data age income:\", raw_data)\n",
    "\n",
    "# Compute column wise minimum and maximum values.\n",
    "col_min = raw_data.min(axis=0)\n",
    "col_max = raw_data.max(axis=0)\n",
    "\n",
    "# Apply simple min max normalization per column.\n",
    "minmax_data = (raw_data - col_min) / (col_max - col_min)\n",
    "\n",
    "# Print normalized data to compare scales.\n",
    "print(\"Minmax normalized data values:\", minmax_data)\n",
    "\n",
    "# Compute column wise mean and standard deviation.\n",
    "col_mean = raw_data.mean(axis=0)\n",
    "col_std = raw_data.std(axis=0)\n",
    "\n",
    "# Avoid division by zero using safe replacement.\n",
    "col_std_safe = np.where(col_std == 0.0, 1.0, col_std)\n",
    "\n",
    "# Apply zscore normalization centering and scaling.\n",
    "zscore_data = (raw_data - col_mean) / col_std_safe\n",
    "\n",
    "# Print zscore normalized data for comparison.\n",
    "print(\"Zscore normalized data values:\", zscore_data)\n",
    "\n",
    "# Show how a simple distance changes after scaling.\n",
    "point_a = raw_data[0]\n",
    "point_b = raw_data[3]\n",
    "\n",
    "# Compute Euclidean distance on raw features.\n",
    "raw_distance = np.linalg.norm(point_a - point_b)\n",
    "\n",
    "# Compute Euclidean distance on minmax normalized features.\n",
    "minmax_distance = np.linalg.norm(\n",
    "    minmax_data[0] - minmax_data[3]\n",
    ")\n",
    "\n",
    "# Print both distances to highlight scale effects.\n",
    "print(\"Raw distance value:\", float(raw_distance))\n",
    "print(\"Minmax distance value:\", float(minmax_distance))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22ddf96",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Cleaning And Scaling**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1939d3fe",
   "metadata": {},
   "source": [
    "\n",
    "In this lecture, you learned to:\n",
    "- Identify common data issues such as missing values and inconsistent formats. \n",
    "- Describe simple strategies for handling missing values in features. \n",
    "- Explain why scaling numeric features can help some models behave more sensibly. \n",
    "\n",
    "In the next Module (Module 8), we will go over 'Nonlinear Models'"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
