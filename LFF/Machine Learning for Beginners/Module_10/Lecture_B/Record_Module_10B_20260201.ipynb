{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41d92f85",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**From Idea To Use**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab4b1bb",
   "metadata": {},
   "source": [
    ">Last update: 20260201.\n",
    "    \n",
    "By the end of this Lecture, you will be able to:\n",
    "- Outline the main stages of a simple machine learning project lifecycle. \n",
    "- Relate each stage of the lifecycle to concepts learned earlier in the course. \n",
    "- Explain why ongoing monitoring and revision are important after a model is first deployed. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9827e0c",
   "metadata": {},
   "source": [
    "## **1. ML Project Lifecycle**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81583cf4",
   "metadata": {},
   "source": [
    "### **1.1. Defining the ML Problem**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f680e105",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_10/Lecture_B/image_01_01.jpg?v=1769975369\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Turn vague AI ideas into specific problems\n",
    ">* Define inputs, outputs, stakeholders, and real use\n",
    "\n",
    ">* Turn plain-language goals into specific ML tasks\n",
    ">* Define examples, features, labels, and practical constraints\n",
    "\n",
    ">* Identify ethical, legal, and data constraints early\n",
    ">* Ensure models remain fair, transparent, and accountable\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cc32ae",
   "metadata": {},
   "source": [
    "### **1.2. Building Models from Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0598dca",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_10/Lecture_B/image_01_02.jpg?v=1769975380\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Prepare and clean data into useful features\n",
    ">* Choose suitable models, balancing performance and interpretability\n",
    "\n",
    ">* Model training adjusts parameters to match patterns\n",
    ">* Hyperparameter choices affect fit and generalization\n",
    "\n",
    ">* Model building is iterative, testing many options\n",
    ">* Aim for models balancing accuracy, robustness, fairness\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7341e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Building Models from Data\n",
    "\n",
    "# This script shows building simple models from data.\n",
    "# We use tiny synthetic data for clarity.\n",
    "# Focus is on training and evaluating responsibly.\n",
    "\n",
    "# import required built in and numerical libraries.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# set deterministic random seed for reproducibility.\n",
    "np.random.seed(42)\n",
    "\n",
    "# create tiny synthetic dataset for a regression task.\n",
    "size = 30\n",
    "x = np.linspace(0, 10, size)\n",
    "\n",
    "# generate target values with a simple linear pattern.\n",
    "noise = np.random.normal(loc=0.0, scale=1.0, size=size)\n",
    "y = 2.0 * x + 3.0 + noise\n",
    "\n",
    "# place data into a pandas dataframe structure.\n",
    "data = pd.DataFrame({\"feature_x\": x, \"target_y\": y})\n",
    "\n",
    "# check dataset shape to ensure expected small size.\n",
    "rows, cols = data.shape\n",
    "assert rows == size and cols == 2\n",
    "\n",
    "# split data into training and testing subsets.\n",
    "train_fraction = 0.7\n",
    "train_size = int(train_fraction * size)\n",
    "\n",
    "# use simple index based splitting for clarity.\n",
    "train_data = data.iloc[:train_size].copy()\n",
    "test_data = data.iloc[train_size:].copy()\n",
    "\n",
    "# separate features and targets for training and testing.\n",
    "X_train = train_data[\"feature_x\"].values\n",
    "y_train = train_data[\"target_y\"].values\n",
    "\n",
    "# reshape features to column vectors for matrix operations.\n",
    "X_train_matrix = np.vstack([np.ones_like(X_train), X_train]).T\n",
    "\n",
    "# compute linear regression parameters using normal equation.\n",
    "XtX = X_train_matrix.T @ X_train_matrix\n",
    "Xty = X_train_matrix.T @ y_train\n",
    "\n",
    "# solve for parameters with small regularization for stability.\n",
    "lambda_identity = 1e-6 * np.eye(XtX.shape[0])\n",
    "params = np.linalg.solve(XtX + lambda_identity, Xty)\n",
    "\n",
    "# extract intercept and slope from learned parameters.\n",
    "intercept, slope = params[0], params[1]\n",
    "\n",
    "# prepare testing features and targets for evaluation.\n",
    "X_test = test_data[\"feature_x\"].values\n",
    "y_test = test_data[\"target_y\"].values\n",
    "\n",
    "# reshape testing features for prediction matrix multiplication.\n",
    "X_test_matrix = np.vstack([np.ones_like(X_test), X_test]).T\n",
    "\n",
    "# generate predictions on testing data using learned model.\n",
    "y_pred = X_test_matrix @ params\n",
    "\n",
    "# compute mean squared error as evaluation metric.\n",
    "errors = y_test - y_pred\n",
    "mse = float(np.mean(errors ** 2))\n",
    "\n",
    "# print concise summary of lifecycle style steps.\n",
    "print(\"Data rows and columns:\", rows, cols)\n",
    "print(\"Training size and testing size:\", train_size, size - train_size)\n",
    "print(\"Learned intercept and slope:\", round(intercept, 2), round(slope, 2))\n",
    "print(\"Test mean squared error:\", round(mse, 3))\n",
    "\n",
    "# create scatter plot of data and learned regression line.\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.scatter(data[\"feature_x\"], data[\"target_y\"], label=\"data points\")\n",
    "\n",
    "# create line values using learned model parameters.\n",
    "line_x = np.linspace(0, 10, 50)\n",
    "line_y = intercept + slope * line_x\n",
    "\n",
    "# plot learned regression line for visual comparison.\n",
    "plt.plot(line_x, line_y, color=\"red\", label=\"learned model\")\n",
    "\n",
    "# add labels and legend to explain the visualization.\n",
    "plt.xlabel(\"feature_x value\")\n",
    "plt.ylabel(\"target_y value\")\n",
    "\n",
    "# add title connecting plot to project lifecycle stage.\n",
    "plt.title(\"Building a simple model from data\")\n",
    "plt.legend()\n",
    "\n",
    "# display the final plot to complete this teaching example.\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca020e1d",
   "metadata": {},
   "source": [
    "### **1.3. Model Evaluation Choices**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd0d955",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_10/Lecture_B/image_01_03.jpg?v=1769975425\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Define good performance for the specific context\n",
    ">* Choose metrics based on acceptable and harmful errors\n",
    "\n",
    ">* Split data into training and unseen test sets\n",
    ">* Testing on unseen data prevents overoptimistic performance\n",
    "\n",
    ">* Compare models, balancing accuracy, simplicity, and transparency\n",
    ">* Include fairness, robustness, and real-world trade-offs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a351fcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Model Evaluation Choices\n",
    "\n",
    "# This script illustrates simple model evaluation choices.\n",
    "# We compare two tiny models on a toy dataset.\n",
    "# Focus on accuracy versus different error types.\n",
    "\n",
    "# No extra installs are required for this script.\n",
    "# Required libraries are available in this environment.\n",
    "\n",
    "# Import numpy for simple numeric operations.\n",
    "import numpy as np\n",
    "\n",
    "# Set a deterministic random seed for reproducibility.\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create a tiny synthetic binary classification dataset.\n",
    "features = np.array([\n",
    "    [0.1, 0.2],\n",
    "    [0.2, 0.1],\n",
    "    [0.8, 0.9],\n",
    "    [0.9, 0.8],\n",
    "\n",
    "    [0.15, 0.18],\n",
    "    [0.82, 0.88],\n",
    "    [0.4, 0.3],\n",
    "    [0.7, 0.6],\n",
    "])\n",
    "\n",
    "# Create labels where 1 means positive class and 0 negative.\n",
    "labels = np.array([0, 0, 1, 1, 0, 1, 0, 1])\n",
    "\n",
    "# Validate shapes before continuing to avoid mistakes.\n",
    "assert features.shape[0] == labels.shape[0]\n",
    "\n",
    "# Define a simple function to split train and test.\n",
    "def train_test_split_indices(n_samples, test_fraction):\n",
    "    # Compute test size using fraction and integer rounding.\n",
    "    test_size = max(1, int(n_samples * test_fraction))\n",
    "\n",
    "    # Create shuffled indices deterministically using permutation.\n",
    "    indices = np.random.permutation(n_samples)\n",
    "\n",
    "    # Split indices into train and test subsets.\n",
    "    test_idx = indices[:test_size]\n",
    "    train_idx = indices[test_size:]\n",
    "\n",
    "    return train_idx, test_idx\n",
    "\n",
    "# Get train and test indices using the helper function.\n",
    "train_idx, test_idx = train_test_split_indices(len(labels), 0.25)\n",
    "\n",
    "# Create train and test sets using the computed indices.\n",
    "X_train, y_train = features[train_idx], labels[train_idx]\n",
    "\n",
    "# Create held out test features and labels for evaluation.\n",
    "X_test, y_test = features[test_idx], labels[test_idx]\n",
    "\n",
    "# Define a simple threshold based model using feature sums.\n",
    "def predict_threshold(data, threshold):\n",
    "    # Compute scores as sum of feature values per row.\n",
    "    scores = data.sum(axis=1)\n",
    "\n",
    "    # Predict positive class when score exceeds threshold.\n",
    "    predictions = (scores >= threshold).astype(int)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "# Define a helper to compute confusion counts.\n",
    "def confusion_counts(y_true, y_pred):\n",
    "    # True positives are predicted one and actually one.\n",
    "    tp = int(((y_true == 1) & (y_pred == 1)).sum())\n",
    "\n",
    "    # False positives are predicted one but actually zero.\n",
    "    fp = int(((y_true == 0) & (y_pred == 1)).sum())\n",
    "\n",
    "    # False negatives are predicted zero but actually one.\n",
    "    fn = int(((y_true == 1) & (y_pred == 0)).sum())\n",
    "\n",
    "    # True negatives are predicted zero and actually zero.\n",
    "    tn = int(((y_true == 0) & (y_pred == 0)).sum())\n",
    "\n",
    "    return tp, fp, fn, tn\n",
    "\n",
    "# Define a helper to compute accuracy safely.\n",
    "def accuracy_score(y_true, y_pred):\n",
    "    # Ensure shapes match before computing accuracy.\n",
    "    assert y_true.shape == y_pred.shape\n",
    "\n",
    "    # Compute proportion of correct predictions overall.\n",
    "    correct = (y_true == y_pred).sum()\n",
    "\n",
    "    return float(correct) / float(len(y_true))\n",
    "\n",
    "# Model A prefers catching positives using lower threshold.\n",
    "model_a_threshold = 1.0\n",
    "\n",
    "# Model B prefers avoiding false alarms using higher threshold.\n",
    "model_b_threshold = 1.5\n",
    "\n",
    "# Get predictions for both models on the test set.\n",
    "y_pred_a = predict_threshold(X_test, model_a_threshold)\n",
    "\n",
    "# Compute predictions for the second model on test data.\n",
    "y_pred_b = predict_threshold(X_test, model_b_threshold)\n",
    "\n",
    "# Compute confusion counts for both models.\n",
    "metrics_a = confusion_counts(y_test, y_pred_a)\n",
    "\n",
    "# Compute confusion counts for the second model.\n",
    "metrics_b = confusion_counts(y_test, y_pred_b)\n",
    "\n",
    "# Compute accuracy values for both models.\n",
    "acc_a = accuracy_score(y_test, y_pred_a)\n",
    "\n",
    "# Compute accuracy for the second model on test data.\n",
    "acc_b = accuracy_score(y_test, y_pred_b)\n",
    "\n",
    "# Unpack confusion counts for readability in printing.\n",
    "tp_a, fp_a, fn_a, tn_a = metrics_a\n",
    "\n",
    "# Unpack confusion counts for the second model.\n",
    "tp_b, fp_b, fn_b, tn_b = metrics_b\n",
    "\n",
    "# Print a short header explaining the comparison.\n",
    "print(\"Comparing two simple models on held out test data.\")\n",
    "\n",
    "# Print accuracy for both models using formatted strings.\n",
    "print(\"Model A accuracy prioritizing recall:\", round(acc_a, 3))\n",
    "\n",
    "# Print accuracy for the second model with different threshold.\n",
    "print(\"Model B accuracy prioritizing precision:\", round(acc_b, 3))\n",
    "\n",
    "# Print confusion details for model A showing error types.\n",
    "print(\"Model A TP FP FN TN:\", tp_a, fp_a, fn_a, tn_a)\n",
    "\n",
    "# Print confusion details for model B showing error types.\n",
    "print(\"Model B TP FP FN TN:\", tp_b, fp_b, fn_b, tn_b)\n",
    "\n",
    "# Final line prints a reminder about evaluation choices.\n",
    "print(\"Choose thresholds based on which mistakes matter more in context.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c291c28",
   "metadata": {},
   "source": [
    "## **2. Linking Core Concepts**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cc7011",
   "metadata": {},
   "source": [
    "### **2.1. Connecting Features And Targets**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1023397",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_10/Lecture_B/image_02_01.jpg?v=1769975482\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Features are inputs; targets are desired outcomes\n",
    ">* Choosing them well underpins safe, reliable projects\n",
    "\n",
    ">* Define the decision, then choose a target\n",
    ">* Select reliable features that reflect real goals\n",
    "\n",
    ">* Features and targets embed historical values and biases\n",
    ">* Rechecking them ensures fair, ethical model decisions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad588f1",
   "metadata": {},
   "source": [
    "### **2.2. Training and Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449a845c",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_10/Lecture_B/image_02_02.jpg?v=1769975493\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Training combines data splits, loss, optimization, generalization\n",
    ">* Model iteratively adjusts parameters to reduce prediction errors\n",
    "\n",
    ">* Hold out data to test generalization performance\n",
    ">* Use metrics and splits to spot overfitting, failures\n",
    "\n",
    ">* Balance bias and variance using training, evaluation\n",
    ">* Check fairness so models meet ethical standards\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9fbad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Training and Evaluation\n",
    "\n",
    "# This script illustrates simple training and evaluation stages.\n",
    "# We use tiny synthetic data for a regression example.\n",
    "# Focus on connecting lifecycle ideas to earlier concepts.\n",
    "\n",
    "# Required third party libraries would be installed like this.\n",
    "# pip install numpy.\n",
    "\n",
    "# Import numpy for numeric arrays and simple math.\n",
    "import numpy as np\n",
    "\n",
    "# Set deterministic random seed for reproducible tiny dataset.\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create small synthetic feature data and target values.\n",
    "X_all = np.linspace(0.0, 10.0, 20).reshape(-1, 1)\n",
    "\n",
    "y_all = 2.0 * X_all.reshape(-1) + 1.0 + np.random.normal(\n",
    "    loc=0.0,\n",
    "    scale=1.0,\n",
    "    size=X_all.shape[0],\n",
    ")\n",
    "\n",
    "# Validate shapes before splitting into training and test sets.\n",
    "assert X_all.shape[0] == y_all.shape[0]\n",
    "\n",
    "# Define simple index based split for training and test sets.\n",
    "split_index = int(0.7 * X_all.shape[0])\n",
    "\n",
    "# Slice arrays to obtain training subset for model fitting.\n",
    "X_train = X_all[:split_index]\n",
    "\n",
    "y_train = y_all[:split_index]\n",
    "\n",
    "# Slice arrays to obtain test subset for later evaluation.\n",
    "X_test = X_all[split_index:]\n",
    "\n",
    "y_test = y_all[split_index:]\n",
    "\n",
    "# Initialize linear model parameters for slope and intercept.\n",
    "weight = 0.0\n",
    "\n",
    "# Initialize bias term representing intercept of linear model.\n",
    "bias = 0.0\n",
    "\n",
    "# Set learning rate and training epochs for gradient descent.\n",
    "learning_rate = 0.01\n",
    "\n",
    "epochs = 200\n",
    "\n",
    "# Define helper function to compute mean squared error loss.\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    # Compute squared differences and average them for loss.\n",
    "    return float(np.mean((y_true - y_pred) ** 2))\n",
    "\n",
    "\n",
    "# Training loop performs gradient descent on training data.\n",
    "for epoch in range(epochs):\n",
    "    # Compute current predictions using linear model parameters.\n",
    "    y_pred_train = weight * X_train.reshape(-1) + bias\n",
    "\n",
    "    # Compute gradients of loss with respect to parameters.\n",
    "    error = y_pred_train - y_train\n",
    "\n",
    "    # Gradient for weight uses feature values and error term.\n",
    "    grad_w = float(2.0 * np.mean(error * X_train.reshape(-1)))\n",
    "\n",
    "    # Gradient for bias uses average error across training examples.\n",
    "    grad_b = float(2.0 * np.mean(error))\n",
    "\n",
    "    # Update parameters by stepping opposite gradient direction.\n",
    "    weight -= learning_rate * grad_w\n",
    "\n",
    "    # Update bias similarly to reduce training loss gradually.\n",
    "    bias -= learning_rate * grad_b\n",
    "\n",
    "# After training compute predictions on both training and test sets.\n",
    "y_pred_train_final = weight * X_train.reshape(-1) + bias\n",
    "\n",
    "y_pred_test_final = weight * X_test.reshape(-1) + bias\n",
    "\n",
    "# Compute mean squared error on training data for comparison.\n",
    "train_mse = mean_squared_error(y_train, y_pred_train_final)\n",
    "\n",
    "# Compute mean squared error on test data for generalization.\n",
    "test_mse = mean_squared_error(y_test, y_pred_test_final)\n",
    "\n",
    "# Print framework information and learned parameters succinctly.\n",
    "print(\"Numpy version and learned parameters:\", np.__version__, weight, bias)\n",
    "\n",
    "# Print training and test losses to compare fit and generalization.\n",
    "print(\"Training MSE and test MSE values:\", round(train_mse, 3), round(test_mse, 3))\n",
    "\n",
    "# Print short interpretation linking training and evaluation concepts.\n",
    "print(\"Lower training loss shows fitting, test loss shows generalization performance.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f075928d",
   "metadata": {},
   "source": [
    "### **2.3. Better Data Better Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b161c25a",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_10/Lecture_B/image_02_03.jpg?v=1769975533\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Model quality depends on clean, representative data\n",
    ">* Data collection choices shape accuracy and fairness\n",
    "\n",
    ">* Data quality needs careful measurement and representation choices\n",
    ">* Biased training data encodes discrimination into models\n",
    "\n",
    ">* Unbalanced data can hide real fairness problems\n",
    ">* Ongoing, reflective data work builds trustworthy models\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8bd6cf",
   "metadata": {},
   "source": [
    "## **3. Monitoring Deployed Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d04d06",
   "metadata": {},
   "source": [
    "### **3.1. Adapting To Data Shifts**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707e8128",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_10/Lecture_B/image_03_01.jpg?v=1769975547\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Real-world data changes, causing data shifts\n",
    ">* Unmonitored models use outdated patterns, harming decisions\n",
    "\n",
    ">* User groups and contexts change, causing data shifts\n",
    ">* Unupdated models become inaccurate, unfair, and unreliable\n",
    "\n",
    ">* Plan strategies to detect and handle data change\n",
    ">* Update, validate, and review models to stay ethical\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7174a9ae",
   "metadata": {},
   "source": [
    "### **3.2. Spotting Performance Drift**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd80baf5",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_10/Lecture_B/image_03_02.jpg?v=1769975559\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Performance drift is gradual loss of model reliability\n",
    ">* Monitoring drift prevents hidden harm and lost trust\n",
    "\n",
    ">* Compare current model performance to original baseline\n",
    ">* Track metrics over time to catch emerging problems\n",
    "\n",
    ">* Drift can change errors, fairness, and impact\n",
    ">* Monitor many metrics to catch subtle misalignment\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc4f7a4",
   "metadata": {},
   "source": [
    "### **3.3. Scheduled Model Checkups**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92a9005",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_10/Lecture_B/image_03_03.jpg?v=1769975569\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Plan regular reviews of deployed modelsâ€™ behavior\n",
    ">* Routine checks catch hidden issues before serious harm\n",
    "\n",
    ">* Plan fixed metrics, datasets, and review questions\n",
    ">* Use consistent checkups to spot trends and disparities\n",
    "\n",
    ">* Regular checkups trigger updates, retraining, and documentation\n",
    ">* They align model changes with goals and users\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc51c919",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**From Idea To Use**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922bedd8",
   "metadata": {},
   "source": [
    "\n",
    "In this lecture, you learned to:\n",
    "- Outline the main stages of a simple machine learning project lifecycle. \n",
    "- Relate each stage of the lifecycle to concepts learned earlier in the course. \n",
    "- Explain why ongoing monitoring and revision are important after a model is first deployed. \n",
    "\n",
    "<font color='yellow'>Congratulations on completing this course!</font>"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
