{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de2913f5",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Bias And Fairness**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb1e3b8",
   "metadata": {},
   "source": [
    ">Last update: 20260201.\n",
    "    \n",
    "By the end of this Lecture, you will be able to:\n",
    "- Identify potential sources of bias in datasets used for machine learning. \n",
    "- Explain how biased data can lead to unfair model predictions for certain groups. \n",
    "- Propose simple steps a beginner can take to check for and reduce unfairness. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a80055",
   "metadata": {},
   "source": [
    "## **1. Sources of Data Bias**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720ed293",
   "metadata": {},
   "source": [
    "### **1.1. Uneven Data Sampling**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25287512",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_10/Lecture_A/image_01_01.jpg?v=1769974188\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Some groups appear much more in data\n",
    ">* Models then favor majority groups, disadvantaging minorities\n",
    "\n",
    ">* Data sources can quietly exclude many groups\n",
    ">* Models work best for those most frequently recorded\n",
    "\n",
    ">* Large, advanced datasets can still be skewed\n",
    ">* Ask who’s missing and how that harms\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1420b270",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Uneven Data Sampling\n",
    "\n",
    "# This script illustrates uneven data sampling clearly.\n",
    "# We use a tiny synthetic dataset example.\n",
    "# Focus is on group counts and simple accuracy.\n",
    "\n",
    "# Import required libraries for data handling.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Set deterministic random seed for reproducibility.\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create a small balanced dataset with two groups.\n",
    "balanced_size_per_group = 20\n",
    "ages_group_a = np.random.normal(loc=30, scale=5, size=balanced_size_per_group)\n",
    "\n",
    "# Create slightly different ages for second group.\n",
    "ages_group_b = np.random.normal(loc=50, scale=5, size=balanced_size_per_group)\n",
    "\n",
    "# Create simple outcome higher for older ages.\n",
    "outcome_a = (ages_group_a > 35).astype(int)\n",
    "outcome_b = (ages_group_b > 35).astype(int)\n",
    "\n",
    "# Build balanced dataframe with equal group representation.\n",
    "data_balanced = pd.DataFrame(\n",
    "    {\n",
    "        \"age\": np.concatenate([ages_group_a, ages_group_b]),\n",
    "        \"group\": [\"A\"] * balanced_size_per_group\n",
    "        + [\"B\"] * balanced_size_per_group,\n",
    "        \"outcome\": np.concatenate([outcome_a, outcome_b]),\n",
    "    }\n",
    ")\n",
    "\n",
    "# Show group counts for the balanced dataset.\n",
    "print(\"Balanced dataset group counts:\")\n",
    "print(data_balanced[\"group\"].value_counts())\n",
    "\n",
    "# Create an uneven sampled dataset favoring group A heavily.\n",
    "uneven_size_a = 35\n",
    "uneven_size_b = 5\n",
    "\n",
    "# Sample without replacement from original arrays safely.\n",
    "indices_a = np.random.choice(balanced_size_per_group, size=uneven_size_a, replace=True)\n",
    "indices_b = np.random.choice(balanced_size_per_group, size=uneven_size_b, replace=True)\n",
    "\n",
    "# Build uneven dataframe using sampled indices.\n",
    "data_uneven = pd.DataFrame(\n",
    "    {\n",
    "        \"age\": np.concatenate([\n",
    "            ages_group_a[indices_a],\n",
    "            ages_group_b[indices_b],\n",
    "        ]),\n",
    "        \"group\": [\"A\"] * uneven_size_a + [\"B\"] * uneven_size_b,\n",
    "        \"outcome\": np.concatenate([\n",
    "            outcome_a[indices_a],\n",
    "            outcome_b[indices_b],\n",
    "        ]),\n",
    "    }\n",
    ")\n",
    "\n",
    "# Show group counts for the uneven dataset.\n",
    "print(\"\\nUneven dataset group counts:\")\n",
    "print(data_uneven[\"group\"].value_counts())\n",
    "\n",
    "# Define a naive rule based only on majority group pattern.\n",
    "threshold_age = data_uneven[data_uneven[\"group\"] == \"A\"][\"age\"].mean()\n",
    "\n",
    "# Predict outcome using the single learned threshold.\n",
    "predictions = (data_balanced[\"age\"] > threshold_age).astype(int)\n",
    "\n",
    "# Check shapes before computing accuracy values.\n",
    "if predictions.shape[0] == data_balanced.shape[0]:\n",
    "\n",
    "    # Compute accuracy separately for each group.\n",
    "    mask_a = data_balanced[\"group\"] == \"A\"\n",
    "    mask_b = data_balanced[\"group\"] == \"B\"\n",
    "\n",
    "    # Calculate accuracy for group A.\n",
    "    acc_a = (predictions[mask_a].values\n",
    "             == data_balanced.loc[mask_a, \"outcome\"].values).mean()\n",
    "\n",
    "    # Calculate accuracy for group B.\n",
    "    acc_b = (predictions[mask_b].values\n",
    "             == data_balanced.loc[mask_b, \"outcome\"].values).mean()\n",
    "\n",
    "    # Print simple comparison of group accuracies.\n",
    "    print(\"\\nNaive rule accuracy using uneven sampling threshold:\")\n",
    "    print(\"Group A accuracy:\", round(float(acc_a), 3))\n",
    "    print(\"Group B accuracy:\", round(float(acc_b), 3))\n",
    "\n",
    "# Final print summarizing what changed between datasets.\n",
    "print(\"\\nNotice how uneven sampling changed who the rule fits.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c864ca",
   "metadata": {},
   "source": [
    "### **1.2. Legacy Bias in Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650d815b",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_10/Lecture_A/image_01_02.jpg?v=1769974234\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Historical inequalities get baked into training data\n",
    ">* Models then repeat and strengthen those past injustices\n",
    "\n",
    ">* Old hiring and promotion data can encode favoritism\n",
    ">* Healthcare records may hide illness in underserved groups\n",
    "\n",
    ">* Historical data can hide deep, persistent bias\n",
    ">* Question data origins and context to spot unfairness\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45df69af",
   "metadata": {},
   "source": [
    "### **1.3. Unequal Data Measurement**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7800a0",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_10/Lecture_A/image_01_03.jpg?v=1769974246\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Measurement methods differ across groups, creating bias\n",
    ">* Models learn and reinforce these distorted measurements\n",
    "\n",
    ">* Subjective ratings and records can encode hidden bias\n",
    ">* Models trained on them may unfairly punish groups\n",
    "\n",
    ">* Tech, language, environment can distort data capture\n",
    ">* Uneven measurement creates hidden bias in models\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d922388",
   "metadata": {},
   "source": [
    "## **2. Unequal Model Outcomes**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4403ab26",
   "metadata": {},
   "source": [
    "### **2.1. Error Rate Gaps**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d7ce67",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_10/Lecture_A/image_02_01.jpg?v=1769974262\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Overall accuracy can hide group performance differences\n",
    ">* Biased training data creates unequal error rates\n",
    "\n",
    ">* Different tasks show unequal false positives or negatives\n",
    ">* Historical data imbalances cause clustered mistakes by group\n",
    "\n",
    ">* Error gaps cause unequal treatment and harm\n",
    ">* They mirror and worsen existing social inequalities\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68437f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Error Rate Gaps\n",
    "\n",
    "# This script shows simple error rate gaps.\n",
    "# We compare model mistakes across two groups.\n",
    "# Focus is on unequal outcomes not algorithms.\n",
    "\n",
    "# import required built in and numerical libraries.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# set deterministic random seed for reproducibility.\n",
    "np.random.seed(42)\n",
    "\n",
    "# create tiny dataset with group labels and outcomes.\n",
    "data = {\n",
    "    \"group\": [\"A\"] * 20 + [\"B\"] * 20,\n",
    "    \"true_label\": [1] * 10 + [0] * 10 + [1] * 10 + [0] * 10,\n",
    "}\n",
    "\n",
    "# create biased predictions with higher errors for group B.\n",
    "predictions = []\n",
    "for i, g in enumerate(data[\"group\"]):\n",
    "    true = data[\"true_label\"][i]\n",
    "    if g == \"A\":\n",
    "        flip_prob = 0.1\n",
    "\n",
    "    else:\n",
    "        flip_prob = 0.4\n",
    "\n",
    "    if np.random.rand() < flip_prob:\n",
    "        pred = 1 - true\n",
    "\n",
    "    else:\n",
    "        pred = true\n",
    "\n",
    "    predictions.append(pred)\n",
    "\n",
    "# build dataframe from constructed columns and predictions.\n",
    "df = pd.DataFrame(data)\n",
    "df[\"prediction\"] = predictions\n",
    "\n",
    "# verify dataframe shape is as expected.\n",
    "assert df.shape == (40, 3)\n",
    "\n",
    "\n",
    "# function to compute error rate for each group.\n",
    "def compute_error_rate(group_name):\n",
    "    subset = df[df[\"group\"] == group_name]\n",
    "    errors = subset[\"prediction\"] != subset[\"true_label\"]\n",
    "    return errors.mean()\n",
    "\n",
    "\n",
    "# calculate error rates for both groups separately.\n",
    "error_A = compute_error_rate(\"A\")\n",
    "error_B = compute_error_rate(\"B\")\n",
    "\n",
    "# calculate overall error rate across all individuals.\n",
    "overall_error = (df[\"prediction\"] != df[\"true_label\"]).mean()\n",
    "\n",
    "# print concise summary highlighting error rate gaps.\n",
    "print(\"Overall error rate:\", round(overall_error, 3))\n",
    "print(\"Group A error rate:\", round(error_A, 3))\n",
    "print(\"Group B error rate:\", round(error_B, 3))\n",
    "print(\"Error rate gap B minus A:\", round(error_B - error_A, 3))\n",
    "\n",
    "# prepare values for simple bar chart comparison.\n",
    "labels = [\"Overall\", \"Group A\", \"Group B\"]\n",
    "values = [overall_error, error_A, error_B]\n",
    "\n",
    "# create bar plot to visualize unequal error rates.\n",
    "fig, ax = plt.subplots(figsize=(5, 4))\n",
    "ax.bar(labels, values, color=[\"gray\", \"skyblue\", \"salmon\"])\n",
    "\n",
    "# label axes and title clearly for beginners.\n",
    "ax.set_ylabel(\"Error rate (proportion wrong)\")\n",
    "ax.set_title(\"Error rate gaps between groups A and B\")\n",
    "\n",
    "# add horizontal line showing overall error reference.\n",
    "ax.axhline(overall_error, color=\"black\", linestyle=\"--\", linewidth=1)\n",
    "\n",
    "# adjust layout and display the single plot.\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9c9dd8",
   "metadata": {},
   "source": [
    "### **2.2. Real World Consequences**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5888e959",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_10/Lecture_A/image_02_02.jpg?v=1769974304\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Biased hiring models quietly limit people’s opportunities\n",
    ">* They reinforce inequality and reduce workplace diversity\n",
    "\n",
    ">* Biased models misjudge health and crime risks\n",
    ">* Unequal errors worsen care, punishment, and resources\n",
    "\n",
    ">* Biased systems harm confidence, opportunities, and participation\n",
    ">* Communities may distrust technology, worsening social division\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c61d7ec",
   "metadata": {},
   "source": [
    "### **2.3. Real World Bias Examples**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de8f534",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_10/Lecture_A/image_02_03.jpg?v=1769974315\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Hiring algorithms learn past gender-skewed patterns\n",
    ">* Qualified women and non-binary people get unfairly rejected\n",
    "\n",
    ">* Biased policing data makes some communities seem riskier\n",
    ">* Models reinforce bias, driving harsher future treatment\n",
    "\n",
    ">* Healthcare and credit models can misjudge groups\n",
    ">* Biased training data amplifies existing social inequalities\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5ae470",
   "metadata": {},
   "source": [
    "## **3. Practical Fairness Checks**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41d2c52",
   "metadata": {},
   "source": [
    "### **3.1. Group Fairness Checks**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebd8848",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_10/Lecture_A/image_03_01.jpg?v=1769974327\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Check model performance separately for different groups\n",
    ">* Compare error patterns to spot possible unfairness\n",
    "\n",
    ">* Compare key outcome rates across different groups\n",
    ">* Unequal rates reveal hidden patterns of unfairness\n",
    "\n",
    ">* Adjust data and thresholds to reduce unfairness\n",
    ">* Monitor group gaps over time and document limitations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dc7230",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Group Fairness Checks\n",
    "\n",
    "# This script shows simple group fairness checks.\n",
    "# We use tiny synthetic hiring recommendation data.\n",
    "# Focus on comparing model errors across demographic groups.\n",
    "\n",
    "# Required libraries are available in Colab by default.\n",
    "# import statements use only allowed safe libraries.\n",
    "\n",
    "# Import numpy and pandas for simple data handling.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Set deterministic random seed for reproducible results.\n",
    "rng = np.random.default_rng(seed=42)\n",
    "\n",
    "# Create a tiny synthetic dataset for hiring.\n",
    "num_rows = 40\n",
    "qualified = rng.integers(low=0, high=2, size=num_rows)\n",
    "\n",
    "# Create a simple gender group column for fairness checks.\n",
    "gender = rng.choice([\"female\", \"male\"], size=num_rows)\n",
    "\n",
    "# Simulate a biased model score higher for one group.\n",
    "base_scores = qualified + rng.normal(loc=0.0, scale=0.4, size=num_rows)\n",
    "\n",
    "# Add a small unfair bonus to one gender group.\n",
    "bonus = np.where(gender == \"male\", 0.3, 0.0)\n",
    "model_score = base_scores + bonus\n",
    "\n",
    "# Build a pandas DataFrame to hold our dataset.\n",
    "data = pd.DataFrame({\n",
    "    \"qualified\": qualified,\n",
    "    \"gender\": gender,\n",
    "    \"score\": model_score,\n",
    "})\n",
    "\n",
    "# Validate dataset shape before continuing with analysis.\n",
    "assert data.shape[0] == num_rows\n",
    "\n",
    "# Set one global decision threshold for interview recommendation.\n",
    "threshold = 0.8\n",
    "\n",
    "# Create model decision column based on the threshold.\n",
    "data[\"recommended\"] = (data[\"score\"] >= threshold).astype(int)\n",
    "\n",
    "# Define a helper to compute group fairness summary metrics.\n",
    "def group_fairness_summary(df, group_col):\n",
    "    # Prepare a list for storing summary rows.\n",
    "    rows = []\n",
    "\n",
    "    # Loop through each group value in the column.\n",
    "    for group_value, group_df in df.groupby(group_col):\n",
    "        # Skip groups that are too tiny for stable rates.\n",
    "        if len(group_df) < 3:\n",
    "            continue\n",
    "\n",
    "        # Compute basic counts for this group.\n",
    "        total = len(group_df)\n",
    "        positives = group_df[\"qualified\"].sum()\n",
    "\n",
    "        # Avoid division by zero when no positives exist.\n",
    "        if positives == 0:\n",
    "            true_positive_rate = np.nan\n",
    "        else:\n",
    "            # Share of qualified correctly recommended.\n",
    "            true_positive_rate = (\n",
    "                group_df[(group_df[\"qualified\"] == 1) &\n",
    "                         (group_df[\"recommended\"] == 1)].shape[0]\n",
    "                / positives\n",
    "            )\n",
    "\n",
    "        # Compute false positive rate for unqualified candidates.\n",
    "        negatives = total - positives\n",
    "        if negatives == 0:\n",
    "            false_positive_rate = np.nan\n",
    "        else:\n",
    "            false_positive_rate = (\n",
    "                group_df[(group_df[\"qualified\"] == 0) &\n",
    "                         (group_df[\"recommended\"] == 1)].shape[0]\n",
    "                / negatives\n",
    "            )\n",
    "\n",
    "        # Compute overall accuracy for this group.\n",
    "        accuracy = (\n",
    "            (group_df[\"qualified\"] == group_df[\"recommended\"]).sum()\n",
    "            / total\n",
    "        )\n",
    "\n",
    "        # Append summary row for this group.\n",
    "        rows.append({\n",
    "            group_col: group_value,\n",
    "            \"group_size\": total,\n",
    "            \"true_positive_rate\": round(float(true_positive_rate), 3)\n",
    "            if not np.isnan(true_positive_rate) else np.nan,\n",
    "            \"false_positive_rate\": round(float(false_positive_rate), 3)\n",
    "            if not np.isnan(false_positive_rate) else np.nan,\n",
    "            \"accuracy\": round(float(accuracy), 3),\n",
    "        })\n",
    "\n",
    "    # Return a DataFrame with one row per group.\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# Compute fairness summary by gender group.\n",
    "summary = group_fairness_summary(data, \"gender\")\n",
    "\n",
    "# Select only a few columns for compact printing.\n",
    "summary_to_print = summary[[\"gender\", \"group_size\", \"true_positive_rate\",\n",
    "                            \"false_positive_rate\", \"accuracy\"]]\n",
    "\n",
    "# Print a short explanation header for learners.\n",
    "print(\"Group fairness check for a simple hiring model:\")\n",
    "\n",
    "# Print the summary table with basic group metrics.\n",
    "print(summary_to_print.to_string(index=False))\n",
    "\n",
    "# Print a brief interpretation hint for beginners.\n",
    "print(\"Compare rates across groups to spot possible unfair gaps.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0ecd2c",
   "metadata": {},
   "source": [
    "### **3.2. Broadening Data Representation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac35978",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_10/Lecture_A/image_03_02.jpg?v=1769974376\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Check if training data reflects real users\n",
    ">* Look for missing groups and potential unfair gaps\n",
    "\n",
    ">* Add ethical data from underrepresented groups\n",
    ">* Reduce overfitting and unfair errors across groups\n",
    "\n",
    ">* Match training data to real usage contexts\n",
    ">* Fill missing scenarios or narrow model scope\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd7f849",
   "metadata": {},
   "source": [
    "### **3.3. Communicating Model Limitations**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f389d5fa",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_10/Lecture_A/image_03_03.jpg?v=1769974387\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* State where the model works and fails\n",
    ">* Help users question predictions, not trust blindly\n",
    "\n",
    ">* Describe model purpose, data, and weak spots\n",
    ">* Warn about unfairness for underrepresented applicant groups\n",
    "\n",
    ">* Explain safe use, limits, and override situations\n",
    ">* Encourage human review, feedback, and ongoing improvement\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c38f505",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Bias And Fairness**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be188d9a",
   "metadata": {},
   "source": [
    "\n",
    "In this lecture, you learned to:\n",
    "- Identify potential sources of bias in datasets used for machine learning. \n",
    "- Explain how biased data can lead to unfair model predictions for certain groups. \n",
    "- Propose simple steps a beginner can take to check for and reduce unfairness. \n",
    "\n",
    "<font color='yellow'>Congratulations on completing this course!</font>"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
