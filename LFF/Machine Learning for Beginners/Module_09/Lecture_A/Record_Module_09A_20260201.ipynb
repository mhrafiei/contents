{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "223d79c8",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Clustering Concepts**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfa2dac",
   "metadata": {},
   "source": [
    ">Last update: 20260201.\n",
    "    \n",
    "By the end of this Lecture, you will be able to:\n",
    "- Describe clustering as grouping similar examples based on feature values. \n",
    "- Use simple distance-based reasoning to decide which examples belong together. \n",
    "- Interpret clusters in terms of meaningful patterns in a given context. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3bd53f",
   "metadata": {},
   "source": [
    "## **1. Clustering Without Labels**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a791432e",
   "metadata": {},
   "source": [
    "### **1.1. No target column**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002b1fc7",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_09/Lecture_A/image_01_01.jpg?v=1769971477\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Clustering works without labels or target column\n",
    ">* Algorithm groups examples using only feature similarities\n",
    "\n",
    ">* No labels, so results lack fixed answers\n",
    ">* Algorithm groups by similarity; humans interpret meanings\n",
    "\n",
    ">* Analysts make creative choices when forming clusters\n",
    ">* No labels; clusters reveal useful hidden structure\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9242d838",
   "metadata": {},
   "source": [
    "### **1.2. Similarity Based Groups**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c1d2bb",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_09/Lecture_A/image_01_02.jpg?v=1769971488\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Clustering groups examples using their feature values\n",
    ">* Similar points in feature space form groups\n",
    "\n",
    ">* Fruits are positioned by features in 3D space\n",
    ">* Algorithm finds dense, similar groups without labels\n",
    "\n",
    ">* Clustering finds hidden behavior patterns in data\n",
    ">* Groups form from similar features, not labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f9e4ca",
   "metadata": {},
   "source": [
    "### **1.3. Clustering for Exploration**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72539d2d",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_09/Lecture_A/image_01_03.jpg?v=1769971499\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Clustering explores data by letting groups emerge\n",
    ">* Reveals hidden patterns that guide deeper analysis\n",
    "\n",
    ">* Clustering reveals community or visitor groups from features\n",
    ">* Clusters guide follow-up questions and targeted actions\n",
    "\n",
    ">* Clustering supports iterative, reflective data exploration cycles\n",
    ">* Refined clusters reveal patterns that inspire new hypotheses\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2b5dbd",
   "metadata": {},
   "source": [
    "## **2. Distance and Similarity**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b2b107",
   "metadata": {},
   "source": [
    "### **2.1. Point to Point Distance**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6057415",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_09/Lecture_A/image_02_01.jpg?v=1769971510\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Examples become points; distance measures their closeness\n",
    ">* Distances help form clusters and spot different groups\n",
    "\n",
    ">* Song features define distances that reflect similarity\n",
    ">* Closer songs join the same playlist-style cluster\n",
    "\n",
    ">* Distance depends on chosen features and scales\n",
    ">* Comparing distances reveals dense clusters and separations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd06d63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Point to Point Distance\n",
    "\n",
    "# This script shows simple point distance calculations.\n",
    "# We compare customers using tiny numeric feature vectors.\n",
    "# Focus on how distance reflects similarity between examples.\n",
    "\n",
    "# import math for square root distance calculations.\n",
    "import math\n",
    "\n",
    "# define two simple customers with age and purchases features.\n",
    "customer_a = {\"name\": \"Alice\", \"features\": (25, 5)}\n",
    "customer_b = {\"name\": \"Bob\", \"features\": (27, 6)}\n",
    "\n",
    "# define a third customer intentionally more different.\n",
    "customer_c = {\"name\": \"Cara\", \"features\": (40, 1)}\n",
    "\n",
    "# define a function computing Euclidean distance between two points.\n",
    "def euclidean_distance(point_one, point_two):\n",
    "    # unpack coordinates from the two feature tuples.\n",
    "    x1, y1 = point_one\n",
    "    x2, y2 = point_two\n",
    "\n",
    "    # compute squared differences along each feature dimension.\n",
    "    squared_sum = ((x1 - x2) ** 2) + ((y1 - y2) ** 2)\n",
    "\n",
    "    # return the square root as final distance value.\n",
    "    return math.sqrt(squared_sum)\n",
    "\n",
    "# collect all customers into a small list for iteration.\n",
    "customers = [customer_a, customer_b, customer_c]\n",
    "\n",
    "# print a short header explaining the feature meaning.\n",
    "print(\"Features: age in years, number of products purchased.\")\n",
    "\n",
    "# compute and print pairwise distances between all customers.\n",
    "for i in range(len(customers)):\n",
    "    # select the first customer in the pair.\n",
    "    first = customers[i]\n",
    "\n",
    "    # compare with later customers to avoid duplicates.\n",
    "    for j in range(i + 1, len(customers)):\n",
    "        # select the second customer in the pair.\n",
    "        second = customers[j]\n",
    "\n",
    "        # compute distance using the helper function.\n",
    "        dist = euclidean_distance(first[\"features\"], second[\"features\"])\n",
    "\n",
    "        # print a readable summary of the distance value.\n",
    "        print(\n",
    "            f\"Distance between {first['name']} and {second['name']} is {dist:.2f}.\"\n",
    "        )\n",
    "\n",
    "# interpret which pair is most similar based on smallest distance.\n",
    "print(\"Smaller distance means customers are more similar overall.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b0ff6a",
   "metadata": {},
   "source": [
    "### **2.2. Scaling and Distance**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516fd53d",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_09/Lecture_A/image_02_02.jpg?v=1769971543\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Feature scale strongly affects distance-based grouping\n",
    ">* Large-range features dominate, hiding small-scale information\n",
    "\n",
    ">* Rescale features so typical differences are comparable\n",
    ">* Scaling balances feature influence, producing fairer clusters\n",
    "\n",
    ">* Scaling prevents one feature dominating many others\n",
    ">* It lets us choose and balance feature importance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f368eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Scaling and Distance\n",
    "\n",
    "# This script shows how scaling changes distances.\n",
    "# We compare raw and scaled customer feature distances.\n",
    "# Focus on income and satisfaction feature distance contributions.\n",
    "\n",
    "# import required numerical and plotting libraries.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# set deterministic random seed for reproducible behavior.\n",
    "np.random.seed(42)\n",
    "\n",
    "# create tiny synthetic customer feature data array.\n",
    "customers = np.array(\n",
    "    [[20000, 2],\n",
    "     [50000, 4],\n",
    "     [80000, 3],\n",
    "     [120000, 5]],\n",
    "    dtype=float,\n",
    ")\n",
    "\n",
    "# print original customer feature table with labels.\n",
    "print(\"Customers as [income_dollars, satisfaction_score]:\")\n",
    "print(customers)\n",
    "\n",
    "# define simple function computing euclidean distance values.\n",
    "def euclidean_distance(a, b):\n",
    "    diff = a - b\n",
    "\n",
    "    return float(np.sqrt(np.sum(diff ** 2)))\n",
    "\n",
    "# choose reference customer index for distance comparison.\n",
    "ref_index = 0\n",
    "ref_customer = customers[ref_index]\n",
    "\n",
    "# compute distances from reference using raw features.\n",
    "raw_distances = []\n",
    "for i in range(customers.shape[0]):\n",
    "    dist = euclidean_distance(ref_customer, customers[i])\n",
    "    raw_distances.append(dist)\n",
    "\n",
    "# convert raw distances list into numpy array.\n",
    "raw_distances = np.array(raw_distances, dtype=float)\n",
    "\n",
    "# compute feature means and standard deviations for scaling.\n",
    "feature_means = customers.mean(axis=0)\n",
    "feature_stds = customers.std(axis=0, ddof=0)\n",
    "\n",
    "# avoid division by zero using safe standard deviation values.\n",
    "feature_stds_safe = np.where(feature_stds == 0, 1.0, feature_stds)\n",
    "\n",
    "# scale features to zero mean and unit variance values.\n",
    "customers_scaled = (customers - feature_means) / feature_stds_safe\n",
    "\n",
    "# compute distances from reference using scaled features.\n",
    "scaled_ref_customer = customers_scaled[ref_index]\n",
    "scaled_distances = []\n",
    "for i in range(customers_scaled.shape[0]):\n",
    "    dist = euclidean_distance(scaled_ref_customer, customers_scaled[i])\n",
    "    scaled_distances.append(dist)\n",
    "\n",
    "# convert scaled distances list into numpy array.\n",
    "scaled_distances = np.array(scaled_distances, dtype=float)\n",
    "\n",
    "# print raw and scaled distances for interpretation.\n",
    "print(\"\\nRaw distances from customer 0:\")\n",
    "print(raw_distances)\n",
    "print(\"\\nScaled distances from customer 0:\")\n",
    "print(scaled_distances)\n",
    "\n",
    "# prepare x positions for bar plot comparison.\n",
    "indices = np.arange(customers.shape[0])\n",
    "\n",
    "# create bar width and figure for distance comparison.\n",
    "bar_width = 0.35\n",
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "# plot raw distances as left bars.\n",
    "plt.bar(indices - bar_width / 2, raw_distances, width=bar_width, label=\"Raw\")\n",
    "\n",
    "# plot scaled distances as right bars.\n",
    "plt.bar(indices + bar_width / 2, scaled_distances, width=bar_width, label=\"Scaled\")\n",
    "\n",
    "# label axes and title for clarity.\n",
    "plt.xlabel(\"Customer index\")\n",
    "plt.ylabel(\"Distance from customer zero\")\n",
    "plt.title(\"Effect of feature scaling on distance values\")\n",
    "\n",
    "# add legend and layout adjustment.\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "# display the final comparison plot.\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbf1289",
   "metadata": {},
   "source": [
    "### **2.3. Visual 2D Examples**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a925b2d",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_09/Lecture_A/image_02_03.jpg?v=1769971588\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Plot examples as dots on two features\n",
    ">* Nearby dots form clusters; distant dots are different\n",
    "\n",
    ">* Plot students by two exam scores visually\n",
    ">* Assign each student to cluster of nearest neighbors\n",
    "\n",
    ">* Imagine restaurant points, connect nearby ones into groups\n",
    ">* Gaps between point groups mark cluster boundaries visually\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4e8f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Visual 2D Examples\n",
    "\n",
    "# This script visualizes simple two dimensional distance based clustering examples.\n",
    "# It shows how nearby points can form intuitive visual clusters.\n",
    "# Use it to connect distance ideas with real looking scatter plots.\n",
    "\n",
    "# !pip install numpy matplotlib seaborn.\n",
    "\n",
    "# Import required numerical and plotting libraries.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set deterministic random seed for reproducible point locations.\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create grocery versus entertainment spending cluster centers.\n",
    "centers_spend = np.array([[20, 40], [60, 20], [55, 70]])\n",
    "\n",
    "# Generate small clouds of spending points around each center.\n",
    "points_spend = []\n",
    "for center in centers_spend:\n",
    "    cloud = center + np.random.normal(loc=0.0, scale=5.0, size=(15, 2))\n",
    "    points_spend.append(cloud)\n",
    "\n",
    "# Stack all spending points into one array for plotting.\n",
    "points_spend = np.vstack(points_spend)\n",
    "\n",
    "# Create math versus writing exam score cluster centers.\n",
    "centers_exam = np.array([[80, 80], [30, 30], [80, 40], [40, 80]])\n",
    "\n",
    "# Generate small clouds of exam points around each center.\n",
    "points_exam = []\n",
    "for center in centers_exam:\n",
    "    cloud = center + np.random.normal(loc=0.0, scale=4.0, size=(10, 2))\n",
    "    points_exam.append(cloud)\n",
    "\n",
    "# Stack all exam points into one array for plotting.\n",
    "points_exam = np.vstack(points_exam)\n",
    "\n",
    "# Choose one grocery customer and one student for distance highlighting.\n",
    "customer_index = 5\n",
    "student_index = 12\n",
    "\n",
    "# Extract chosen reference points from arrays safely.\n",
    "customer_point = points_spend[customer_index]\n",
    "student_point = points_exam[student_index]\n",
    "\n",
    "# Compute Euclidean distances from reference customer to all customers.\n",
    "distances_customer = np.sqrt(np.sum((points_spend - customer_point) ** 2, axis=1))\n",
    "\n",
    "# Compute Euclidean distances from reference student to all students.\n",
    "distances_student = np.sqrt(np.sum((points_exam - student_point) ** 2, axis=1))\n",
    "\n",
    "# Find three nearest neighbors for customer excluding itself.\n",
    "neighbor_indices_customer = np.argsort(distances_customer)[1:4]\n",
    "\n",
    "# Find three nearest neighbors for student excluding itself.\n",
    "neighbor_indices_student = np.argsort(distances_student)[1:4]\n",
    "\n",
    "# Print short summary explaining selected nearest neighbors.\n",
    "print(\"Customer nearest neighbor distances:\", np.round(distances_customer[neighbor_indices_customer], 2))\n",
    "print(\"Student nearest neighbor distances:\", np.round(distances_student[neighbor_indices_student], 2))\n",
    "\n",
    "# Create a figure with two side by side scatter subplots.\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\n",
    "\n",
    "# Plot grocery versus entertainment spending scatter with neighbors highlighted.\n",
    "ax0 = axes[0]\n",
    "ax0.scatter(points_spend[:, 0], points_spend[:, 1], c=\"lightgray\", label=\"Other customers\")\n",
    "\n",
    "# Highlight reference customer with a distinct color and marker.\n",
    "ax0.scatter(customer_point[0], customer_point[1], c=\"red\", marker=\"x\", s=80, label=\"Chosen customer\")\n",
    "\n",
    "# Highlight nearest neighbor customers using another color.\n",
    "ax0.scatter(points_spend[neighbor_indices_customer, 0], points_spend[neighbor_indices_customer, 1], c=\"blue\", marker=\"o\", s=60, label=\"Nearest neighbors\")\n",
    "\n",
    "# Label axes and title for spending subplot.\n",
    "ax0.set_xlabel(\"Grocery spending per month\")\n",
    "ax0.set_ylabel(\"Entertainment spending per month\")\n",
    "ax0.set_title(\"Customers grouped by similar spending patterns\")\n",
    "ax0.legend(loc=\"best\")\n",
    "\n",
    "# Plot math versus writing exam scatter with neighbors highlighted.\n",
    "ax1 = axes[1]\n",
    "ax1.scatter(points_exam[:, 0], points_exam[:, 1], c=\"lightgray\", label=\"Other students\")\n",
    "\n",
    "# Highlight reference student with a distinct color and marker.\n",
    "ax1.scatter(student_point[0], student_point[1], c=\"red\", marker=\"x\", s=80, label=\"Chosen student\")\n",
    "\n",
    "# Highlight nearest neighbor students using another color.\n",
    "ax1.scatter(points_exam[neighbor_indices_student, 0], points_exam[neighbor_indices_student, 1], c=\"green\", marker=\"o\", s=60, label=\"Nearest neighbors\")\n",
    "\n",
    "# Label axes and title for exam subplot.\n",
    "ax1.set_xlabel(\"Math exam score\")\n",
    "ax1.set_ylabel(\"Writing exam score\")\n",
    "ax1.set_title(\"Students grouped by similar exam performance\")\n",
    "ax1.legend(loc=\"best\")\n",
    "\n",
    "# Adjust layout and display the combined figure clearly.\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ec330d",
   "metadata": {},
   "source": [
    "## **3. Making Sense of Clusters**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b1d5f2",
   "metadata": {},
   "source": [
    "### **3.1. Cluster summaries**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b851e7b9",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_09/Lecture_A/image_03_01.jpg?v=1769971639\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Summarize each clusterâ€™s typical shared characteristics clearly\n",
    ">* Turn feature statistics into a simple, meaningful story\n",
    "\n",
    ">* Focus on features where clusters stand out\n",
    ">* Use contrasts to explain meaningful group differences\n",
    "\n",
    ">* Cluster summaries must match real-world goals and context\n",
    ">* Contextual summaries guide targeted actions and decisions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3cc08d",
   "metadata": {},
   "source": [
    "### **3.2. Labeling Cluster Groups**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f251d57",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_09/Lecture_A/image_03_02.jpg?v=1769971650\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Create human-friendly labels describing each cluster\n",
    ">* Summarize key feature patterns in short phrases\n",
    "\n",
    ">* Compare clusters to find what makes them unique\n",
    ">* Use clear, contrastive labels that aid decisions\n",
    "\n",
    ">* Use cautious, unbiased labels based on observable data\n",
    ">* Check labels with experts and note limitations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68e296c",
   "metadata": {},
   "source": [
    "### **3.3. Applying Clusters to Decisions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09481fda",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_09/Lecture_A/image_03_03.jpg?v=1769971659\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Use each cluster as a distinct segment\n",
    ">* Tailor actions to clusters for targeted decisions\n",
    "\n",
    ">* Design different actions for each customer cluster\n",
    ">* Balance personalization with ethics, cost, and practicality\n",
    "\n",
    ">* Treat clusters as tentative patterns, not facts\n",
    ">* Keep monitoring, updating, and revising cluster-based decisions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522c3ba8",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Clustering Concepts**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6cc19e",
   "metadata": {},
   "source": [
    "\n",
    "In this lecture, you learned to:\n",
    "- Describe clustering as grouping similar examples based on feature values. \n",
    "- Use simple distance-based reasoning to decide which examples belong together. \n",
    "- Interpret clusters in terms of meaningful patterns in a given context. \n",
    "\n",
    "In the next Lecture (Lecture B), we will go over 'Simplifying Features'"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
