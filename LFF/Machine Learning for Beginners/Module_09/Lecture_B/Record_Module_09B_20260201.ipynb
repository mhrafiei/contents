{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52ef274f",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Simplifying Features**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5c0b7b",
   "metadata": {},
   "source": [
    ">Last update: 20260201.\n",
    "    \n",
    "By the end of this Lecture, you will be able to:\n",
    "- Explain why high-dimensional data can be hard to visualize and reason about. \n",
    "- Describe the idea of representing data with fewer combined features. \n",
    "- Interpret simple low-dimensional visualizations that summarize higher-dimensional data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6ec21f",
   "metadata": {},
   "source": [
    "## **1. High Dimensional Intuition**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cf411c",
   "metadata": {},
   "source": [
    "### **1.1. Intuition for Many Features**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cffa9c",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_09/Lecture_B/image_01_01.jpg?v=1769972596\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Many features describe each item as coordinates\n",
    ">* Human intuition fails beyond three or four dimensions\n",
    "\n",
    ">* Few features are easy to plot visually\n",
    ">* Many features break our ability to see structure\n",
    "\n",
    ">* Many features make comparisons and decisions overwhelming\n",
    ">* Our intuition misses subtle multi-feature patterns and structure\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad921ea",
   "metadata": {},
   "source": [
    "### **1.2. Visualizing Many Dimensions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cca5542",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_09/Lecture_B/image_01_02.jpg?v=1769972609\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* We easily understand data in two or three dimensions\n",
    ">* High-dimensional datasets exceed our visual and cognitive limits\n",
    "\n",
    ">* We use 2D plots and color encodings\n",
    ">* These views fragment patterns and strain our reasoning\n",
    "\n",
    ">* Complex multi-axis charts quickly become cluttered\n",
    ">* Human perception struggles to see subtle high-dimensional patterns\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e192ee78",
   "metadata": {},
   "source": [
    "### **1.3. Noise and Clutter**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1646897b",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_09/Lecture_B/image_01_03.jpg?v=1769972620\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Many features add noise and overwhelming clutter\n",
    ">* Extra weak features hide the real underlying patterns\n",
    "\n",
    ">* Extra dimensions add small noises that accumulate\n",
    ">* Noisy features distort distances and visual projections\n",
    "\n",
    ">* Too many weak cues overwhelm human reasoning\n",
    ">* High dimensions blur signal, distances, and patterns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427d9c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Noise and Clutter\n",
    "\n",
    "# This script shows noise clutter in high dimensions.\n",
    "# We compare clean and noisy features visually and numerically.\n",
    "# Use this to build intuition about confusing extra dimensions.\n",
    "\n",
    "# Required scientific plotting libraries are already available in Colab.\n",
    "# They provide arrays, dataframes, and simple visualization tools.\n",
    "# No additional installations are needed for this short example.\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set a deterministic random seed for reproducible results.\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create a small number of clean two dimensional points.\n",
    "clean_points = np.random.normal(loc=0.0, scale=1.0, size=(40, 2))\n",
    "\n",
    "# Create several extra noisy dimensions with weak information.\n",
    "noisy_extra = np.random.normal(loc=0.0, scale=3.0, size=(40, 8))\n",
    "\n",
    "# Combine clean and noisy features into one high dimensional array.\n",
    "high_dim_points = np.concatenate((clean_points, noisy_extra), axis=1)\n",
    "\n",
    "# Validate that shapes match expectations before further operations.\n",
    "assert clean_points.shape[0] == high_dim_points.shape[0]\n",
    "\n",
    "# Compute pairwise distances using only the two clean dimensions.\n",
    "clean_distances = np.linalg.norm(\n",
    "    clean_points[None, :, :] - clean_points[:, None, :], axis=2\n",
    ")\n",
    "\n",
    "# Compute pairwise distances using all noisy high dimensional features.\n",
    "high_dim_distances = np.linalg.norm(\n",
    "    high_dim_points[None, :, :] - high_dim_points[:, None, :], axis=2\n",
    ")\n",
    "\n",
    "# Select a reference point index for distance comparison.\n",
    "reference_index = 0\n",
    "\n",
    "# Extract distances from the reference point in both spaces.\n",
    "clean_from_ref = clean_distances[reference_index]\n",
    "\n",
    "# Extract high dimensional distances from the same reference point.\n",
    "high_from_ref = high_dim_distances[reference_index]\n",
    "\n",
    "# Sort indices by distance in the clean two dimensional space.\n",
    "clean_order = np.argsort(clean_from_ref)\n",
    "\n",
    "# Sort indices by distance in the noisy high dimensional space.\n",
    "high_order = np.argsort(high_from_ref)\n",
    "\n",
    "# Print nearest neighbors in clean space and noisy space.\n",
    "print(\"Nearest neighbors using only two clean features:\")\n",
    "print(clean_order[:6])\n",
    "print(\"Nearest neighbors using all noisy extra features:\")\n",
    "print(high_order[:6])\n",
    "\n",
    "# Create a simple scatter plot of the clean two dimensional points.\n",
    "plt.scatter(clean_points[:, 0], clean_points[:, 1], c=\"lightgray\", label=\"all points\")\n",
    "\n",
    "# Highlight the reference point in the clean two dimensional view.\n",
    "plt.scatter(\n",
    "    clean_points[reference_index, 0], clean_points[reference_index, 1], c=\"red\", label=\"reference\"\n",
    ")\n",
    "\n",
    "# Highlight the nearest neighbor in clean space for visual comparison.\n",
    "plt.scatter(\n",
    "    clean_points[clean_order[1], 0], clean_points[clean_order[1], 1], c=\"blue\", label=\"clean neighbor\"\n",
    ")\n",
    "\n",
    "# Add labels and title explaining noise clutter intuition.\n",
    "plt.xlabel(\"Feature one (clean signal)\")\n",
    "plt.ylabel(\"Feature two (clean signal)\")\n",
    "\n",
    "# Show how extra noisy dimensions can reshuffle neighbor relationships.\n",
    "plt.title(\"Clean view versus hidden noisy clutter in extra dimensions\")\n",
    "\n",
    "# Display legend to distinguish reference and neighbor points.\n",
    "plt.legend()\n",
    "\n",
    "# Render the final plot window for this short teaching script.\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefb5d23",
   "metadata": {},
   "source": [
    "## **2. Summarizing Features**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbeb780",
   "metadata": {},
   "source": [
    "### **2.1. Merging Related Features**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9076a66b",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_09/Lecture_B/image_02_01.jpg?v=1769972662\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Combine measurements that tell the same story\n",
    ">* Create one summary feature to capture broad patterns\n",
    "\n",
    ">* Combine related variables into one composite score\n",
    ">* Composite scores simplify comparisons while keeping key information\n",
    "\n",
    ">* Merging correlated features cuts noise and redundancy\n",
    ">* Combined features reveal clearer, simpler data patterns\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ec54f5",
   "metadata": {},
   "source": [
    "### **2.2. Key Patterns in Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42af83ab",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_09/Lecture_B/image_02_02.jpg?v=1769972672\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Look for underlying themes across many features\n",
    ">* Use pattern scores to summarize each person\n",
    "\n",
    ">* Patterns appear from features changing together across data\n",
    ">* These patterns become new combined features describing behavior\n",
    "\n",
    ">* Focus on main variation patterns, not measurements\n",
    ">* Pattern scores summarize patients with fewer features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c94cfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Key Patterns in Data\n",
    "\n",
    "# This script shows key patterns using combined features.\n",
    "# We create lifestyle data and summarize main patterns.\n",
    "# Focus is representing many features with fewer pattern scores.\n",
    "\n",
    "# import required numerical and plotting libraries.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# set deterministic random seed for reproducible results.\n",
    "np.random.seed(42)\n",
    "\n",
    "# create small synthetic lifestyle dataset with three features.\n",
    "num_people = 30\n",
    "exercise_hours = np.random.normal(loc=4.0, scale=1.0, size=num_people)\n",
    "\n",
    "# create sleep hours positively related to exercise hours.\n",
    "sleep_hours = 6.0 + 0.5 * exercise_hours + np.random.normal(\n",
    "    loc=0.0, scale=0.5, size=num_people\n",
    ")\n",
    "\n",
    "# create stress score negatively related to exercise and sleep.\n",
    "stress_score = 8.0 - 0.6 * exercise_hours - 0.4 * sleep_hours + np.random.normal(\n",
    "    loc=0.0, scale=0.7, size=num_people\n",
    ")\n",
    "\n",
    "# stack features into one data matrix for processing.\n",
    "X = np.column_stack((exercise_hours, sleep_hours, stress_score))\n",
    "\n",
    "# validate matrix shape before further operations.\n",
    "assert X.shape == (num_people, 3)\n",
    "\n",
    "# center each feature by subtracting its mean value.\n",
    "X_mean = X.mean(axis=0)\n",
    "X_centered = X - X_mean\n",
    "\n",
    "# compute covariance matrix capturing how features move together.\n",
    "cov_matrix = np.cov(X_centered.T)\n",
    "\n",
    "# compute eigenvalues and eigenvectors of covariance matrix.\n",
    "values, vectors = np.linalg.eig(cov_matrix)\n",
    "\n",
    "# sort eigenvalues to find strongest shared variation direction.\n",
    "order = np.argsort(values)[::-1]\n",
    "values = values[order]\n",
    "vectors = vectors[:, order]\n",
    "\n",
    "# take first eigenvector as main lifestyle pattern direction.\n",
    "main_pattern_direction = vectors[:, 0]\n",
    "\n",
    "# project each person onto this main pattern direction.\n",
    "pattern_scores = X_centered.dot(main_pattern_direction)\n",
    "\n",
    "# print brief explanation and first few pattern scores.\n",
    "print(\"Each person now has one main pattern score summarizing lifestyle.\")\n",
    "print(\"First five pattern scores (higher means healthier overall pattern):\")\n",
    "print(np.round(pattern_scores[:5], 2))\n",
    "\n",
    "# create scatter plot comparing exercise and stress colored by pattern score.\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.scatter(\n",
    "    exercise_hours,\n",
    "    stress_score,\n",
    "    c=pattern_scores,\n",
    "    cmap=\"viridis\",\n",
    ")\n",
    "\n",
    "# label axes and add colorbar explaining pattern strength.\n",
    "plt.xlabel(\"Exercise hours per week\")\n",
    "plt.ylabel(\"Stress score (higher means more stress)\")\n",
    "plt.title(\"One combined pattern summarizing three lifestyle features\")\n",
    "plt.colorbar(label=\"Main lifestyle pattern score\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52b850d",
   "metadata": {},
   "source": [
    "### **2.3. Balancing Detail Loss**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8e5867",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_09/Lecture_B/image_02_03.jpg?v=1769972707\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Combining features trades detail for simpler patterns\n",
    ">* Check simplified data still supports reliable conclusions\n",
    "\n",
    ">* Reducing features should remove noise, not signal\n",
    ">* Broader features reveal patterns while hiding tiny details\n",
    "\n",
    ">* Over-simplifying features can hide important local patterns\n",
    ">* We must test simplifications for accuracy and fairness\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664010a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Balancing Detail Loss\n",
    "\n",
    "# This script shows balancing detail loss visually.\n",
    "# We compare original features and simplified features.\n",
    "# Focus on keeping signal while discarding noisy detail.\n",
    "\n",
    "# Required libraries are available in Colab by default.\n",
    "# Uncomment next lines only if running elsewhere.\n",
    "# import sys for environment specific installation.\n",
    "# import subprocess for optional manual package installation.\n",
    "\n",
    "# Import required numerical and plotting libraries.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set deterministic random seed for reproducible behavior.\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create small synthetic dataset with three features.\n",
    "num_points = 40\n",
    "base_trend = np.linspace(0, 10, num_points)\n",
    "\n",
    "# Add two noisy features around the base trend.\n",
    "feature_one = base_trend + np.random.normal(0, 0.8, num_points)\n",
    "feature_two = base_trend + np.random.normal(0, 1.2, num_points)\n",
    "\n",
    "# Stack features into a matrix and validate shape.\n",
    "data_matrix = np.column_stack((feature_one, feature_two))\n",
    "assert data_matrix.shape == (num_points, 2)\n",
    "\n",
    "# Create a combined feature by averaging both features.\n",
    "combined_feature = data_matrix.mean(axis=1)\n",
    "\n",
    "# Compute simple detail loss measure using variance difference.\n",
    "original_variance = data_matrix.var(axis=0, ddof=1).mean()\n",
    "combined_variance = combined_feature.var(ddof=1)\n",
    "variance_ratio = combined_variance / original_variance\n",
    "\n",
    "# Print short summary about simplification and variance.\n",
    "print(\"Original average variance across features:\", round(original_variance, 3))\n",
    "print(\"Combined feature variance after simplification:\", round(combined_variance, 3))\n",
    "print(\"Variance kept ratio close to one means less detail loss.\")\n",
    "print(\"Here ratio value is:\", round(variance_ratio, 3))\n",
    "\n",
    "# Prepare figure with two subplots for comparison.\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "# Plot original two features against base trend.\n",
    "axes[0].scatter(base_trend, feature_one, color=\"tab:blue\", label=\"Feature one\")\n",
    "axes[0].scatter(base_trend, feature_two, color=\"tab:orange\", label=\"Feature two\")\n",
    "axes[0].set_title(\"Original separate noisy features\")\n",
    "axes[0].set_xlabel(\"Underlying trend index value\")\n",
    "axes[0].set_ylabel(\"Measured feature values scale\")\n",
    "axes[0].legend(loc=\"upper left\")\n",
    "\n",
    "# Plot combined feature showing simplified representation.\n",
    "axes[1].scatter(base_trend, combined_feature, color=\"tab:green\", label=\"Combined feature\")\n",
    "axes[1].set_title(\"Simplified combined feature view\")\n",
    "axes[1].set_xlabel(\"Underlying trend index value\")\n",
    "axes[1].set_ylabel(\"Combined feature values scale\")\n",
    "axes[1].legend(loc=\"upper left\")\n",
    "\n",
    "# Adjust layout and display the single figure.\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05647c36",
   "metadata": {},
   "source": [
    "## **3. Reading 2D Projections**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c2aac4",
   "metadata": {},
   "source": [
    "### **3.1. Basic 2D Plots**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c02c6cd",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_09/Lecture_B/image_03_01.jpg?v=1769972746\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* 2D plots are snapshots of complex data\n",
    ">* Each dot summarizes many original feature values\n",
    "\n",
    ">* Abstract axes summarize key patterns in data\n",
    ">* Translate axis directions into real-world meanings mentally\n",
    "\n",
    ">* Look for dense areas, gaps, and directions\n",
    ">* Treat distance as similarity to spot patterns, outliers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99d94e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Basic 2D Plots\n",
    "\n",
    "# This script shows a simple two dimensional projection plot.\n",
    "# We summarize higher dimensional style scores into two combined axes.\n",
    "# Use this to practice reading basic two dimensional scatter plots.\n",
    "\n",
    "# Import required numerical and plotting libraries.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set a deterministic random seed for reproducible synthetic data.\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create small synthetic data representing three customer style features.\n",
    "base_points = np.array([\n",
    "    [2.0, 7.0, 3.0],\n",
    "    [6.0, 2.0, 8.0],\n",
    "    [4.0, 5.0, 6.0],\n",
    "    [8.0, 1.0, 9.0],\n",
    "    [1.0, 8.0, 2.0],\n",
    "    [7.0, 3.0, 7.0],\n",
    "])\n",
    "\n",
    "# Check that the data has expected two dimensional shape.\n",
    "assert base_points.shape[1] == 3\n",
    "\n",
    "# Define simple weights to create two combined projection directions.\n",
    "weights_one = np.array([0.6, 0.3, 0.1])\n",
    "weights_two = np.array([-0.2, 0.5, 0.7])\n",
    "\n",
    "# Compute first projection as weighted sum for each record.\n",
    "proj_one = base_points.dot(weights_one)\n",
    "\n",
    "# Compute second projection as another weighted sum for each record.\n",
    "proj_two = base_points.dot(weights_two)\n",
    "\n",
    "# Stack projections into two dimensional coordinates for plotting.\n",
    "projected_points = np.column_stack((proj_one, proj_two))\n",
    "\n",
    "# Confirm projected data has two columns representing two axes.\n",
    "assert projected_points.shape[1] == 2\n",
    "\n",
    "# Prepare short labels describing each synthetic customer record.\n",
    "labels = [\n",
    "    \"Eco casual\", \"Luxury formal\", \"Balanced style\",\n",
    "    \"Bold luxury\", \"Relaxed eco\", \"Modern luxury\",\n",
    "]\n",
    "\n",
    "# Create a scatter plot showing the two dimensional projection.\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.scatter(projected_points[:, 0], projected_points[:, 1], color=\"teal\")\n",
    "\n",
    "# Label each point to support interpretation of plot positions.\n",
    "for point, label in zip(projected_points, labels):\n",
    "    plt.text(point[0] + 0.05, point[1] + 0.05, label, fontsize=8)\n",
    "\n",
    "# Add axis labels that describe abstract combined style directions.\n",
    "plt.xlabel(\"Component one budget to luxury style direction\")\n",
    "plt.ylabel(\"Component two relaxed to bold style direction\")\n",
    "\n",
    "# Add a concise title explaining this two dimensional projection.\n",
    "plt.title(\"Reading a simple two dimensional projection of style data\")\n",
    "\n",
    "# Adjust layout and display the final scatter plot figure.\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90fe2a4",
   "metadata": {},
   "source": [
    "### **3.2. Seeing clusters and trends**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27ea38b",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_09/Lecture_B/image_03_02.jpg?v=1769972785\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Look for tight groups of nearby points\n",
    ">* Separated dense groups suggest meaningful data subgroups\n",
    "\n",
    ">* Projections can show smooth gradients, not just clusters\n",
    ">* Gradual changes reveal fuzzy boundaries and related groups\n",
    "\n",
    ">* Use density, overlap, and outliers to interpret\n",
    ">* Form hypotheses about structure, reliability, and anomalies\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d52085f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Seeing clusters and trends\n",
    "\n",
    "# This script shows clusters and trends visually.\n",
    "# We use simple synthetic data with two projections.\n",
    "# You will compare clusters and gradients between projections.\n",
    "\n",
    "# import required libraries for arrays and plotting.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# set deterministic random seed for reproducible results.\n",
    "np.random.seed(42)\n",
    "\n",
    "# create two tight clusters and one gradient cluster.\n",
    "cluster_one = np.random.normal(loc=(-2, -2), scale=0.4, size=(40, 2))\n",
    "\n",
    "# create second cluster with different center location.\n",
    "cluster_two = np.random.normal(loc=(2, 2), scale=0.4, size=(40, 2))\n",
    "\n",
    "# create gradient points stretching between clusters.\n",
    "gradient_line = np.linspace(-2.0, 2.0, 40)\n",
    "\n",
    "# stack gradient coordinates into two dimensional array.\n",
    "gradient_cluster = np.column_stack((gradient_line, gradient_line))\n",
    "\n",
    "# combine all points into one dataset array.\n",
    "data = np.vstack((cluster_one, cluster_two, gradient_cluster))\n",
    "\n",
    "# validate dataset shape before plotting anything.\n",
    "assert data.shape == (120, 2), \"Unexpected data shape detected\"\n",
    "\n",
    "# create figure with two side by side subplots.\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\n",
    "\n",
    "# first subplot shows original coordinates scatter.\n",
    "axes[0].scatter(data[:, 0], data[:, 1], c=\"steelblue\", s=20, alpha=0.8)\n",
    "\n",
    "# add helpful title and axis labels for first subplot.\n",
    "axes[0].set_title(\"Projection A: clear clusters and gradient\")\n",
    "\n",
    "# label axes to remind these are combined features.\n",
    "axes[0].set_xlabel(\"Combined feature one\")\n",
    "axes[0].set_ylabel(\"Combined feature two\")\n",
    "\n",
    "# build a second projection mixing original coordinates.\n",
    "proj_x = (0.7 * data[:, 0]) + (0.3 * data[:, 1])\n",
    "\n",
    "# build second projection y using different mixing weights.\n",
    "proj_y = (0.2 * data[:, 0]) - (0.6 * data[:, 1])\n",
    "\n",
    "# second subplot shows mixed projection scatter.\n",
    "axes[1].scatter(proj_x, proj_y, c=\"darkorange\", s=20, alpha=0.8)\n",
    "\n",
    "# add title describing weaker separation in second projection.\n",
    "axes[1].set_title(\"Projection B: fuzzier clusters and trends\")\n",
    "\n",
    "# label axes to emphasize different combined features.\n",
    "axes[1].set_xlabel(\"Combined feature three\")\n",
    "axes[1].set_ylabel(\"Combined feature four\")\n",
    "\n",
    "# adjust layout for readability and display the figure.\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb8e1b1",
   "metadata": {},
   "source": [
    "### **3.3. Insights From Plots**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93ee68f",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_09/Lecture_B/image_03_03.jpg?v=1769972806\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Treat projections as clues about data organization\n",
    ">* Clusters, gradients, and shapes reveal hidden relationships\n",
    "\n",
    ">* Axes are abstract combined features, not originals\n",
    ">* Use axis movement and distances to interpret patterns\n",
    "\n",
    ">* 2D projections lose information, so interpret cautiously\n",
    ">* Use plots to form hypotheses, then verify elsewhere\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315c550f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Insights From Plots\n",
    "\n",
    "# This script shows simple two dimensional projections visually.\n",
    "# It helps connect scatter patterns with underlying feature structure.\n",
    "# We use synthetic customers to keep ideas very clear.\n",
    "\n",
    "# !pip install numpy.\n",
    "# !pip install matplotlib.\n",
    "# !pip install seaborn.\n",
    "\n",
    "# Import required libraries for numerical work and plotting.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set deterministic random seed for reproducible synthetic data.\n",
    "rng = np.random.default_rng(seed=42)\n",
    "\n",
    "# Create three small customer groups with simple spending patterns.\n",
    "cluster_centers = np.array([[2.0, 8.0, 3.0], [7.0, 2.0, 6.0], [4.5, 5.0, 8.0]])\n",
    "\n",
    "# Choose number of customers per group and total count.\n",
    "points_per_cluster = 20\n",
    "num_clusters = cluster_centers.shape[0]\n",
    "\n",
    "# Generate noisy points around each cluster center deterministically.\n",
    "noise = rng.normal(loc=0.0, scale=0.6, size=(points_per_cluster * num_clusters, 3))\n",
    "\n",
    "# Repeat centers and add noise to create full dataset.\n",
    "base = np.repeat(cluster_centers, repeats=points_per_cluster, axis=0)\n",
    "\n",
    "# Combine base and noise to obtain final three dimensional features.\n",
    "data = base + noise\n",
    "\n",
    "# Validate shape to avoid unexpected broadcasting mistakes.\n",
    "assert data.shape == (points_per_cluster * num_clusters, 3)\n",
    "\n",
    "# Manually build two combined features as simple projections.\n",
    "combined_one = (0.5 * data[:, 0]) + (0.5 * data[:, 1])\n",
    "\n",
    "# Build second combined feature emphasizing third original feature strongly.\n",
    "combined_two = (0.2 * data[:, 1]) + (0.8 * data[:, 2])\n",
    "\n",
    "# Stack combined features into two dimensional projection array.\n",
    "projection_2d = np.column_stack((combined_one, combined_two))\n",
    "\n",
    "# Validate projection shape before plotting for safety.\n",
    "assert projection_2d.shape[1] == 2 and projection_2d.shape[0] == data.shape[0]\n",
    "\n",
    "# Build simple cluster labels to color points by original group.\n",
    "labels = np.repeat(np.arange(num_clusters), repeats=points_per_cluster)\n",
    "\n",
    "# Print short explanation connecting plot to high dimensional structure.\n",
    "print(\"Each point is a customer summarized by two combined features.\")\n",
    "print(\"Colors show original groups that lived in three feature dimensions.\")\n",
    "print(\"Look for separated colored clouds or smooth gradients across axes.\")\n",
    "\n",
    "# Create scatter plot showing two dimensional projection of customers.\n",
    "plt.figure(figsize=(6, 5))\n",
    "\n",
    "# Use seaborn scatterplot for nicer default styling and legend.\n",
    "sns.scatterplot(x=projection_2d[:, 0], y=projection_2d[:, 1], hue=labels, palette=\"deep\")\n",
    "\n",
    "# Label axes to emphasize they are abstract combined features.\n",
    "plt.xlabel(\"Combined feature one summarizing two spending types\")\n",
    "plt.ylabel(\"Combined feature two summarizing another spending mix\")\n",
    "\n",
    "# Add title reminding viewers this is a compressed projection.\n",
    "plt.title(\"Reading a two dimensional projection of three dimensional customers\")\n",
    "\n",
    "# Show legend and final plot for visual interpretation practice.\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f266009",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Simplifying Features**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1f34c9",
   "metadata": {},
   "source": [
    "\n",
    "In this lecture, you learned to:\n",
    "- Explain why high-dimensional data can be hard to visualize and reason about. \n",
    "- Describe the idea of representing data with fewer combined features. \n",
    "- Interpret simple low-dimensional visualizations that summarize higher-dimensional data. \n",
    "\n",
    "In the next Module (Module 10), we will go over 'Responsible Practice'"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
