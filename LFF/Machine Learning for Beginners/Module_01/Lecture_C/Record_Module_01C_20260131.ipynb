{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4374cbf0",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**ML Problem Types**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acef8a9",
   "metadata": {},
   "source": [
    ">Last update: 20260131.\n",
    "    \n",
    "By the end of this Lecture, you will be able to:\n",
    "- Describe supervised, unsupervised, and reinforcement learning in intuitive terms. \n",
    "- Match simple problem descriptions to the appropriate machine learning category. \n",
    "- Identify the type of data and feedback signal required for each learning category. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d0b518",
   "metadata": {},
   "source": [
    "## **1. Supervised Learning Basics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39272e3",
   "metadata": {},
   "source": [
    "### **1.1. Inputs and Labels**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184fa735",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_01/Lecture_C/image_01_01.jpg?v=1769916633\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Model learns from paired inputs and answers\n",
    ">* Labels guide learning like a teacher’s corrections\n",
    "\n",
    ">* Inputs and labels can take many forms\n",
    ">* Models learn patterns from many input–label examples\n",
    "\n",
    ">* Messy inputs or wrong labels confuse models\n",
    ">* Carefully chosen features and accurate labels boost performance\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1b9ce0",
   "metadata": {},
   "source": [
    "### **1.2. Classification and Regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39a608d",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_01/Lecture_C/image_01_02.jpg?v=1769916644\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Classification predicts discrete category labels from examples\n",
    ">* Regression predicts continuous numeric values from features\n",
    "\n",
    ">* Classification chooses from fixed category options\n",
    ">* Regression predicts numeric values on continuous scales\n",
    "\n",
    ">* Same data can support both prediction types\n",
    ">* Choosing type guides data prep and evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11798b49",
   "metadata": {},
   "source": [
    "### **1.3. Loan Approval Prediction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12b484c",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_01/Lecture_C/image_01_03.jpg?v=1769916654\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Bank uses past applicants’ details and outcomes\n",
    ">* Model learns from feature–label pairs to predict\n",
    "\n",
    ">* Model repeatedly adjusts itself using labeled examples\n",
    ">* Learns numeric patterns to predict loan outcomes\n",
    "\n",
    ">* Uses labeled past data to learn decisions\n",
    ">* Applies learned patterns for accurate future predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9f854a",
   "metadata": {},
   "source": [
    "## **2. Finding Hidden Groups**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81418306",
   "metadata": {},
   "source": [
    "### **2.1. Input Only Scenarios**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebb2510",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_01/Lecture_C/image_02_01.jpg?v=1769916669\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Only inputs, no known correct labels given\n",
    ">* Model finds hidden patterns; this is unsupervised\n",
    "\n",
    ">* Supervised uses labeled examples to predict answers\n",
    ">* Unsupervised finds patterns from unlabeled input data\n",
    "\n",
    ">* Reinforcement learning involves actions, rewards, and sequences\n",
    ">* Unsupervised input scenarios use static data without labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611fcf0e",
   "metadata": {},
   "source": [
    "### **2.2. Discovering Hidden Structure**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337d0f54",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_01/Lecture_C/image_02_02.jpg?v=1769916682\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Many unlabeled examples; we suspect hidden patterns\n",
    ">* Algorithms group raw data, revealing underlying structure\n",
    "\n",
    ">* Exploratory questions signal hidden-structure style problems\n",
    ">* Algorithms group unlabeled data into meaningful clusters\n",
    "\n",
    ">* Prediction with known labels is different\n",
    ">* Grouping unlabeled data means discovering hidden structure\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaafa7fd",
   "metadata": {},
   "source": [
    "### **2.3. Customer Grouping Example**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd67d3f",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_01/Lecture_C/image_02_03.jpg?v=1769916694\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Retailer uses rich customer data without labels\n",
    ">* Algorithm finds unexpected customer groups for marketing\n",
    "\n",
    ">* Clustering groups customers with similar shopping behavior\n",
    ">* Humans interpret clusters and design targeted marketing actions\n",
    "\n",
    ">* Unsupervised learning finds patterns in unlabeled data\n",
    ">* Discovered clusters guide decisions across many domains\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2b6696",
   "metadata": {},
   "source": [
    "## **3. Reinforcement Learning Basics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdbfe04",
   "metadata": {},
   "source": [
    "### **3.1. Agent Environment Interaction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f82c64",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_01/Lecture_C/image_03_01.jpg?v=1769916708\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Agent repeatedly observes state and chooses actions\n",
    ">* Environment returns new state and reward over time\n",
    "\n",
    ">* Agent gets time-linked states, actions, rewards\n",
    ">* Feedback is delayed, noisy; learning uses ongoing interaction\n",
    "\n",
    ">* Agent actions create a continuous feedback loop\n",
    ">* Traffic light example shows learning from changing rewards\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d286a5d8",
   "metadata": {},
   "source": [
    "### **3.2. Rewards and Actions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976e5dee",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_01/Lecture_C/image_03_02.jpg?v=1769916719\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Agent acts, environment returns simple numeric rewards\n",
    ">* Agent learns to favor actions with higher rewards\n",
    "\n",
    ">* Actions are discrete or continuous ways to act\n",
    ">* Agent learns which actions pay off through rewards\n",
    "\n",
    ">* Agent creates data; rewards are sparse, delayed\n",
    ">* Learns policies maximizing long-term total reward\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65a1ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Rewards and Actions\n",
    "\n",
    "# This script illustrates reinforcement learning rewards and actions.\n",
    "# We simulate a tiny grid world environment example.\n",
    "# Focus on data feedback signals instead of model details.\n",
    "\n",
    "# Import required built in and numerical libraries.\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Set deterministic random seed for reproducible behavior.\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define grid world size and terminal goal position.\n",
    "GRID_ROWS, GRID_COLS = 2, 3\n",
    "GOAL_STATE = (0, 2)\n",
    "\n",
    "# Define available discrete actions for the agent.\n",
    "ACTIONS = [\"left\", \"right\", \"up\", \"down\"]\n",
    "\n",
    "# Define function returning next state and reward signal.\n",
    "def step(state, action):\n",
    "    row, col = state\n",
    "\n",
    "    # Compute candidate next position based on chosen action.\n",
    "    if action == \"left\":\n",
    "        col = max(col - 1, 0)\n",
    "    elif action == \"right\":\n",
    "        col = min(col + 1, GRID_COLS - 1)\n",
    "\n",
    "    if action == \"up\":\n",
    "        row = max(row - 1, 0)\n",
    "    elif action == \"down\":\n",
    "        row = min(row + 1, GRID_ROWS - 1)\n",
    "\n",
    "    next_state = (row, col)\n",
    "\n",
    "    # Assign reward based on reaching goal or moving otherwise.\n",
    "    if next_state == GOAL_STATE:\n",
    "        reward = 1.0\n",
    "    else:\n",
    "        reward = -0.1\n",
    "\n",
    "    return next_state, reward\n",
    "\n",
    "# Define simple policy choosing random actions uniformly.\n",
    "def random_policy(state):\n",
    "    _ = state\n",
    "    return random.choice(ACTIONS)\n",
    "\n",
    "# Run several episodes to collect actions and rewards.\n",
    "num_episodes = 5\n",
    "max_steps_per_episode = 6\n",
    "\n",
    "# Store trajectories for later inspection and printing.\n",
    "trajectories = []\n",
    "\n",
    "# Simulate episodes where agent explores environment.\n",
    "for episode in range(num_episodes):\n",
    "    state = (1, 0)\n",
    "    episode_steps = []\n",
    "\n",
    "    for step_index in range(max_steps_per_episode):\n",
    "        action = random_policy(state)\n",
    "        next_state, reward = step(state, action)\n",
    "\n",
    "        episode_steps.append((state, action, reward, next_state))\n",
    "        state = next_state\n",
    "\n",
    "        if state == GOAL_STATE:\n",
    "            break\n",
    "\n",
    "    trajectories.append(episode_steps)\n",
    "\n",
    "# Print concise summary emphasizing actions and rewards.\n",
    "print(\"Episode summaries showing actions and reward feedback:\")\n",
    "\n",
    "# Iterate through trajectories and display key information.\n",
    "for episode_index, episode_steps in enumerate(trajectories):\n",
    "    total_reward = sum(step_info[2] for step_info in episode_steps)\n",
    "\n",
    "    first_step = episode_steps[0]\n",
    "    last_step = episode_steps[-1]\n",
    "\n",
    "    print(\n",
    "        \"Episode\", episode_index + 1,\n",
    "        \"started\", first_step[0],\n",
    "        \"ended\", last_step[3],\n",
    "        \"total_reward\", round(total_reward, 2),\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f5b23f",
   "metadata": {},
   "source": [
    "### **3.3. Game Playing Agent**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02404153",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_01/Lecture_C/image_03_03.jpg?v=1769916762\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Agent sees game state and chooses moves\n",
    ">* Learns from delayed rewards instead of correct labels\n",
    "\n",
    ">* Agent experiences state, chooses actions, receives rewards\n",
    ">* Interactive trajectories teach strategies for long-term reward\n",
    "\n",
    ">* RL uses delayed, sparse, sometimes noisy rewards\n",
    ">* Agent learns policies from long histories of interactions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61249a8e",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**ML Problem Types**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2a5708",
   "metadata": {},
   "source": [
    "\n",
    "In this lecture, you learned to:\n",
    "- Describe supervised, unsupervised, and reinforcement learning in intuitive terms. \n",
    "- Match simple problem descriptions to the appropriate machine learning category. \n",
    "- Identify the type of data and feedback signal required for each learning category. \n",
    "\n",
    "In the next Module (Module 2), we will go over 'Data And Features'"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
