{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb6a7919",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Classification Metrics**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599da524",
   "metadata": {},
   "source": [
    ">Last update: 20260201.\n",
    "    \n",
    "By the end of this Lecture, you will be able to:\n",
    "- Define accuracy as the fraction of correctly classified examples. \n",
    "- Construct simple confusion-style summaries from prediction results. \n",
    "- Explain why accuracy alone may be insufficient in some classification problems. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e18657",
   "metadata": {},
   "source": [
    "## **1. Understanding Accuracy**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cf40e5",
   "metadata": {},
   "source": [
    "### **1.1. Correct prediction fraction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb353a91",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_06/Lecture_B/image_01_01.jpg?v=1769962652\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Accuracy is correct predictions over all predictions\n",
    ">* Each example is one chance to be right\n",
    "\n",
    ">* Accuracy is simple, everyday performance comparison\n",
    ">* Count correct predictions, divide by total cases\n",
    "\n",
    ">* Accuracy depends on the specific evaluation dataset\n",
    ">* Different example sets can change measured accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d3385b",
   "metadata": {},
   "source": [
    "### **1.2. Accuracy as Percentage**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965819b4",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_06/Lecture_B/image_01_02.jpg?v=1769962662\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Accuracy is the percentage of correct predictions\n",
    ">* Percentages give a familiar, easy-to-read summary\n",
    "\n",
    ">* Percent accuracy makes model comparisons immediately clear\n",
    ">* Percentages summarize progress and support business decisions\n",
    "\n",
    ">* Percent accuracy hides large error counts at scale\n",
    ">* Small percentage changes can greatly impact real outcomes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e4a426",
   "metadata": {},
   "source": [
    "### **1.3. Limits of Accuracy**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e76a37e",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_06/Lecture_B/image_01_03.jpg?v=1769962675\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* High accuracy can hide costly classification mistakes\n",
    ">* Rare but important cases may be missed\n",
    "\n",
    ">* Accuracy hides which specific errors are made\n",
    ">* Can mask unfair treatment of certain groups\n",
    "\n",
    ">* Accuracy ignores different costs of prediction errors\n",
    ">* Threshold choices and real impacts require extra metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f69ffd",
   "metadata": {},
   "source": [
    "## **2. Classification Error Breakdown**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5dfdda",
   "metadata": {},
   "source": [
    "### **2.1. Predicted Class Counts**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180e1786",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_06/Lecture_B/image_02_01.jpg?v=1769962691\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Count how many predictions fall in each class\n",
    ">* These counts show decision distribution and guide analysis\n",
    "\n",
    ">* Predicted label counts reveal imbalance and bias\n",
    ">* Rarely used labels signal collapsed or missing classes\n",
    "\n",
    ">* Predicted counts link model decisions to real impacts\n",
    ">* They guide deeper analysis of important classification errors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4179f594",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Predicted Class Counts\n",
    "\n",
    "# This script explores predicted class counts simply.\n",
    "# We use tiny example predictions for clarity.\n",
    "# Focus on counting how often each label appears.\n",
    "\n",
    "# Import collections for convenient counting tools.\n",
    "from collections import Counter\n",
    "\n",
    "# Create a tiny list of predicted labels.\n",
    "predicted_labels = [\n",
    "    \"spam\",\n",
    "    \"not_spam\",\n",
    "    \"spam\",\n",
    "    \"spam\",\n",
    "\n",
    "    \"not_spam\",\n",
    "    \"spam\",\n",
    "    \"not_spam\",\n",
    "    \"not_spam\",\n",
    "]\n",
    "\n",
    "# Confirm the list has the expected small size.\n",
    "if not isinstance(predicted_labels, list):\n",
    "    raise TypeError(\"predicted_labels must be a list here\")\n",
    "\n",
    "# Use Counter to count how often each label appears.\n",
    "predicted_counts = Counter(predicted_labels)\n",
    "\n",
    "# Convert counts into a sorted list of (label, count) pairs.\n",
    "sorted_counts = sorted(\n",
    "    predicted_counts.items(),\n",
    "    key=lambda pair: pair[0],\n",
    ")\n",
    "\n",
    "# Compute the total number of predictions for later percentages.\n",
    "total_predictions = len(predicted_labels)\n",
    "\n",
    "# Guard against division by zero in case list is empty.\n",
    "if total_predictions == 0:\n",
    "    raise ValueError(\"There must be at least one prediction here\")\n",
    "\n",
    "# Print a short header explaining the upcoming summary.\n",
    "print(\"Predicted class counts for our tiny spam classifier:\")\n",
    "\n",
    "# Loop through each label and show count and percentage.\n",
    "for label, count in sorted_counts:\n",
    "    percentage = (count / total_predictions) * 100.0\n",
    "    print(\n",
    "        f\"Label '{label}' predicted {count} times, {percentage:.1f}% of predictions.\"\n",
    "    )\n",
    "\n",
    "# Print a final line summarizing the total number of predictions.\n",
    "print(\"Total number of predictions considered:\", total_predictions)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94c9e55",
   "metadata": {},
   "source": [
    "### **2.2. Per Class Outcomes**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d4436c",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_06/Lecture_B/image_02_02.jpg?v=1769962722\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Look at results for each class separately\n",
    ">* Compare correct hits, misses, false alarms, rejections\n",
    "\n",
    ">* Pick one target class, group all others\n",
    ">* Count correct, missed, and mistaken predictions per class\n",
    "\n",
    ">* Per class results reveal domain-specific behavior\n",
    ">* They show strengths, failures, and improvement opportunities\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b562a09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Per Class Outcomes\n",
    "\n",
    "# This script explores per class classification outcomes.\n",
    "# We manually build a tiny prediction example.\n",
    "# Then we compute confusion style counts per class.\n",
    "\n",
    "# Required libraries are already available in Colab.\n",
    "# No additional installations are necessary here.\n",
    "\n",
    "# Define tiny true labels for a three class problem.\n",
    "true_labels = [\"bear\", \"bear\", \"deer\", \"fox\", \"bear\", \"deer\"]\n",
    "\n",
    "# Define corresponding model predictions for each example.\n",
    "pred_labels = [\"bear\", \"deer\", \"deer\", \"bear\", \"bear\", \"fox\"]\n",
    "\n",
    "# Collect the unique classes appearing in true labels.\n",
    "classes = sorted(list(set(true_labels)))\n",
    "\n",
    "# Validate that labels and predictions have equal lengths.\n",
    "assert len(true_labels) == len(pred_labels)\n",
    "\n",
    "# Define a function computing per class outcome counts.\n",
    "def per_class_outcomes(true_list, pred_list, class_name):\n",
    "    # Initialize counters for four outcome types.\n",
    "    tp_count = 0\n",
    "    fn_count = 0\n",
    "    fp_count = 0\n",
    "    tn_count = 0\n",
    "\n",
    "    # Loop through all paired true and predicted labels.\n",
    "    for true_value, pred_value in zip(true_list, pred_list):\n",
    "        # Check if current example truly belongs to class.\n",
    "        is_true_class = true_value == class_name\n",
    "        # Check if prediction assigns current example to class.\n",
    "        is_pred_class = pred_value == class_name\n",
    "\n",
    "        # Update counts based on true and predicted membership.\n",
    "        if is_true_class and is_pred_class:\n",
    "            tp_count += 1\n",
    "        elif is_true_class and not is_pred_class:\n",
    "            fn_count += 1\n",
    "        elif (not is_true_class) and is_pred_class:\n",
    "            fp_count += 1\n",
    "        else:\n",
    "            tn_count += 1\n",
    "\n",
    "    # Return a dictionary summarizing the four outcomes.\n",
    "    return {\"TP\": tp_count, \"FN\": fn_count, \"FP\": fp_count, \"TN\": tn_count}\n",
    "\n",
    "# Print a short header describing the tiny dataset.\n",
    "print(\"True labels:\", true_labels, \"Predicted labels:\", pred_labels)\n",
    "\n",
    "# Loop over each class and display its outcome summary.\n",
    "for current_class in classes:\n",
    "    # Compute outcomes for the current focus class.\n",
    "    outcomes = per_class_outcomes(true_labels, pred_labels, current_class)\n",
    "\n",
    "    # Nicely format the per class confusion style summary.\n",
    "    print(\"Class\", current_class, \"->\", outcomes)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9202cff",
   "metadata": {},
   "source": [
    "### **2.3. Finding Systematic Errors**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9768742a",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_06/Lecture_B/image_02_03.jpg?v=1769962759\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Use confusion summaries to spot repeated misclassifications\n",
    ">* Scan rows and columns to locate error clusters\n",
    "\n",
    ">* Relate confusion cells to real-world class meanings\n",
    ">* Use patterns to find and prioritize serious errors\n",
    "\n",
    ">* Inspect examples from confusion cells to diagnose causes\n",
    ">* Use insights to improve data and model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1338b644",
   "metadata": {},
   "source": [
    "## **3. Why Context Matters**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5417ffe3",
   "metadata": {},
   "source": [
    "### **3.1. Imbalanced Class Pitfalls**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc39374",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_06/Lecture_B/image_03_01.jpg?v=1769962771\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* High accuracy can hide imbalanced class problems\n",
    ">* Model may miss rare, important minority cases\n",
    "\n",
    ">* High accuracy can ignore rare critical events\n",
    ">* Minority class errors matter more than accuracy\n",
    "\n",
    ">* Similar accuracy can hide minority class failures\n",
    ">* Use class-specific metrics to avoid misleading accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0323d088",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Imbalanced Class Pitfalls\n",
    "\n",
    "# This script shows imbalanced class accuracy pitfalls.\n",
    "# We use a tiny medical style disease example.\n",
    "# Focus on accuracy and confusion style summaries.\n",
    "\n",
    "# No extra installations are required for this script.\n",
    "# All used libraries are available by default.\n",
    "# You can run everything directly in Colab.\n",
    "\n",
    "# Import numpy for simple array handling.\n",
    "import numpy as np\n",
    "\n",
    "# Set a deterministic random seed value.\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create labels for one thousand patients.\n",
    "num_patients = 1000\n",
    "\n",
    "# Define rare disease count and healthy count.\n",
    "num_disease = 10\n",
    "num_healthy = num_patients - num_disease\n",
    "\n",
    "# Build true labels array with zeros and ones.\n",
    "true_labels = np.array(([1] * num_disease) + ([0] * num_healthy))\n",
    "\n",
    "# Shuffle labels to avoid ordered structure.\n",
    "np.random.shuffle(true_labels)\n",
    "\n",
    "# Validate labels shape before further operations.\n",
    "assert true_labels.shape == (num_patients,)\n",
    "\n",
    "# Model A predicts everyone as healthy always.\n",
    "pred_all_healthy = np.zeros_like(true_labels)\n",
    "\n",
    "# Model B randomly guesses disease with small probability.\n",
    "pred_random = (np.random.rand(num_patients) < 0.02).astype(int)\n",
    "\n",
    "# Define a helper function computing accuracy safely.\n",
    "def compute_accuracy(y_true, y_pred):\n",
    "    assert y_true.shape == y_pred.shape\n",
    "    correct = np.sum(y_true == y_pred)\n",
    "    return correct / y_true.size\n",
    "\n",
    "\n",
    "# Define a helper function building confusion style counts.\n",
    "def confusion_counts(y_true, y_pred):\n",
    "    assert y_true.shape == y_pred.shape\n",
    "    tp = int(np.sum((y_true == 1) & (y_pred == 1)))\n",
    "    fn = int(np.sum((y_true == 1) & (y_pred == 0)))\n",
    "    tn = int(np.sum((y_true == 0) & (y_pred == 0)))\n",
    "    fp = int(np.sum((y_true == 0) & (y_pred == 1)))\n",
    "    return tp, fn, tn, fp\n",
    "\n",
    "\n",
    "# Compute accuracy for both simple models.\n",
    "acc_all_healthy = compute_accuracy(true_labels, pred_all_healthy)\n",
    "acc_random = compute_accuracy(true_labels, pred_random)\n",
    "\n",
    "# Compute confusion style summaries for both models.\n",
    "conf_all_healthy = confusion_counts(true_labels, pred_all_healthy)\n",
    "conf_random = confusion_counts(true_labels, pred_random)\n",
    "\n",
    "# Unpack confusion counts for readability and printing.\n",
    "(tp_a, fn_a, tn_a, fp_a) = conf_all_healthy\n",
    "(tp_b, fn_b, tn_b, fp_b) = conf_random\n",
    "\n",
    "# Print overall accuracy for both models.\n",
    "print(\"Model A accuracy value:\", round(acc_all_healthy, 4))\n",
    "print(\"Model B accuracy value:\", round(acc_random, 4))\n",
    "\n",
    "# Print confusion style summary for model A.\n",
    "print(\"Model A counts tp fn tn fp:\", tp_a, fn_a, tn_a, fp_a)\n",
    "\n",
    "# Print confusion style summary for model B.\n",
    "print(\"Model B counts tp fn tn fp:\", tp_b, fn_b, tn_b, fp_b)\n",
    "\n",
    "# Print short explanation highlighting minority class performance.\n",
    "print(\"Notice similar accuracy but very different disease detection performance.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f84b8e",
   "metadata": {},
   "source": [
    "### **3.2. Error Costs in Classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1017592a",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_06/Lecture_B/image_03_02.jpg?v=1769962818\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Different error types can have unequal consequences\n",
    ">* Relying only on accuracy can misguide models\n",
    "\n",
    ">* Different errors have very different real costs\n",
    ">* Accuracy can hide costly mistakes like missed fraud\n",
    "\n",
    ">* Different mistakes have very different real impacts\n",
    ">* Choose metrics that match real-world error costs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75c067f",
   "metadata": {},
   "source": [
    "### **3.3. Selecting Appropriate Metrics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093a544d",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_06/Lecture_B/image_03_03.jpg?v=1769962828\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Choose metrics based on task and consequences\n",
    ">* Different problems prioritize different error types, metrics\n",
    "\n",
    ">* Metrics must handle rare classes and imbalance\n",
    ">* Threshold-based metrics reveal trade-offs and guide choices\n",
    "\n",
    ">* Balance easy-to-explain metrics with technical detail\n",
    ">* Use multiple metrics to capture fairness and costs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ae5e76",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Classification Metrics**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e4ed16",
   "metadata": {},
   "source": [
    "\n",
    "In this lecture, you learned to:\n",
    "- Define accuracy as the fraction of correctly classified examples. \n",
    "- Construct simple confusion-style summaries from prediction results. \n",
    "- Explain why accuracy alone may be insufficient in some classification problems. \n",
    "\n",
    "In the next Module (Module 7), we will go over 'Data Preparation'"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
