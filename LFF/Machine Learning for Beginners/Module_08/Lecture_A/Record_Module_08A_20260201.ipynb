{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39132f02",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Curved Relationships**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea15aa28",
   "metadata": {},
   "source": [
    ">Last update: 20260201.\n",
    "    \n",
    "By the end of this Lecture, you will be able to:\n",
    "- Recognize visual patterns in data that suggest nonlinear relationships. \n",
    "- Describe how adding transformed features can create curved prediction shapes. \n",
    "- Explain the trade-off between flexibility and overfitting in nonlinear models. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13034cb9",
   "metadata": {},
   "source": [
    "## **1. Seeing Curved Patterns**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40117110",
   "metadata": {},
   "source": [
    "### **1.1. Recognizing Curved Trends**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef45bc5a",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_08/Lecture_A/image_01_01.jpg?v=1769966838\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Look for changing direction or steepness in points\n",
    ">* Curved patterns show straight-line models may fail\n",
    "\n",
    ">* Curved trends appear as arches, rises, plateaus\n",
    ">* Key sign is changing, non-constant rate of change\n",
    "\n",
    ">* Check if points systematically dodge a straight line\n",
    ">* Look for bending patterns and wave-like residuals\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4393ddcd",
   "metadata": {},
   "source": [
    "### **1.2. Limits of Straight Lines**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3f610c",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_08/Lecture_A/image_01_02.jpg?v=1769966851\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Straight lines assume a constant rate of change\n",
    ">* They fail when relationships speed up or reverse\n",
    "\n",
    ">* Real processes often speed up then level off\n",
    ">* Straight lines miss plateaus and turning point effects\n",
    "\n",
    ">* Residual patterns reveal when straight lines fail\n",
    ">* Curved data like U-shapes need flexible models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c9b1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Limits of Straight Lines\n",
    "\n",
    "# This script shows limits of straight lines visually.\n",
    "# We compare a straight line to a curved pattern.\n",
    "# Focus on seeing mismatches between line and data.\n",
    "\n",
    "# Required plotting libraries are already available in Colab.\n",
    "# You could install them with pip if needed.\n",
    "# Example commented command is shown for completeness.\n",
    "# !pip install matplotlib seaborn numpy.\n",
    "\n",
    "# Import required libraries for numbers and plotting.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set a deterministic random seed for reproducibility.\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create predictor values evenly spaced along one dimension.\n",
    "x_values = np.linspace(-3.0, 3.0, 40)\n",
    "\n",
    "# Generate a curved true relationship using a simple square.\n",
    "true_curve = 0.6 * (x_values ** 2) + 1.0\n",
    "\n",
    "# Add small random noise to create scattered data points.\n",
    "noise_values = np.random.normal(loc=0.0, scale=0.8, size=x_values.shape)\n",
    "\n",
    "# Combine curve and noise to form observed outcomes.\n",
    "y_observed = true_curve + noise_values\n",
    "\n",
    "# Fit a simple straight line using numpy polyfit function.\n",
    "line_coeffs = np.polyfit(x_values, y_observed, deg=1)\n",
    "\n",
    "# Evaluate the fitted straight line across all x values.\n",
    "y_line = np.polyval(line_coeffs, x_values)\n",
    "\n",
    "# Compute residuals as observed minus straight line predictions.\n",
    "residuals = y_observed - y_line\n",
    "\n",
    "# Print a short summary describing the fitted straight line.\n",
    "print(\"Fitted straight line slope and intercept:\", line_coeffs)\n",
    "\n",
    "# Print a simple check on residuals average magnitude.\n",
    "print(\"Average absolute residual size:\", np.mean(np.abs(residuals)))\n",
    "\n",
    "# Create a new figure with a clear size for visibility.\n",
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "# Plot the noisy curved data as scattered blue points.\n",
    "plt.scatter(x_values, y_observed, color=\"blue\", label=\"Observed data\")\n",
    "\n",
    "# Plot the true underlying curve as a smooth green line.\n",
    "plt.plot(x_values, true_curve, color=\"green\", label=\"True curved pattern\")\n",
    "\n",
    "# Plot the fitted straight line as a red dashed line.\n",
    "plt.plot(x_values, y_line, color=\"red\", linestyle=\"--\", label=\"Straight line fit\")\n",
    "\n",
    "# Label axes to remind viewers what each direction represents.\n",
    "plt.xlabel(\"Predictor value (x)\")\n",
    "\n",
    "# Label vertical axis to show outcome measurements.\n",
    "plt.ylabel(\"Outcome value (y)\")\n",
    "\n",
    "# Add a title emphasizing limits of straight line models.\n",
    "plt.title(\"Straight line struggling to follow a curved relationship\")\n",
    "\n",
    "# Show legend so learners can distinguish elements clearly.\n",
    "plt.legend()\n",
    "\n",
    "# Display the final plot to visually inspect mismatches.\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7c7fd8",
   "metadata": {},
   "source": [
    "### **1.3. Spotting Nonlinear Shapes**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e6e710",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_08/Lecture_A/image_01_03.jpg?v=1769966874\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Look at the overall scatterplot shape, not points\n",
    ">* Curved, hill-like patterns signal nonlinear relationships\n",
    "\n",
    ">* Look for U, inverted U, or S-shapes\n",
    ">* Notice systematic direction changes no straight line fits\n",
    "\n",
    ">* Compare data to an imagined straight line\n",
    ">* Look for consistent misses that suggest curvature\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d2079d",
   "metadata": {},
   "source": [
    "## **2. Curved Predictions with Features**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9471d780",
   "metadata": {},
   "source": [
    "### **2.1. Squared Features Intuition**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337dec6b",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_08/Lecture_A/image_02_01.jpg?v=1769966888\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Linear models give constant-slope, straight-line predictions\n",
    ">* Adding squared features lets linear models create curves\n",
    "\n",
    ">* Squaring makes large predictor values stand out\n",
    ">* Model captures study benefits that rise then fade\n",
    "\n",
    ">* Squared features model optimal middle, extremes worse\n",
    ">* They capture smooth curved patterns across many domains\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b8281c",
   "metadata": {},
   "source": [
    "### **2.2. Feature Interactions Explained**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a958810d",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_08/Lecture_A/image_02_02.jpg?v=1769966899\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Interaction means one featureâ€™s effect depends on another\n",
    ">* Interaction terms create curved, non-additive prediction surfaces\n",
    "\n",
    ">* Interaction between dosage and age changes effects\n",
    ">* Creates twisted, curved prediction surfaces across patients\n",
    "\n",
    ">* Interactions capture effects of combined conditions across domains\n",
    ">* They create curved prediction shapes by features working together\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dfdab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Feature Interactions Explained\n",
    "\n",
    "# This script shows simple feature interactions visually.\n",
    "# We compare additive and interaction based predictions.\n",
    "# Focus on how interaction creates curved prediction surfaces.\n",
    "\n",
    "# import required numerical and plotting libraries.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# set deterministic random seed for reproducible behavior.\n",
    "np.random.seed(0)\n",
    "\n",
    "# create small grids for dosage and age features.\n",
    "dosage_values = np.linspace(0.0, 10.0, 30)\n",
    "\n",
    "# create age values representing younger and older patients.\n",
    "age_values = np.linspace(20.0, 80.0, 30)\n",
    "\n",
    "# build two dimensional meshgrid for surface calculations.\n",
    "D_grid, A_grid = np.meshgrid(dosage_values, age_values)\n",
    "\n",
    "# define simple additive model without interaction feature.\n",
    "additive_effect = 0.3 * D_grid + 0.02 * A_grid\n",
    "\n",
    "# define interaction model including dosage age product term.\n",
    "interaction_effect = (\n",
    "    0.1 * D_grid + 0.01 * A_grid + 0.002 * D_grid * A_grid\n",
    ")\n",
    "\n",
    "# verify shapes match to avoid broadcasting surprises.\n",
    "assert additive_effect.shape == interaction_effect.shape\n",
    "\n",
    "# create figure and two side by side subplots.\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10.0, 4.0))\n",
    "\n",
    "# plot additive model as simple straight prediction surface.\n",
    "add_plot = axes[0].contourf(\n",
    "    D_grid,\n",
    "    A_grid,\n",
    "    additive_effect,\n",
    "    levels=15,\n",
    "    cmap=\"Blues\",\n",
    ")\n",
    "\n",
    "# label first subplot to highlight additive straight behavior.\n",
    "axes[0].set_title(\"Additive model surface\")\n",
    "axes[0].set_xlabel(\"Dosage level\")\n",
    "axes[0].set_ylabel(\"Age in years\")\n",
    "\n",
    "# plot interaction model showing curved prediction surface.\n",
    "int_plot = axes[1].contourf(\n",
    "    D_grid,\n",
    "    A_grid,\n",
    "    interaction_effect,\n",
    "    levels=15,\n",
    "    cmap=\"Reds\",\n",
    ")\n",
    "\n",
    "# label second subplot to emphasize interaction curvature.\n",
    "axes[1].set_title(\"Interaction model surface\")\n",
    "axes[1].set_xlabel(\"Dosage level\")\n",
    "axes[1].set_ylabel(\"Age in years\")\n",
    "\n",
    "# adjust layout and display the final comparison figure.\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c098646",
   "metadata": {},
   "source": [
    "### **2.3. How Features Bend Predictions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd72ed6",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_08/Lecture_A/image_02_03.jpg?v=1769966936\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Transformed features push and pull prediction surfaces\n",
    ">* Different transforms create distinct curves and twists\n",
    "\n",
    ">* Transformed features bend flat prediction surfaces into curves\n",
    ">* Squared and interaction terms create realistic peaks and valleys\n",
    "\n",
    ">* Transformed features curve risk differently across groups\n",
    ">* They act like levers shaping prediction geometry\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e57511",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - How Features Bend Predictions\n",
    "\n",
    "# This script shows how features bend predictions.\n",
    "# We compare linear and curved feature based predictions.\n",
    "# Focus on simple squared feature bending a straight line.\n",
    "\n",
    "# import required numerical and plotting libraries.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# set deterministic random seed for reproducible noise.\n",
    "np.random.seed(0)\n",
    "\n",
    "# create simple feature values representing sunlight levels.\n",
    "sunlight = np.linspace(0.0, 10.0, 30)\n",
    "\n",
    "# define true curved relationship using a squared term.\n",
    "true_growth = 4.0 * sunlight - 0.4 * (sunlight ** 2)\n",
    "\n",
    "# add small noise to simulate observed plant growth.\n",
    "noise = np.random.normal(loc=0.0, scale=1.0, size=sunlight.shape)\n",
    "\n",
    "# compute observed growth values with noise included.\n",
    "observed_growth = true_growth + noise\n",
    "\n",
    "# build linear prediction using only raw sunlight feature.\n",
    "linear_pred = 1.0 + 1.0 * sunlight\n",
    "\n",
    "# build curved prediction using sunlight and sunlight squared.\n",
    "curved_pred = 1.0 + 1.0 * sunlight - 0.1 * (sunlight ** 2)\n",
    "\n",
    "# print short explanation of both prediction shapes.\n",
    "print(\"Linear prediction changes at a constant rate with sunlight.\")\n",
    "print(\"Curved prediction bends because of the squared sunlight feature.\")\n",
    "\n",
    "# create figure and axis for visual comparison plot.\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "# plot noisy observed growth as scatter points.\n",
    "ax.scatter(sunlight, observed_growth, color=\"black\", label=\"Observed growth\")\n",
    "\n",
    "# plot linear prediction line showing straight pattern.\n",
    "ax.plot(sunlight, linear_pred, color=\"blue\", label=\"Linear prediction\")\n",
    "\n",
    "# plot curved prediction line showing bending pattern.\n",
    "ax.plot(sunlight, curved_pred, color=\"red\", label=\"Curved prediction\")\n",
    "\n",
    "# label axes and add descriptive title.\n",
    "ax.set_xlabel(\"Sunlight level (arbitrary units)\")\n",
    "ax.set_ylabel(\"Plant growth (arbitrary units)\")\n",
    "\n",
    "# add legend explaining each curve on plot.\n",
    "ax.legend()\n",
    "\n",
    "# display the final plot to visualize bending effect.\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69737ce",
   "metadata": {},
   "source": [
    "## **3. Balancing Flexibility and Overfitting**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7805052e",
   "metadata": {},
   "source": [
    "### **3.1. Fitting complex patterns**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c234b477",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_08/Lecture_A/image_03_01.jpg?v=1769966956\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Real-world data often follow curved, changing patterns\n",
    ">* Nonlinear models flexibly capture curves, plateaus, turning points\n",
    "\n",
    ">* Nonlinear models capture rise, peak, and decline\n",
    ">* Extra flexibility improves prediction accuracy and insights\n",
    "\n",
    ">* Match model complexity to data signal complexity\n",
    ">* Capture thresholds, saturation, and nuanced curved behaviors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569bdd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Fitting complex patterns\n",
    "\n",
    "# This script shows fitting simple and complex curves.\n",
    "# It illustrates flexibility versus overfitting with polynomials.\n",
    "# We use tiny synthetic data for clear visualization.\n",
    "\n",
    "# import required numerical and plotting libraries.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# set deterministic random seed for reproducibility.\n",
    "np.random.seed(0)\n",
    "\n",
    "# create one dimensional feature values with clear curvature.\n",
    "x_values = np.linspace(0, 10, 20)\n",
    "\n",
    "# generate true curved relationship with small noise.\n",
    "true_y = 0.5 * (x_values ** 2) - 3 * x_values\n",
    "\n",
    "# add controlled noise to simulate measurement imperfections.\n",
    "noise = np.random.normal(loc=0.0, scale=5.0, size=x_values.shape)\n",
    "\n",
    "# compute observed targets as true curve plus noise.\n",
    "y_observed = true_y + noise\n",
    "\n",
    "# build polynomial design matrix for chosen degree.\n",
    "def build_polynomial_features(x_array, degree_value):\n",
    "    # validate input shapes before feature construction.\n",
    "    x_array = np.asarray(x_array).reshape(-1, 1)\n",
    "\n",
    "    # start with bias column of ones for intercept.\n",
    "    features = [np.ones_like(x_array)]\n",
    "\n",
    "    # iteratively append higher power columns up to degree.\n",
    "    for power in range(1, degree_value + 1):\n",
    "        features.append(x_array ** power)\n",
    "\n",
    "    # concatenate all feature columns horizontally.\n",
    "    design_matrix = np.concatenate(features, axis=1)\n",
    "\n",
    "    # return final two dimensional design matrix.\n",
    "    return design_matrix\n",
    "\n",
    "# compute least squares solution for polynomial regression.\n",
    "def fit_polynomial_regression(x_array, y_array, degree_value):\n",
    "    # build polynomial features for given degree.\n",
    "    X_design = build_polynomial_features(x_array, degree_value)\n",
    "\n",
    "    # ensure target vector has correct shape.\n",
    "    y_vector = np.asarray(y_array).reshape(-1, 1)\n",
    "\n",
    "    # solve normal equations using numpy linear algebra.\n",
    "    coefficients, *_ = np.linalg.lstsq(X_design, y_vector, rcond=None)\n",
    "\n",
    "    # return flattened coefficient vector for convenience.\n",
    "    return coefficients.flatten()\n",
    "\n",
    "# evaluate polynomial model predictions on new grid.\n",
    "def predict_polynomial(x_array, coefficients):\n",
    "    # determine polynomial degree from coefficient length.\n",
    "    degree_value = len(coefficients) - 1\n",
    "\n",
    "    # rebuild design matrix using same degree.\n",
    "    X_design = build_polynomial_features(x_array, degree_value)\n",
    "\n",
    "    # compute predictions using matrix multiplication.\n",
    "    predictions = X_design @ coefficients.reshape(-1, 1)\n",
    "\n",
    "    # return flattened prediction array for plotting.\n",
    "    return predictions.flatten()\n",
    "\n",
    "# choose two degrees representing simple and complex models.\n",
    "degrees = [2, 10]\n",
    "\n",
    "# prepare smooth grid for drawing prediction curves.\n",
    "x_grid = np.linspace(0, 10, 200)\n",
    "\n",
    "# create figure and axis for single comparison plot.\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "# scatter plot observed noisy data points.\n",
    "ax.scatter(x_values, y_observed, color=\"black\", label=\"data points\")\n",
    "\n",
    "# loop over degrees and plot corresponding fitted curves.\n",
    "for degree_value in degrees:\n",
    "    # fit polynomial model for current degree.\n",
    "    coeffs = fit_polynomial_regression(x_values, y_observed, degree_value)\n",
    "\n",
    "    # compute predictions across smooth grid.\n",
    "    y_pred_grid = predict_polynomial(x_grid, coeffs)\n",
    "\n",
    "    # choose linestyle based on model complexity.\n",
    "    style = \"-\" if degree_value == 2 else \"--\"\n",
    "\n",
    "    # add curve to plot with informative label.\n",
    "    ax.plot(x_grid, y_pred_grid, linestyle=style, label=f\"degree {degree_value}\")\n",
    "\n",
    "# add axis labels describing variables clearly.\n",
    "ax.set_xlabel(\"feature x representing input quantity\")\n",
    "\n",
    "# label vertical axis as predicted outcome values.\n",
    "ax.set_ylabel(\"predicted outcome based on model\")\n",
    "\n",
    "# add concise title emphasizing flexibility versus overfitting.\n",
    "ax.set_title(\"Simple versus very flexible polynomial fits\")\n",
    "\n",
    "# display legend to distinguish model curves.\n",
    "ax.legend()\n",
    "\n",
    "# print short explanation connecting plot to lecture ideas.\n",
    "print(\"Degree 2 follows main curve without chasing every noisy point.\")\n",
    "\n",
    "# print second line describing highly flexible degree ten behavior.\n",
    "print(\"Degree 10 twists sharply, showing risk of overfitting noise.\")\n",
    "\n",
    "# finally display the constructed comparison plot.\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243e5daf",
   "metadata": {},
   "source": [
    "### **3.2. Overfitting Dangers**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5d6a5a",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_08/Lecture_A/image_03_02.jpg?v=1769966987\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Flexible nonlinear models can closely follow training data\n",
    ">* They may memorize noise, hurting performance on new data\n",
    "\n",
    ">* Flexible models may chase temporary housing price dips\n",
    ">* They fit noise as pattern, hurting future predictions\n",
    "\n",
    ">* Overfit curves can look realistic yet misleading\n",
    ">* They fail on new data, causing harmful decisions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc18b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Overfitting Dangers\n",
    "\n",
    "# This script shows nonlinear overfitting dangers visually.\n",
    "# We compare simple and wiggly curves on noisy data.\n",
    "# Focus on training versus testing prediction errors carefully.\n",
    "\n",
    "# Required libraries are available in Colab environment already.\n",
    "# Uncomment next lines only if running elsewhere needs installs.\n",
    "# import pip for manual installations if really necessary.\n",
    "# pip.install('matplotlib') would be an example installation.\n",
    "\n",
    "# Import required numerical and plotting libraries.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set deterministic random seed for reproducibility.\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create one dimensional input values for training data.\n",
    "x_train = np.linspace(-3.0, 3.0, 20)\n",
    "\n",
    "# Generate underlying smooth nonlinear signal values.\n",
    "y_true_train = np.sin(x_train)\n",
    "\n",
    "# Add random noise to create realistic observations.\n",
    "noise_train = np.random.normal(loc=0.0, scale=0.3, size=x_train.shape)\n",
    "\n",
    "# Combine signal and noise for observed training targets.\n",
    "y_train = y_true_train + noise_train\n",
    "\n",
    "# Create separate test inputs to check generalization.\n",
    "x_test = np.linspace(-3.0, 3.0, 100)\n",
    "\n",
    "# Compute true signal values for test inputs.\n",
    "y_true_test = np.sin(x_test)\n",
    "\n",
    "# Define helper function for polynomial feature expansion.\n",
    "def make_poly_features(x_values, degree):\n",
    "    # Validate that input is one dimensional array.\n",
    "    x_values = np.asarray(x_values).reshape(-1)\n",
    "\n",
    "    # Build design matrix with powers of x values.\n",
    "    features = [np.ones_like(x_values)]\n",
    "\n",
    "    # Append higher powers up to chosen degree.\n",
    "    for power in range(1, degree + 1):\n",
    "        features.append(x_values ** power)\n",
    "\n",
    "    # Stack features column wise into final matrix.\n",
    "    return np.vstack(features).T\n",
    "\n",
    "# Define helper function for least squares polynomial fitting.\n",
    "def fit_polynomial(x_values, y_values, degree):\n",
    "    # Build polynomial feature matrix for inputs.\n",
    "    X = make_poly_features(x_values, degree)\n",
    "\n",
    "    # Validate shapes before solving linear system.\n",
    "    assert X.shape[0] == y_values.shape[0]\n",
    "\n",
    "    # Solve normal equations using numpy least squares.\n",
    "    coeffs, *_ = np.linalg.lstsq(X, y_values, rcond=None)\n",
    "\n",
    "    # Return learned polynomial coefficient vector.\n",
    "    return coeffs\n",
    "\n",
    "# Define helper function to compute predictions from coefficients.\n",
    "def predict_polynomial(x_values, coeffs):\n",
    "    # Determine polynomial degree from coefficient length.\n",
    "    degree = len(coeffs) - 1\n",
    "\n",
    "    # Build matching feature matrix for new inputs.\n",
    "    X = make_poly_features(x_values, degree)\n",
    "\n",
    "    # Compute predictions using matrix multiplication.\n",
    "    return X @ coeffs\n",
    "\n",
    "# Fit a simple low degree polynomial model.\n",
    "coeffs_simple = fit_polynomial(x_train, y_train, degree=2)\n",
    "\n",
    "# Fit a very flexible high degree polynomial model.\n",
    "coeffs_complex = fit_polynomial(x_train, y_train, degree=12)\n",
    "\n",
    "# Compute training predictions for both models.\n",
    "y_pred_train_simple = predict_polynomial(x_train, coeffs_simple)\n",
    "\n",
    "# Compute complex model predictions on training data.\n",
    "y_pred_train_complex = predict_polynomial(x_train, coeffs_complex)\n",
    "\n",
    "# Compute test predictions for both models.\n",
    "y_pred_test_simple = predict_polynomial(x_test, coeffs_simple)\n",
    "\n",
    "# Compute complex model predictions on test inputs.\n",
    "y_pred_test_complex = predict_polynomial(x_test, coeffs_complex)\n",
    "\n",
    "# Define helper function for mean squared error calculation.\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    # Ensure shapes match before computing error.\n",
    "    y_true = np.asarray(y_true).reshape(-1)\n",
    "\n",
    "    # Reshape predictions to one dimensional array.\n",
    "    y_pred = np.asarray(y_pred).reshape(-1)\n",
    "\n",
    "    # Validate equal lengths for safe subtraction.\n",
    "    assert y_true.shape == y_pred.shape\n",
    "\n",
    "    # Return average squared difference between arrays.\n",
    "    return float(np.mean((y_true - y_pred) ** 2))\n",
    "\n",
    "# Compute training error for simple model.\n",
    "train_mse_simple = mean_squared_error(y_train, y_pred_train_simple)\n",
    "\n",
    "# Compute training error for complex model.\n",
    "train_mse_complex = mean_squared_error(y_train, y_pred_train_complex)\n",
    "\n",
    "# Compute test error for simple model.\n",
    "test_mse_simple = mean_squared_error(y_true_test, y_pred_test_simple)\n",
    "\n",
    "# Compute test error for complex model.\n",
    "test_mse_complex = mean_squared_error(y_true_test, y_pred_test_complex)\n",
    "\n",
    "# Print concise comparison of training and test errors.\n",
    "print(\"Simple model training MSE:\", round(train_mse_simple, 3))\n",
    "\n",
    "# Print complex model training error showing apparent improvement.\n",
    "print(\"Complex model training MSE:\", round(train_mse_complex, 3))\n",
    "\n",
    "# Print simple model test error for generalization performance.\n",
    "print(\"Simple model test MSE:\", round(test_mse_simple, 3))\n",
    "\n",
    "# Print complex model test error revealing overfitting danger.\n",
    "print(\"Complex model test MSE:\", round(test_mse_complex, 3))\n",
    "\n",
    "# Create figure and axis for visual comparison plot.\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "# Plot noisy training data as scatter points.\n",
    "ax.scatter(x_train, y_train, color=\"black\", label=\"Training data\")\n",
    "\n",
    "# Plot true underlying smooth sine relationship.\n",
    "ax.plot(x_test, y_true_test, color=\"green\", label=\"True pattern\")\n",
    "\n",
    "# Plot simple model curve showing stable behavior.\n",
    "ax.plot(x_test, y_pred_test_simple, color=\"blue\", label=\"Simple model\")\n",
    "\n",
    "# Plot complex model curve showing wiggly overfitting.\n",
    "ax.plot(x_test, y_pred_test_complex, color=\"red\", label=\"Complex model\")\n",
    "\n",
    "# Add title emphasizing overfitting dangers visually.\n",
    "ax.set_title(\"Overfitting: complex curve fits noise, hurts test performance\")\n",
    "\n",
    "# Label axes for clarity about inputs and outputs.\n",
    "ax.set_xlabel(\"Input feature x value\")\n",
    "\n",
    "# Label vertical axis describing predicted target variable.\n",
    "ax.set_ylabel(\"Predicted y value or true signal\")\n",
    "\n",
    "# Add legend to distinguish curves and data points.\n",
    "ax.legend(loc=\"best\")\n",
    "\n",
    "# Display the final plot to the learner.\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad43e3b",
   "metadata": {},
   "source": [
    "### **3.3. Model Validation Essentials**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7574c88",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Machine Learning for Beginners/Module_08/Lecture_A/image_03_03.jpg?v=1769967011\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Validation checks performance on unseen data\n",
    ">* Separate validation data helps detect nonlinear overfitting\n",
    "\n",
    ">* Compare models with different flexibility using validation\n",
    ">* Pick complexity where validation performance peaks, avoiding overfit\n",
    "\n",
    ">* Use repeated, realistic splits to test stability\n",
    ">* Ongoing validation guides safe model flexibility choices\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502309de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Model Validation Essentials\n",
    "\n",
    "# This script shows basic model validation ideas.\n",
    "# We compare flexible and simple polynomial curve fits.\n",
    "# Focus on overfitting versus generalization using validation.\n",
    "\n",
    "# !pip install numpy pandas matplotlib seaborn.\n",
    "\n",
    "# Import required numerical and plotting libraries.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set deterministic random seed for reproducibility.\n",
    "rng = np.random.default_rng(seed=42)\n",
    "\n",
    "# Create one dimensional input values for our dataset.\n",
    "x_all = np.linspace(-3.0, 3.0, num=60)\n",
    "\n",
    "# Generate noisy targets from a smooth cubic function.\n",
    "true_y = 0.5 * x_all ** 3 - x_all\n",
    "\n",
    "# Add random noise to create realistic observations.\n",
    "noise = rng.normal(loc=0.0, scale=3.0, size=x_all.shape)\n",
    "\n",
    "# Combine true values and noise for final targets.\n",
    "y_all = true_y + noise\n",
    "\n",
    "# Split indices into training and validation subsets.\n",
    "indices = np.arange(x_all.shape[0])\n",
    "\n",
    "# Shuffle indices deterministically using the generator.\n",
    "rng.shuffle(indices)\n",
    "\n",
    "# Select first half for training and remaining for validation.\n",
    "train_size = x_all.shape[0] // 2\n",
    "\n",
    "# Create boolean mask arrays for train and validation.\n",
    "train_idx = indices[:train_size]\n",
    "\n",
    "# Define complementary validation indices for evaluation.\n",
    "val_idx = indices[train_size:]\n",
    "\n",
    "# Extract training inputs and targets using indices.\n",
    "x_train, y_train = x_all[train_idx], y_all[train_idx]\n",
    "\n",
    "# Extract validation inputs and targets using indices.\n",
    "x_val, y_val = x_all[val_idx], y_all[val_idx]\n",
    "\n",
    "# Confirm shapes are consistent before fitting models.\n",
    "assert x_train.shape == y_train.shape\n",
    "\n",
    "# Confirm validation shapes also match correctly.\n",
    "assert x_val.shape == y_val.shape\n",
    "\n",
    "# Define polynomial degrees representing flexibility levels.\n",
    "degrees = [1, 3, 9]\n",
    "\n",
    "# Prepare containers for training and validation errors.\n",
    "train_mse_list, val_mse_list = [], []\n",
    "\n",
    "# Loop over each degree and fit polynomial coefficients.\n",
    "for deg in degrees:\n",
    "\n",
    "    # Fit polynomial using least squares on training data.\n",
    "    coeffs = np.polyfit(x_train, y_train, deg=deg)\n",
    "\n",
    "    # Create polynomial function from fitted coefficients.\n",
    "    poly_fn = np.poly1d(coeffs)\n",
    "\n",
    "    # Compute predictions for training and validation sets.\n",
    "    y_train_pred, y_val_pred = poly_fn(x_train), poly_fn(x_val)\n",
    "\n",
    "    # Calculate mean squared error for training predictions.\n",
    "    train_mse = float(np.mean((y_train_pred - y_train) ** 2))\n",
    "\n",
    "    # Calculate mean squared error for validation predictions.\n",
    "    val_mse = float(np.mean((y_val_pred - y_val) ** 2))\n",
    "\n",
    "    # Store errors for later printing and discussion.\n",
    "    train_mse_list.append(train_mse)\n",
    "\n",
    "    # Store validation error to compare flexibility tradeoffs.\n",
    "    val_mse_list.append(val_mse)\n",
    "\n",
    "# Print concise summary of training and validation errors.\n",
    "print(\"Degree, Train MSE, Validation MSE\")\n",
    "\n",
    "# Iterate through degrees and corresponding error values.\n",
    "for d, tr, va in zip(degrees, train_mse_list, val_mse_list):\n",
    "\n",
    "    # Show how errors change as flexibility increases.\n",
    "    print(f\"{d}, {tr:.2f}, {va:.2f}\")\n",
    "\n",
    "# Create dense grid for plotting fitted prediction curves.\n",
    "x_plot = np.linspace(-3.0, 3.0, num=200)\n",
    "\n",
    "# Initialize figure and axis for a single plot.\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "# Plot noisy training data as blue scatter points.\n",
    "ax.scatter(x_train, y_train, color=\"blue\", label=\"Train data\")\n",
    "\n",
    "# Plot validation data as orange scatter points.\n",
    "ax.scatter(x_val, y_val, color=\"orange\", label=\"Validation data\")\n",
    "\n",
    "# Plot true underlying cubic function for reference.\n",
    "ax.plot(x_plot, 0.5 * x_plot ** 3 - x_plot, color=\"black\", label=\"True curve\")\n",
    "\n",
    "# Plot fitted curves for each polynomial degree.\n",
    "for deg, color in zip(degrees, [\"green\", \"red\", \"purple\"]):\n",
    "\n",
    "    # Refit polynomial on all training data for plotting.\n",
    "    coeffs = np.polyfit(x_train, y_train, deg=deg)\n",
    "\n",
    "    # Evaluate polynomial on dense grid for smooth curve.\n",
    "    y_plot = np.polyval(coeffs, x_plot)\n",
    "\n",
    "    # Add curve to plot with appropriate label text.\n",
    "    ax.plot(x_plot, y_plot, color=color, linestyle=\"--\", label=f\"Degree {deg}\")\n",
    "\n",
    "# Add axis labels and descriptive plot title.\n",
    "ax.set_xlabel(\"Input feature x\")\n",
    "\n",
    "# Label vertical axis to indicate target variable values.\n",
    "ax.set_ylabel(\"Target y\")\n",
    "\n",
    "# Add legend to distinguish datasets and model curves.\n",
    "ax.legend(loc=\"best\")\n",
    "\n",
    "# Display the final plot to visualize validation behavior.\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6fdbc5",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Curved Relationships**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19587c9",
   "metadata": {},
   "source": [
    "\n",
    "In this lecture, you learned to:\n",
    "- Recognize visual patterns in data that suggest nonlinear relationships. \n",
    "- Describe how adding transformed features can create curved prediction shapes. \n",
    "- Explain the trade-off between flexibility and overfitting in nonlinear models. \n",
    "\n",
    "In the next Lecture (Lecture B), we will go over 'Piecewise Decisions'"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
