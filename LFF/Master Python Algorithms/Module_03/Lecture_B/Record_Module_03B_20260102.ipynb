{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b408142e",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Dicts And Sets**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9400f214",
   "metadata": {},
   "source": [
    ">Last update: 20260102.\n",
    "    \n",
    "By the end of this Lecture, you will be able to:\n",
    "- Explain how hash tables underpin Python dict and set behavior. \n",
    "- Analyze the expected time complexity of lookups, insertions, and deletions in dicts and sets. \n",
    "- Select appropriate key types and usage patterns to maintain efficient hash-based operations. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19dfe1a7",
   "metadata": {},
   "source": [
    "## **1. Hash Table Foundations**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f250ef",
   "metadata": {},
   "source": [
    "### **1.1. Hashing and Buckets**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cac619",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master Python Algorithms/Module_03/Lecture_B/image_01_01.jpg?v=1767330609\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Hashing maps each key to a bucket\n",
    ">* Buckets give fast direct access without scanning\n",
    "\n",
    ">* Buckets store entries that share a hash\n",
    ">* Hashing jumps directly to buckets for fast lookups\n",
    "\n",
    ">* Library catalog numbers mirror hash-based bucket lookup\n",
    ">* Even key spread keeps operations fast and predictable\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23183466",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Hashing and Buckets\n",
    "\n",
    "# Demonstrate hashing mapping keys into buckets visually.\n",
    "# Show how different keys share or use separate buckets.\n",
    "# Connect dictionary behavior with underlying hash bucket positions.\n",
    "# pip install some_required_library_if_needed.\n",
    "\n",
    "# Define a simple bucketed table using a small size.\n",
    "table_size = 8\n",
    "buckets = [[] for index in range(table_size)]\n",
    "\n",
    "# Define a helper function computing bucket index from Python hash.\n",
    "def bucket_index(key, size):\n",
    "    return hash(key) % size\n",
    "\n",
    "# Insert several keys and show their chosen buckets.\n",
    "keys = [\"alice\", \"bob\", \"carol\", \"dave\"]\n",
    "for key in keys:\n",
    "    index = bucket_index(key, table_size)\n",
    "    buckets[index].append(key)\n",
    "\n",
    "# Print a header explaining upcoming bucket layout.\n",
    "print(\"Bucket index and stored keys inside each bucket:\")\n",
    "\n",
    "# Loop through buckets and display their contents clearly.\n",
    "for index, bucket in enumerate(buckets):\n",
    "    print(f\"Bucket {index}: {bucket}\")\n",
    "\n",
    "# Show that looking up a key jumps directly to its bucket.\n",
    "lookup_key = \"carol\"\n",
    "lookup_index = bucket_index(lookup_key, table_size)\n",
    "print(f\"Key '{lookup_key}' goes directly to bucket {lookup_index}.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1100ca1c",
   "metadata": {},
   "source": [
    "### **1.2. Handling Collisions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ecf35b",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master Python Algorithms/Module_03/Lecture_B/image_01_02.jpg?v=1767330624\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Collisions happen when different keys share buckets\n",
    ">* Good collision handling keeps dict and set fast\n",
    "\n",
    ">* Two main collision strategies: chaining and probing\n",
    ">* Open addressing probes new buckets following repeatable pattern\n",
    "\n",
    ">* Too many collisions create slow clustered lookups\n",
    ">* Python limits clustering with probing and resizing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d1460a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Handling Collisions\n",
    "\n",
    "# Demonstrate simple hash collisions using a tiny custom table.\n",
    "# Show how probing searches next slots when collisions occur.\n",
    "# Compare our table behavior with normal Python dictionary behavior.\n",
    "# pip install commands are unnecessary because script uses only built in features.\n",
    "\n",
    "# Define a tiny hash table size to force frequent collisions.\n",
    "TABLE_SIZE = 5\n",
    "\n",
    "# Define a simple hash function using modulo operation for bucket index.\n",
    "def simple_hash(key):\n",
    "    return hash(key) % TABLE_SIZE\n",
    "\n",
    "# Define an insert function using linear probing for collisions.\n",
    "def insert(table, key, value):\n",
    "    index = simple_hash(key)\n",
    "    start_index = index\n",
    "\n",
    "    # Loop until an empty slot or matching key is found.\n",
    "    while table[index] is not None and table[index][0] != key:\n",
    "        index = (index + 1) % TABLE_SIZE\n",
    "        if index == start_index:\n",
    "            raise RuntimeError(\"Table is full, cannot insert more items.\")\n",
    "\n",
    "    # Store the key value pair at the chosen index after probing.\n",
    "    table[index] = (key, value)\n",
    "\n",
    "# Define a lookup function that follows the same probing pattern.\n",
    "def lookup(table, key):\n",
    "    index = simple_hash(key)\n",
    "    start_index = index\n",
    "\n",
    "    # Probe sequentially until key is found or empty slot encountered.\n",
    "    while table[index] is not None:\n",
    "        if table[index][0] == key:\n",
    "            return table[index][1]\n",
    "        index = (index + 1) % TABLE_SIZE\n",
    "        if index == start_index:\n",
    "            break\n",
    "\n",
    "    # Return None when key is not found after probing.\n",
    "    return None\n",
    "\n",
    "# Create an empty table with None entries representing empty buckets.\n",
    "custom_table = [None] * TABLE_SIZE\n",
    "\n",
    "# Insert keys that intentionally collide into the tiny table.\n",
    "keys = [\"AA\", \"BB\", \"CC\"]\n",
    "values = [10, 20, 30]\n",
    "\n",
    "# Insert each key value pair while printing chosen bucket index.\n",
    "for key, value in zip(keys, values):\n",
    "    index = simple_hash(key)\n",
    "    print(f\"Planned bucket for {key} is index {index}.\")\n",
    "    insert(custom_table, key, value)\n",
    "\n",
    "# Show final table layout after handling collisions with probing.\n",
    "print(\"Final custom table layout with possible collisions handled:\")\n",
    "print(custom_table)\n",
    "\n",
    "# Demonstrate lookup following the same probing pattern for a colliding key.\n",
    "search_key = \"CC\"\n",
    "found_value = lookup(custom_table, search_key)\n",
    "print(f\"Lookup for {search_key} returned value {found_value}.\")\n",
    "\n",
    "# Compare with normal Python dictionary behavior using same keys and values.\n",
    "py_dict = {k: v for k, v in zip(keys, values)}\n",
    "print(\"Python dict lookup for CC gives:\", py_dict[\"CC\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816c804e",
   "metadata": {},
   "source": [
    "### **1.3. Growth and Capacity**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fac581",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master Python Algorithms/Module_03/Lecture_B/image_01_03.jpg?v=1767330643\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Hash tables have fixed slots and capacity\n",
    ">* Load factor controls collisions, speed, and resizing\n",
    "\n",
    ">* Hash tables sometimes resize and rehash keys\n",
    ">* Rare resizes keep operations fast and efficient\n",
    "\n",
    ">* Resizing causes memory jumps but adds spare capacity\n",
    ">* Extra capacity keeps operations fast as sizes change\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f881f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Growth and Capacity\n",
    "\n",
    "# Demonstrate dictionary growth and capacity behavior with simple insert operations.\n",
    "# Show how size jumps occasionally while many insertions happen smoothly overall.\n",
    "# Illustrate that occasional expensive growth keeps average access time nearly constant.\n",
    "# pip install commands are unnecessary because this script uses only built in features.\n",
    "\n",
    "# Import sys module for checking approximate dictionary memory size in bytes.\n",
    "import sys\n",
    "\n",
    "# Create empty dictionary representing a growing address book over time.\n",
    "address_book = {}\n",
    "\n",
    "# Define total number of insert operations for this simple demonstration.\n",
    "total_inserts = 5000\n",
    "\n",
    "# Define checkpoints where we will inspect dictionary size and memory usage.\n",
    "checkpoints = [10, 50, 100, 500, 1000, 2500, 5000]\n",
    "\n",
    "# Print header describing columns for later checkpoint information.\n",
    "print(\"entries\\tapprox_bytes\\tcomment\")\n",
    "\n",
    "# Loop through range and insert fake entries representing house numbers and names.\n",
    "for house_number in range(1, total_inserts + 1):\n",
    "\n",
    "    # Insert simple key value pair representing a resident at some street address.\n",
    "    address_book[f\"{house_number} Main Street\"] = f\"Resident {house_number}\"\n",
    "\n",
    "    # When current count hits checkpoint, print approximate size and short explanation.\n",
    "    if house_number in checkpoints:\n",
    "\n",
    "        # Get approximate dictionary size using sys.getsizeof for rough capacity hint.\n",
    "        approx_bytes = sys.getsizeof(address_book)\n",
    "\n",
    "        # Prepare short comment describing growth stage and potential resize behavior.\n",
    "        if house_number == 10:\n",
    "            comment = \"Very small table, first growth steps probably just started.\"\n",
    "        elif house_number == 50:\n",
    "            comment = \"Still light load, spare capacity keeps lookups very fast.\"\n",
    "        elif house_number == 100:\n",
    "            comment = \"More entries, internal table maybe resized once already.\"\n",
    "        elif house_number == 500:\n",
    "            comment = \"Notice memory jump, table expanded to reduce collisions.\"\n",
    "        elif house_number == 1000:\n",
    "            comment = \"Large spare capacity now, many inserts before next resize.\"\n",
    "        elif house_number == 2500:\n",
    "            comment = \"Load factor rising again, another growth step may be near.\"\n",
    "        else:\n",
    "            comment = \"Reached final size, capacity will handle more future growth.\"\n",
    "\n",
    "        # Print current number of entries, approximate bytes, and growth comment.\n",
    "        print(f\"{house_number}\\t{approx_bytes}\\t{comment}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb236055",
   "metadata": {},
   "source": [
    "## **2. Dict Operation Costs**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323d058d",
   "metadata": {},
   "source": [
    "### **2.1. Lookup Time Costs**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e077e08",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master Python Algorithms/Module_03/Lecture_B/image_02_01.jpg?v=1767330662\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Dict lookups use hashing to find keys\n",
    ">* Expected lookup time stays constant as size grows\n",
    "\n",
    ">* Constant-time lookups are average-case, not guaranteed\n",
    ">* Collisions can slow lookups; worst-case becomes linear\n",
    "\n",
    ">* Treat frequent dict lookups as usually constant-time\n",
    ">* Avoid hash collisions and rely on resizing behavior\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce7b922",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Lookup Time Costs\n",
    "\n",
    "# Demonstrate dictionary lookup time staying similar for different dictionary sizes.\n",
    "# Compare lookup times for small and large dictionaries using time measurements.\n",
    "# Show that average lookup cost barely grows even with many stored items.\n",
    "\n",
    "# pip install commands are unnecessary because this script uses only built in modules.\n",
    "\n",
    "# Import time module for measuring elapsed lookup durations.\n",
    "import time\n",
    "\n",
    "# Create small dictionary with one thousand key value pairs.\n",
    "small_dict = {f\"user_{i}\": i for i in range(1000)}\n",
    "\n",
    "# Create large dictionary with one million key value pairs.\n",
    "large_dict = {f\"user_{i}\": i for i in range(1000000)}\n",
    "\n",
    "# Define helper function that measures average lookup time for given dictionary.\n",
    "def measure_lookup_time(given_dict, label, repetitions):\n",
    "    # Choose existing key and missing key for realistic lookup scenarios.\n",
    "    existing_key = next(iter(given_dict.keys()))\n",
    "    missing_key = \"user_missing_key_value_here\"\n",
    "\n",
    "    # Measure time for repeated existing key lookups.\n",
    "    start_existing = time.perf_counter()\n",
    "    for _ in range(repetitions):\n",
    "        _ = given_dict[existing_key]\n",
    "    duration_existing = time.perf_counter() - start_existing\n",
    "\n",
    "    # Measure time for repeated missing key membership checks.\n",
    "    start_missing = time.perf_counter()\n",
    "    for _ in range(repetitions):\n",
    "        _ = missing_key in given_dict\n",
    "    duration_missing = time.perf_counter() - start_missing\n",
    "\n",
    "    # Compute average microseconds per lookup for both scenarios.\n",
    "    avg_existing_us = duration_existing / repetitions * 1_000_000\n",
    "    avg_missing_us = duration_missing / repetitions * 1_000_000\n",
    "\n",
    "    # Print formatted summary showing approximate constant lookup behavior.\n",
    "    print(f\"{label}: existing lookup ≈ {avg_existing_us:.3f} microseconds each.\")\n",
    "    print(f\"{label}: missing lookup ≈ {avg_missing_us:.3f} microseconds each.\")\n",
    "\n",
    "# Set number of repetitions for stable timing without long runtime.\n",
    "repetitions = 200000\n",
    "\n",
    "# Measure and display lookup times for small dictionary.\n",
    "measure_lookup_time(small_dict, \"Small dict (1k keys)\", repetitions)\n",
    "\n",
    "# Measure and display lookup times for large dictionary.\n",
    "measure_lookup_time(large_dict, \"Large dict (1M keys)\", repetitions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0d25a1",
   "metadata": {},
   "source": [
    "### **2.2. Update Operation Costs**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5458bdde",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master Python Algorithms/Module_03/Lecture_B/image_02_02.jpg?v=1767330687\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Inserts, updates, deletes share hash-based probing\n",
    ">* Each update is amortized constant time, scalable\n",
    "\n",
    ">* Dictionaries sometimes resize when load factor grows\n",
    ">* Resizes are rare, so average insertion stays constant\n",
    "\n",
    ">* Deletes use tombstones, keeping probes working correctly\n",
    ">* Occasional housekeeping keeps average update time constant\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508ca99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Update Operation Costs\n",
    "\n",
    "# Demonstrate dictionary update costs with many insertions and occasional resizing.\n",
    "# Show that single updates feel fast even as the dictionary grows larger.\n",
    "# Compare time per operation for small and large update batches.\n",
    "# pip install numpy matplotlib pandas seaborn.\n",
    "\n",
    "# Import required standard library modules for timing operations.\n",
    "import time\n",
    "\n",
    "# Define a helper function that measures average insertion time.\n",
    "def measure_insert_time(count, label):\n",
    "    start_time = time.perf_counter()\n",
    "    data = {}\n",
    "    for number in range(count):\n",
    "        data[number] = number\n",
    "    end_time = time.perf_counter()\n",
    "    average = (end_time - start_time) / count\n",
    "    print(f\"{label}: {count} inserts, average seconds {average:.8f}\")\n",
    "    return average\n",
    "\n",
    "# Warm up the interpreter to reduce one time overhead effects.\n",
    "_ = measure_insert_time(1000, \"Warmup batch small\")\n",
    "\n",
    "# Measure average insertion time for a medium sized batch.\n",
    "medium_average = measure_insert_time(50000, \"Medium batch inserts\")\n",
    "\n",
    "# Measure average insertion time for a larger batch.\n",
    "large_average = measure_insert_time(200000, \"Large batch inserts\")\n",
    "\n",
    "# Show ratio between large and medium average times for clarity.\n",
    "ratio = large_average / medium_average if medium_average else 0.0\n",
    "print(f\"Large versus medium average time ratio: {ratio:.2f}\")\n",
    "\n",
    "# Demonstrate that overwriting existing keys remains very fast overall.\n",
    "user_scores = {f\"user_{i}\": i for i in range(100000)}\n",
    "\n",
    "# Time updating existing keys inside the prepared dictionary.\n",
    "start_update = time.perf_counter()\n",
    "for i in range(100000):\n",
    "    user_scores[f\"user_{i}\"] = i + 1\n",
    "end_update = time.perf_counter()\n",
    "\n",
    "# Print average update time per overwrite operation.\n",
    "avg_update = (end_update - start_update) / 100000\n",
    "print(f\"Overwrite existing keys average seconds {avg_update:.8f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e930f3f9",
   "metadata": {},
   "source": [
    "### **2.3. Iterating Dictionary Views**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1d3d4a",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master Python Algorithms/Module_03/Lecture_B/image_02_03.jpg?v=1767330703\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Iterating visits each stored entry once, linearly\n",
    ">* Hash-table layout makes iteration fast and cache-friendly\n",
    "\n",
    ">* Dict view objects are cheap, reference-based windows\n",
    ">* Iteration over views is linear and can dominate time\n",
    "\n",
    ">* Mixing iteration with operations increases total cost\n",
    ">* Design algorithms to avoid repeated full scans\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b9942b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Iterating Dictionary Views\n",
    "\n",
    "# Demonstrate iterating dictionary views and their linear time behavior.\n",
    "# Compare iteration over keys, values, and items with growing dictionary sizes.\n",
    "# Show that doubling entries roughly doubles iteration time in simple examples.\n",
    "\n",
    "# pip install commands are unnecessary because this script uses only builtins.\n",
    "\n",
    "# Import time module for simple timing measurements.\n",
    "import time\n",
    "\n",
    "# Create a helper function that builds a dictionary with given size.\n",
    "def build_inventory(size):\n",
    "    inventory = {}\n",
    "    for i in range(size):\n",
    "        inventory[f\"product_{i}\"] = i\n",
    "    return inventory\n",
    "\n",
    "# Create two dictionaries with different sizes for comparison.\n",
    "small_inventory = build_inventory(1_000)\n",
    "large_inventory = build_inventory(10_000)\n",
    "\n",
    "# Define a function that times iterating over a given dictionary view.\n",
    "def time_iteration(label, view):\n",
    "    start = time.perf_counter()\n",
    "    total = 0\n",
    "    for item in view:\n",
    "        total += 1\n",
    "    duration = time.perf_counter() - start\n",
    "    print(f\"{label}: visited {total} entries in {duration:.6f} seconds\")\n",
    "\n",
    "# Time iterating over keys for small and large dictionaries.\n",
    "print(\"Iterating over keys view timings:\")\n",
    "time_iteration(\"Small keys view\", small_inventory.keys())\n",
    "time_iteration(\"Large keys view\", large_inventory.keys())\n",
    "\n",
    "# Time iterating over values for small and large dictionaries.\n",
    "print(\"\\nIterating over values view timings:\")\n",
    "time_iteration(\"Small values view\", small_inventory.values())\n",
    "time_iteration(\"Large values view\", large_inventory.values())\n",
    "\n",
    "# Time iterating over items for small and large dictionaries.\n",
    "print(\"\\nIterating over items view timings:\")\n",
    "time_iteration(\"Small items view\", small_inventory.items())\n",
    "time_iteration(\"Large items view\", large_inventory.items())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847b1427",
   "metadata": {},
   "source": [
    "## **3. Efficient Set Keys**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22238c63",
   "metadata": {},
   "source": [
    "### **3.1. Fast Membership Checks**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff746e7",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master Python Algorithms/Module_03/Lecture_B/image_03_01.jpg?v=1767330726\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Sets are optimized for rapid membership checks\n",
    ">* Use sets as yes-or-no filters efficiently\n",
    "\n",
    ">* Convert repeated checks on lists into sets\n",
    ">* Build sets once, reuse for many lookups\n",
    "\n",
    ">* Avoid constantly rebuilding or churning large sets\n",
    ">* Keep long-lived sets stable to preserve speed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d753c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Fast Membership Checks\n",
    "\n",
    "# Demonstrate fast membership checks using sets versus lists in Python.\n",
    "# Show why converting to a set once speeds up repeated lookups.\n",
    "# Compare lookup times for a list and a set with many elements.\n",
    "\n",
    "# pip install commands are not required because this script uses only standard libraries.\n",
    "\n",
    "# Import time module for simple timing measurements.\n",
    "import time\n",
    "\n",
    "# Create a large list of integers representing many user IDs.\n",
    "user_ids_list = list(range(0, 500000))\n",
    "\n",
    "# Convert the list into a set once for fast membership checks.\n",
    "user_ids_set = set(user_ids_list)\n",
    "\n",
    "# Choose some test IDs including present and absent values.\n",
    "test_ids = [10, 250000, 499999, 750000]\n",
    "\n",
    "# Define a helper function to time membership checks for any collection.\n",
    "def time_membership_checks(collection, label):\n",
    "    # Record start time before performing membership checks.\n",
    "    start = time.perf_counter()\n",
    "    \n",
    "    # Perform many repeated membership checks using the same collection.\n",
    "    for _ in range(2000):\n",
    "        for value in test_ids:\n",
    "            value in collection\n",
    "    \n",
    "    # Record end time after all membership checks complete.\n",
    "    end = time.perf_counter()\n",
    "    \n",
    "    # Compute elapsed time in milliseconds for readability.\n",
    "    elapsed_ms = (end - start) * 1000\n",
    "    \n",
    "    # Print timing result with a clear label for the collection type.\n",
    "    print(f\"{label} membership time: {elapsed_ms:.2f} ms\")\n",
    "\n",
    "# Time membership checks using the list collection first.\n",
    "time_membership_checks(user_ids_list, \"List\")\n",
    "\n",
    "# Time membership checks using the set collection second.\n",
    "time_membership_checks(user_ids_set, \"Set\")\n",
    "\n",
    "# Print a short conclusion highlighting which structure was faster overall.\n",
    "print(\"Set membership is usually much faster than list membership for repeated checks.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46a1306",
   "metadata": {},
   "source": [
    "### **3.2. Choosing Hashable Keys**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666f762a",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master Python Algorithms/Module_03/Lecture_B/image_03_02.jpg?v=1767330758\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Use simple, immutable, hashable values as keys\n",
    ">* They keep membership checks fast and predictable\n",
    "\n",
    ">* Use immutable keys so identity never changes\n",
    ">* Choose keys that match your domain’s uniqueness rules\n",
    "\n",
    ">* Heavy, complex keys slow large set operations\n",
    ">* Prefer short, unique, lightweight keys for performance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f621e3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Choosing Hashable Keys\n",
    "\n",
    "# Demonstrate choosing simple hashable keys for efficient Python set membership checks.\n",
    "# Compare performance and behavior of simple keys versus heavier composite keys.\n",
    "# Show why stable, meaningful, lightweight keys keep membership checks efficient.\n",
    "\n",
    "# pip install commands are not required because this script uses only standard libraries.\n",
    "\n",
    "# Import time module for simple timing measurements.\n",
    "import time\n",
    "\n",
    "# Create simple immutable keys representing student ID numbers.\n",
    "student_ids_simple = {101, 205, 309, 412, 518}\n",
    "\n",
    "# Create heavier keys using tuples containing ID and long descriptive strings.\n",
    "student_ids_heavy = {\n",
    "    (101, \"student from north campus with long description\"),\n",
    "    (205, \"student from south campus with long description\"),\n",
    "}\n",
    "\n",
    "# Define a function that checks membership many times using given key and set.\n",
    "def check_membership_many_times(target_set, target_key, repeat_count):\n",
    "    start_time = time.time()\n",
    "    found_count = 0\n",
    "    for _ in range(repeat_count):\n",
    "        if target_key in target_set:\n",
    "            found_count += 1\n",
    "    end_time = time.time()\n",
    "    return found_count, end_time - start_time\n",
    "\n",
    "# Choose a simple key and a heavy key that both represent the same student.\n",
    "simple_key = 205\n",
    "heavy_key = (205, \"student from south campus with long description\")\n",
    "\n",
    "# Run membership checks many times for both key styles.\n",
    "repetitions = 200000\n",
    "simple_found, simple_seconds = check_membership_many_times(student_ids_simple, simple_key, repetitions)\n",
    "heavy_found, heavy_seconds = check_membership_many_times(student_ids_heavy, heavy_key, repetitions)\n",
    "\n",
    "# Print results showing that both keys work but heavier keys cost more time.\n",
    "print(\"Simple key found count and seconds:\", simple_found, round(simple_seconds, 4))\n",
    "print(\"Heavy key found count and seconds:\", heavy_found, round(heavy_seconds, 4))\n",
    "\n",
    "# Show that using stable, meaningful, lightweight keys keeps membership checks predictable.\n",
    "print(\"Simple keys are shorter, immutable, and usually faster for large sets.\")\n",
    "print(\"Heavy composite keys are valid but can be slower and more memory hungry.\")\n",
    "print(\"Choose keys that match your equality rules but remain compact and stable.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f44f8c",
   "metadata": {},
   "source": [
    "### **3.3. Safe Mutable Key Usage**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37293ea7",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master Python Algorithms/Module_03/Lecture_B/image_03_03.jpg?v=1767330775\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Set elements need stable, unchanging hash values\n",
    ">* Changing hashed attributes breaks membership and removal\n",
    "\n",
    ">* Store immutable identifiers instead of mutable objects\n",
    ">* Keeps membership checks fast and avoids hash bugs\n",
    "\n",
    ">* Mutable objects in sets need stable identity fields\n",
    ">* Prefer immutable keys; keep mutable data separate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cde1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Safe Mutable Key Usage\n",
    "\n",
    "# Demonstrate safe mutable key usage with sets and stable identifiers.\n",
    "# Show how changing hashed fields breaks membership and removal operations.\n",
    "# Show safer pattern using immutable identifiers for mutable session objects.\n",
    "# pip install some_required_library_if_needed.\n",
    "\n",
    "# Define a simple mutable Session class with mutable attributes.\n",
    "class Session:\n",
    "    def __init__(self, session_id, status):\n",
    "        self.session_id = session_id\n",
    "        self.status = status\n",
    "\n",
    "    # Hash and equality depend only on immutable session_id field.\n",
    "    def __hash__(self):\n",
    "        return hash(self.session_id)\n",
    "\n",
    "    # Equality also compares only session_id for identity stability.\n",
    "    def __eq__(self, other):\n",
    "        return isinstance(other, Session) and self.session_id == other.session_id\n",
    "\n",
    "# Create one session object and place it inside a set.\n",
    "session = Session(\"ABC123\", \"active\")\n",
    "active_sessions_objects = {session}\n",
    "\n",
    "# Change mutable status field, which does not affect hashing behavior.\n",
    "session.status = \"paused\"\n",
    "\n",
    "# Membership check still works because session_id remained unchanged.\n",
    "print(\"Session object still found in set:\", session in active_sessions_objects)\n",
    "\n",
    "# Now demonstrate safer pattern using only immutable identifiers.\n",
    "active_sessions_ids = {\"ABC123\", \"XYZ999\"}\n",
    "\n",
    "# Simulate session status changes stored separately in a dictionary.\n",
    "session_status = {\"ABC123\": \"paused\", \"XYZ999\": \"active\"}\n",
    "\n",
    "# Membership uses stable identifiers, while details remain mutable elsewhere.\n",
    "print(\"Identifier 'ABC123' in active set:\", \"ABC123\" in active_sessions_ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978bbef9",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Dicts And Sets**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8037391e",
   "metadata": {},
   "source": [
    "\n",
    "In this lecture, you learned to:\n",
    "- Explain how hash tables underpin Python dict and set behavior. \n",
    "- Analyze the expected time complexity of lookups, insertions, and deletions in dicts and sets. \n",
    "- Select appropriate key types and usage patterns to maintain efficient hash-based operations. \n",
    "\n",
    "In the next Module (Module 4), we will go over 'Searching Techniques'"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
