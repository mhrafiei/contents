{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f6ad5db",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Importing Data**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c697603",
   "metadata": {},
   "source": [
    ">Last update: 20251225.\n",
    "    \n",
    "By the end of this Lecture, you will be able to:\n",
    "- Load tabular data from CSV, Excel, and Parquet files into Pandas 2.3.1 DataFrames using appropriate read_* functions. \n",
    "- Configure import options such as separators, headers, dtypes, and date parsing to correctly interpret raw files. \n",
    "- Diagnose and fix common import issues including bad encodings, unexpected missing values, and mixed-type columns. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46118009",
   "metadata": {},
   "source": [
    "## **1. Reading CSV Files**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddde6d9",
   "metadata": {},
   "source": [
    "### **1.1. Core read csv options**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b450fd7",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Pandas (2.3.1) A-Z/Module_02/Lecture_A/image_01_01.jpg?v=1766706987\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* CSV is plain text forming table-like rows\n",
    ">* Defaults usually create a usable, well-typed DataFrame\n",
    "\n",
    ">* Choose and position headers to match data\n",
    ">* Limit or skip rows so DataFrame structure fits\n",
    "\n",
    ">* Control dtypes, missing values, and parsing behavior\n",
    ">* Careful options create accurate, analysis-ready DataFrames\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb5b950",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Core read csv options\n",
    "\n",
    "# Demonstrate core pandas read_csv options with simple in memory CSV text.\n",
    "# Show header handling, row skipping, and column data type control clearly.\n",
    "# Designed for beginners using Google Colab with minimal printed output.\n",
    "\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "csv_text = \"\"\"Note line before header\n",
    "Another note describing file\n",
    "id,name,zip_code,age\n",
    "001,Anna,02115,28\n",
    "002,Bob,30301,35\n",
    "003,Cara,10001,41\n",
    "\"\"\"\n",
    "\n",
    "csv_buffer = StringIO(csv_text)\n",
    "\n",
    "print(\"Original CSV text preview:\")\n",
    "print(\"\\n\".join(csv_text.splitlines()[:4]))\n",
    "\n",
    "csv_buffer.seek(0)\n",
    "\n",
    "basic_df = pd.read_csv(csv_buffer, header=2)\n",
    "\n",
    "print(\"\\nDataFrame with inferred types:\")\n",
    "print(basic_df.head())\n",
    "\n",
    "csv_buffer.seek(0)\n",
    "\n",
    "dtyped_df = pd.read_csv(csv_buffer, header=2, dtype={\"id\": \"string\", \"zip_code\": \"string\"})\n",
    "\n",
    "print(\"\\nDataFrame with controlled types:\")\n",
    "print(dtyped_df.dtypes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da64b186",
   "metadata": {},
   "source": [
    "### **1.2. Handling delimiters and headers**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd72dae",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Pandas (2.3.1) A-Z/Module_02/Lecture_A/image_01_02.jpg?v=1766707004\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Check actual delimiter and header row structure\n",
    ">* These choices control column names and field splitting\n",
    "\n",
    ">* Clean, clear headers make analysis easier, safer\n",
    ">* Tidy messy headers or replace them entirely\n",
    "\n",
    ">* Embedded delimiters need correct quoting to parse\n",
    ">* Check rows, headers, and delimiters to avoid misalignment\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec73c8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Handling delimiters and headers\n",
    "\n",
    "# Demonstrate reading CSV files with different delimiters and header configurations.\n",
    "# Show how wrong delimiter assumptions collapse columns into single wide column.\n",
    "# Show how header options change column names and skipped descriptive rows.\n",
    "\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "csv_semicolon_text = \"Name;Age;City\\nAlice;30;Boston\\nBob;25;Chicago\"\n",
    "print(\"Raw semicolon separated text preview:\")\n",
    "print(csv_semicolon_text.split(\"\\n\")[0])\n",
    "\n",
    "wrong_delimiter_buffer = StringIO(csv_semicolon_text)\n",
    "df_wrong = pd.read_csv(wrong_delimiter_buffer, delimiter=\",\")\n",
    "print(\"\\nUsing comma delimiter, columns look collapsed:\")\n",
    "print(df_wrong.head())\n",
    "\n",
    "correct_delimiter_buffer = StringIO(csv_semicolon_text)\n",
    "df_correct = pd.read_csv(correct_delimiter_buffer, delimiter=\";\")\n",
    "print(\"\\nUsing semicolon delimiter, columns separate correctly:\")\n",
    "print(df_correct.head())\n",
    "\n",
    "messy_header_text = \"Sales report for 2024\\nValues in US dollars\\nProduct,Units,Revenue\\nWidget,10,250.0\\nGadget,5,150.0\"\n",
    "print(\"\\nRaw messy header first three lines:\")\n",
    "for line in messy_header_text.split(\"\\n\")[:3]:\n",
    "    print(line)\n",
    "\n",
    "messy_buffer = StringIO(messy_header_text)\n",
    "df_skip_header = pd.read_csv(messy_buffer, skiprows=2)\n",
    "print(\"\\nAfter skipping two lines, header row becomes column names:\")\n",
    "print(df_skip_header.head())\n",
    "\n",
    "messy_buffer_custom = StringIO(messy_header_text)\n",
    "custom_names = [\"product_name\", \"units_sold\", \"revenue_usd\"]\n",
    "df_custom_header = pd.read_csv(messy_buffer_custom, skiprows=3, header=None, names=custom_names)\n",
    "print(\"\\nUsing custom header names after skipping three lines:\")\n",
    "print(df_custom_header.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf15c39",
   "metadata": {},
   "source": [
    "### **1.3. Efficient Large CSV Loading**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ce3305",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Pandas (2.3.1) A-Z/Module_02/Lecture_A/image_01_03.jpg?v=1766707034\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Treat huge CSVs as row streams, not wholes\n",
    ">* Process chunks, keep summaries or cleaned subsets only\n",
    "\n",
    ">* Read only needed columns to save resources\n",
    ">* Set dtypes upfront to speed and stabilize imports\n",
    "\n",
    ">* Simplify parsing options and delay complex cleaning\n",
    ">* Process chunks efficiently and convert to columnar format\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec44ca71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Efficient Large CSV Loading\n",
    "\n",
    "# Demonstrate reading large CSV files using chunks efficiently.\n",
    "# Show selecting useful columns and specifying data types explicitly.\n",
    "# Summarize chunked data while keeping memory usage comfortably small.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create a sample CSV file that imitates a larger dataset.\n",
    "# We use many rows but keep values simple for quick processing.\n",
    "num_rows = 50000\n",
    "\n",
    "np.random.seed(0)\n",
    "customer_ids = np.random.randint(10000, 20000, size=num_rows)\n",
    "\n",
    "signup_days = np.random.randint(1, 28, size=num_rows)\n",
    "signup_months = np.random.randint(1, 12, size=num_rows)\n",
    "\n",
    "signup_dates = [f\"2024-{m:02d}-{d:02d}\" for m, d in zip(signup_months, signup_days)]\n",
    "\n",
    "countries = np.random.choice([\"US\", \"CA\", \"UK\", \"MX\"], size=num_rows)\n",
    "\n",
    "spend_dollars = np.random.gamma(shape=2.0, scale=50.0, size=num_rows)\n",
    "\n",
    "full_df = pd.DataFrame({\n",
    "    \"customer_id\": customer_ids,\n",
    "    \"signup_date\": signup_dates,\n",
    "    \"country\": countries,\n",
    "    \"spend_usd\": spend_dollars,\n",
    "})\n",
    "\n",
    "csv_path = \"large_customers.csv\"\n",
    "full_df.to_csv(csv_path, index=False)\n",
    "\n",
    "# Now read the CSV efficiently using chunks and selected columns.\n",
    "# We also specify data types to avoid expensive automatic inference.\n",
    "use_columns = [\"customer_id\", \"country\", \"spend_usd\"]\n",
    "\n",
    "dtype_map = {\"customer_id\": \"int32\", \"country\": \"category\", \"spend_usd\": \"float32\"}\n",
    "\n",
    "chunk_size = 10000\n",
    "\n",
    "reader = pd.read_csv(csv_path, usecols=use_columns, dtype=dtype_map, chunksize=chunk_size)\n",
    "\n",
    "total_spend_by_country = {}\n",
    "\n",
    "for chunk in reader:\n",
    "    grouped = chunk.groupby(\"country\")[\"spend_usd\"].sum()\n",
    "    for country, value in grouped.items():\n",
    "        total_spend_by_country[country] = total_spend_by_country.get(country, 0.0) + float(value)\n",
    "\n",
    "print(\"Total spend by country using chunked loading:\")\n",
    "\n",
    "for country, value in total_spend_by_country.items():\n",
    "    print(f\"{country}: ${value:,.2f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2150a1e2",
   "metadata": {},
   "source": [
    "## **2. Excel and Parquet Imports**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bc9eed",
   "metadata": {},
   "source": [
    "### **2.1. Selecting Excel Sheets**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e158ea",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Pandas (2.3.1) A-Z/Module_02/Lecture_A/image_02_01.jpg?v=1766707067\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Excel files can contain many different sheets\n",
    ">* Careful sheet choice ensures you load intended data\n",
    "\n",
    ">* Identify sheets by index or clear names\n",
    ">* Inspect workbook, then explicitly import the correct sheet\n",
    "\n",
    ">* Import and combine related sheets for analysis\n",
    ">* Use or ignore metadata sheets to guide interpretation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70caa11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Selecting Excel Sheets\n",
    "\n",
    "# Demonstrate selecting Excel sheets when importing data with pandas.\n",
    "# Create a sample Excel file containing multiple related sheets.\n",
    "# Load specific sheets by name and index, then display combined results.\n",
    "\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "\n",
    "# Create example DataFrames representing different Excel workbook sheets.\n",
    "usa_data = pd.DataFrame({\"City\": [\"Boston\", \"Dallas\"], \"Sales_dollars\": [1200, 1500]})\n",
    "canada_data = pd.DataFrame({\"City\": [\"Toronto\", \"Calgary\"], \"Sales_dollars\": [900, 1100]})\n",
    "notes_data = pd.DataFrame({\"Note\": [\"Internal summary only\"]})\n",
    "\n",
    "# Save DataFrames into one in_memory Excel file with multiple named sheets.\n",
    "excel_buffer = BytesIO()\n",
    "with pd.ExcelWriter(excel_buffer, engine=\"xlsxwriter\") as writer:\n",
    "    usa_data.to_excel(writer, sheet_name=\"USA_Sales\", index=False)\n",
    "    canada_data.to_excel(writer, sheet_name=\"Canada_Sales\", index=False)\n",
    "    notes_data.to_excel(writer, sheet_name=\"Notes\", index=False)\n",
    "\n",
    "# Move buffer position to start, then read workbook using pandas read_excel.\n",
    "excel_buffer.seek(0)\n",
    "all_sheets = pd.read_excel(excel_buffer, sheet_name=None)\n",
    "print(\"Available sheet names in workbook:\")\n",
    "print(list(all_sheets.keys()))\n",
    "\n",
    "# Load a single sheet by visible sheet name, then display small preview.\n",
    "excel_buffer.seek(0)\n",
    "usa_only = pd.read_excel(excel_buffer, sheet_name=\"USA_Sales\")\n",
    "print(\"\\nUSA sheet preview after selecting by name:\")\n",
    "print(usa_only)\n",
    "\n",
    "# Load multiple sheets at once, then concatenate them into one DataFrame.\n",
    "excel_buffer.seek(0)\n",
    "selected = pd.read_excel(excel_buffer, sheet_name=[\"USA_Sales\", \"Canada_Sales\"])\n",
    "combined = pd.concat(selected.values(), ignore_index=True)\n",
    "print(\"\\nCombined sales from selected sheets:\")\n",
    "print(combined)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1881b9",
   "metadata": {},
   "source": [
    "### **2.2. Parsing Dates and Numbers**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a1f267",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Pandas (2.3.1) A-Z/Module_02/Lecture_A/image_02_02.jpg?v=1766707086\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Decide how Excel dates and numbers import\n",
    ">* Consistent parsing prevents confusing, incorrect analysis results\n",
    "\n",
    ">* Same-looking Excel values can mean different things\n",
    ">* Control parsing to avoid date and number mistakes\n",
    "\n",
    ">* Plan for mixed, invalid, or placeholder values\n",
    ">* Sample data, set types, and handle inconsistencies\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c4c824",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Parsing Dates and Numbers\n",
    "\n",
    "# Demonstrate parsing Excel dates and numbers with pandas read_excel options.\n",
    "# Show problems with default parsing and then fix them using parse_dates options.\n",
    "# Highlight numeric parsing with thousands separators and custom decimal characters.\n",
    "\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "\n",
    "# Create a small DataFrame with mixed date and numeric representations.\n",
    "data = {\n",
    "    \"order_date\": [\"2024-01-02\", \"01/03/2024\", \"2024-01-04\"],\n",
    "    \"delivery_date\": [\"02-01-2024\", \"03-01-2024\", \"TBD\"],\n",
    "    \"budget_text\": [\"1,234.50\", \"2,500.00\", \"750.25\"],\n",
    "}\n",
    "\n",
    "# Convert the DataFrame into an in memory Excel file using BytesIO buffer.\n",
    "df_original = pd.DataFrame(data)\n",
    "excel_buffer = BytesIO()\n",
    "df_original.to_excel(excel_buffer, index=False)\n",
    "\n",
    "# Move buffer position back to start before reading with pandas read_excel function.\n",
    "excel_buffer.seek(0)\n",
    "\n",
    "# Read Excel file with default options to inspect inferred column data types.\n",
    "df_default = pd.read_excel(excel_buffer)\n",
    "print(\"Default dtypes and values:\")\n",
    "print(df_default.dtypes)\n",
    "\n",
    "# Reset buffer position again because previous read moved internal pointer forward.\n",
    "excel_buffer.seek(0)\n",
    "\n",
    "# Read Excel file again, now explicitly parsing order_date and delivery_date columns.\n",
    "df_parsed = pd.read_excel(excel_buffer, parse_dates=[\"order_date\", \"delivery_date\"], dayfirst=False)\n",
    "print(\"\\nParsed dates dtypes and values:\")\n",
    "print(df_parsed.dtypes)\n",
    "\n",
    "# Convert budget_text column into numeric values handling thousands separators and decimals.\n",
    "df_parsed[\"budget_numeric\"] = pd.to_numeric(df_parsed[\"budget_text\"].str.replace(\",\", \"\"), errors=\"coerce\")\n",
    "print(\"\\nParsed numeric budget column:\")\n",
    "print(df_parsed[[\"budget_text\", \"budget_numeric\"]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c624d3",
   "metadata": {},
   "source": [
    "### **2.3. Parquet Data Imports**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7b14f5",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Pandas (2.3.1) A-Z/Module_02/Lecture_A/image_02_03.jpg?v=1766707105\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Parquet stores columnar data with built-in metadata\n",
    ">* Import focuses on using or overriding this schema\n",
    "\n",
    ">* Check Parquet datetime encodings and timezones on import\n",
    ">* Validate numeric dtypes to avoid precision or rounding\n",
    "\n",
    ">* Check and standardize categorical and string columns\n",
    ">* Treat import as schema negotiation for consistency\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29f49c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Parquet Data Imports\n",
    "\n",
    "# Demonstrate basic Parquet imports with pandas DataFrames.\n",
    "# Show how Parquet preserves schema and data types.\n",
    "# Adjust dtypes after import for correct downstream analysis.\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# Create a small DataFrame with mixed types.\n",
    "data = {\n",
    "    \"order_id\": [\"0001\", \"0002\", \"0003\"],\n",
    "    \"amount_usd\": [19.99, 5.50, 120.00],\n",
    "    \"purchased_at_utc\": [\n",
    "        datetime(2024, 1, 1, 15, 0, tzinfo=timezone.utc),\n",
    "        datetime(2024, 1, 2, 18, 30, tzinfo=timezone.utc),\n",
    "        datetime(2024, 1, 3, 20, 45, tzinfo=timezone.utc),\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Build the DataFrame and inspect original dtypes.\n",
    "df_original = pd.DataFrame(data)\n",
    "print(\"Original DataFrame dtypes:\")\n",
    "print(df_original.dtypes)\n",
    "\n",
    "# Save the DataFrame to a Parquet file in the current directory.\n",
    "parquet_path = \"orders_example.parquet\"\n",
    "df_original.to_parquet(parquet_path, index=False)\n",
    "\n",
    "# Read the Parquet file back into a new DataFrame.\n",
    "df_loaded = pd.read_parquet(parquet_path)\n",
    "print(\"\\nLoaded DataFrame dtypes:\")\n",
    "print(df_loaded.dtypes)\n",
    "\n",
    "# Convert UTC timestamps to America New_York timezone for analysis.\n",
    "df_loaded[\"purchased_at_local\"] = df_loaded[\"purchased_at_utc\"].dt.tz_convert(\"America/New_York\")\n",
    "\n",
    "# Ensure order_id stays string, even if numeric looking.\n",
    "df_loaded[\"order_id\"] = df_loaded[\"order_id\"].astype(\"string\")\n",
    "\n",
    "# Show final dtypes and a small preview.\n",
    "print(\"\\nAdjusted DataFrame dtypes:\")\n",
    "print(df_loaded.dtypes)\n",
    "print(\"\\nAdjusted DataFrame preview:\")\n",
    "print(df_loaded.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3aa0acd",
   "metadata": {},
   "source": [
    "## **3. Troubleshooting Data Imports**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d817f5",
   "metadata": {},
   "source": [
    "### **3.1. Encoding and Missing Values**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e465f1f2",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Pandas (2.3.1) A-Z/Module_02/Lecture_A/image_03_01.jpg?v=1766707126\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Encoding mismatches cause garbled text or errors\n",
    ">* Common with international characters; recognize mismatch symptoms\n",
    "\n",
    ">* Infer likely encoding from data source\n",
    ">* Test encodings and inspect characters to confirm\n",
    "\n",
    ">* Placeholder codes can hide or fake missing data\n",
    ">* Use domain knowledge to map placeholders to NA\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069f06a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Encoding and Missing Values\n",
    "\n",
    "# Demonstrate encoding issues and missing value placeholders during data import.\n",
    "# Create small CSV files with different encodings and placeholder missing values.\n",
    "# Show how encoding and na_values options fix garbled text and fake numbers.\n",
    "\n",
    "import pandas as pd\n",
    "import io\n",
    "\n",
    "# Create a small CSV string with accented names and placeholder values.\n",
    "csv_text = \"name,age,height_inches\\nJosé,29,70\\nMüller,NA,0\\nAna,35,-1\\n\"\n",
    "\n",
    "# Save the CSV bytes using Latin-1 encoding to simulate legacy system output.\n",
    "latin1_bytes = csv_text.encode(\"latin-1\")\n",
    "\n",
    "# Read the bytes incorrectly as UTF-8 using BytesIO, causing decoding problems.\n",
    "try:\n",
    "    bad_df = pd.read_csv(io.BytesIO(latin1_bytes), encoding=\"utf-8\")\n",
    "    print(\"Incorrect encoding import, names look wrong:\")\n",
    "    print(bad_df[\"name\"])\n",
    "except UnicodeDecodeError as error:\n",
    "    print(\"Import failed due to UnicodeDecodeError:\")\n",
    "    print(error)\n",
    "\n",
    "# Read the same bytes using correct Latin-1 encoding to restore proper characters.\n",
    "correct_df = pd.read_csv(io.BytesIO(latin1_bytes), encoding=\"latin-1\")\n",
    "print(\"\\nCorrect encoding import, names look correct:\")\n",
    "print(correct_df[\"name\"])\n",
    "\n",
    "# Read again while telling pandas which placeholders represent missing values.\n",
    "na_df = pd.read_csv(io.BytesIO(latin1_bytes), encoding=\"latin-1\", na_values=[\"NA\", 0, -1])\n",
    "print(\"\\nMissing values correctly recognized using na_values option:\")\n",
    "print(na_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86021633",
   "metadata": {},
   "source": [
    "### **3.2. Fixing Mixed Types**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db88dee",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Pandas (2.3.1) A-Z/Module_02/Lecture_A/image_03_02.jpg?v=1766707143\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Mixed-type columns mix numbers, text, and dates\n",
    ">* They break calculations, so inspect types and values\n",
    "\n",
    ">* Check inferred column dtypes against expected meanings\n",
    ">* Scan values and docs to find inconsistent entries\n",
    "\n",
    ">* Choose a target type and clean values\n",
    ">* Standardize formats, handle exceptions, or split columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a077af22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Fixing Mixed Types\n",
    "\n",
    "# Demonstrate detecting mixed types in imported columns using pandas DataFrames.\n",
    "# Show how non numeric placeholders break numeric operations during analysis.\n",
    "# Clean placeholders, convert column types, and compare behavior before and after.\n",
    "\n",
    "import pandas as pandas_library\n",
    "import numpy as numeric_library\n",
    "\n",
    "# Create example data with mixed types in numeric and date columns.\n",
    "data_dictionary = {\n",
    "    \"order_id\": [\"A001\", \"A002\", \"A003\", \"A004\"],\n",
    "    \"weight_pounds\": [\"10.5\", \"unknown\", \"8.0\", \"-\"],\n",
    "    \"ship_date\": [\"2024-01-05\", \"01/06/2024\", \"pending\", \"2024-01-08\"],\n",
    "}\n",
    "\n",
    "mixed_frame = pandas_library.DataFrame(data_dictionary)\n",
    "print(\"Original DataFrame with mixed types:\")\n",
    "print(mixed_frame)\n",
    "\n",
    "# Show inferred dtypes, weight and ship_date appear as generic object types.\n",
    "print(\"\\nInferred dtypes before cleaning:\")\n",
    "print(mixed_frame.dtypes)\n",
    "\n",
    "# Attempt numeric mean on weight column, this fails due to mixed string values.\n",
    "try:\n",
    "    print(\"\\nAttempting numeric mean on weight_pounds column:\")\n",
    "    print(mixed_frame[\"weight_pounds\"].mean())\n",
    "except TypeError as error_object:\n",
    "    print(\"Operation failed due to mixed types:\", error_object)\n",
    "\n",
    "# Replace placeholders with proper missing values, then convert to numeric type.\n",
    "clean_frame = mixed_frame.copy()\n",
    "clean_frame[\"weight_pounds\"] = clean_frame[\"weight_pounds\"].replace([\n",
    "    \"unknown\",\n",
    "    \"-\",\n",
    "], numeric_library.nan)\n",
    "\n",
    "clean_frame[\"weight_pounds\"] = pandas_library.to_numeric(\n",
    "    clean_frame[\"weight_pounds\"], errors=\"coerce\"\n",
    ")\n",
    "\n",
    "# Parse ship_date column, coercing invalid or pending entries to missing values.\n",
    "clean_frame[\"ship_date\"] = pandas_library.to_datetime(\n",
    "    clean_frame[\"ship_date\"], errors=\"coerce\", infer_datetime_format=True\n",
    ")\n",
    "\n",
    "print(\"\\nCleaned DataFrame with consistent types:\")\n",
    "print(clean_frame)\n",
    "\n",
    "print(\"\\nDtypes after cleaning and conversion:\")\n",
    "print(clean_frame.dtypes)\n",
    "\n",
    "# Now numeric operations and date filters behave correctly on cleaned columns.\n",
    "print(\"\\nMean shipped weight in pounds, ignoring missing values:\")\n",
    "print(clean_frame[\"weight_pounds\"].mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb9ff39",
   "metadata": {},
   "source": [
    "### **3.3. Sampling Imports with nrows**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921ae578",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Pandas (2.3.1) A-Z/Module_02/Lecture_A/image_03_03.jpg?v=1766707162\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Start by importing only a small sample\n",
    ">* Use samples to tune settings before full load\n",
    "\n",
    ">* Sample a few rows to spot irregularities\n",
    ">* Adjust import settings before loading full dataset\n",
    "\n",
    ">* Small samples can miss rare data problems\n",
    ">* Sample multiple file sections before full import\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e053e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Sampling Imports with nrows\n",
    "\n",
    "# Demonstrate sampling imports using nrows with pandas DataFrame.\n",
    "# Create a fake large CSV file and read only a few rows.\n",
    "# Compare sampled import with full import for understanding potential issues.\n",
    "\n",
    "import pandas as pd\n",
    "import io\n",
    "\n",
    "csv_text = \"\"\"id,date,temperature_f,notes\n",
    "1,2024-01-01,72.5,Normal reading\n",
    "2,2024-01-02,73.0,Missing value later\n",
    "3,2024-01-03,,Sensor glitch\n",
    "4,2024-01-04,75.2,Strange character � here\n",
    "5,2024-01-05,not_a_number,Corrupted temperature\n",
    "6,2024-01-06,71.8,Normal reading\n",
    "\"\"\"\n",
    "\n",
    "csv_buffer = io.StringIO(csv_text)\n",
    "\n",
    "sample_df = pd.read_csv(csv_buffer, nrows=3)\n",
    "\n",
    "print(\"Sampled first three rows only:\")\n",
    "print(sample_df)\n",
    "\n",
    "csv_buffer.seek(0)\n",
    "\n",
    "full_df = pd.read_csv(csv_buffer)\n",
    "\n",
    "print(\"\\nFull import row count:\", len(full_df))\n",
    "print(\"Full import temperature column dtype:\", full_df[\"temperature_f\"].dtype)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c48957",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Importing Data**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53230b05",
   "metadata": {},
   "source": [
    "\n",
    "In this lecture, you learned to:\n",
    "- Load tabular data from CSV, Excel, and Parquet files into Pandas 2.3.1 DataFrames using appropriate read_* functions. \n",
    "- Configure import options such as separators, headers, dtypes, and date parsing to correctly interpret raw files. \n",
    "- Diagnose and fix common import issues including bad encodings, unexpected missing values, and mixed-type columns. \n",
    "\n",
    "In the next Lecture (Lecture B), we will go over 'Cleaning Columns'"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
