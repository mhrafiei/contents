{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e6e0554",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Autograd with TF**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3ceb25",
   "metadata": {},
   "source": [
    ">Last update: 20260125.\n",
    "    \n",
    "By the end of this Lecture, you will be able to:\n",
    "- Use tf.GradientTape to compute gradients of scalar losses with respect to TensorFlow variables. \n",
    "- Interpret gradient values to understand how parameter changes affect a loss function. \n",
    "- Implement a simple manual training step using gradients and an optimizer. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b220db6",
   "metadata": {},
   "source": [
    "## **1. GradientTape Essentials**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f842d638",
   "metadata": {},
   "source": [
    "### **1.1. Tracing Operations**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7087dc43",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_02/Lecture_B/image_01_01.jpg?v=1769368894\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* GradientTape records every differentiable operation on variables\n",
    ">* This computational graph lets TensorFlow apply chain rule\n",
    "\n",
    ">* Tracing builds a new graph each run\n",
    ">* Gradients follow that path, supporting flexible control flow\n",
    "\n",
    ">* All gradient-relevant ops must run during tracing\n",
    ">* Otherwise steps are untracked, causing missing gradients\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885ccb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Tracing Operations\n",
    "\n",
    "# This script demonstrates TensorFlow GradientTape tracing.\n",
    "# It focuses on how operations are recorded dynamically.\n",
    "# Run cells to see gradients from traced computations.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import TensorFlow with a defensive try block.\n",
    "import importlib, sys\n",
    "\n",
    "# Try importing tensorflow safely.\n",
    "spec = importlib.util.find_spec(\"tensorflow\")\n",
    "\n",
    "# Handle missing TensorFlow with clear message.\n",
    "if spec is None:\n",
    "    raise ImportError(\"TensorFlow is required for this script.\")\n",
    "\n",
    "# Import tensorflow after confirming availability.\n",
    "import tensorflow as tf\n",
    "\n",
    "# Force TensorFlow to use CPU only to avoid CUDA errors.\n",
    "tf.config.set_visible_devices([], \"GPU\")\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Set a deterministic random seed for reproducibility.\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Create a simple scalar variable for tracing.\n",
    "w = tf.Variable(2.0, dtype=tf.float32, name=\"weight\")\n",
    "\n",
    "# Define a small helper to print gradients.\n",
    "def show_gradients(tag, grad_value):\n",
    "    print(tag, \"gradient dw =\", float(grad_value))\n",
    "\n",
    "# Define a simple scalar loss function.\n",
    "def loss_fn(x_value):\n",
    "    return (w * x_value - 3.0) ** 2\n",
    "\n",
    "# Choose a scalar input tensor for experiments.\n",
    "x = tf.constant(1.5, dtype=tf.float32)\n",
    "\n",
    "# Use GradientTape to trace all operations.\n",
    "with tf.GradientTape() as tape:\n",
    "    # Ensure tape watches the variable w.\n",
    "    tape.watch(w)\n",
    "    # Compute loss inside the tracing context.\n",
    "    loss = loss_fn(x)\n",
    "\n",
    "# Compute gradient of loss with respect to w.\n",
    "grad_w = tape.gradient(loss, w)\n",
    "\n",
    "# Show gradient from the traced computation.\n",
    "show_gradients(\"Case 1 traced:\", grad_w)\n",
    "\n",
    "# Demonstrate missing tracing when operating outside.\n",
    "with tf.GradientTape() as tape_outside:\n",
    "    # Watch w for potential gradient computation.\n",
    "    tape_outside.watch(w)\n",
    "    # Compute intermediate value inside context.\n",
    "    y_inside = w * x\n",
    "\n",
    "# Compute loss outside the tracing context.\n",
    "loss_outside = (y_inside - 3.0) ** 2\n",
    "\n",
    "# Try to get gradient for loss_outside with respect to w.\n",
    "grad_missing = tape_outside.gradient(loss_outside, w)\n",
    "\n",
    "# Print gradient, expected to be None due to missing trace.\n",
    "print(\"Case 2 outside trace gradient:\", grad_missing)\n",
    "\n",
    "# Show how control flow is traced dynamically.\n",
    "with tf.GradientTape() as tape_branch:\n",
    "    # Watch w for this new tape.\n",
    "    tape_branch.watch(w)\n",
    "    # Use simple branch depending on x value.\n",
    "    if x > 1.0:\n",
    "        loss_branch = (w * x) ** 2\n",
    "    else:\n",
    "        loss_branch = (w * x - 1.0) ** 2\n",
    "\n",
    "# Compute gradient for the actually executed branch.\n",
    "grad_branch = tape_branch.gradient(loss_branch, w)\n",
    "\n",
    "# Display gradient from dynamic control flow.\n",
    "show_gradients(\"Case 3 branch:\", grad_branch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d2cfee",
   "metadata": {},
   "source": [
    "### **1.2. Persistent And Nonpersistent Tapes**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004142c3",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_02/Lecture_B/image_01_02.jpg?v=1769368960\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Autograd has nonpersistent and persistent recording modes\n",
    ">* Nonpersistent records once, returns gradients, then forgets\n",
    "\n",
    ">* Persistent tapes reuse one computation for many gradients\n",
    ">* They cost more memory, so release them carefully\n",
    "\n",
    ">* Choose mode balancing efficiency and flexibility needs\n",
    ">* Use persistent tapes for deeper analysis workflows\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b786e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Persistent And Nonpersistent Tapes\n",
    "\n",
    "# This script demonstrates TensorFlow GradientTape basics.\n",
    "# We compare nonpersistent and persistent tape behaviors.\n",
    "# Focus on simple gradients for clear understanding.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Additionally disable XLA via environment variable to avoid JIT pow error.\n",
    "import os\n",
    "os.environ[\"TF_XLA_FLAGS\"] = \"--tf_xla_auto_jit=0\"\n",
    "os.environ[\"TF_ENABLE_ONEDNN_OPTS\"] = \"0\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"  # disable GPU to avoid JIT pow error\n",
    "\n",
    "# Import TensorFlow with a clear alias.\n",
    "import tensorflow as tf\n",
    "\n",
    "# Disable XLA JIT to avoid GPU JIT compilation issues.\n",
    "try:\n",
    "    tf.config.optimizer.set_jit(False)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Print TensorFlow version briefly.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Create a simple scalar variable for experiments.\n",
    "w = tf.Variable(3.0, dtype=tf.float32)\n",
    "\n",
    "# Define a simple scalar function of w.\n",
    "def simple_loss(x):\n",
    "    return (x ** 2) + (2.0 * x)\n",
    "\n",
    "# Confirm the loss returns a scalar tensor.\n",
    "loss_value = simple_loss(w)\n",
    "print(\"Initial loss value:\", float(loss_value))\n",
    "\n",
    "# Use nonpersistent GradientTape for one gradient computation.\n",
    "with tf.GradientTape() as tape_non:\n",
    "    loss_non = simple_loss(w)\n",
    "\n",
    "grad_non = tape_non.gradient(loss_non, w)\n",
    "print(\"Nonpersistent gradient:\", float(grad_non))\n",
    "\n",
    "# Show that nonpersistent tape cannot be reused safely.\n",
    "try:\n",
    "    grad_non_again = tape_non.gradient(loss_non, w)\n",
    "    print(\"Second nonpersistent gradient:\", grad_non_again)\n",
    "except Exception as e:\n",
    "    print(\"Nonpersistent reuse error type:\", type(e).__name__)\n",
    "\n",
    "# Use persistent GradientTape to allow multiple gradient calls.\n",
    "with tf.GradientTape(persistent=True) as tape_persist:\n",
    "    loss_persist = simple_loss(w)\n",
    "\n",
    "# First gradient from persistent tape.\n",
    "grad_first = tape_persist.gradient(loss_persist, w)\n",
    "print(\"Persistent first gradient:\", float(grad_first))\n",
    "\n",
    "# Second gradient call using the same persistent tape.\n",
    "grad_second = tape_persist.gradient(loss_persist, w)\n",
    "print(\"Persistent second gradient:\", float(grad_second))\n",
    "\n",
    "# Delete persistent tape to free resources explicitly.\n",
    "del tape_persist\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d93ac2d",
   "metadata": {},
   "source": [
    "### **1.3. Watching non variable tensors**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d9a34d",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_02/Lecture_B/image_01_03.jpg?v=1769369076\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Gradients normally track losses with respect to variables\n",
    ">* Tell GradientTape to watch regular tensors for gradients\n",
    "\n",
    ">* Watch non variables to study model behavior\n",
    ">* Enables sensitivity analysis and robustness checks via gradients\n",
    "\n",
    ">* Watch tensors during tracing or gradients vanish\n",
    ">* Plan which tensors to watch for targeted gradients\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f55c62",
   "metadata": {},
   "source": [
    "## **2. Interpreting TensorFlow Gradients**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9e2440",
   "metadata": {},
   "source": [
    "### **2.1. GradientTape Usage Patterns**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587f6468",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_02/Lecture_B/image_02_01.jpg?v=1769369204\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Record one forward pass and scalar loss\n",
    ">* Gradients show loss sensitivity to each parameter\n",
    "\n",
    ">* Choose which tensors GradientTape should track\n",
    ">* Use tracked tensors to diagnose influential computations\n",
    "\n",
    ">* Single-use tapes give one clear gradient snapshot\n",
    ">* Persistent tapes compare objectives and reveal competing signals\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1da583f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - GradientTape Usage Patterns\n",
    "\n",
    "# This script shows basic TensorFlow gradient usage.\n",
    "# It focuses on GradientTape usage patterns.\n",
    "# Run cells to see gradients and interpretations.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import TensorFlow and check version.\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Create a simple scalar input tensor.\n",
    "x = tf.constant(2.0, dtype=tf.float32)\n",
    "\n",
    "# Create a trainable variable representing a weight.\n",
    "w = tf.Variable(1.0, dtype=tf.float32)\n",
    "\n",
    "# Define a simple quadratic loss function.\n",
    "def loss_fn(weight, input_x):\n",
    "    return (weight * input_x - 3.0) ** 2\n",
    "\n",
    "# Use GradientTape to record one forward pass.\n",
    "with tf.GradientTape() as tape:\n",
    "    loss_value = loss_fn(w, x)\n",
    "\n",
    "# Compute gradient of loss with respect to weight.\n",
    "grad_w = tape.gradient(loss_value, w)\n",
    "\n",
    "# Print loss and gradient for interpretation.\n",
    "print(\"Single pass loss:\", float(loss_value))\n",
    "print(\"Single pass gradient:\", float(grad_w))\n",
    "\n",
    "# Manually interpret gradient sign and magnitude.\n",
    "if grad_w > 0.0:\n",
    "    direction = \"decrease\"\n",
    "else:\n",
    "    direction = \"increase\"\n",
    "\n",
    "# Show how gradient suggests changing the weight.\n",
    "print(\"To reduce loss, you should\", direction, \"w.\")\n",
    "\n",
    "# Demonstrate explicitly watching a non variable tensor.\n",
    "base_feature = tf.constant(1.5, dtype=tf.float32)\n",
    "\n",
    "# Use GradientTape and watch the feature tensor.\n",
    "with tf.GradientTape() as tape_feature:\n",
    "    tape_feature.watch(base_feature)\n",
    "    loss_feature = loss_fn(w, base_feature)\n",
    "\n",
    "# Compute gradient with respect to the feature.\n",
    "grad_feature = tape_feature.gradient(loss_feature, base_feature)\n",
    "\n",
    "# Print gradient to see feature sensitivity.\n",
    "print(\"Gradient with respect to feature:\", float(grad_feature))\n",
    "\n",
    "# Show a tiny manual training step using gradient.\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Perform one GradientTape pass for training step.\n",
    "with tf.GradientTape() as tape_train:\n",
    "    current_loss = loss_fn(w, x)\n",
    "\n",
    "# Compute gradient for training update.\n",
    "train_grad = tape_train.gradient(current_loss, w)\n",
    "\n",
    "# Apply a simple gradient descent update.\n",
    "new_w = w - learning_rate * train_grad\n",
    "\n",
    "# Print old weight, gradient, and new weight.\n",
    "print(\"Old w:\", float(w), \"Gradient:\", float(train_grad))\n",
    "print(\"Updated w:\", float(new_w))\n",
    "\n",
    "# Compute new loss to confirm improvement.\n",
    "new_loss = loss_fn(new_w, x)\n",
    "print(\"Loss before:\", float(current_loss), \"Loss after:\", float(new_loss))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcec1528",
   "metadata": {},
   "source": [
    "### **2.2. Multiple Variable Gradients**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d7dbda",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_02/Lecture_B/image_02_02.jpg?v=1769369264\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Gradients form a structured set for all parameters\n",
    ">* Signs and sizes show best loss-reducing direction\n",
    "\n",
    ">* Different weights get different gradient sizes and signs\n",
    ">* Comparing gradients shows which parameters most affect loss\n",
    "\n",
    ">* Gradients form structured patterns across model layers\n",
    ">* Patterns reveal learning focus and gradient-related problems\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963d0605",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Multiple Variable Gradients\n",
    "\n",
    "# This script shows multiple variable gradients.\n",
    "# We use TensorFlow GradientTape for clarity.\n",
    "# Focus is on interpreting gradient values.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import TensorFlow and check version.\n",
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Disable GPU to avoid CUDA_ERROR_INVALID_HANDLE runtime error.\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "# Set a deterministic random seed value.\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Create two trainable scalar variables.\n",
    "w_size = tf.Variable(0.5, dtype=tf.float32)\n",
    "\n",
    "# Create another variable representing age weight.\n",
    "w_age = tf.Variable(-0.3, dtype=tf.float32)\n",
    "\n",
    "# Create a small batch of feature data.\n",
    "features = tf.constant([[80.0, 5.0], [60.0, 20.0]], dtype=tf.float32)\n",
    "\n",
    "# Create simple target prices for the batch.\n",
    "prices = tf.constant([[300.0], [180.0]], dtype=tf.float32)\n",
    "\n",
    "# Stack variables into a single weight vector.\n",
    "w = tf.stack([w_size, w_age])\n",
    "\n",
    "# Validate that feature and weight shapes align.\n",
    "assert features.shape[1] == w.shape[0]\n",
    "\n",
    "# Define a simple prediction function.\n",
    "def predict(features_batch, weight_vector):\n",
    "    return tf.matmul(features_batch, tf.reshape(weight_vector, (-1, 1)))\n",
    "\n",
    "# Use GradientTape to compute gradients.\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch([w_size, w_age])\n",
    "    w_current = tf.stack([w_size, w_age])\n",
    "    preds = predict(features, w_current)\n",
    "    errors = preds - prices\n",
    "    loss = tf.reduce_mean(errors ** 2)\n",
    "\n",
    "# Compute gradients of loss with respect to both variables.\n",
    "grads = tape.gradient(loss, [w_size, w_age])\n",
    "\n",
    "# Print current parameter values and loss.\n",
    "print(\"w_size:\", float(w_size.numpy()), \"w_age:\", float(w_age.numpy()))\n",
    "print(\"Loss value:\", float(loss.numpy()))\n",
    "\n",
    "# Print gradients for each variable separately.\n",
    "print(\"Gradient for w_size:\", float(grads[0].numpy()))\n",
    "print(\"Gradient for w_age:\", float(grads[1].numpy()))\n",
    "\n",
    "# Explain how gradient signs guide parameter updates.\n",
    "step_size = 0.01\n",
    "new_w_size = w_size - step_size * grads[0]\n",
    "new_w_age = w_age - step_size * grads[1]\n",
    "\n",
    "# Show one manual gradient descent update step.\n",
    "print(\"Updated w_size:\", float(new_w_size.numpy()))\n",
    "print(\"Updated w_age:\", float(new_w_age.numpy()))\n",
    "\n",
    "# Summarize which feature currently influences loss more.\n",
    "print(\"Abs gradients:\", float(abs(grads[0]).numpy()), float(abs(grads[1]).numpy()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a016d8",
   "metadata": {},
   "source": [
    "### **2.3. Handling None Gradients**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030ba3ff",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_02/Lecture_B/image_02_03.jpg?v=1769369324\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* None gradient means no differentiable path exists\n",
    ">* Some parameters simply do not affect the loss\n",
    "\n",
    ">* Zero gradient means loss depends but is flat\n",
    ">* None gradient means parameter never affects the loss\n",
    "\n",
    ">* None gradients reveal unused or nondifferentiable parameters\n",
    ">* Fix architecture so all trainable parameters affect loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb87a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Handling None Gradients\n",
    "\n",
    "# This script shows TensorFlow None gradients clearly.\n",
    "# It focuses on interpreting gradient values safely.\n",
    "# Run each part and read the printed explanations.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import TensorFlow with a clear alias.\n",
    "import tensorflow as tf\n",
    "\n",
    "# Disable GPU to avoid CUDA_ERROR_INVALID_HANDLE runtime error.\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "\n",
    "# Print TensorFlow version briefly.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Create two scalar variables for demonstration.\n",
    "w_used = tf.Variable(2.0, dtype=tf.float32)\n",
    "\n",
    "# Create another variable that will be unused.\n",
    "w_unused = tf.Variable(5.0, dtype=tf.float32)\n",
    "\n",
    "# Define a simple input tensor.\n",
    "x = tf.constant(3.0, dtype=tf.float32)\n",
    "\n",
    "# Use GradientTape to track computations.\n",
    "with tf.GradientTape() as tape:\n",
    "    # Watch both variables for potential gradients.\n",
    "    tape.watch([w_used, w_unused])\n",
    "\n",
    "    # Compute a loss that depends only on w_used.\n",
    "    y = w_used * x\n",
    "\n",
    "    # Define a scalar loss from the output.\n",
    "    loss = (y - 10.0) ** 2\n",
    "\n",
    "# Compute gradients with respect to both variables.\n",
    "grads = tape.gradient(loss, [w_used, w_unused])\n",
    "\n",
    "# Unpack gradients for clarity.\n",
    "grad_used, grad_unused = grads\n",
    "\n",
    "# Confirm shapes are as expected scalars.\n",
    "assert grad_used.shape == (), \"grad_used must be scalar.\"\n",
    "\n",
    "# Check that unused gradient is actually None.\n",
    "assert grad_unused is None, \"grad_unused should be None here.\"\n",
    "\n",
    "# Print the numeric gradient for the used variable.\n",
    "print(\"Gradient for w_used (numeric):\", float(grad_used))\n",
    "\n",
    "# Explain why this gradient is numeric.\n",
    "print(\"w_used affects loss, so gradient is a number.\")\n",
    "\n",
    "# Print the gradient for the unused variable.\n",
    "print(\"Gradient for w_unused:\", grad_unused)\n",
    "\n",
    "# Explain why this gradient is None, not zero.\n",
    "print(\"w_unused never affects loss, so gradient is None.\")\n",
    "\n",
    "# Show that changing w_unused does not change the loss.\n",
    "old_loss = float(loss.numpy())\n",
    "\n",
    "# Manually change w_unused value.\n",
    "w_unused.assign(100.0)\n",
    "\n",
    "# Recompute loss to confirm it is unchanged.\n",
    "with tf.GradientTape() as tape2:\n",
    "    tape2.watch([w_used, w_unused])\n",
    "    y2 = w_used * x\n",
    "    loss2 = (y2 - 10.0) ** 2\n",
    "\n",
    "# Print both losses to compare.\n",
    "print(\"Loss before changing w_unused:\", old_loss)\n",
    "\n",
    "# Final print shows loss after change.\n",
    "print(\"Loss after changing w_unused:\", float(loss2.numpy()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edee9948",
   "metadata": {},
   "source": [
    "## **3. Manual Gradient Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a58a775",
   "metadata": {},
   "source": [
    "### **3.1. Using Keras Optimizers**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4165f4",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_02/Lecture_B/image_03_01.jpg?v=1769369396\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Keras optimizers turn gradients into parameter updates\n",
    ">* They automate step size, momentum, and stability details\n",
    "\n",
    ">* Different Keras optimizers change how training progresses\n",
    ">* They enable faster, stable learning with less effort\n",
    "\n",
    ">* Manual loops control loss, gradients, and updates\n",
    ">* Experiment, tune optimizers, and build training intuition\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89426942",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Using Keras Optimizers\n",
    "\n",
    "# This script shows manual training with optimizers.\n",
    "# We use TensorFlow GradientTape for simple regression.\n",
    "# Focus on Keras optimizers updating trainable variables.\n",
    "\n",
    "# Install TensorFlow if not already available.\n",
    "# pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import TensorFlow and NumPy for computations.\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "tf.random.set_seed(7)\n",
    "np.random.seed(7)\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Create tiny synthetic data for y = 3x + 2.\n",
    "x_data = np.linspace(-1.0, 1.0, 8, dtype=np.float32)\n",
    "y_data = 3.0 * x_data + 2.0\n",
    "\n",
    "# Convert data to TensorFlow tensors.\n",
    "x_train = tf.constant(x_data.reshape(-1, 1))\n",
    "y_train = tf.constant(y_data.reshape(-1, 1))\n",
    "\n",
    "# Validate shapes before training operations.\n",
    "assert x_train.shape == y_train.shape\n",
    "\n",
    "# Define trainable variables for weight and bias.\n",
    "w = tf.Variable(tf.random.normal(shape=(1, 1)))\n",
    "b = tf.Variable(tf.zeros(shape=(1,)))\n",
    "\n",
    "# Define a simple mean squared error loss.\n",
    "def compute_loss(predictions, targets):\n",
    "    error = predictions - targets\n",
    "    return tf.reduce_mean(tf.square(error))\n",
    "\n",
    "# Create a Keras optimizer with small learning rate.\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)\n",
    "\n",
    "# Define one manual training step using GradientTape.\n",
    "@tf.function\n",
    "def train_step(inputs, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        preds = tf.matmul(inputs, w) + b\n",
    "        loss = compute_loss(preds, targets)\n",
    "    grads = tape.gradient(loss, [w, b])\n",
    "    optimizer.apply_gradients(zip(grads, [w, b]))\n",
    "    return loss, grads\n",
    "\n",
    "# Run a few training steps and print progress.\n",
    "for step in range(5):\n",
    "    loss_value, gradients = train_step(x_train, y_train)\n",
    "    w_grad, b_grad = gradients\n",
    "    print(\n",
    "        f\"Step {step}: loss={loss_value.numpy():.4f}, \"\n",
    "        f\"dw={w_grad.numpy()[0,0]:.4f}, db={b_grad.numpy()[0]:.4f}\"\n",
    "    )\n",
    "\n",
    "# Show final learned parameters and compare to true values.\n",
    "print(\"Learned w, b:\", float(w.numpy()[0, 0]), float(b.numpy()[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1aedb80",
   "metadata": {},
   "source": [
    "### **3.2. Applying Gradients to Variables**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cf5233",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_02/Lecture_B/image_03_02.jpg?v=1769369452\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Gradients tell how to adjust each parameter\n",
    ">* We pair gradients with variables and optimizers update\n",
    "\n",
    ">* Gradients show how each parameter affects error\n",
    ">* Optimizer uses gradients, learning rate to reduce loss\n",
    "\n",
    ">* Modify gradients before updating to improve learning\n",
    ">* Repeated updates gradually refine the modelâ€™s behavior\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e110da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Applying Gradients to Variables\n",
    "\n",
    "# This script shows applying gradients to variables.\n",
    "# We use TensorFlow GradientTape for manual updates.\n",
    "# Focus on a tiny linear model training step.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import TensorFlow with a clear alias.\n",
    "import tensorflow as tf\n",
    "\n",
    "# Force TensorFlow to use CPU to avoid CUDA errors.\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "\n",
    "# Print TensorFlow version briefly.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Set deterministic random seed for reproducibility.\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Create a tiny synthetic dataset for y = 3x + 2.\n",
    "x_data = tf.constant([[0.0], [1.0], [2.0], [3.0]])\n",
    "\n",
    "# Create matching target outputs as a column vector.\n",
    "y_true = tf.constant([[2.0], [5.0], [8.0], [11.0]])\n",
    "\n",
    "# Define a trainable weight variable initialized randomly.\n",
    "w = tf.Variable(tf.random.normal(shape=(1, 1)))\n",
    "\n",
    "# Define a trainable bias variable initialized to zero.\n",
    "b = tf.Variable(tf.zeros(shape=(1,)))\n",
    "\n",
    "# Choose a simple optimizer with a small learning rate.\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)\n",
    "\n",
    "# Define a function to compute predictions from inputs.\n",
    "def model(x):\n",
    "    # Compute linear model output y = xw + b.\n",
    "    return tf.matmul(x, w) + b\n",
    "\n",
    "# Define a function to compute mean squared error loss.\n",
    "def loss_fn(y_pred, y_target):\n",
    "    # Compute squared differences and then mean.\n",
    "    return tf.reduce_mean(tf.square(y_pred - y_target))\n",
    "\n",
    "# Run one manual training step using GradientTape.\n",
    "with tf.GradientTape() as tape:\n",
    "    # Compute predictions for current parameters.\n",
    "    y_pred = model(x_data)\n",
    "\n",
    "    # Compute scalar loss from predictions and targets.\n",
    "    loss_value = loss_fn(y_pred, y_true)\n",
    "\n",
    "# Collect trainable variables in a list.\n",
    "variables = [w, b]\n",
    "\n",
    "# Compute gradients of loss with respect to variables.\n",
    "gradients = tape.gradient(loss_value, variables)\n",
    "\n",
    "# Validate that gradients and variables have matching lengths.\n",
    "if gradients is None or len(gradients) != len(variables):\n",
    "    raise RuntimeError(\"Gradient and variable lengths mismatch.\")\n",
    "\n",
    "# Pair each gradient with its corresponding variable.\n",
    "grad_var_pairs = list(zip(gradients, variables))\n",
    "\n",
    "# Apply gradients to update variables using the optimizer.\n",
    "optimizer.apply_gradients(grad_var_pairs)\n",
    "\n",
    "# Print loss and parameter values before and after update.\n",
    "print(\"Loss after one step:\", float(loss_value))\n",
    "\n",
    "# Print updated weight and bias values clearly.\n",
    "print(\"Updated weight:\", float(w.numpy()[0][0]))\n",
    "\n",
    "# Print updated bias value as a simple float.\n",
    "print(\"Updated bias:\", float(b.numpy()[0]))\n",
    "\n",
    "# Show a prediction after the update for x = 4.\n",
    "x_test = tf.constant([[4.0]])\n",
    "\n",
    "# Compute prediction using updated parameters.\n",
    "y_test_pred = model(x_test)\n",
    "\n",
    "# Print the new prediction to observe learning effect.\n",
    "print(\"Prediction for x=4 after update:\", float(y_test_pred.numpy()[0][0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46080af",
   "metadata": {},
   "source": [
    "### **3.3. Stabilizing Gradient Behavior**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a214019",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_02/Lecture_B/image_03_03.jpg?v=1769369509\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Unstable gradients make training explode or stall\n",
    ">* We stabilize updates so loss decreases smoothly\n",
    "\n",
    ">* Learning rate size strongly affects training stability\n",
    ">* Use tuned optimizers with momentum or adaptive scaling\n",
    "\n",
    ">* Use gradient clipping and monitor gradient norms\n",
    ">* Adjust models and preprocessing to keep training stable\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371a636a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Stabilizing Gradient Behavior\n",
    "\n",
    "# This script shows stabilizing gradient behavior.\n",
    "# We use TensorFlow GradientTape for manual updates.\n",
    "# We compare training with and without gradient clipping.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "\n",
    "# Disable GPU to avoid CUDA errors in some environments.\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "# Import TensorFlow for tensors and gradients.\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "random.seed(7)\n",
    "\n",
    "# Set TensorFlow random seed for reproducibility.\n",
    "tf.random.set_seed(7)\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Create simple synthetic input data tensor.\n",
    "x_data = tf.constant([[1.0], [2.0], [3.0], [4.0]], dtype=tf.float32)\n",
    "\n",
    "# Create simple synthetic target data tensor.\n",
    "y_true = tf.constant([[3.0], [5.0], [7.0], [9.0]], dtype=tf.float32)\n",
    "\n",
    "# Check that input and target shapes match.\n",
    "assert x_data.shape == y_true.shape\n",
    "\n",
    "# Define a simple linear model using variables.\n",
    "w = tf.Variable(tf.random.normal(shape=(1, 1)))\n",
    "\n",
    "# Define bias variable for the linear model.\n",
    "b = tf.Variable(tf.zeros(shape=(1,)))\n",
    "\n",
    "# Define mean squared error loss function.\n",
    "def mse_loss(y_pred, y_target):\n",
    "    # Compute squared differences between predictions and targets.\n",
    "    squared = tf.square(y_pred - y_target)\n",
    "\n",
    "    # Return mean of squared differences.\n",
    "    return tf.reduce_mean(squared)\n",
    "\n",
    "# Define a function to run one training step.\n",
    "def train_step(clip_gradients=False):\n",
    "    # Record operations for automatic differentiation.\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Compute model predictions for current parameters.\n",
    "        y_pred = tf.matmul(x_data, w) + b\n",
    "\n",
    "        # Compute scalar loss from predictions and targets.\n",
    "        loss_value = mse_loss(y_pred, y_true)\n",
    "\n",
    "    # Compute gradients of loss with respect to parameters.\n",
    "    grads = tape.gradient(loss_value, [w, b])\n",
    "\n",
    "    # Optionally clip gradients to stabilize updates.\n",
    "    if clip_gradients:\n",
    "        # Clip gradients by global norm to a maximum value.\n",
    "        grads, _ = tf.clip_by_global_norm(grads, 1.0)\n",
    "\n",
    "    # Apply a simple manual gradient descent update.\n",
    "    learning_rate = 0.5\n",
    "\n",
    "    # Update weight variable using gradient and learning rate.\n",
    "    w.assign_sub(learning_rate * grads[0])\n",
    "\n",
    "    # Update bias variable using gradient and learning rate.\n",
    "    b.assign_sub(learning_rate * grads[1])\n",
    "\n",
    "    # Return current loss and gradient norms for inspection.\n",
    "    grad_norm = tf.linalg.global_norm(grads)\n",
    "\n",
    "    # Return scalar loss and gradient norm values.\n",
    "    return float(loss_value.numpy()), float(grad_norm.numpy())\n",
    "\n",
    "# Run a few unstable steps without gradient clipping.\n",
    "print(\"\\nTraining without gradient clipping:\")\n",
    "\n",
    "# Perform several manual updates and observe behavior.\n",
    "for step in range(3):\n",
    "    # Call training step without clipping enabled.\n",
    "    loss_value, grad_norm = train_step(clip_gradients=False)\n",
    "\n",
    "    # Print step, loss, and gradient norm values.\n",
    "    print(f\"Step {step}: loss={loss_value:.3f}, grad_norm={grad_norm:.3f}\")\n",
    "\n",
    "# Reset model parameters for clipped training run.\n",
    "w.assign(tf.random.normal(shape=(1, 1)))\n",
    "\n",
    "# Reset bias parameter to zero for fair comparison.\n",
    "b.assign(tf.zeros(shape=(1,)))\n",
    "\n",
    "# Run a few stable steps with gradient clipping enabled.\n",
    "print(\"\\nTraining with gradient clipping:\")\n",
    "\n",
    "# Perform several manual updates and observe behavior.\n",
    "for step in range(3):\n",
    "    # Call training step with clipping enabled.\n",
    "    loss_value, grad_norm = train_step(clip_gradients=True)\n",
    "\n",
    "    # Print step, loss, and gradient norm values.\n",
    "    print(f\"Step {step}: loss={loss_value:.3f}, grad_norm={grad_norm:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786b0687",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Autograd with TF**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70187706",
   "metadata": {},
   "source": [
    "\n",
    "In this lecture, you learned to:\n",
    "- Use tf.GradientTape to compute gradients of scalar losses with respect to TensorFlow variables. \n",
    "- Interpret gradient values to understand how parameter changes affect a loss function. \n",
    "- Implement a simple manual training step using gradients and an optimizer. \n",
    "\n",
    "In the next Module (Module 3), we will go over 'Keras Model Building'"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
