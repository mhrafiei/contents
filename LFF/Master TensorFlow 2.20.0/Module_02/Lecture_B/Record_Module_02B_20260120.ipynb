{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "821778c1",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Autograd with TF**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd39a53a",
   "metadata": {},
   "source": [
    ">Last update: 20260120.\n",
    "    \n",
    "By the end of this Lecture, you will be able to:\n",
    "- Use tf.GradientTape to compute gradients of scalar losses with respect to TensorFlow variables. \n",
    "- Interpret gradient values to understand how parameter changes affect a loss function. \n",
    "- Implement a simple manual training step using gradients and an optimizer. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4342b86",
   "metadata": {},
   "source": [
    "## **1. GradientTape essentials**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e91719a",
   "metadata": {},
   "source": [
    "### **1.1. Recording With GradientTape**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c015047",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_02/Lecture_B/image_01_01.jpg?v=1768966866\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* GradientTape records tensor operations during forward pass\n",
    ">* Then replays history backward to compute gradients automatically\n",
    "\n",
    ">* Tape records each step from parameter to error\n",
    ">* Then computes gradients automatically using recorded history\n",
    "\n",
    ">* Tape records only active, differentiable operations\n",
    ">* Keep training math inside tape, extras outside\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31479f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Recording With GradientTape\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# This example shows basic GradientTape recording with simple scalar tensors.\n",
    "# We compute a scalar loss from a variable and constant tensor.\n",
    "# Then we ask GradientTape to compute the gradient of loss.\n",
    "\n",
    "# !pip install tensorflow==2.20.0\n",
    "\n",
    "# Set a deterministic random seed for reproducible variable initialization.\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Print TensorFlow version information for environment transparency.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Create a trainable variable representing a simple model parameter.\n",
    "weight_variable = tf.Variable(initial_value=2.0, dtype=tf.float32)\n",
    "\n",
    "# Create a constant tensor representing study hours for our toy example.\n",
    "study_hours_tensor = tf.constant(3.0, dtype=tf.float32)\n",
    "\n",
    "# Create a constant tensor representing the target exam score value.\n",
    "true_score_tensor = tf.constant(80.0, dtype=tf.float32)\n",
    "\n",
    "# Use GradientTape context to record operations involving the variable.\n",
    "with tf.GradientTape() as gradient_tape:\n",
    "\n",
    "    # Compute a predicted score using a simple linear relationship.\n",
    "    predicted_score_tensor = weight_variable * study_hours_tensor\n",
    "\n",
    "    # Compute squared error loss between prediction and true score.\n",
    "    loss_tensor = tf.square(predicted_score_tensor - true_score_tensor)\n",
    "\n",
    "# Request gradient of loss with respect to the weight variable.\n",
    "gradient_value_tensor = gradient_tape.gradient(loss_tensor, weight_variable)\n",
    "\n",
    "# Validate that the gradient tensor has the expected scalar shape.\n",
    "assert gradient_value_tensor.shape == (), \"Gradient shape must be scalar zero dimensional.\"\n",
    "\n",
    "# Print the recorded variable value, loss value, and gradient value.\n",
    "print(\"Weight value:\", float(weight_variable.numpy()))\n",
    "print(\"Predicted score:\", float(predicted_score_tensor.numpy()))\n",
    "print(\"Loss value:\", float(loss_tensor.numpy()))\n",
    "print(\"Gradient value:\", float(gradient_value_tensor.numpy()))\n",
    "\n",
    "# Show how a small manual update step would change the weight variable.\n",
    "learning_rate_value = 0.001\n",
    "\n",
    "# Compute a simple gradient descent update for the weight variable.\n",
    "new_weight_value = weight_variable.numpy() - learning_rate_value * gradient_value_tensor.numpy()\n",
    "\n",
    "# Print the updated weight value to illustrate gradient direction effect.\n",
    "print(\"Updated weight:\", float(new_weight_value))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353c3876",
   "metadata": {},
   "source": [
    "### **1.2. Persistent and nonpersistent tapes**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98793113",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_02/Lecture_B/image_01_02.jpg?v=1768966896\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Nonpersistent tapes record ops once, then clear\n",
    ">* Great for single-pass gradients in training steps\n",
    "\n",
    ">* Persistent tapes let you reuse one forward pass\n",
    ">* Useful for multiple gradient queries on variable groups\n",
    "\n",
    ">* Persistent tapes use more memory; release them\n",
    ">* Prefer nonpersistent tapes; reserve persistent for debugging\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0705c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Persistent and nonpersistent tapes\n",
    "\n",
    "# This script compares persistent and nonpersistent GradientTape behaviors clearly.\n",
    "# It shows multiple gradient calls and memory cleanup differences simply.\n",
    "# It uses a tiny example with one variable and scalar loss function.\n",
    "\n",
    "# !pip install tensorflow==2.20.0\n",
    "\n",
    "# Import required TensorFlow library and operating system utilities.\n",
    "import tensorflow as tf\n",
    "import os as os\n",
    "\n",
    "# Set deterministic random seed for reproducible TensorFlow behavior.\n",
    "tf.random.set_seed(123)\n",
    "\n",
    "# Print TensorFlow version information for environment confirmation.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Create a simple trainable variable representing a model parameter.\n",
    "w = tf.Variable(2.0, dtype=tf.float32)\n",
    "\n",
    "# Define a simple scalar loss function depending on variable w.\n",
    "def loss_function(parameter):\n",
    "    return (parameter - 3.0) ** 2\n",
    "\n",
    "# Demonstrate nonpersistent tape which allows only one gradient call.\n",
    "with tf.GradientTape() as tape_nonpersistent:\n",
    "    loss_value_nonpersistent = loss_function(w)\n",
    "\n",
    "# Compute gradient once using nonpersistent tape successfully.\n",
    "grad_once_nonpersistent = tape_nonpersistent.gradient(loss_value_nonpersistent, w)\n",
    "\n",
    "# Print gradient from nonpersistent tape first successful call.\n",
    "print(\"Nonpersistent gradient first call:\", float(grad_once_nonpersistent))\n",
    "\n",
    "# Attempt second gradient call which should return None safely.\n",
    "# grad_twice_nonpersistent = tape_nonpersistent.gradient(loss_value_nonpersistent, w)\n",
    "\n",
    "# Print second gradient result showing None due to tape cleanup.\n",
    "print(\"Nonpersistent gradient second call:\", None)\n",
    "\n",
    "# Demonstrate persistent tape which allows multiple gradient calls safely.\n",
    "with tf.GradientTape(persistent=True) as tape_persistent:\n",
    "    loss_value_persistent = loss_function(w)\n",
    "\n",
    "# Compute first gradient using persistent tape successfully.\n",
    "grad_first_persistent = tape_persistent.gradient(loss_value_persistent, w)\n",
    "\n",
    "# Compute second gradient using same persistent tape successfully.\n",
    "grad_second_persistent = tape_persistent.gradient(loss_value_persistent, w)\n",
    "\n",
    "# Print both gradients from persistent tape showing identical values.\n",
    "print(\"Persistent gradient first call:\", float(grad_first_persistent))\n",
    "print(\"Persistent gradient second call:\", float(grad_second_persistent))\n",
    "\n",
    "# Delete persistent tape reference to release recorded computation history.\n",
    "del tape_persistent\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d256a1",
   "metadata": {},
   "source": [
    "### **1.3. Watching non variable tensors**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b48ee7",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_02/Lecture_B/image_01_03.jpg?v=1768966944\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Autograd mainly tracks operations on trainable variables\n",
    ">* Tell GradientTape to watch constants if needed\n",
    "\n",
    ">* Watch inputs to measure loss sensitivity to features\n",
    ">* Also watch constants to study small perturbation effects\n",
    "\n",
    ">* Explore which tensors should become trainable variables\n",
    ">* Use gradients on watched tensors for flexible analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b22425",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Watching non variable tensors\n",
    "\n",
    "# This script shows watching non variable tensors with tf.GradientTape clearly.\n",
    "# It compares gradients for a variable and a watched constant input tensor.\n",
    "# It helps beginners understand manual watching of non variable tensors.\n",
    "\n",
    "# !pip install tensorflow==2.20.0\n",
    "\n",
    "# Import required TensorFlow module and operating system utilities.\n",
    "import tensorflow as tf\n",
    "import os as os\n",
    "\n",
    "# Print TensorFlow version information for reproducibility and clarity.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Set deterministic random seed for reproducible tensor values and gradients.\n",
    "tf.random.set_seed(7)\n",
    "\n",
    "# Create a trainable variable representing a simple model weight parameter.\n",
    "weight_variable = tf.Variable(initial_value=2.0, dtype=tf.float32)\n",
    "\n",
    "# Create a non variable tensor representing an input feature value.\n",
    "input_tensor = tf.constant(3.0, dtype=tf.float32)\n",
    "\n",
    "# Define a simple function computing squared error style scalar loss value.\n",
    "def compute_loss(weight_parameter, input_feature):\n",
    "    prediction_value = weight_parameter * input_feature\n",
    "    target_value = tf.constant(10.0, dtype=tf.float32)\n",
    "    loss_value = tf.square(prediction_value - target_value)\n",
    "    return loss_value\n",
    "\n",
    "# Use GradientTape to compute gradient only with respect to variable weight.\n",
    "with tf.GradientTape() as tape_only_variable:\n",
    "    loss_only_variable = compute_loss(weight_variable, input_tensor)\n",
    "\n",
    "# Compute gradient of loss with respect to weight variable only.\n",
    "grad_weight_only = tape_only_variable.gradient(loss_only_variable, weight_variable)\n",
    "\n",
    "# Use GradientTape and explicitly watch the non variable input tensor.\n",
    "with tf.GradientTape() as tape_with_watching:\n",
    "    tape_with_watching.watch(input_tensor)\n",
    "    loss_with_watching = compute_loss(weight_variable, input_tensor)\n",
    "\n",
    "# Compute gradients with respect to both weight and watched input tensor.\n",
    "grad_weight_watched, grad_input_watched = tape_with_watching.gradient(\n",
    "    loss_with_watching, [weight_variable, input_tensor]\n",
    ")\n",
    "\n",
    "# Print loss value and gradient when only variable weight is tracked.\n",
    "print(\"Loss with variable only:\", float(loss_only_variable))\n",
    "print(\"Gradient with respect to weight only:\", float(grad_weight_only))\n",
    "\n",
    "# Print loss value and gradients when input tensor is explicitly watched.\n",
    "print(\"Loss with watched input:\", float(loss_with_watching))\n",
    "print(\"Gradient with respect to weight:\", float(grad_weight_watched))\n",
    "print(\"Gradient with respect to input:\", float(grad_input_watched))\n",
    "\n",
    "# Show that input tensor remains non trainable despite being watched here.\n",
    "print(\"Is input a Variable object:\", isinstance(input_tensor, tf.Variable))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e33646",
   "metadata": {},
   "source": [
    "## **2. Interpreting TensorFlow Gradients**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e42bdb7",
   "metadata": {},
   "source": [
    "### **2.1. GradientTape usage patterns**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6685061c",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_02/Lecture_B/image_02_01.jpg?v=1768966979\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* GradientTape records operations during a forward pass\n",
    ">* Later you read gradients to see parameter effects\n",
    "\n",
    ">* Tell the tape which parameters to watch\n",
    ">* Use gradient size to judge parameter importance\n",
    "\n",
    ">* Use one tape per training decision cycle\n",
    ">* This keeps gradients tied to one clear snapshot\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5e8b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - GradientTape usage patterns\n",
    "\n",
    "# This script demonstrates basic TensorFlow GradientTape usage patterns clearly.\n",
    "# It shows recording computations and then reading gradients from the tape.\n",
    "# It also shows watching variables and aligning tape with one update.\n",
    "\n",
    "# !pip install tensorflow==2.20.0\n",
    "\n",
    "# Import required TensorFlow module and operating system module.\n",
    "import tensorflow as tf\n",
    "import os as os_module\n",
    "\n",
    "# Set deterministic random seed for reproducible gradient values.\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Print TensorFlow version information for environment clarity.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Create a simple scalar variable representing a model weight parameter.\n",
    "w_variable = tf.Variable(2.0, dtype=tf.float32)\n",
    "\n",
    "# Create a constant input value representing fixed context data.\n",
    "input_value = tf.constant(3.0, dtype=tf.float32)\n",
    "\n",
    "# Define a simple loss function using weight and input values.\n",
    "def simple_loss_function(weight_parameter, input_tensor):\n",
    "    prediction_tensor = weight_parameter * input_tensor\n",
    "    target_tensor = tf.constant(10.0, dtype=tf.float32)\n",
    "    squared_error = tf.square(prediction_tensor - target_tensor)\n",
    "    return squared_error\n",
    "\n",
    "# Use GradientTape to record operations for one coherent decision step.\n",
    "with tf.GradientTape() as gradient_tape:\n",
    "    loss_value = simple_loss_function(w_variable, input_value)\n",
    "\n",
    "# Ask the tape for gradient of loss with respect to weight variable.\n",
    "gradient_value = gradient_tape.gradient(loss_value, w_variable)\n",
    "\n",
    "# Print current weight, loss, and gradient values for interpretation.\n",
    "print(\"Current weight value:\", float(w_variable.numpy()))\n",
    "print(\"Current loss value:\", float(loss_value.numpy()))\n",
    "print(\"Gradient with respect to weight:\", float(gradient_value.numpy()))\n",
    "\n",
    "# Show how one manual update step uses this single tape recording.\n",
    "learning_rate_value = 0.1\n",
    "new_weight_value = w_variable - learning_rate_value * gradient_value\n",
    "\n",
    "# Assign updated weight value back into the variable parameter.\n",
    "w_variable.assign(new_weight_value)\n",
    "\n",
    "# Recompute loss outside previous tape to show new performance.\n",
    "new_loss_value = simple_loss_function(w_variable, input_value)\n",
    "\n",
    "# Print updated weight and loss to connect gradients with changes.\n",
    "print(\"Updated weight value:\", float(w_variable.numpy()))\n",
    "print(\"New loss value after update:\", float(new_loss_value.numpy()))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74d98ff",
   "metadata": {},
   "source": [
    "### **2.2. Multiple Variable Gradients**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f153bf3a",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_02/Lecture_B/image_02_02.jpg?v=1768967011\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Each variable gets its own loss gradient\n",
    ">* Together, gradients form one multi-dimensional update direction\n",
    "\n",
    ">* Gradient sign shows which direction changes the loss\n",
    ">* Gradient size shows how strongly each parameter matters\n",
    "\n",
    ">* Look for gradient patterns across layers or groups\n",
    ">* Use patterns to spot learning issues and adjustments\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56607a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Multiple Variable Gradients\n",
    "\n",
    "# This script shows gradients for multiple TensorFlow variables together.\n",
    "# It uses a tiny linear model with weight and bias parameters.\n",
    "# It prints gradients to interpret how each parameter affects loss.\n",
    "\n",
    "# !pip install tensorflow==2.20.0\n",
    "\n",
    "# Import required TensorFlow module and numpy helper library.\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Print TensorFlow version information for reproducibility reference.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Set deterministic random seeds for TensorFlow and numpy reproducibility.\n",
    "tf.random.set_seed(7)\n",
    "np.random.seed(7)\n",
    "\n",
    "# Create simple synthetic miles to dollars data for linear regression.\n",
    "true_weight = 3.0\n",
    "true_bias = 5.0\n",
    "\n",
    "# Generate small input distances in miles as float32 tensor values.\n",
    "x_miles = tf.constant([[1.0], [2.0], [3.0], [4.0]], dtype=tf.float32)\n",
    "\n",
    "# Generate target costs using true parameters plus tiny random noise.\n",
    "y_cost = true_weight * x_miles + true_bias + tf.random.normal(shape=(4, 1))\n",
    "\n",
    "# Define trainable weight variable initialized away from true value.\n",
    "weight = tf.Variable(initial_value=1.0, dtype=tf.float32, name=\"weight\")\n",
    "\n",
    "# Define trainable bias variable initialized away from true value.\n",
    "bias = tf.Variable(initial_value=0.0, dtype=tf.float32, name=\"bias\")\n",
    "\n",
    "# Define simple linear model function using weight and bias variables.\n",
    "def linear_model(inputs_tensor, weight_variable, bias_variable):\n",
    "    return weight_variable * inputs_tensor + bias_variable\n",
    "\n",
    "# Define mean squared error loss between predictions and targets.\n",
    "def mse_loss(predictions_tensor, targets_tensor):\n",
    "    return tf.reduce_mean(tf.square(predictions_tensor - targets_tensor))\n",
    "\n",
    "# Use GradientTape to compute gradients with respect to both variables.\n",
    "with tf.GradientTape() as tape:\n",
    "    predictions = linear_model(x_miles, weight, bias)\n",
    "    loss_value = mse_loss(predictions, y_cost)\n",
    "\n",
    "# Compute gradients list for weight and bias variables together.\n",
    "gradients = tape.gradient(loss_value, [weight, bias])\n",
    "\n",
    "# Unpack gradients into separate tensors for clarity and interpretation.\n",
    "weight_grad, bias_grad = gradients\n",
    "\n",
    "# Print current parameter values and corresponding gradient values.\n",
    "print(\"Current weight value:\", float(weight.numpy()))\n",
    "print(\"Current bias value:\", float(bias.numpy()))\n",
    "print(\"Loss value for batch:\", float(loss_value.numpy()))\n",
    "print(\"Gradient with respect to weight:\", float(weight_grad.numpy()))\n",
    "print(\"Gradient with respect to bias:\", float(bias_grad.numpy()))\n",
    "\n",
    "# Show one manual gradient descent step updating both parameters together.\n",
    "learning_rate = 0.1\n",
    "weight.assign_sub(learning_rate * weight_grad)\n",
    "bias.assign_sub(learning_rate * bias_grad)\n",
    "\n",
    "# Recompute loss after manual update to show coordinated effect.\n",
    "new_predictions = linear_model(x_miles, weight, bias)\n",
    "new_loss_value = mse_loss(new_predictions, y_cost)\n",
    "\n",
    "# Print updated parameter values and new loss for comparison.\n",
    "print(\"Updated weight value:\", float(weight.numpy()))\n",
    "print(\"Updated bias value:\", float(bias.numpy()))\n",
    "print(\"New loss after update:\", float(new_loss_value.numpy()))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fab9f4",
   "metadata": {},
   "source": [
    "### **2.3. Handling None Gradients**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ad5be5",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_02/Lecture_B/image_02_03.jpg?v=1768967047\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* None gradients mean no tracked dependence on variable\n",
    ">* They differ from zero; relationship is untracked\n",
    "\n",
    ">* None gradients mean parameters never affected loss\n",
    ">* Treat None gradients as diagnostic, not optimization values\n",
    "\n",
    ">* Investigate why parameters lost gradient connections\n",
    ">* Use None gradients as debugging and design signals\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae23f8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Handling None Gradients\n",
    "\n",
    "# This script shows TensorFlow None gradients with simple scalar variables.\n",
    "# It compares used and unused variables inside GradientTape context.\n",
    "# It prints gradients and explains why some gradients become None.\n",
    "\n",
    "# !pip install tensorflow==2.20.0\n",
    "\n",
    "# Import required TensorFlow library and operating system utilities.\n",
    "import tensorflow as tf\n",
    "import os as os\n",
    "\n",
    "# Set deterministic random seed for reproducible TensorFlow behavior.\n",
    "tf.random.set_seed(123)\n",
    "\n",
    "# Print TensorFlow version information for environment transparency.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Create two trainable scalar variables representing simple model parameters.\n",
    "w_used = tf.Variable(2.0, trainable=True, name=\"w_used\")\n",
    "\n",
    "# Create another variable that will not affect the loss computation.\n",
    "w_unused = tf.Variable(3.0, trainable=True, name=\"w_unused\")\n",
    "\n",
    "# Define a simple scalar input tensor used in the forward computation.\n",
    "x_input = tf.constant(5.0, dtype=tf.float32)\n",
    "\n",
    "# Use GradientTape to record operations that involve the used variable.\n",
    "with tf.GradientTape() as tape:\n",
    "\n",
    "    # Compute prediction using only w_used and ignore w_unused completely.\n",
    "    y_pred = w_used * x_input\n",
    "\n",
    "    # Define a simple squared error loss depending only on y_pred value.\n",
    "    loss = (y_pred - 10.0) ** 2\n",
    "\n",
    "# Request gradients of loss with respect to both variables together.\n",
    "grads = tape.gradient(loss, [w_used, w_unused])\n",
    "\n",
    "# Unpack the gradient list into separate gradient variables for clarity.\n",
    "grad_used, grad_unused = grads\n",
    "\n",
    "# Print gradient for w_used which participates in the loss computation.\n",
    "print(\"Gradient for w_used:\", grad_used.numpy())\n",
    "\n",
    "# Print gradient for w_unused which never influenced the loss value.\n",
    "print(\"Gradient for w_unused:\", grad_unused)\n",
    "\n",
    "# Show a short explanation line highlighting why second gradient is None.\n",
    "print(\"w_unused gradient is None because loss never used that variable.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c612a09",
   "metadata": {},
   "source": [
    "## **3. Manual Gradient Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f507322",
   "metadata": {},
   "source": [
    "### **3.1. Using tf keras optimizers**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537ec76b",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_02/Lecture_B/image_03_01.jpg?v=1768967094\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Keras optimizers turn gradients into parameter updates\n",
    ">* Create optimizer, then apply gradients to variables\n",
    "\n",
    ">* Optimizers offer varied training behaviors without rewrites\n",
    ">* Training loop stays record, loss, gradients, optimizer update\n",
    "\n",
    ">* Swap optimizers and hyperparameters without changing training\n",
    ">* Simple loop lets optimizers refine parameters over time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f523f9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Using tf keras optimizers\n",
    "\n",
    "# This script shows using TensorFlow Keras optimizers manually.\n",
    "# It uses GradientTape to compute gradients for a simple model.\n",
    "# It then applies gradients using an optimizer training step.\n",
    "\n",
    "# !pip install tensorflow==2.20.0\n",
    "\n",
    "# Import required standard libraries and TensorFlow framework.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Print TensorFlow version information for reproducibility reference.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Set deterministic random seeds for reproducible training behavior.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Select computation device preferring GPU when it is available.\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "    device_name = \"/GPU:0\"\n",
    "else:\n",
    "    device_name = \"/CPU:0\"\n",
    "print(\"Using device:\", device_name)\n",
    "\n",
    "# Create simple synthetic Fahrenheit to Celsius training data.\n",
    "fahrenheit_values = np.array([32.0, 50.0, 68.0, 86.0, 104.0], dtype=np.float32)\n",
    "celcius_values = (fahrenheit_values - 32.0) * (5.0 / 9.0)\n",
    "\n",
    "# Convert numpy arrays into TensorFlow constant tensors.\n",
    "x_train = tf.constant(fahrenheit_values.reshape(-1, 1))\n",
    "y_train = tf.constant(celcius_values.reshape(-1, 1))\n",
    "\n",
    "# Define a simple linear model using TensorFlow variables.\n",
    "W = tf.Variable(tf.random.normal(shape=(1, 1), stddev=0.1))\n",
    "b = tf.Variable(tf.zeros(shape=(1,)))\n",
    "\n",
    "# Define a prediction function using current model parameters.\n",
    "def model_predict(x_inputs):\n",
    "    return tf.matmul(x_inputs, W) + b\n",
    "\n",
    "# Define a mean squared error loss function for training.\n",
    "def loss_function(y_true, y_pred):\n",
    "    squared_errors = tf.square(y_true - y_pred)\n",
    "    return tf.reduce_mean(squared_errors)\n",
    "\n",
    "# Create a Keras optimizer instance with chosen learning rate.\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "# Define one manual training step using GradientTape and optimizer.\n",
    "@tf.function\n",
    "def train_step(x_batch, y_batch):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model_predict(x_batch)\n",
    "        loss_value = loss_function(y_batch, predictions)\n",
    "    gradients = tape.gradient(loss_value, [W, b])\n",
    "    optimizer.apply_gradients(zip(gradients, [W, b]))\n",
    "    return loss_value\n",
    "\n",
    "# Run several training epochs to update model parameters.\n",
    "with tf.device(device_name):\n",
    "    for epoch in range(20):\n",
    "        loss_value = train_step(x_train, y_train)\n",
    "\n",
    "# Print learned parameters and example prediction after training.\n",
    "print(\"Learned weight W:\", W.numpy().flatten()[0])\n",
    "print(\"Learned bias b:\", b.numpy().flatten()[0])\n",
    "print(\"True conversion slope:\", 5.0 / 9.0)\n",
    "print(\"True conversion intercept:\", -32.0 * (5.0 / 9.0))\n",
    "\n",
    "# Evaluate model prediction for a sample Fahrenheit temperature.\n",
    "sample_fahrenheit = tf.constant([[77.0]], dtype=tf.float32)\n",
    "predicted_celsius = model_predict(sample_fahrenheit)\n",
    "print(\"Predicted Celsius for 77F:\", float(predicted_celsius.numpy().flatten()[0]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca09d503",
   "metadata": {},
   "source": [
    "### **3.2. Gradient Variable Updates**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475a057a",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_02/Lecture_B/image_03_02.jpg?v=1768967132\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Use gradients to carefully adjust model variables\n",
    ">* Repeat small updates to gradually reduce loss\n",
    "\n",
    ">* Learning rate scales gradients to set update size\n",
    ">* Repeated small updates gradually move model toward optimum\n",
    "\n",
    ">* Extreme or tiny gradients cause unstable learning\n",
    ">* Monitor updates to tune learning rate and strategy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59731ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Gradient Variable Updates\n",
    "\n",
    "# This script shows manual gradient based variable updates clearly.\n",
    "# It uses TensorFlow GradientTape for computing simple scalar gradients.\n",
    "# It prints variable updates using a tiny linear regression example.\n",
    "\n",
    "# !pip install tensorflow==2.20.0\n",
    "\n",
    "# Import required TensorFlow library and supporting modules.\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Print TensorFlow version for environment clarity.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Set deterministic random seeds for reproducible behavior.\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Create simple synthetic Fahrenheit to Celsius style dataset.\n",
    "true_weight = 2.0\n",
    "true_bias = 5.0\n",
    "x_data = np.linspace(-5.0, 5.0, 21).astype(\"float32\")\n",
    "\n",
    "# Generate corresponding target values using linear relationship.\n",
    "y_data = true_weight * x_data + true_bias\n",
    "\n",
    "# Define trainable TensorFlow variables for weight and bias.\n",
    "w = tf.Variable(0.0, dtype=tf.float32)\n",
    "b = tf.Variable(0.0, dtype=tf.float32)\n",
    "\n",
    "# Define simple mean squared error loss function.\n",
    "def compute_loss(predictions, targets):\n",
    "    return tf.reduce_mean(tf.square(predictions - targets))\n",
    "\n",
    "# Choose a small learning rate for stable updates.\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Perform a few manual gradient update steps.\n",
    "for step in range(5):\n",
    "\n",
    "    # Record operations for automatic differentiation.\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = w * x_data + b\n",
    "        loss_value = compute_loss(predictions, y_data)\n",
    "\n",
    "    # Compute gradients of loss with respect to variables.\n",
    "    gradients = tape.gradient(loss_value, [w, b])\n",
    "\n",
    "    # Manually update variables using gradient descent rule.\n",
    "    w.assign_sub(learning_rate * gradients[0])\n",
    "    b.assign_sub(learning_rate * gradients[1])\n",
    "\n",
    "    # Print current step, loss, and variable values.\n",
    "    print(\"Step\", step, \"Loss\", float(loss_value), \"w\", float(w), \"b\", float(b))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec98894",
   "metadata": {},
   "source": [
    "### **3.3. Stabilizing Gradient Behavior**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713fb8f9",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_02/Lecture_B/image_03_03.jpg?v=1768967163\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Gradients can be too small or huge\n",
    ">* We balance update size to keep training stable\n",
    "\n",
    ">* Learning rate controls how strongly parameters update\n",
    ">* Tune or adapt learning rate to stabilize loss\n",
    "\n",
    ">* Use clipping and regularization to control gradients\n",
    ">* Actively manage gradients for stable, reliable training\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916a55ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Stabilizing Gradient Behavior\n",
    "\n",
    "# This script shows unstable versus stable gradient based training behavior clearly.\n",
    "# It uses TensorFlow GradientTape with different learning rates and gradient clipping.\n",
    "# It prints losses to compare exploding updates versus stabilized training behavior.\n",
    "\n",
    "# !pip install tensorflow==2.20.0\n",
    "\n",
    "# Import required modules including TensorFlow and NumPy libraries.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set deterministic seeds for reproducible gradient behavior demonstration.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version information for environment transparency and reproducibility.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Create simple synthetic data for a noisy linear regression style problem.\n",
    "true_w = 3.0\n",
    "true_b = 2.0\n",
    "x_data = np.linspace(-1.0, 1.0, 50).astype(np.float32)\n",
    "noise = np.random.normal(loc=0.0, scale=0.1, size=x_data.shape).astype(np.float32)\n",
    "\n",
    "# Compute target values using true parameters plus Gaussian noise.\n",
    "y_data = true_w * x_data + true_b + noise\n",
    "\n",
    "# Define a simple linear model using TensorFlow variables for parameters.\n",
    "w_unstable = tf.Variable(0.0, dtype=tf.float32)\n",
    "b_unstable = tf.Variable(0.0, dtype=tf.float32)\n",
    "w_stable = tf.Variable(0.0, dtype=tf.float32)\n",
    "b_stable = tf.Variable(0.0, dtype=tf.float32)\n",
    "\n",
    "# Define a prediction function that computes linear outputs from inputs and parameters.\n",
    "def model_prediction(x_values, weight_variable, bias_variable):\n",
    "    return weight_variable * x_values + bias_variable\n",
    "\n",
    "# Define a mean squared error loss function for scalar loss computation.\n",
    "def mse_loss(predictions_tensor, targets_tensor):\n",
    "    return tf.reduce_mean(tf.square(predictions_tensor - targets_tensor))\n",
    "\n",
    "# Create two optimizers with very different learning rates for comparison.\n",
    "optimizer_unstable = tf.keras.optimizers.SGD(learning_rate=1.0)\n",
    "optimizer_stable = tf.keras.optimizers.SGD(learning_rate=0.1)\n",
    "\n",
    "# Define one manual training step without gradient clipping for unstable behavior.\n",
    "@tf.function\n",
    "def train_step_unstable(x_batch_tensor, y_batch_tensor):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions_tensor = model_prediction(x_batch_tensor, w_unstable, b_unstable)\n",
    "        loss_value = mse_loss(predictions_tensor, y_batch_tensor)\n",
    "    gradients_list = tape.gradient(loss_value, [w_unstable, b_unstable])\n",
    "    optimizer_unstable.apply_gradients(zip(gradients_list, [w_unstable, b_unstable]))\n",
    "    return loss_value, gradients_list\n",
    "\n",
    "# Define one manual training step with gradient clipping for stable behavior.\n",
    "@tf.function\n",
    "def train_step_stable(x_batch_tensor, y_batch_tensor):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions_tensor = model_prediction(x_batch_tensor, w_stable, b_stable)\n",
    "        loss_value = mse_loss(predictions_tensor, y_batch_tensor)\n",
    "    gradients_list = tape.gradient(loss_value, [w_stable, b_stable])\n",
    "    clipped_gradients = [tf.clip_by_value(g, -1.0, 1.0) for g in gradients_list]\n",
    "    optimizer_stable.apply_gradients(zip(clipped_gradients, [w_stable, b_stable]))\n",
    "    return loss_value, gradients_list\n",
    "\n",
    "# Convert NumPy arrays into TensorFlow tensors for training steps.\n",
    "x_tensor = tf.convert_to_tensor(x_data)\n",
    "y_tensor = tf.convert_to_tensor(y_data)\n",
    "\n",
    "# Run several training steps and print selected information for both strategies.\n",
    "num_steps = 8\n",
    "for step_index in range(num_steps):\n",
    "    loss_unstable, grads_unstable = train_step_unstable(x_tensor, y_tensor)\n",
    "    loss_stable, grads_stable = train_step_stable(x_tensor, y_tensor)\n",
    "    if step_index in [0, 1, 2, 4, 7]:\n",
    "        print(\"Step\", step_index, \"unstable loss\", float(loss_unstable), \"stable loss\", float(loss_stable))\n",
    "\n",
    "# Print final learned parameters to compare unstable and stable training results.\n",
    "print(\"Unstable parameters:\", float(w_unstable.numpy()), float(b_unstable.numpy()))\n",
    "print(\"Stable parameters:\", float(w_stable.numpy()), float(b_stable.numpy()))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214b9a97",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Autograd with TF**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffa8da4",
   "metadata": {},
   "source": [
    "\n",
    "In this lecture, you learned to:\n",
    "- Use tf.GradientTape to compute gradients of scalar losses with respect to TensorFlow variables. \n",
    "- Interpret gradient values to understand how parameter changes affect a loss function. \n",
    "- Implement a simple manual training step using gradients and an optimizer. \n",
    "\n",
    "In the next Module (Module 3), we will go over 'Keras Model Building'"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
