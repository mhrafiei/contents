{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc7c4297",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Tensors and Ops**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a113b5b",
   "metadata": {},
   "source": [
    ">Last update: 20260120.\n",
    "    \n",
    "By the end of this Lecture, you will be able to:\n",
    "- Create and manipulate TensorFlow tensors with specified shapes and dtypes. \n",
    "- Apply common TensorFlow math and array operations to transform tensors. \n",
    "- Explain TensorFlow broadcasting and shape inference in simple expressions. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd2a03c",
   "metadata": {},
   "source": [
    "## **1. Creating TensorFlow Tensors**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4032ec37",
   "metadata": {},
   "source": [
    "### **1.1. Constants and Variables**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54304136",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_02/Lecture_A/image_01_01.jpg?v=1768963343\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Constants stay fixed; variables change during computation\n",
    ">* Both have shapes, dtypes; differ in mutability\n",
    "\n",
    ">* Plan tensor shapes to match all operations\n",
    ">* Choose dtypes carefully to avoid computation errors\n",
    "\n",
    ">* Constants store fixed facts; variables store learnable parameters\n",
    ">* Clear tensor roles aid debugging, saving, and deployment\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fb5569",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Constants and Variables\n",
    "\n",
    "# This script shows TensorFlow constants and variables with simple numeric examples.\n",
    "# It highlights shapes and dtypes for both constants and variables clearly.\n",
    "# It also demonstrates updating variables while constants remain unchanged.\n",
    "\n",
    "# !pip install tensorflow==2.20.0\n",
    "\n",
    "# Import required TensorFlow module and operating system utilities.\n",
    "import tensorflow as tf\n",
    "import os as operating_system\n",
    "\n",
    "# Print TensorFlow version information for reproducibility and clarity.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Set deterministic random seed for reproducible variable initialization.\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Create a constant tensor representing fixed Fahrenheit temperature values.\n",
    "fahrenheit_constant = tf.constant([32.0, 68.0, 86.0], dtype=tf.float32)\n",
    "\n",
    "# Display constant tensor values, shape information, and data type details.\n",
    "print(\"Constant values:\", fahrenheit_constant.numpy())\n",
    "\n",
    "# Show constant tensor shape and dtype to emphasize tensor properties.\n",
    "print(\"Constant shape:\", fahrenheit_constant.shape, \"dtype:\", fahrenheit_constant.dtype)\n",
    "\n",
    "# Create a variable tensor representing adjustable temperature correction offsets.\n",
    "correction_variable = tf.Variable([0.5, -1.0, 2.0], dtype=tf.float32, name=\"correction_offsets\")\n",
    "\n",
    "# Display variable initial values, shape information, and data type details.\n",
    "print(\"Variable initial:\", correction_variable.numpy())\n",
    "\n",
    "# Show variable tensor shape and dtype to compare with constant tensor properties.\n",
    "print(\"Variable shape:\", correction_variable.shape, \"dtype:\", correction_variable.dtype)\n",
    "\n",
    "# Validate that constant and variable shapes match for elementwise operations.\n",
    "assert fahrenheit_constant.shape == correction_variable.shape\n",
    "\n",
    "# Compute adjusted Fahrenheit values using constant plus variable offsets.\n",
    "adjusted_fahrenheit = fahrenheit_constant + correction_variable\n",
    "\n",
    "# Display adjusted Fahrenheit values after applying variable based corrections.\n",
    "print(\"Adjusted Fahrenheit:\", adjusted_fahrenheit.numpy())\n",
    "\n",
    "# Update variable values in place to simulate learning or calibration changes.\n",
    "correction_variable.assign(correction_variable * 0.5)\n",
    "\n",
    "# Display updated variable values to show successful in place modification.\n",
    "print(\"Variable updated:\", correction_variable.numpy())\n",
    "\n",
    "# Recompute adjusted Fahrenheit values using updated variable corrections.\n",
    "new_adjusted_fahrenheit = fahrenheit_constant + correction_variable\n",
    "\n",
    "# Display new adjusted Fahrenheit values while constant tensor remains unchanged.\n",
    "print(\"New adjusted Fahrenheit:\", new_adjusted_fahrenheit.numpy())\n",
    "\n",
    "# Confirm constant tensor values remain unchanged after variable updates.\n",
    "print(\"Constant unchanged:\", fahrenheit_constant.numpy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13af3353",
   "metadata": {},
   "source": [
    "### **1.2. Random and Zero Tensors**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6187b2fe",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_02/Lecture_A/image_01_02.jpg?v=1768963377\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Random tensors initialize models with varied values\n",
    ">* Zero tensors give neutral placeholders with chosen shapes\n",
    "\n",
    ">* Random tensors enable exploration in models and simulations\n",
    ">* Choose tensor shape and dtype to match problem\n",
    "\n",
    ">* Zero tensors give a neutral, unbiased baseline\n",
    ">* Used for padding, accumulators, and shape-safe storage\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138670b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Random and Zero Tensors\n",
    "\n",
    "# This script shows TensorFlow random tensors and zero tensors together clearly.\n",
    "# It demonstrates shapes, dtypes, and reproducible random values for beginners.\n",
    "# It prints small summaries so outputs stay readable and easy to compare.\n",
    "\n",
    "# !pip install tensorflow==2.20.0\n",
    "\n",
    "# Import required TensorFlow module with standard alias for usage.\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set global random seed for deterministic random tensor generation.\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Print TensorFlow version information for environment transparency purposes.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Create a random normal tensor with specific shape and dtype float32.\n",
    "random_normal_tensor = tf.random.normal(shape=(2, 3), mean=0.0, stddev=1.0, dtype=tf.float32)\n",
    "\n",
    "# Create a random uniform tensor with specific shape and dtype float32.\n",
    "random_uniform_tensor = tf.random.uniform(shape=(2, 3), minval=-1.0, maxval=1.0, dtype=tf.float32)\n",
    "\n",
    "# Create a zero tensor with same shape and dtype as random normal tensor.\n",
    "zero_tensor_same_shape = tf.zeros(shape=random_normal_tensor.shape, dtype=random_normal_tensor.dtype)\n",
    "\n",
    "# Verify shapes and dtypes for all created tensors using TensorFlow attributes.\n",
    "print(\"Random normal shape and dtype:\", random_normal_tensor.shape, random_normal_tensor.dtype)\n",
    "\n",
    "# Print shape and dtype for random uniform tensor to compare with others.\n",
    "print(\"Random uniform shape and dtype:\", random_uniform_tensor.shape, random_uniform_tensor.dtype)\n",
    "\n",
    "# Print shape and dtype for zero tensor confirming correct matching properties.\n",
    "print(\"Zero tensor shape and dtype:\", zero_tensor_same_shape.shape, zero_tensor_same_shape.dtype)\n",
    "\n",
    "# Show small random normal tensor values for understanding distribution visually.\n",
    "print(\"Random normal values sample:\")\n",
    "\n",
    "# Print the actual random normal tensor values with limited size for clarity.\n",
    "print(random_normal_tensor.numpy())\n",
    "\n",
    "# Show small random uniform tensor values for understanding distribution visually.\n",
    "print(\"Random uniform values sample:\")\n",
    "\n",
    "# Print the actual random uniform tensor values with limited size for clarity.\n",
    "print(random_uniform_tensor.numpy())\n",
    "\n",
    "# Demonstrate adding zero tensor does not change random normal tensor values.\n",
    "unchanged_tensor = random_normal_tensor + zero_tensor_same_shape\n",
    "\n",
    "# Confirm equality between original random tensor and result after zero addition.\n",
    "print(\"Random equals random plus zeros:\", tf.reduce_all(random_normal_tensor == unchanged_tensor).numpy())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3c5364",
   "metadata": {},
   "source": [
    "### **1.3. NumPy to Tensors**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7e907f",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_02/Lecture_A/image_01_03.jpg?v=1768963410\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Convert NumPy arrays to tensors seamlessly\n",
    ">* Enables TensorFlow computation, acceleration, and autograd\n",
    "\n",
    ">* Tensor shape usually matches the NumPy array\n",
    ">* Choose dtypes carefully to avoid bugs, precision loss\n",
    "\n",
    ">* Plan when to convert NumPy data to tensors\n",
    ">* Avoid frequent conversions; keep heavy work in one framework\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2cc8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - NumPy to Tensors\n",
    "\n",
    "# This script shows converting NumPy arrays into TensorFlow tensors.\n",
    "# It also demonstrates shape preservation and explicit dtype conversion.\n",
    "# Finally it compares NumPy and tensor operations side by side.\n",
    "\n",
    "# !pip install tensorflow==2.20.0\n",
    "\n",
    "# Import required libraries for NumPy and TensorFlow usage.\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set deterministic random seeds for reproducible random array values.\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Create a simple NumPy array representing daily sales counts.\n",
    "sales_numpy_array = np.array([[10, 12, 9], [8, 15, 11]], dtype=np.int32)\n",
    "\n",
    "# Print the NumPy array shape and dtype for initial inspection.\n",
    "print(\"NumPy shape and dtype:\", sales_numpy_array.shape, sales_numpy_array.dtype)\n",
    "\n",
    "# Convert the NumPy array into a TensorFlow tensor directly.\n",
    "sales_tensor_default = tf.convert_to_tensor(sales_numpy_array)\n",
    "\n",
    "# Print tensor shape and dtype to confirm preservation from NumPy.\n",
    "print(\"Tensor default shape and dtype:\", sales_tensor_default.shape, sales_tensor_default.dtype)\n",
    "\n",
    "# Convert the same NumPy array but request float32 dtype explicitly.\n",
    "sales_tensor_float = tf.convert_to_tensor(sales_numpy_array, dtype=tf.float32)\n",
    "\n",
    "# Print tensor shape and dtype to observe explicit dtype conversion.\n",
    "print(\"Tensor float shape and dtype:\", sales_tensor_float.shape, sales_tensor_float.dtype)\n",
    "\n",
    "# Verify that the underlying numerical values remain equal after conversion.\n",
    "print(\"Values equal after conversion:\", np.allclose(sales_numpy_array, sales_tensor_default.numpy()))\n",
    "\n",
    "# Perform a simple TensorFlow operation using broadcasting with a scalar.\n",
    "scaled_sales_tensor = sales_tensor_float * tf.constant(1.1, dtype=tf.float32)\n",
    "\n",
    "# Print a small summary of original and scaled tensor values.\n",
    "print(\"Original tensor values:\", sales_tensor_float.numpy())\n",
    "print(\"Scaled tensor values:\", scaled_sales_tensor.numpy())\n",
    "\n",
    "# Demonstrate round trip conversion from tensor back to NumPy array.\n",
    "roundtrip_numpy_array = scaled_sales_tensor.numpy()\n",
    "\n",
    "# Print final round trip shape and dtype to confirm consistency.\n",
    "print(\"Roundtrip NumPy shape and dtype:\", roundtrip_numpy_array.shape, roundtrip_numpy_array.dtype)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caabfd8c",
   "metadata": {},
   "source": [
    "## **2. Core Tensor Operations**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90038c55",
   "metadata": {},
   "source": [
    "### **2.1. Elementwise Tensor Operations**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fd4f85",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_02/Lecture_A/image_02_01.jpg?v=1768963450\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Same operation applied independently to every element\n",
    ">* Supports arithmetic and math functions without mixing positions\n",
    "\n",
    ">* Combine simple elementwise steps to build transformations\n",
    ">* Chained operations keep tensor shape while changing values\n",
    "\n",
    ">* Elementwise ops power activations and loss calculations\n",
    ">* They enable parallel, scalable transformations across tensors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6d5ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Elementwise Tensor Operations\n",
    "\n",
    "# This script demonstrates basic TensorFlow elementwise tensor operations clearly and simply.\n",
    "# It shows how adding and multiplying tensors transform every element independently.\n",
    "# It also compares manual Python loops with fast vectorized TensorFlow operations.\n",
    "\n",
    "# !pip install tensorflow==2.20.0\n",
    "\n",
    "# Import TensorFlow and NumPy for numerical tensor operations.\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Print TensorFlow version information for reproducibility and clarity.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Set deterministic random seeds for NumPy and TensorFlow reproducibility.\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "# Create a simple 2D tensor representing daily temperatures in Fahrenheit.\n",
    "fahrenheit_values = tf.constant([[70.0, 75.0, 80.0], [65.0, 60.0, 55.0]])\n",
    "\n",
    "# Print the original Fahrenheit tensor values for reference and understanding.\n",
    "print(\"Original Fahrenheit tensor:\")\n",
    "print(fahrenheit_values.numpy())\n",
    "\n",
    "# Create a tensor representing a constant temperature increase for every reading.\n",
    "increase_tensor = tf.constant([[5.0, 5.0, 5.0], [5.0, 5.0, 5.0]])\n",
    "\n",
    "# Apply elementwise addition to increase every temperature reading by five degrees.\n",
    "warmer_fahrenheit = fahrenheit_values + increase_tensor\n",
    "\n",
    "# Print the warmer Fahrenheit tensor after elementwise addition operation.\n",
    "print(\"Warmer Fahrenheit tensor:\")\n",
    "print(warmer_fahrenheit.numpy())\n",
    "\n",
    "# Convert Fahrenheit to Celsius using elementwise subtraction and multiplication.\n",
    "celcius_values = (fahrenheit_values - 32.0) * (5.0 / 9.0)\n",
    "\n",
    "# Print the Celsius tensor values to show elementwise transformation results.\n",
    "print(\"Converted Celsius tensor:\")\n",
    "print((tf.round(celcius_values * 100) / 100.0).numpy())\n",
    "\n",
    "# Apply an elementwise nonlinear operation using TensorFlow relu activation function.\n",
    "shifted_temperatures = fahrenheit_values - 72.0\n",
    "\n",
    "# Use relu to zero out temperatures below seventy two degrees Fahrenheit threshold.\n",
    "relu_temperatures = tf.nn.relu(shifted_temperatures)\n",
    "\n",
    "# Print the relu transformed tensor to observe elementwise thresholding behavior.\n",
    "print(\"ReLU transformed tensor:\")\n",
    "print(relu_temperatures.numpy())\n",
    "\n",
    "# Demonstrate manual Python loop equivalent for elementwise Fahrenheit increase operation.\n",
    "manual_warmer_list = []\n",
    "for row in fahrenheit_values.numpy():\n",
    "    manual_row = [value + 5.0 for value in row]\n",
    "    manual_warmer_list.append(manual_row)\n",
    "\n",
    "# Convert manual list result back into a TensorFlow tensor for comparison.\n",
    "manual_warmer_tensor = tf.constant(manual_warmer_list, dtype=tf.float32)\n",
    "\n",
    "# Verify equality between TensorFlow vectorized result and manual loop result.\n",
    "comparison_result = tf.reduce_all(tf.equal(warmer_fahrenheit, manual_warmer_tensor))\n",
    "\n",
    "# Print comparison result showing both methods produce identical elementwise outputs.\n",
    "print(\"Vectorized and manual results match:\", bool(comparison_result.numpy()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa2fff5",
   "metadata": {},
   "source": [
    "### **2.2. Matrix Multiplication Basics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50037c6",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_02/Lecture_A/image_02_02.jpg?v=1768963515\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Matrix multiplication combines tensor rows and columns\n",
    ">* Transforms data representations in many TensorFlow models\n",
    "\n",
    ">* Scores times weights give scholarship-specific combinations\n",
    ">* Inner dimensions must match; outer dimensions shape output\n",
    "\n",
    ">* TensorFlow multiplies whole batches of matrices efficiently\n",
    ">* Batched matmuls power many core deep learning transformations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a823e680",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Matrix Multiplication Basics\n",
    "\n",
    "# This script demonstrates basic TensorFlow matrix multiplication operations clearly.\n",
    "# It shows shapes for student scores and scholarship weight matrices.\n",
    "# It computes weighted scores and batched matrix multiplication examples.\n",
    "\n",
    "# !pip install tensorflow==2.20.0\n",
    "\n",
    "# Import TensorFlow and NumPy libraries for tensor operations.\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Set deterministic random seeds for reproducible tensor values.\n",
    "np.random.seed(7)\n",
    "tf.random.set_seed(7)\n",
    "\n",
    "# Print TensorFlow version information for environment confirmation.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Create a scores matrix representing students and subject test scores.\n",
    "scores_matrix = tf.constant([[80.0, 90.0, 70.0], [88.0, 76.0, 92.0]], dtype=tf.float32)\n",
    "\n",
    "# Create a weights matrix representing scholarship criteria importance values.\n",
    "weights_matrix = tf.constant([[0.5, 0.2], [0.3, 0.4], [0.2, 0.4]], dtype=tf.float32)\n",
    "\n",
    "# Print shapes to verify inner dimensions match for multiplication.\n",
    "print(\"Scores shape:\", scores_matrix.shape)\n",
    "print(\"Weights shape:\", weights_matrix.shape)\n",
    "\n",
    "# Perform matrix multiplication to compute weighted scholarship scores.\n",
    "weighted_scores_matrix = tf.matmul(scores_matrix, weights_matrix)\n",
    "\n",
    "# Print resulting matrix and shape to understand transformation effect.\n",
    "print(\"Weighted scores matrix:\")\n",
    "print(weighted_scores_matrix.numpy())\n",
    "print(\"Result shape:\", weighted_scores_matrix.shape)\n",
    "\n",
    "# Create a batched scores tensor representing multiple groups of students.\n",
    "batched_scores_tensor = tf.stack([scores_matrix, scores_matrix], axis=0)\n",
    "\n",
    "# Create a batched weights tensor applying same weights for each batch.\n",
    "batched_weights_tensor = tf.stack([weights_matrix, weights_matrix], axis=0)\n",
    "\n",
    "# Print batched shapes to confirm compatibility for batched multiplication.\n",
    "print(\"Batched scores shape:\", batched_scores_tensor.shape)\n",
    "print(\"Batched weights shape:\", batched_weights_tensor.shape)\n",
    "\n",
    "# Perform batched matrix multiplication across leading batch dimension.\n",
    "batched_result_tensor = tf.matmul(batched_scores_tensor, batched_weights_tensor)\n",
    "\n",
    "# Print final batched result shape to show transformation across batches.\n",
    "print(\"Batched result shape:\", batched_result_tensor.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a590ff",
   "metadata": {},
   "source": [
    "### **2.3. Tensor Reduction Operations**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb26fbe",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_02/Lecture_A/image_02_03.jpg?v=1768963550\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Reductions summarize many tensor values into fewer\n",
    ">* They compress high‑dimensional data into useful summaries\n",
    "\n",
    ">* Choosing reduction axes changes the output shape\n",
    ">* Different axes answer different questions from data\n",
    "\n",
    ">* Reductions find extremes, counts, and statistical measures\n",
    ">* They turn complex tensors into useful numeric summaries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f37e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Tensor Reduction Operations\n",
    "\n",
    "# Demonstrate basic TensorFlow tensor reduction operations clearly.\n",
    "# Show how axis choices change reduced tensor shapes.\n",
    "# Summarize example batch data using several reduction operations.\n",
    "\n",
    "# !pip install tensorflow==2.20.0\n",
    "\n",
    "# Import TensorFlow library and numpy helper library.\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Print TensorFlow version information for reproducibility.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Set deterministic random seeds for reproducible tensor values.\n",
    "np.random.seed(7)\n",
    "tf.random.set_seed(7)\n",
    "\n",
    "# Create example batch data tensor with shape batch, height, width.\n",
    "data_array = np.random.randint(0, 256, size=(2, 3, 4))\n",
    "\n",
    "tensor_data = tf.constant(data_array, dtype=tf.float32)\n",
    "\n",
    "# Print original tensor shape and a small preview summary.\n",
    "print(\"Original shape:\", tensor_data.shape)\n",
    "print(\"First batch slice:\", tensor_data[0])\n",
    "\n",
    "# Compute global mean reduction over all tensor axes.\n",
    "global_mean = tf.reduce_mean(tensor_data)\n",
    "\n",
    "# Print global mean scalar value summarizing entire tensor.\n",
    "print(\"Global mean value:\", float(global_mean.numpy()))\n",
    "\n",
    "# Compute per image mean brightness using batch axis only.\n",
    "per_image_mean = tf.reduce_mean(tensor_data, axis=[1, 2])\n",
    "\n",
    "# Print per image mean values and resulting shape.\n",
    "print(\"Per image mean shape:\", per_image_mean.shape)\n",
    "print(\"Per image mean values:\", per_image_mean.numpy())\n",
    "\n",
    "# Compute per pixel maximum across batch axis only.\n",
    "per_pixel_max = tf.reduce_max(tensor_data, axis=0)\n",
    "\n",
    "# Print per pixel maximum shape and a small preview slice.\n",
    "print(\"Per pixel max shape:\", per_pixel_max.shape)\n",
    "print(\"Per pixel max first row:\", per_pixel_max[0].numpy())\n",
    "\n",
    "# Compute count of elements above threshold using boolean reduction.\n",
    "threshold_value = 200.0\n",
    "\n",
    "mask_high = tensor_data > threshold_value\n",
    "\n",
    "count_high = tf.reduce_sum(tf.cast(mask_high, tf.int32))\n",
    "\n",
    "print(\"Count values above threshold:\", int(count_high.numpy()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95894115",
   "metadata": {},
   "source": [
    "## **3. Tensor Shapes and Broadcasting**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca76062c",
   "metadata": {},
   "source": [
    "### **3.1. Static and Dynamic Shapes**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782aadf2",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_02/Lecture_A/image_03_01.jpg?v=1768963589\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Static shapes are known structure checked before runtime\n",
    ">* Dynamic shapes depend on runtime data, remain flexible\n",
    "\n",
    ">* Static shapes propagate known dimensions through ops\n",
    ">* Dynamic shapes fill missing details at runtime\n",
    "\n",
    ">* Static shapes catch impossible operations early\n",
    ">* Dynamic shapes decide broadcasting and compatibility at runtime\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb44d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Static and Dynamic Shapes\n",
    "\n",
    "# This script demonstrates static and dynamic tensor shapes clearly using TensorFlow operations.\n",
    "# It shows how TensorFlow infers shapes before and during actual tensor computations.\n",
    "# It prints shapes and values to connect static information with dynamic runtime behavior.\n",
    "\n",
    "# !pip install tensorflow==2.20.0\n",
    "\n",
    "# Import required TensorFlow module and operating system utilities.\n",
    "import tensorflow as tf\n",
    "import os as os\n",
    "\n",
    "# Set deterministic random seed for reproducible tensor values.\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Print TensorFlow version information for environment clarity and reproducibility.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Create a constant tensor with fully known static shape information.\n",
    "features_tensor = tf.constant([[1.0, 2.0], [3.0, 4.0]])\n",
    "\n",
    "# Show static shape known from graph construction before any computation happens.\n",
    "print(\"Static shape features:\", features_tensor.shape)\n",
    "\n",
    "# Show dynamic shape obtained at runtime using TensorFlow shape operation.\n",
    "print(\"Dynamic shape features:\", tf.shape(features_tensor).numpy())\n",
    "\n",
    "# Create a placeholder like input using tf.function with partially unknown shape.\n",
    "@tf.function\n",
    "def process_batch(batch_tensor):\n",
    "\n",
    "    # Inside function, inspect static shape information available to TensorFlow.\n",
    "    static_shape = batch_tensor.shape\n",
    "\n",
    "    # Inside function, inspect dynamic shape using runtime TensorFlow operation.\n",
    "    dynamic_shape = tf.shape(batch_tensor)\n",
    "\n",
    "    # Return both shapes and a simple doubled tensor for clarity.\n",
    "    return static_shape, dynamic_shape, batch_tensor * 2.0\n",
    "\n",
    "# Create first batch tensor with three examples and two features each.\n",
    "batch_one = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "\n",
    "# Call function with first batch and capture returned shapes and values.\n",
    "static_one, dynamic_one, doubled_one = process_batch(batch_one)\n",
    "\n",
    "# Print static and dynamic shapes for first batch to compare behavior.\n",
    "print(\"Batch one static shape:\", static_one)\n",
    "\n",
    "# Print dynamic shape for first batch using numpy conversion for readability.\n",
    "print(\"Batch one dynamic shape:\", dynamic_one.numpy())\n",
    "\n",
    "# Create second batch tensor with different batch dimension but same feature dimension.\n",
    "batch_two = tf.constant([[10.0, 20.0], [30.0, 40.0]])\n",
    "\n",
    "# Call function with second batch and capture returned shapes and values.\n",
    "static_two, dynamic_two, doubled_two = process_batch(batch_two)\n",
    "\n",
    "# Print static and dynamic shapes for second batch to highlight dynamic flexibility.\n",
    "print(\"Batch two static shape:\", static_two)\n",
    "\n",
    "# Print dynamic shape for second batch using numpy conversion for readability.\n",
    "print(\"Batch two dynamic shape:\", dynamic_two.numpy())\n",
    "\n",
    "# Finally print doubled batch values to confirm computations still work correctly.\n",
    "print(\"Doubled batch two values:\", doubled_two.numpy())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50ed1ef",
   "metadata": {},
   "source": [
    "### **3.2. Broadcasting Rules**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d403af5",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_02/Lecture_A/image_03_02.jpg?v=1768963619\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Broadcasting virtually expands smaller tensors to match\n",
    ">* Lets one value efficiently apply across larger data\n",
    "\n",
    ">* Align shapes from right; check each dimension\n",
    ">* Ones or missing dims stretch; otherwise error\n",
    "\n",
    ">* Broadcasting applies shared values across larger tensors\n",
    ">* Check shapes right-to-left to prevent mismatch errors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbfd952",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Broadcasting Rules\n",
    "\n",
    "# This script demonstrates TensorFlow broadcasting rules with simple temperature examples.\n",
    "# It shows how smaller tensors expand to match larger tensor shapes automatically.\n",
    "# It also prints resulting shapes to explain TensorFlow broadcasting behavior clearly.\n",
    "\n",
    "# !pip install tensorflow==2.20.0\n",
    "\n",
    "# Import TensorFlow library and NumPy helper utilities.\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Set deterministic random seeds for reproducible tensor values.\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Print TensorFlow version information for environment confirmation.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Create base Fahrenheit temperatures tensor for two cities and three days.\n",
    "base_temps = tf.constant([[70.0, 72.0, 68.0], [65.0, 67.0, 66.0]])\n",
    "\n",
    "# Create single daily offset tensor representing uniform heat wave effect.\n",
    "offset_scalar = tf.constant(5.0)\n",
    "\n",
    "# Add scalar offset to base temperatures using broadcasting behavior.\n",
    "result_scalar = base_temps + offset_scalar\n",
    "\n",
    "# Print shapes and small sample values for scalar broadcasting demonstration.\n",
    "print(\"Base temps shape:\", base_temps.shape)\n",
    "print(\"Offset scalar shape:\", offset_scalar.shape)\n",
    "print(\"Result scalar shape:\", result_scalar.shape)\n",
    "\n",
    "# Create per day offset tensor representing different daily adjustments.\n",
    "offset_per_day = tf.constant([2.0, -1.0, 3.0])\n",
    "\n",
    "# Add per day offsets to base temperatures using right aligned broadcasting.\n",
    "result_per_day = base_temps + offset_per_day\n",
    "\n",
    "# Print shapes and example values for per day broadcasting demonstration.\n",
    "print(\"Offset per day shape:\", offset_per_day.shape)\n",
    "print(\"Result per day shape:\", result_per_day.shape)\n",
    "\n",
    "# Create per city offset tensor using column vector shaped for broadcasting.\n",
    "offset_per_city = tf.constant([[10.0], [20.0]])\n",
    "\n",
    "# Add per city offsets to base temperatures using left side broadcasting.\n",
    "result_per_city = base_temps + offset_per_city\n",
    "\n",
    "# Print shapes and example values for per city broadcasting demonstration.\n",
    "print(\"Offset per city shape:\", offset_per_city.shape)\n",
    "print(\"Result per city shape:\", result_per_city.shape)\n",
    "\n",
    "# Demonstrate incompatible shapes causing broadcasting error with try except handling.\n",
    "try:\n",
    "    bad_offset = tf.constant([[1.0, 2.0]])\n",
    "    bad_result = base_temps + bad_offset\n",
    "except Exception as error:\n",
    "    print(\"Broadcasting error type:\", type(error).__name__)\n",
    "\n",
    "# Confirm final tensor values remain finite and correctly shaped after operations.\n",
    "print(\"Final result scalar sample:\", result_scalar[0, 0].numpy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bcc469",
   "metadata": {},
   "source": [
    "### **3.3. Reshaping Tensors Safely**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeff18c0",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_02/Lecture_A/image_03_03.jpg?v=1768963657\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Reshaping changes how TensorFlow views existing data\n",
    ">* Total element count must stay the same\n",
    "\n",
    ">* Keep each tensor dimension’s real meaning intact\n",
    ">* Check dimensions before and after reshape match intent\n",
    "\n",
    ">* Plan reshapes so dimensions broadcast correctly\n",
    ">* Check shapes before and after to avoid bugs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb7b56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Reshaping Tensors Safely\n",
    "\n",
    "# This script demonstrates safe tensor reshaping with TensorFlow examples.\n",
    "# It shows how element counts must match during reshape operations.\n",
    "# It also shows how reshaping affects broadcasting and later computations.\n",
    "\n",
    "# !pip install tensorflow==2.20.0\n",
    "\n",
    "# Import TensorFlow and NumPy with clear deterministic behavior.\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Print TensorFlow version for reproducibility and environment clarity.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Set deterministic random seeds for TensorFlow and NumPy reproducibility.\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create a simple one dimensional tensor representing hourly temperatures.\n",
    "hourly_temps = tf.constant([70.0, 71.5, 69.0, 68.5, 72.0, 73.0], dtype=tf.float32)\n",
    "\n",
    "# Print original tensor values and shape for clear understanding.\n",
    "print(\"Original temps:\", hourly_temps.numpy(), \"shape:\", hourly_temps.shape)\n",
    "\n",
    "# Safely reshape into days by hours while preserving element count.\n",
    "days_by_hours = tf.reshape(hourly_temps, shape=(2, 3))\n",
    "\n",
    "# Print reshaped tensor and shape to show reinterpretation only.\n",
    "print(\"Days by hours:\", days_by_hours.numpy(), \"shape:\", days_by_hours.shape)\n",
    "\n",
    "# Verify element counts before and after reshape are exactly identical.\n",
    "print(\"Element counts equal:\", hourly_temps.shape.num_elements() == days_by_hours.shape.num_elements())\n",
    "\n",
    "# Demonstrate unsafe reshape attempt with mismatched element count.\n",
    "try:\n",
    "    bad_shape = tf.reshape(hourly_temps, shape=(4, 2))\n",
    "except Exception as error:\n",
    "    print(\"Bad reshape error:\", type(error).__name__)\n",
    "\n",
    "# Create a tensor representing Fahrenheit adjustment per hour dimension.\n",
    "hour_adjust = tf.constant([[1.0, 0.0, -1.0]], dtype=tf.float32)\n",
    "\n",
    "# Show broadcasting working correctly with matching trailing dimensions.\n",
    "correct_adjusted = days_by_hours + hour_adjust\n",
    "\n",
    "# Print correctly adjusted tensor and shape after broadcasting.\n",
    "print(\"Correct adjusted:\", correct_adjusted.numpy(), \"shape:\", correct_adjusted.shape)\n",
    "\n",
    "# Reshape incorrectly so hours and days semantics become mixed.\n",
    "wrong_view = tf.reshape(days_by_hours, shape=(3, 2))\n",
    "\n",
    "# Show shape that will not broadcast correctly with hour adjustment tensor.\n",
    "print(\"Wrong view shape:\", wrong_view.shape, \"hour_adjust shape:\", hour_adjust.shape)\n",
    "\n",
    "# Attempt broadcasting with wrong semantics and catch resulting error.\n",
    "try:\n",
    "    wrong_adjusted = wrong_view + hour_adjust\n",
    "except Exception as error:\n",
    "    print(\"Broadcasting error:\", type(error).__name__)\n",
    "\n",
    "# Final print confirming script finished without unexpected failures.\n",
    "print(\"Script finished successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df50025",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Tensors and Ops**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f187275",
   "metadata": {},
   "source": [
    "\n",
    "In this lecture, you learned to:\n",
    "- Create and manipulate TensorFlow tensors with specified shapes and dtypes. \n",
    "- Apply common TensorFlow math and array operations to transform tensors. \n",
    "- Explain TensorFlow broadcasting and shape inference in simple expressions. \n",
    "\n",
    "In the next Lecture (Lecture B), we will go over 'Autograd with TF'"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
