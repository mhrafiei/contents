{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26d9a196",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Tensors and Ops**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7842506d",
   "metadata": {},
   "source": [
    ">Last update: 20260125.\n",
    "    \n",
    "By the end of this Lecture, you will be able to:\n",
    "- Create and manipulate TensorFlow tensors with specified shapes and dtypes. \n",
    "- Apply common TensorFlow math and array operations to transform tensors. \n",
    "- Explain TensorFlow broadcasting and shape inference in simple expressions. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c5bc39",
   "metadata": {},
   "source": [
    "## **1. Creating Tensors**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929ad3df",
   "metadata": {},
   "source": [
    "### **1.1. Constants and Variables**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa384fa",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_02/Lecture_A/image_01_01.jpg?v=1769366658\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Constant tensors hold fixed values during computation\n",
    ">* Variable tensors store updatable values for learning\n",
    "\n",
    ">* Define constants with values, shape, and dtype\n",
    ">* Used in computations while their values stay fixed\n",
    "\n",
    ">* Variable tensors change values but keep shape\n",
    ">* They start from initial values and update during training\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254504d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Constants and Variables\n",
    "\n",
    "# This script introduces TensorFlow constants and variables.\n",
    "# It focuses on shapes and data types for tensors.\n",
    "# Run each section and observe the printed outputs.\n",
    "\n",
    "# Install TensorFlow if not already available in the environment.\n",
    "# !pip install tensorflow==2.20.0 --quiet.\n",
    "\n",
    "# Import TensorFlow with a short alias.\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "import tensorflow as tf\n",
    "\n",
    "# Print TensorFlow version in one concise line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Create a scalar constant with explicit float32 dtype.\n",
    "scalar_const = tf.constant(3.5, dtype=tf.float32)\n",
    "\n",
    "# Create a 1D constant tensor from a Python list.\n",
    "vector_const = tf.constant([1, 2, 3], dtype=tf.int32)\n",
    "\n",
    "# Create a 2D constant tensor representing a small matrix.\n",
    "matrix_const = tf.constant([[1., 2.], [3., 4.]], dtype=tf.float32)\n",
    "\n",
    "# Print basic information about the scalar constant.\n",
    "print(\"Scalar constant:\", scalar_const.numpy(), scalar_const.shape)\n",
    "\n",
    "# Print basic information about the vector constant.\n",
    "print(\"Vector constant:\", vector_const.numpy(), vector_const.shape)\n",
    "\n",
    "# Print basic information about the matrix constant.\n",
    "print(\"Matrix constant:\\n\", matrix_const.numpy())\n",
    "\n",
    "# Demonstrate that constants cannot be reassigned in place.\n",
    "new_matrix_const = matrix_const + tf.constant(1.0, dtype=tf.float32)\n",
    "\n",
    "# Show original and new constant values for comparison.\n",
    "print(\"Original matrix constant:\\n\", matrix_const.numpy())\n",
    "\n",
    "# Show the result of adding one to the matrix constant.\n",
    "print(\"New matrix constant plus one:\\n\", new_matrix_const.numpy())\n",
    "\n",
    "# Create a variable tensor from an initial constant value.\n",
    "var_weights = tf.Variable([[0.5, -0.5], [1.0, -1.0]], dtype=tf.float32)\n",
    "\n",
    "# Print the initial variable value and its shape.\n",
    "print(\"Initial variable weights:\\n\", var_weights.numpy())\n",
    "\n",
    "# Define a small update tensor with the same shape as the variable.\n",
    "update_step = tf.constant([[0.1, 0.1], [0.1, 0.1]], dtype=tf.float32)\n",
    "\n",
    "# Validate that shapes match before applying the update.\n",
    "if var_weights.shape == update_step.shape:\n",
    "    # Apply an in place style update using assign_add.\n",
    "    var_weights.assign_add(update_step)\n",
    "\n",
    "# Print the updated variable values after the assign_add.\n",
    "print(\"Updated variable weights:\\n\", var_weights.numpy())\n",
    "\n",
    "# Create another variable to show dtype must stay consistent.\n",
    "var_bias = tf.Variable([0.0, 0.0], dtype=tf.float32)\n",
    "\n",
    "# Safely update the bias variable with a matching dtype tensor.\n",
    "var_bias.assign(var_bias + tf.constant([0.2, -0.1], dtype=tf.float32))\n",
    "\n",
    "# Print final bias values to confirm the successful update.\n",
    "print(\"Updated bias variable:\", var_bias.numpy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f462efdc",
   "metadata": {},
   "source": [
    "### **1.2. Random and Zero Tensors**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fc8806",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_02/Lecture_A/image_01_02.jpg?v=1769366730\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Use random or zero tensors for initialization\n",
    ">* Choose tensor shape and dtype for compatibility\n",
    "\n",
    ">* Choose random distributions to match modeling needs\n",
    ">* Set shape and dtype; framework fills values\n",
    "\n",
    ">* Zero or constant tensors give neutral placeholders\n",
    ">* Match shape and dtype to avoid errors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8322f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Random and Zero Tensors\n",
    "\n",
    "# This script explores random and zero tensors.\n",
    "# It uses TensorFlow to create simple tensors.\n",
    "# Focus on shapes dtypes and deterministic randomness.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import TensorFlow with a clear alias.\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "import tensorflow as tf\n",
    "\n",
    "# Print TensorFlow version briefly.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Set global random seed for reproducibility.\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Create a small random tensor with normal distribution.\n",
    "random_normal = tf.random.normal(\n",
    "    shape=(2, 3), mean=0.0, stddev=1.0, dtype=tf.float32\n",
    ")\n",
    "\n",
    "# Create a small random tensor with uniform distribution.\n",
    "random_uniform = tf.random.uniform(\n",
    "    shape=(2, 3), minval=0.0, maxval=1.0, dtype=tf.float32\n",
    ")\n",
    "\n",
    "# Create a zero tensor matching a specific shape.\n",
    "zeros_explicit = tf.zeros(shape=(2, 3), dtype=tf.float32)\n",
    "\n",
    "# Create a zero tensor that matches another tensor shape.\n",
    "zeros_like_random = tf.zeros_like(random_normal, dtype=tf.float32)\n",
    "\n",
    "# Show shapes and dtypes for all created tensors.\n",
    "print(\"random_normal shape and dtype:\", random_normal.shape, random_normal.dtype)\n",
    "print(\"random_uniform shape and dtype:\", random_uniform.shape, random_uniform.dtype)\n",
    "print(\"zeros_explicit shape and dtype:\", zeros_explicit.shape, zeros_explicit.dtype)\n",
    "print(\"zeros_like_random shape and dtype:\", zeros_like_random.shape, zeros_like_random.dtype)\n",
    "\n",
    "# Safely check that shapes match before an addition.\n",
    "if random_normal.shape == zeros_like_random.shape:\n",
    "    added_tensor = random_normal + zeros_like_random\n",
    "else:\n",
    "    added_tensor = None\n",
    "\n",
    "# Print a small sample of tensor values.\n",
    "print(\"random_normal values:\\n\", random_normal.numpy())\n",
    "print(\"zeros_like_random values:\\n\", zeros_like_random.numpy())\n",
    "\n",
    "# Confirm that adding zeros keeps values unchanged.\n",
    "print(\"added_tensor equals random_normal:\", tf.reduce_all(\n",
    "    tf.equal(added_tensor, random_normal)\n",
    ").numpy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bf6adb",
   "metadata": {},
   "source": [
    "### **1.3. From NumPy to Tensors**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468d4cda",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_02/Lecture_A/image_01_03.jpg?v=1769366795\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Array libraries handle rich, real-world data structures\n",
    ">* Converting arrays to tensors enables optimized ML computation\n",
    "\n",
    ">* Tensor keeps array values, shape, and structure\n",
    ">* Choose tensor dtypes carefully to avoid subtle bugs\n",
    "\n",
    ">* Converting arrays to tensors connects raw data\n",
    ">* Tensors enable efficient computation while preserving structure\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b4b4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - From NumPy to Tensors\n",
    "\n",
    "# This script shows NumPy to TensorFlow tensor conversion.\n",
    "# It focuses on shapes and data types for tensors.\n",
    "# Run each part and observe the printed outputs.\n",
    "\n",
    "# !pip install tensorflow.\n",
    "# !pip install numpy.\n",
    "\n",
    "# Import required standard libraries safely.\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "\n",
    "# Try importing numpy and handle missing installation.\n",
    "try:\n",
    "    import numpy as np\n",
    "except ImportError as exc:\n",
    "    raise SystemExit(\"NumPy is required for this script.\") from exc\n",
    "\n",
    "# Try importing tensorflow and handle missing installation.\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "except ImportError as exc:\n",
    "    raise SystemExit(\"TensorFlow is required for this script.\") from exc\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Create a small NumPy array with float values.\n",
    "numpy_array = np.array([[1.0, 2.0], [3.0, 4.0]], dtype=np.float32)\n",
    "\n",
    "# Show basic information about the NumPy array.\n",
    "print(\"NumPy array shape:\", numpy_array.shape)\n",
    "print(\"NumPy array dtype:\", numpy_array.dtype)\n",
    "\n",
    "# Convert NumPy array to a TensorFlow tensor.\n",
    "tensor_default = tf.convert_to_tensor(numpy_array)\n",
    "\n",
    "# Show tensor shape and dtype after conversion.\n",
    "print(\"Tensor shape (default):\", tensor_default.shape)\n",
    "print(\"Tensor dtype (default):\", tensor_default.dtype)\n",
    "\n",
    "# Convert NumPy array to tensor with explicit float64 dtype.\n",
    "tensor_float64 = tf.convert_to_tensor(numpy_array, dtype=tf.float64)\n",
    "\n",
    "# Print dtype change while shape stays the same.\n",
    "print(\"Tensor shape (float64):\", tensor_float64.shape)\n",
    "print(\"Tensor dtype (float64):\", tensor_float64.dtype)\n",
    "\n",
    "# Create integer NumPy array to show integer tensor conversion.\n",
    "int_numpy_array = np.array([[0, 1, 2], [3, 4, 5]], dtype=np.int32)\n",
    "\n",
    "# Convert integer NumPy array to integer tensor explicitly.\n",
    "int_tensor = tf.convert_to_tensor(int_numpy_array, dtype=tf.int32)\n",
    "\n",
    "# Print integer tensor properties in a compact way.\n",
    "print(\"Int tensor shape:\", int_tensor.shape)\n",
    "print(\"Int tensor dtype:\", int_tensor.dtype)\n",
    "\n",
    "# Validate that tensor and NumPy shapes are identical.\n",
    "if tensor_default.shape != numpy_array.shape:\n",
    "    raise ValueError(\"Tensor and NumPy shapes do not match.\")\n",
    "\n",
    "# Show that small tensor values match original NumPy values.\n",
    "print(\"First row NumPy:\", numpy_array[0])\n",
    "print(\"First row tensor:\", tensor_default.numpy()[0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f47f227",
   "metadata": {},
   "source": [
    "## **2. Core Tensor Operations**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb2c298",
   "metadata": {},
   "source": [
    "### **2.1. Elementwise Tensor Operations**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6e4578",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_02/Lecture_A/image_02_01.jpg?v=1769366840\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Same operation applied independently to all elements\n",
    ">* Efficient, spreadsheet-like transforms crucial for computation\n",
    "\n",
    ">* Elementwise ops apply many math functions per entry\n",
    ">* They reshape data scale while preserving structure\n",
    "\n",
    ">* Combine data with parameters using elementwise transforms\n",
    ">* Local per-element updates build complex global behavior\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64cb620",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Elementwise Tensor Operations\n",
    "\n",
    "# This script demonstrates elementwise tensor operations.\n",
    "# It uses only built in Python and lists.\n",
    "# Focus is on shapes broadcasting and transformations.\n",
    "\n",
    "# Create two small one dimensional lists as tensors.\n",
    "a = [1.0, 2.0, 3.0]\n",
    "# Create another list with same length for pairing.\n",
    "b = [10.0, 20.0, 30.0]\n",
    "# Show the original lists for reference.\n",
    "print(\"a:\", a)\n",
    "\n",
    "# Show the second list for comparison.\n",
    "print(\"b:\", b)\n",
    "# Perform elementwise addition using list comprehension.\n",
    "add_result = [x + y for x, y in zip(a, b)]\n",
    "# Perform elementwise multiplication similarly.\n",
    "mul_result = [x * y for x, y in zip(a, b)]\n",
    "# Print the elementwise addition result.\n",
    "print(\"a + b:\", add_result)\n",
    "\n",
    "# Print the elementwise multiplication result.\n",
    "print(\"a * b:\", mul_result)\n",
    "# Define a scalar to apply elementwise to list.\n",
    "scalar = 0.5\n",
    "# Scale each element of list a by scalar.\n",
    "scaled_a = [scalar * x for x in a]\n",
    "# Print the scaled list to show transformation.\n",
    "print(\"0.5 * a:\", scaled_a)\n",
    "\n",
    "# Apply an elementwise nonlinear transformation.\n",
    "squared_a = [x ** 2 for x in a]\n",
    "# Print squared values to show nonlinear effect.\n",
    "print(\"a squared:\", squared_a)\n",
    "# Clip values elementwise between given bounds.\n",
    "clipped_a = [min(max(x, 1.5), 2.5) for x in a]\n",
    "# Print clipped values to show independent changes.\n",
    "print(\"clipped a:\", clipped_a)\n",
    "\n",
    "# Demonstrate simple broadcasting with different length lists.\n",
    "short = [1.0, 2.0]\n",
    "# Validate compatible lengths for manual broadcasting.\n",
    "if len(a) % len(short) != 0:\n",
    "    pass\n",
    "# Repeat short list pattern to match length of a.\n",
    "short_tiled = (short * (len(a) // len(short)))[: len(a)]\n",
    "\n",
    "# Perform elementwise multiplication with tiled pattern.\n",
    "broadcast_mul = [x * y for x, y in zip(a, short_tiled)]\n",
    "# Print pattern and result to explain broadcasting.\n",
    "print(\"short tiled:\", short_tiled)\n",
    "print(\"a * short_tiled:\", broadcast_mul)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0286bc5",
   "metadata": {},
   "source": [
    "### **2.2. Matrix Multiplication Basics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a5afbb",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_02/Lecture_A/image_02_02.jpg?v=1769366886\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Matrix multiplication mixes data with learned parameters\n",
    ">* Used to reshape features and propagate network activations\n",
    "\n",
    ">* Matrix multiplication pairs rows with matching columns\n",
    ">* TensorFlow checks inner dimensions and hardware efficiency\n",
    "\n",
    ">* Use batched matrix multiplies for many examples\n",
    ">* Leverage batch dimensions to exploit optimized routines\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11742b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Matrix Multiplication Basics\n",
    "\n",
    "# This script demonstrates basic matrix multiplication.\n",
    "# It uses tiny tensors to keep shapes understandable.\n",
    "# Focus on shapes and results not heavy computation.\n",
    "\n",
    "# import TensorFlow for tensor operations.\n",
    "import tensorflow as tf\n",
    "\n",
    "# print TensorFlow version for reproducibility.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# create a simple 2x3 matrix of features.\n",
    "features = tf.constant([[1.0, 2.0, 3.0],\n",
    "                        [4.0, 5.0, 6.0]])\n",
    "\n",
    "# create a 3x2 weight matrix for transformation.\n",
    "weights = tf.constant([[0.1, 0.2],\n",
    "                       [0.3, 0.4],\n",
    "                       [0.5, 0.6]])\n",
    "\n",
    "# show shapes so multiplication rule is clear.\n",
    "print(\"features shape:\", features.shape)\n",
    "print(\"weights shape:\", weights.shape)\n",
    "\n",
    "# validate inner dimensions match for matmul.\n",
    "if features.shape[1] != weights.shape[0]:\n",
    "    raise ValueError(\"Inner dimensions must match for matmul\")\n",
    "\n",
    "# perform matrix multiplication using tf.matmul.\n",
    "outputs = tf.matmul(features, weights)\n",
    "\n",
    "# show resulting shape after multiplication.\n",
    "print(\"outputs shape:\", outputs.shape)\n",
    "\n",
    "# print small matrices to see numeric effect.\n",
    "print(\"features matrix:\\n\", features.numpy())\n",
    "print(\"weights matrix:\\n\", weights.numpy())\n",
    "print(\"outputs matrix:\\n\", outputs.numpy())\n",
    "\n",
    "# demonstrate batched matrix multiplication basics.\n",
    "batch_features = tf.stack([features, features], axis=0)\n",
    "\n",
    "# confirm new batched tensor shape.\n",
    "print(\"batch_features shape:\", batch_features.shape)\n",
    "\n",
    "# apply same weights to each batch element.\n",
    "batch_outputs = tf.matmul(batch_features, weights)\n",
    "\n",
    "# print final batched outputs shape only.\n",
    "print(\"batch_outputs shape:\", batch_outputs.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251d0ab3",
   "metadata": {},
   "source": [
    "### **2.3. Tensor Reduction Operations**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978be801",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_02/Lecture_A/image_02_03.jpg?v=1769366930\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Reductions summarize many tensor values into fewer\n",
    ">* They compute sums, averages, extremes for model use\n",
    "\n",
    ">* Choosing reduction axes collapses specific tensor dimensions\n",
    ">* Axis choice controls output shape and interpretation\n",
    "\n",
    ">* Use specialized reductions to capture key statistics\n",
    ">* Chain reductions to normalize, aggregate, and guide learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef94f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Tensor Reduction Operations\n",
    "\n",
    "# This script demonstrates TensorFlow tensor reductions.\n",
    "# It focuses on simple shapes and clear outputs.\n",
    "# Run each part and read printed explanations.\n",
    "\n",
    "# Install TensorFlow if not already available.\n",
    "# !pip install tensorflow-cpu==2.20.0.\n",
    "\n",
    "# Import TensorFlow with a short alias.\n",
    "import tensorflow as tf\n",
    "\n",
    "# Disable GPU to avoid CUDA_ERROR_INVALID_HANDLE errors on some systems.\n",
    "try:\n",
    "    tf.config.set_visible_devices([], 'GPU')\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Print TensorFlow version for reproducibility.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Create a small 2D tensor of floats.\n",
    "values_2d = tf.constant([[1.0, 2.0, 3.0],\n",
    "                         [4.0, 5.0, 6.0]])\n",
    "\n",
    "# Show the tensor and its shape briefly.\n",
    "print(\"values_2d:\", values_2d.numpy())\n",
    "\n",
    "# Compute the global sum over all elements.\n",
    "global_sum = tf.reduce_sum(values_2d)\n",
    "\n",
    "# Print the scalar global sum result.\n",
    "print(\"Global sum:\", global_sum.numpy())\n",
    "\n",
    "# Reduce along axis zero keeping column wise sums.\n",
    "col_sum = tf.reduce_sum(values_2d, axis=0)\n",
    "\n",
    "# Print column wise sums and resulting shape.\n",
    "print(\"Column sum, shape\", col_sum.shape, \":\", col_sum.numpy())\n",
    "\n",
    "# Reduce along axis one keeping row wise sums.\n",
    "row_sum = tf.reduce_sum(values_2d, axis=1)\n",
    "\n",
    "# Print row wise sums and resulting shape.\n",
    "print(\"Row sum, shape\", row_sum.shape, \":\", row_sum.numpy())\n",
    "\n",
    "# Compute mean over all elements as scalar.\n",
    "global_mean = tf.reduce_mean(values_2d)\n",
    "\n",
    "# Print the scalar mean value clearly.\n",
    "print(\"Global mean:\", global_mean.numpy())\n",
    "\n",
    "# Compute maximum along axis zero for each column.\n",
    "col_max = tf.reduce_max(values_2d, axis=0)\n",
    "\n",
    "# Print column maximum values and their shape.\n",
    "print(\"Column max, shape\", col_max.shape, \":\", col_max.numpy())\n",
    "\n",
    "# Create a boolean tensor for nonzero counting.\n",
    "nonzero_mask = tf.not_equal(values_2d, 0.0)\n",
    "\n",
    "# Count nonzero elements using reduce_sum on mask.\n",
    "nonzero_count = tf.reduce_sum(tf.cast(nonzero_mask, tf.int32))\n",
    "\n",
    "# Final print summarizing nonzero element count.\n",
    "print(\"Nonzero element count:\", int(nonzero_count.numpy()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638a6f37",
   "metadata": {},
   "source": [
    "## **3. Tensor Shapes and Broadcasting**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2214917",
   "metadata": {},
   "source": [
    "### **3.1. Static and Dynamic Shapes**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fac81a8",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_02/Lecture_A/image_03_01.jpg?v=1769366998\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Static shape is TensorFlow’s known tensor layout\n",
    ">* It catches shape errors early and optimizes computation\n",
    "\n",
    ">* Some tensor dimensions stay unknown until runtime\n",
    ">* TensorFlow uses partial shapes, final sizes resolve later\n",
    "\n",
    ">* Static shapes enable early, predictable broadcasting checks\n",
    ">* Dynamic shapes defer compatibility errors until runtime\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33fb636",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Static and Dynamic Shapes\n",
    "\n",
    "# This script explores static and dynamic shapes.\n",
    "# It uses TensorFlow tensors to show shape behavior.\n",
    "# Focus on broadcasting and simple shape inference.\n",
    "\n",
    "# Uncomment the next line if TensorFlow is missing.\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import TensorFlow with a clear alias.\n",
    "import tensorflow as tf\n",
    "\n",
    "# Force TensorFlow to use CPU only to avoid CUDA errors.\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "# Print TensorFlow version for reproducibility.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Create a simple constant tensor with known shape.\n",
    "static_tensor = tf.constant([[1.0, 2.0], [3.0, 4.0]])\n",
    "\n",
    "# Show the static shape known at definition time.\n",
    "print(\"Static shape of tensor:\", static_tensor.shape)\n",
    "\n",
    "# Show the dynamic shape evaluated at runtime.\n",
    "print(\"Dynamic shape via tf.shape:\", tf.shape(static_tensor))\n",
    "\n",
    "# Create a placeholder like tensor using tf.zeros.\n",
    "batch_size = 3\n",
    "\n",
    "# Define a tensor where batch dimension can change.\n",
    "flexible_tensor = tf.zeros((batch_size, 4))\n",
    "\n",
    "# Show static shape information for flexible tensor.\n",
    "print(\"Static shape flexible:\", flexible_tensor.shape)\n",
    "\n",
    "# Show dynamic shape information for flexible tensor.\n",
    "print(\"Dynamic shape flexible:\", tf.shape(flexible_tensor))\n",
    "\n",
    "# Simulate unknown batch by using None in shape.\n",
    "example_spec = tf.TensorSpec(shape=(None, 4), dtype=tf.float32)\n",
    "\n",
    "# Print the static shape from the TensorSpec.\n",
    "print(\"TensorSpec static shape:\", example_spec.shape)\n",
    "\n",
    "# Create a new batch with different batch size.\n",
    "new_batch = tf.ones((5, 4), dtype=tf.float32)\n",
    "\n",
    "# Confirm that new batch matches TensorSpec rank.\n",
    "print(\"New batch static shape:\", new_batch.shape)\n",
    "\n",
    "# Use tf.shape to get dynamic shape of new batch.\n",
    "print(\"New batch dynamic shape:\", tf.shape(new_batch))\n",
    "\n",
    "# Create a bias vector that will broadcast.\n",
    "bias = tf.constant([0.5, 0.5, 0.5, 0.5], dtype=tf.float32)\n",
    "\n",
    "# Show shapes before broadcasting addition.\n",
    "print(\"Bias static shape:\", bias.shape)\n",
    "\n",
    "# Perform broadcasting addition with new_batch and bias.\n",
    "result = new_batch + bias\n",
    "\n",
    "# Show static shape of the broadcasted result.\n",
    "print(\"Result static shape:\", result.shape)\n",
    "\n",
    "# Show dynamic shape of the broadcasted result.\n",
    "print(\"Result dynamic shape:\", tf.shape(result))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a43b6e",
   "metadata": {},
   "source": [
    "### **3.2. Broadcasting Rules**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15da41b9",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_02/Lecture_A/image_03_02.jpg?v=1769367059\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Broadcasting aligns tensor shapes right to left\n",
    ">* Size-one dimensions stretch to match larger tensors\n",
    "\n",
    ">* Image batch and channel corrections have compatible shapes\n",
    ">* Broadcasting repeats three corrections across every image pixel\n",
    "\n",
    ">* Incompatible dimensions cause broadcasting shape mismatch errors\n",
    ">* Understand which axes stretch to avoid wrong results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309abefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Broadcasting Rules\n",
    "\n",
    "# This script demonstrates TensorFlow broadcasting rules.\n",
    "# It focuses on simple shapes and clear outputs.\n",
    "# Run each part and read printed explanations.\n",
    "\n",
    "# Install TensorFlow if not already available.\n",
    "# pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import TensorFlow with a short alias.\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "import tensorflow as tf\n",
    "\n",
    "# Print TensorFlow version for reproducibility.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Create a base tensor representing a week of sales.\n",
    "week_sales = tf.constant([10., 12., 9., 11., 13., 8., 7.])\n",
    "\n",
    "# Show the shape of the base tensor.\n",
    "print(\"week_sales shape:\", week_sales.shape)\n",
    "\n",
    "# Create a scalar bias that will be broadcast.\n",
    "scalar_bias = tf.constant(1.5)\n",
    "\n",
    "# Add scalar to vector using broadcasting.\n",
    "sales_plus_scalar = week_sales + scalar_bias\n",
    "\n",
    "# Print result and shape after scalar broadcasting.\n",
    "print(\"scalar broadcast result:\", sales_plus_scalar.numpy())\n",
    "\n",
    "# Create a per day adjustment vector with matching shape.\n",
    "per_day_adjust = tf.constant([1., 0., -1., 0., 1., 0., -1.])\n",
    "\n",
    "# Add per day adjustments without broadcasting changes.\n",
    "sales_plus_day = week_sales + per_day_adjust\n",
    "\n",
    "# Print result and confirm same shape as original.\n",
    "print(\"per day result shape:\", sales_plus_day.shape)\n",
    "\n",
    "# Create a matrix representing two weeks of sales data.\n",
    "two_weeks = tf.stack([week_sales, week_sales + 2.])\n",
    "\n",
    "# Print the shape of the two weeks tensor.\n",
    "print(\"two_weeks shape:\", two_weeks.shape)\n",
    "\n",
    "# Reshape per day adjustment for column broadcasting.\n",
    "per_day_column = tf.reshape(per_day_adjust, (1, 7))\n",
    "\n",
    "# Add row vector to each week using broadcasting.\n",
    "weeks_plus_day = two_weeks + per_day_column\n",
    "\n",
    "# Print shape after row wise broadcasting.\n",
    "print(\"weeks_plus_day shape:\", weeks_plus_day.shape)\n",
    "\n",
    "# Create a column bias that broadcasts over days.\n",
    "per_week_bias = tf.reshape(tf.constant([2., -1.]), (2, 1))\n",
    "\n",
    "# Add column bias to two weeks tensor using broadcasting.\n",
    "weeks_plus_bias = two_weeks + per_week_bias\n",
    "\n",
    "# Print shape after column wise broadcasting.\n",
    "print(\"weeks_plus_bias shape:\", weeks_plus_bias.shape)\n",
    "\n",
    "# Try an incompatible shape and handle the error.\n",
    "try:\n",
    "    bad_adjust = tf.constant([1., 2., 3., 4., 5.])\n",
    "    _ = week_sales + bad_adjust\n",
    "except Exception as e:\n",
    "    print(\"incompatible broadcast error type:\", type(e).__name__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1831e165",
   "metadata": {},
   "source": [
    "### **3.3. Reshaping Tensors Safely**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb3b34f",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_02/Lecture_A/image_03_03.jpg?v=1769367122\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Reshaping changes tensor dimensions, not the data\n",
    ">* Total element count must stay exactly constant\n",
    "\n",
    ">* Match reshaped tensors with later operation shapes\n",
    ">* Keep each dimension’s real-world meaning consistent\n",
    "\n",
    ">* Be cautious using inferred dimensions when reshaping\n",
    ">* Verify axes meanings to avoid harmful broadcasting\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e3a07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Reshaping Tensors Safely\n",
    "\n",
    "# This script explores safe tensor reshaping.\n",
    "# It uses only Python lists and reasoning.\n",
    "# Focus on shapes, sizes, and safe changes.\n",
    "\n",
    "# Show an original flat list representing data.\n",
    "original_data = [1, 2, 3, 4, 5, 6]\n",
    "\n",
    "# Explain the original length as total elements.\n",
    "original_length = len(original_data)\n",
    "\n",
    "# Print the original data and its length.\n",
    "print(\"Original data:\", original_data)\n",
    "print(\"Original length:\", original_length)\n",
    "\n",
    "# Define a helper to compute product of dimensions.\n",
    "def product(dims):\n",
    "    result = 1\n",
    "    for d in dims:\n",
    "        result *= d\n",
    "    return result\n",
    "\n",
    "# Choose a target shape that is compatible.\n",
    "target_shape_safe = (2, 3)\n",
    "\n",
    "# Compute element count for the safe shape.\n",
    "safe_count = product(target_shape_safe)\n",
    "\n",
    "# Print the safe shape and its element count.\n",
    "print(\"Safe shape:\", target_shape_safe, \"elements:\", safe_count)\n",
    "\n",
    "# Check if safe shape matches original length.\n",
    "if safe_count == original_length:\n",
    "    print(\"Safe reshape possible, sizes match.\")\n",
    "else:\n",
    "    print(\"Safe reshape not possible here.\")\n",
    "\n",
    "# Choose an unsafe target shape on purpose.\n",
    "target_shape_unsafe = (4, 3)\n",
    "\n",
    "# Compute element count for the unsafe shape.\n",
    "unsafe_count = product(target_shape_unsafe)\n",
    "\n",
    "# Print the unsafe shape and its element count.\n",
    "print(\"Unsafe shape:\", target_shape_unsafe, \"elements:\", unsafe_count)\n",
    "\n",
    "# Check and warn if unsafe reshape is attempted.\n",
    "if unsafe_count != original_length:\n",
    "    print(\"Warning: cannot reshape, element counts differ.\")\n",
    "else:\n",
    "    print(\"Unexpectedly safe, counts are equal.\")\n",
    "\n",
    "# Demonstrate a simple manual reshape into rows.\n",
    "rows, cols = target_shape_safe\n",
    "\n",
    "# Build a nested list to mimic a reshaped tensor.\n",
    "reshaped = [original_data[i * cols:(i + 1) * cols] for i in range(rows)]\n",
    "\n",
    "# Print the reshaped structure and its dimensions.\n",
    "print(\"Reshaped data:\", reshaped, \"with shape\", target_shape_safe)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4253bd42",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Tensors and Ops**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc4c141",
   "metadata": {},
   "source": [
    "\n",
    "In this lecture, you learned to:\n",
    "- Create and manipulate TensorFlow tensors with specified shapes and dtypes. \n",
    "- Apply common TensorFlow math and array operations to transform tensors. \n",
    "- Explain TensorFlow broadcasting and shape inference in simple expressions. \n",
    "\n",
    "In the next Lecture (Lecture B), we will go over 'Autograd with TF'"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
