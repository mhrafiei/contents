{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72a493b3",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Serving and APIs**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b894992a",
   "metadata": {},
   "source": [
    ">Last update: 20260128.\n",
    "    \n",
    "By the end of this Lecture, you will be able to:\n",
    "- Deploy a SavedModel using TensorFlow Serving or a similar serving stack. \n",
    "- Expose a prediction API endpoint that accepts JSON inputs and returns model outputs. \n",
    "- Evaluate basic performance characteristics of a served model, including latency and throughput. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a23cda8",
   "metadata": {},
   "source": [
    "## **1. TensorFlow Serving Basics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab16065",
   "metadata": {},
   "source": [
    "### **1.1. Dockerized TF Serving**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961c8a9c",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_09/Lecture_B/image_01_01.jpg?v=1769605095\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Run TensorFlow Serving inside a Docker container\n",
    ">* Get simple, consistent deployments across all environments\n",
    "\n",
    ">* Containers isolate TensorFlow Serving from host changes\n",
    ">* Multiple models run with separate versions and resources\n",
    "\n",
    ">* Container images integrate with registries and orchestrators\n",
    ">* Enable scalable, reliable model serving across environments\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c3d304",
   "metadata": {},
   "source": [
    "### **1.2. Managing Model Versions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c79cce",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_09/Lecture_B/image_01_02.jpg?v=1769605115\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Treat models as numbered versioned snapshots over time\n",
    ">* Directory structure enables updates, rollback, parallel versions\n",
    "\n",
    ">* New model versions load safely alongside old\n",
    ">* Blue–green style updates reduce risk and downtime\n",
    "\n",
    ">* Versioning enables A/B tests and canary rollouts\n",
    ">* Supports auditability, rollback, and reproducible predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb487f5",
   "metadata": {},
   "source": [
    "### **1.3. REST and gRPC Setup**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e32d2b3",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_09/Lecture_B/image_01_03.jpg?v=1769605131\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* REST exposes models via simple HTTP JSON requests\n",
    ">* Easy integration, debugging, and testing across many apps\n",
    "\n",
    ">* gRPC uses binary, schema-based messages over HTTP/2\n",
    ">* Contracts enable fast, consistent, multi-language service communication\n",
    "\n",
    ">* Use REST or gRPC based on clients\n",
    ">* Plan endpoints, reliability, monitoring, and security carefully\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b39acee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - REST and gRPC Setup\n",
    "\n",
    "# This script shows simple REST and gRPC setup.\n",
    "# It simulates TensorFlow Serving style prediction calls.\n",
    "# Focus is on beginner friendly client side examples.\n",
    "\n",
    "# Required external libraries would be installed like this.\n",
    "# !pip install tensorflow-serving-api.\n",
    "\n",
    "# Import standard libraries for HTTP and environment handling.\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "\n",
    "# Import TensorFlow to show version and basic tensor usage.\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set deterministic seeds for reproducible random behavior.\n",
    "random.seed(7)\n",
    "tf.random.set_seed(7)\n",
    "\n",
    "# Print TensorFlow version in one concise line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Define a tiny helper to pretty print JSON safely.\n",
    "def pretty_json(data_dict):\n",
    "    return json.dumps(data_dict, indent=2, sort_keys=True)\n",
    "\n",
    "# Simulate a SavedModel signature for numeric inputs.\n",
    "MODEL_NAME = \"demo_model\"\n",
    "MODEL_VERSION = \"1\"\n",
    "\n",
    "# Create a small example input vector for predictions.\n",
    "example_features = [0.1, 0.5, 0.9]\n",
    "\n",
    "# Validate the example input length before using it.\n",
    "if len(example_features) != 3:\n",
    "    raise ValueError(\"example_features must have length three\")\n",
    "\n",
    "# Build a fake REST request body similar to TensorFlow Serving.\n",
    "rest_request_body = {\n",
    "    \"signature_name\": \"serving_default\",\n",
    "    \"instances\": [example_features],\n",
    "}\n",
    "\n",
    "# Show the REST URL pattern for a prediction endpoint.\n",
    "rest_url = (\n",
    "    \"http://localhost:8501/v1/models/\" + MODEL_NAME + \":predict\"\n",
    ")\n",
    "\n",
    "# Print a short explanation and the REST request example.\n",
    "print(\"\\nREST endpoint URL example:\")\n",
    "print(rest_url)\n",
    "\n",
    "# Print the JSON body that a REST client would send.\n",
    "print(\"\\nREST JSON request body example:\")\n",
    "print(pretty_json(rest_request_body))\n",
    "\n",
    "# Simulate a simple model prediction using TensorFlow operations.\n",
    "weights = tf.constant([[0.2], [0.4], [0.6]], dtype=tf.float32)\n",
    "\n",
    "# Convert example features into a TensorFlow tensor.\n",
    "features_tensor = tf.constant([example_features], dtype=tf.float32)\n",
    "\n",
    "# Validate tensor shape before matrix multiplication.\n",
    "if features_tensor.shape[1] != weights.shape[0]:\n",
    "    raise ValueError(\"Feature size and weight size must match\")\n",
    "\n",
    "# Compute a fake prediction using matrix multiplication.\n",
    "prediction_tensor = tf.matmul(features_tensor, weights)\n",
    "\n",
    "# Convert prediction tensor to a plain Python value.\n",
    "prediction_value = float(prediction_tensor.numpy()[0][0])\n",
    "\n",
    "# Build a fake REST style prediction response body.\n",
    "rest_response_body = {\"predictions\": [prediction_value]}\n",
    "\n",
    "# Print the simulated REST response JSON body.\n",
    "print(\"\\nSimulated REST JSON response body:\")\n",
    "print(pretty_json(rest_response_body))\n",
    "\n",
    "# Define a minimal gRPC style request dictionary for teaching.\n",
    "grpc_request = {\n",
    "    \"model_spec\": {\"name\": MODEL_NAME, \"version\": MODEL_VERSION},\n",
    "    \"inputs\": {\"features\": example_features},\n",
    "}\n",
    "\n",
    "# Print a short explanation of the gRPC style request.\n",
    "print(\"\\nSimplified gRPC style request example:\")\n",
    "print(pretty_json(grpc_request))\n",
    "\n",
    "# Build a matching gRPC style response dictionary.\n",
    "grpc_response = {\n",
    "    \"model_spec\": {\"name\": MODEL_NAME, \"version\": MODEL_VERSION},\n",
    "    \"outputs\": {\"score\": prediction_value},\n",
    "}\n",
    "\n",
    "# Print the simulated gRPC style response body.\n",
    "print(\"\\nSimulated gRPC style response example:\")\n",
    "print(pretty_json(grpc_response))\n",
    "\n",
    "# Final confirmation line summarizing what was demonstrated.\n",
    "print(\"\\nDemo complete: compared REST and gRPC style payloads.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3dc64a",
   "metadata": {},
   "source": [
    "## **2. Designing Prediction APIs**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a30b8e",
   "metadata": {},
   "source": [
    "### **2.1. JSON Request Design**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4dd9afc",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_09/Lecture_B/image_02_01.jpg?v=1769605178\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Prediction request defines client–server data contract\n",
    ">* Use clear fields to avoid missing, misaligned data\n",
    "\n",
    ">* Support both single and batched prediction requests\n",
    ">* Handle text, numeric, and binary data safely\n",
    "\n",
    ">* Separate required and optional fields with defaults\n",
    ">* Design schema for versioning and future extensions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ab1e59",
   "metadata": {},
   "source": [
    "### **2.2. Service Layer Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acbbfc8",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_09/Lecture_B/image_02_02.jpg?v=1769605196\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Service layer cleans and converts JSON inputs\n",
    ">* Ensures consistent tensors so models focus on inference\n",
    "\n",
    ">* Service preprocessing adds flexibility across model versions\n",
    ">* Must exactly match training transforms to avoid drift\n",
    "\n",
    ">* Preprocessing defends against bad or unusual inputs\n",
    ">* It enriches data and boosts API robustness\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf10af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Service Layer Preprocessing\n",
    "\n",
    "# This script shows service layer preprocessing.\n",
    "# We simulate a tiny JSON prediction request.\n",
    "# Then we convert it into model ready tensors.\n",
    "\n",
    "# Required TensorFlow is usually preinstalled in Colab.\n",
    "# Uncomment next line if TensorFlow is unexpectedly missing.\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import standard libraries for typing and randomness.\n",
    "import json, random, os, math\n",
    "\n",
    "# Import TensorFlow for tensor handling and models.\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set deterministic seeds for reproducible behavior.\n",
    "random.seed(7)\n",
    "\n",
    "# Print TensorFlow version in a compact single line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Define a tiny feature specification for our service.\n",
    "FEATURE_KEYS = [\"age\", \"income\", \"employment_status\"]\n",
    "\n",
    "# Define allowed employment categories for simple encoding.\n",
    "EMPLOYMENT_CATEGORIES = [\"unemployed\", \"part_time\", \"full_time\"]\n",
    "\n",
    "# Define numeric scaling constants for preprocessing logic.\n",
    "AGE_MAX = 100.0\n",
    "\n",
    "# Define income scaling constant to keep values small.\n",
    "INCOME_SCALE = 100000.0\n",
    "\n",
    "# Build a simple Keras model that expects three features.\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(3,)),\n",
    "    tf.keras.layers.Dense(4, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "])\n",
    "\n",
    "# Compile the model with minimal configuration options.\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")\n",
    "\n",
    "# Create a tiny dummy batch for quick warmup training.\n",
    "dummy_x = tf.constant([[0.2, 0.3, 1.0]], dtype=tf.float32)\n",
    "\n",
    "# Create dummy labels so the model can be trained.\n",
    "dummy_y = tf.constant([[1.0]], dtype=tf.float32)\n",
    "\n",
    "# Train briefly with verbose zero to avoid log spam.\n",
    "model.fit(dummy_x, dummy_y, epochs=3, verbose=0)\n",
    "\n",
    "# Define a function that validates incoming JSON payloads.\n",
    "def validate_request(payload: dict) -> None:\n",
    "    # Ensure all required feature keys are present.\n",
    "    missing = [k for k in FEATURE_KEYS if k not in payload]\n",
    "    # Raise a clear error if any keys are missing.\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing keys: {missing}\")\n",
    "    # Validate age is within a safe numeric range.\n",
    "    age = payload[\"age\"]\n",
    "    # Check age type and reasonable numeric bounds.\n",
    "    if not isinstance(age, (int, float)) or not (0 <= age <= 120):\n",
    "        raise ValueError(\"Invalid age value in request\")\n",
    "    # Validate income is non negative and numeric.\n",
    "    income = payload[\"income\"]\n",
    "    # Check income type and simple upper bound.\n",
    "    if not isinstance(income, (int, float)) or income < 0:\n",
    "        raise ValueError(\"Invalid income value in request\")\n",
    "    # Validate employment status is a known category.\n",
    "    status = payload[\"employment_status\"]\n",
    "    # Ensure status is a string and in allowed list.\n",
    "    if status not in EMPLOYMENT_CATEGORIES:\n",
    "        raise ValueError(\"Unknown employment_status category\")\n",
    "\n",
    "\n",
    "# Define a function that converts JSON into model tensors.\n",
    "def preprocess_request(payload: dict) -> tf.Tensor:\n",
    "    # First validate the payload using helper function.\n",
    "    validate_request(payload)\n",
    "    # Normalize age into zero one range using constant.\n",
    "    age_raw = float(payload[\"age\"])\n",
    "    # Clip age to avoid extreme unexpected values.\n",
    "    age_clipped = max(0.0, min(age_raw, AGE_MAX))\n",
    "    # Scale age by maximum to obtain normalized feature.\n",
    "    age_scaled = age_clipped / AGE_MAX\n",
    "    # Scale income into a smaller numeric magnitude.\n",
    "    income_raw = float(payload[\"income\"])\n",
    "    # Use simple division to keep values numerically stable.\n",
    "    income_scaled = income_raw / INCOME_SCALE\n",
    "    # Convert employment status into one hot like value.\n",
    "    status = payload[\"employment_status\"]\n",
    "    # Map status string to index using predefined list.\n",
    "    status_index = EMPLOYMENT_CATEGORIES.index(status)\n",
    "    # Represent status as a single numeric index feature.\n",
    "    status_feature = float(status_index)\n",
    "    # Assemble final feature vector in correct order.\n",
    "    features = [age_scaled, income_scaled, status_feature]\n",
    "    # Convert list into a batch tensor for the model.\n",
    "    tensor = tf.convert_to_tensor([features], dtype=tf.float32)\n",
    "    # Validate tensor shape before returning to caller.\n",
    "    if tensor.shape != (1, 3):\n",
    "        raise ValueError(f\"Unexpected tensor shape {tensor.shape}\")\n",
    "    # Return the prepared tensor ready for inference.\n",
    "    return tensor\n",
    "\n",
    "\n",
    "# Define a simple function that simulates an API call.\n",
    "def predict_from_json(json_string: str) -> float:\n",
    "    # Parse the incoming JSON string into a dictionary.\n",
    "    payload = json.loads(json_string)\n",
    "    # Preprocess payload into a numeric tensor batch.\n",
    "    inputs = preprocess_request(payload)\n",
    "    # Run the model prediction on prepared inputs.\n",
    "    preds = model.predict(inputs, verbose=0)\n",
    "    # Extract scalar prediction value from returned array.\n",
    "    score = float(preds[0, 0])\n",
    "    # Return the prediction score as a Python float.\n",
    "    return score\n",
    "\n",
    "\n",
    "# Create a sample valid JSON request for demonstration.\n",
    "sample_request = json.dumps({\n",
    "    \"age\": 35,\n",
    "    \"income\": 55000,\n",
    "    \"employment_status\": \"full_time\",\n",
    "})\n",
    "\n",
    "# Run the simulated prediction and capture the result.\n",
    "result = predict_from_json(sample_request)\n",
    "\n",
    "# Print the original JSON and the numeric prediction.\n",
    "print(\"Request JSON:\", sample_request)\n",
    "\n",
    "# Print the final prediction score with limited decimals.\n",
    "print(\"Predicted approval score:\", round(result, 4))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e665e7",
   "metadata": {},
   "source": [
    "### **2.3. Interpreting Model Responses**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879f8794",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_09/Lecture_B/image_02_03.jpg?v=1769605313\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Convert raw model numbers into meaningful outputs\n",
    ">* Return clear, consistent, self-describing prediction objects\n",
    "\n",
    ">* Choose which probabilities and signals to expose\n",
    ">* Balance detail, clarity, and safety in responses\n",
    "\n",
    ">* Clearly separate successes, low-confidence results, and errors\n",
    ">* Log response types to refine models and clients\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8185e2f8",
   "metadata": {},
   "source": [
    "## **3. Serving Performance Basics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7fe128",
   "metadata": {},
   "source": [
    "### **3.1. Batching For Higher Throughput**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e20203",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_09/Lecture_B/image_03_01.jpg?v=1769605333\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Batching groups many requests into one pass\n",
    ">* This boosts throughput with little extra latency\n",
    "\n",
    ">* Batching balances higher throughput against added waiting time\n",
    ">* Traffic patterns and latency needs guide batch settings\n",
    "\n",
    ">* Tune batch size and wait time carefully\n",
    ">* Monitor latency and throughput, adjust or use dynamic batching\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e3cb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Batching For Higher Throughput\n",
    "\n",
    "# This script demonstrates batching for throughput.\n",
    "# We compare single and batched inference latency.\n",
    "# Focus is on simple TensorFlow serving concepts.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Import TensorFlow and numpy.\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Print TensorFlow version briefly.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Define a small dense model for demonstration.\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(32,)),\n",
    "    tf.keras.layers.Dense(32, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1, activation=\"linear\"),\n",
    "])\n",
    "\n",
    "# Compile the model with simple configuration.\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "\n",
    "# Create tiny random training data.\n",
    "train_x = np.random.randn(256, 32).astype(\"float32\")\n",
    "train_y = np.random.randn(256, 1).astype(\"float32\")\n",
    "\n",
    "# Train briefly with silent output.\n",
    "model.fit(train_x, train_y, epochs=2, batch_size=32, verbose=0)\n",
    "\n",
    "# Create a small test set for timing.\n",
    "test_x = np.random.randn(128, 32).astype(\"float32\")\n",
    "\n",
    "# Validate test shape before inference.\n",
    "assert test_x.shape == (128, 32)\n",
    "\n",
    "# Define helper to time repeated predictions.\n",
    "def time_predictions(inputs, batch_size, repeats):\n",
    "    # Warm up once to avoid cold start.\n",
    "    _ = model.predict(inputs, batch_size=batch_size, verbose=0)\n",
    "\n",
    "    # Measure total wall time for repeats.\n",
    "    start = time.time()\n",
    "    for _ in range(repeats):\n",
    "        _ = model.predict(inputs, batch_size=batch_size, verbose=0)\n",
    "    end = time.time()\n",
    "\n",
    "    # Compute average latency per repeat.\n",
    "    total_time = end - start\n",
    "    avg_latency = total_time / repeats\n",
    "    return total_time, avg_latency\n",
    "\n",
    "# Configure experiment parameters.\n",
    "repeats = 20\n",
    "single_batch_size = 1\n",
    "large_batch_size = 32\n",
    "\n",
    "# Time single example style inference.\n",
    "single_total, single_avg = time_predictions(\n",
    "    test_x, batch_size=single_batch_size, repeats=repeats\n",
    ")\n",
    "\n",
    "# Time batched inference with larger batch.\n",
    "large_total, large_avg = time_predictions(\n",
    "    test_x, batch_size=large_batch_size, repeats=repeats\n",
    ")\n",
    "\n",
    "# Compute approximate throughput values.\n",
    "num_examples = test_x.shape[0]\n",
    "single_throughput = num_examples / single_avg\n",
    "large_throughput = num_examples / large_avg\n",
    "\n",
    "# Print concise comparison results.\n",
    "print(\"Single batch size latency (s):\", round(single_avg, 5))\n",
    "print(\"Large batch size latency (s):\", round(large_avg, 5))\n",
    "print(\"Single batch size throughput:\", int(single_throughput), \"examples/s\")\n",
    "print(\"Large batch size throughput:\", int(large_throughput), \"examples/s\")\n",
    "print(\"Total time single batch mode:\", round(single_total, 3), \"s\")\n",
    "print(\"Total time large batch mode:\", round(large_total, 3), \"s\")\n",
    "print(\"Note how batching improves overall throughput.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02738082",
   "metadata": {},
   "source": [
    "### **3.2. Measuring Latency and Throughput**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad831686",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_09/Lecture_B/image_03_02.jpg?v=1769605383\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Latency is how fast one prediction returns\n",
    ">* Throughput is how many predictions per time\n",
    "\n",
    ">* Simulate realistic traffic and record request latencies\n",
    ">* Summarize latency percentiles, track throughput versus load\n",
    "\n",
    ">* Relate latency and throughput to application needs\n",
    ">* Use patterns to guide tuning and capacity decisions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22ece6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Measuring Latency and Throughput\n",
    "\n",
    "# This script measures simple serving latency basics.\n",
    "# It simulates a tiny model prediction service.\n",
    "# We focus on latency and throughput concepts.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import time\n",
    "import statistics\n",
    "import random\n",
    "\n",
    "# Import tensorflow for a tiny model.\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set deterministic random seeds.\n",
    "random.seed(42)\n",
    "\n",
    "# Print TensorFlow version once.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Build a tiny dense model.\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(4,)),\n",
    "    tf.keras.layers.Dense(8, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1, activation=\"linear\"),\n",
    "])\n",
    "\n",
    "# Compile model with simple settings.\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "\n",
    "# Create small synthetic training data.\n",
    "x_train = tf.random.uniform((64, 4), minval=0.0, maxval=1.0)\n",
    "\n",
    "# Create matching target values.\n",
    "y_train = tf.reduce_sum(x_train, axis=1, keepdims=True)\n",
    "\n",
    "# Train briefly with silent output.\n",
    "model.fit(x_train, y_train, epochs=3, verbose=0)\n",
    "\n",
    "# Define a simple predict function.\n",
    "def predict_once(batch_size: int) -> tf.Tensor:\n",
    "    # Create random input batch.\n",
    "    inputs = tf.random.uniform((batch_size, 4))\n",
    "    # Run model prediction.\n",
    "    outputs = model(inputs, training=False)\n",
    "    return outputs\n",
    "\n",
    "# Measure latency for single requests.\n",
    "latencies_single = []\n",
    "\n",
    "# Run several single predictions.\n",
    "for _ in range(20):\n",
    "    # Record start time.\n",
    "    start = time.perf_counter()\n",
    "    # Call prediction with batch size one.\n",
    "    _ = predict_once(batch_size=1)\n",
    "    # Record end time.\n",
    "    end = time.perf_counter()\n",
    "    # Store latency in milliseconds.\n",
    "    latencies_single.append((end - start) * 1000.0)\n",
    "\n",
    "# Measure latency for batched requests.\n",
    "latencies_batch = []\n",
    "\n",
    "# Run several batched predictions.\n",
    "for _ in range(20):\n",
    "    # Record start time.\n",
    "    start = time.perf_counter()\n",
    "    # Call prediction with batch size sixteen.\n",
    "    _ = predict_once(batch_size=16)\n",
    "    # Record end time.\n",
    "    end = time.perf_counter()\n",
    "    # Store latency in milliseconds.\n",
    "    latencies_batch.append((end - start) * 1000.0)\n",
    "\n",
    "# Helper to compute simple statistics.\n",
    "def summarize_latencies(values):\n",
    "    # Validate non empty list.\n",
    "    if not values:\n",
    "        return None\n",
    "    # Sort values for percentiles.\n",
    "    sorted_vals = sorted(values)\n",
    "    # Compute median index.\n",
    "    mid = len(sorted_vals) // 2\n",
    "    # Compute median value.\n",
    "    median = sorted_vals[mid]\n",
    "    # Compute p95 index.\n",
    "    p95_index = int(0.95 * (len(sorted_vals) - 1))\n",
    "    # Compute p95 value.\n",
    "    p95 = sorted_vals[p95_index]\n",
    "    # Compute mean value.\n",
    "    mean = statistics.fmean(sorted_vals)\n",
    "    return mean, median, p95\n",
    "\n",
    "# Summarize single request latencies.\n",
    "mean_s, median_s, p95_s = summarize_latencies(latencies_single)\n",
    "\n",
    "# Summarize batched request latencies.\n",
    "mean_b, median_b, p95_b = summarize_latencies(latencies_batch)\n",
    "\n",
    "# Estimate throughput from batched timings.\n",
    "total_requests = 20 * 16\n",
    "\n",
    "# Total time seconds for batched runs.\n",
    "total_time_sec = sum(latencies_batch) / 1000.0\n",
    "\n",
    "# Compute requests per second.\n",
    "throughput_rps = total_requests / total_time_sec\n",
    "\n",
    "# Print concise latency summary.\n",
    "print(\"Single request mean ms:\", round(mean_s, 3))\n",
    "\n",
    "# Print median latency for singles.\n",
    "print(\"Single request median ms:\", round(median_s, 3))\n",
    "\n",
    "# Print p95 latency for singles.\n",
    "print(\"Single request p95 ms:\", round(p95_s, 3))\n",
    "\n",
    "# Print concise batched latency summary.\n",
    "print(\"Batch(16) mean ms:\", round(mean_b, 3))\n",
    "\n",
    "# Print median latency for batches.\n",
    "print(\"Batch(16) median ms:\", round(median_b, 3))\n",
    "\n",
    "# Print p95 latency for batches.\n",
    "print(\"Batch(16) p95 ms:\", round(p95_b, 3))\n",
    "\n",
    "# Print approximate throughput estimate.\n",
    "print(\"Approx throughput rps:\", round(throughput_rps, 2))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8abed8a",
   "metadata": {},
   "source": [
    "### **3.3. Scaling Model Replicas**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a9e426",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_09/Lecture_B/image_03_03.jpg?v=1769605503\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Run multiple identical model servers behind load balancing\n",
    ">* Handle higher traffic and avoid single-instance failures\n",
    "\n",
    ">* More replicas boost throughput and sometimes latency\n",
    ">* Poor balancing or autoscaling can hurt tail latency\n",
    "\n",
    ">* Balance horizontal, vertical scaling with batching, hardware\n",
    ">* Watch for saturation where replicas stop improving throughput\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f49460",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Serving and APIs**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613033ee",
   "metadata": {},
   "source": [
    "\n",
    "In this lecture, you learned to:\n",
    "- Deploy a SavedModel using TensorFlow Serving or a similar serving stack. \n",
    "- Expose a prediction API endpoint that accepts JSON inputs and returns model outputs. \n",
    "- Evaluate basic performance characteristics of a served model, including latency and throughput. \n",
    "\n",
    "In the next Module (Module 10), we will go over 'Advanced Topics'"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
