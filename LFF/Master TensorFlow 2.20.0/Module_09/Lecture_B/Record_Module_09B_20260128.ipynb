{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef4d4ce1",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Serving and APIs**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f9917f",
   "metadata": {},
   "source": [
    ">Last update: 20260128.\n",
    "    \n",
    "By the end of this Lecture, you will be able to:\n",
    "- Deploy a SavedModel using TensorFlow Serving or a similar serving stack. \n",
    "- Expose a prediction API endpoint that accepts JSON inputs and returns model outputs. \n",
    "- Evaluate basic performance characteristics of a served model, including latency and throughput. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b94ff3a",
   "metadata": {},
   "source": [
    "## **1. TensorFlow Serving Basics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22f31a0",
   "metadata": {},
   "source": [
    "### **1.1. Dockerized TF Serving**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae1011d",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_09/Lecture_B/image_01_01.jpg?v=1769620985\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Run TensorFlow Serving inside a prebuilt container\n",
    ">* Get portable, reproducible deployments across all environments\n",
    "\n",
    ">* Mount SavedModel, container exposes prediction endpoint\n",
    ">* Separates ML work from ops, enables safe updates\n",
    "\n",
    ">* Orchestration tools scale and manage serving containers\n",
    ">* Containerization plus orchestration ensures robust, maintainable serving\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cca317a",
   "metadata": {},
   "source": [
    "### **1.2. Managing Model Versions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f22174",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_09/Lecture_B/image_01_02.jpg?v=1769620999\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Models stored as numbered versioned directories\n",
    ">* Server switches versions while keeping client interface stable\n",
    "\n",
    ">* Configure which model versions load and default\n",
    ">* Use policies for canaries, A/B tests, rollbacks\n",
    "\n",
    ">* Plan storage, memory, and version retention carefully\n",
    ">* Monitor, automate promotion, and retire unused versions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf0ab36",
   "metadata": {},
   "source": [
    "### **1.3. Serving API Protocols**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65a76c8",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_09/Lecture_B/image_01_03.jpg?v=1769621021\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Serving protocols define request, response, error formats\n",
    ">* They connect clients to models via REST or gRPC\n",
    "\n",
    ">* Protocols support many input types and shapes\n",
    ">* Strict schemas ensure correct mapping and interoperability\n",
    "\n",
    ">* Protocols define batching, metadata, and shared tooling\n",
    ">* Design stable APIs that evolve without breaking clients\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c5f52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Serving API Protocols\n",
    "\n",
    "# This script demonstrates serving API protocols basics.\n",
    "# It simulates REST and gRPC style prediction requests.\n",
    "# Focus is on request shapes not real networking.\n",
    "\n",
    "# !pip install tensorflow-serving-api.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Set deterministic random seeds.\n",
    "random.seed(7)\n",
    "np.random.seed(7)\n",
    "\n",
    "# Define a tiny dummy prediction function.\n",
    "def dummy_model_predict(features_batch):\n",
    "    features_array = np.asarray(features_batch, dtype=float)\n",
    "    if features_array.ndim != 2:\n",
    "        raise ValueError(\"features_batch must be 2D array\")\n",
    "    weights = np.array([[0.3], [0.7]], dtype=float)\n",
    "    scores = features_array @ weights\n",
    "    probs = 1.0 / (1.0 + np.exp(-scores))\n",
    "    return probs.squeeze(axis=-1)\n",
    "\n",
    "# Build a fake REST JSON request body.\n",
    "def build_rest_request(user_ids, item_ids, scores):\n",
    "    if not (len(user_ids) == len(item_ids) == len(scores)):\n",
    "        raise ValueError(\"All feature lists must share length\")\n",
    "    instances = []\n",
    "    for u, i, s in zip(user_ids, item_ids, scores):\n",
    "        instance = {\"user_id\": int(u), \"item_id\": int(i), \"score\": float(s)}\n",
    "        instances.append(instance)\n",
    "    request_body = {\"instances\": instances}\n",
    "    return json.dumps(request_body)\n",
    "\n",
    "# Parse REST JSON and map to model inputs.\n",
    "def parse_rest_request(json_body):\n",
    "    parsed = json.loads(json_body)\n",
    "    if \"instances\" not in parsed:\n",
    "        raise KeyError(\"Request must contain 'instances' field\")\n",
    "    instances = parsed[\"instances\"]\n",
    "    features_batch = []\n",
    "    for inst in instances:\n",
    "        for key in (\"user_id\", \"item_id\", \"score\"):\n",
    "            if key not in inst:\n",
    "                raise KeyError(f\"Missing required field: {key}\")\n",
    "        features = [inst[\"user_id\"], inst[\"item_id\"]]\n",
    "        features_batch.append(features)\n",
    "    return np.array(features, dtype=float), np.array(features_batch, dtype=float)\n",
    "\n",
    "# Build a fake gRPC style request dictionary.\n",
    "def build_grpc_request(user_ids, item_ids, scores):\n",
    "    if not (len(user_ids) == len(item_ids) == len(scores)):\n",
    "        raise ValueError(\"All feature lists must share length\")\n",
    "    tensor = {\n",
    "        \"user_id\": {\"dtype\": \"INT32\", \"values\": list(map(int, user_ids))},\n",
    "        \"item_id\": {\"dtype\": \"INT32\", \"values\": list(map(int, item_ids))},\n",
    "        \"score\": {\"dtype\": \"FLOAT\", \"values\": list(map(float, scores))},\n",
    "    }\n",
    "    request = {\"model_name\": \"demo_recommender\", \"inputs\": tensor}\n",
    "    return request\n",
    "\n",
    "# Parse gRPC style request into model inputs.\n",
    "def parse_grpc_request(request):\n",
    "    if \"inputs\" not in request:\n",
    "        raise KeyError(\"Request must contain 'inputs' field\")\n",
    "    inputs = request[\"inputs\"]\n",
    "    for key in (\"user_id\", \"item_id\", \"score\"):\n",
    "        if key not in inputs:\n",
    "            raise KeyError(f\"Missing required tensor: {key}\")\n",
    "    user_ids = np.array(inputs[\"user_id\"][\"values\"], dtype=float)\n",
    "    item_ids = np.array(inputs[\"item_id\"][\"values\"], dtype=float)\n",
    "    if user_ids.shape != item_ids.shape:\n",
    "        raise ValueError(\"user_id and item_id shapes must match\")\n",
    "    features_batch = np.stack([user_ids, item_ids], axis=1)\n",
    "    return features_batch\n",
    "\n",
    "# Create a tiny batch of example features.\n",
    "user_ids = [1, 2, 3]\n",
    "item_ids = [10, 20, 30]\n",
    "scores = [0.1, 0.5, 0.9]\n",
    "\n",
    "# Build and parse REST style request.\n",
    "rest_body = build_rest_request(user_ids, item_ids, scores)\n",
    "last_instance, rest_features = parse_rest_request(rest_body)\n",
    "\n",
    "# Build and parse gRPC style request.\n",
    "grpc_request = build_grpc_request(user_ids, item_ids, scores)\n",
    "grpc_features = parse_grpc_request(grpc_request)\n",
    "\n",
    "# Run dummy predictions for both protocols.\n",
    "rest_predictions = dummy_model_predict(rest_features)\n",
    "grpc_predictions = dummy_model_predict(grpc_features)\n",
    "\n",
    "# Print concise comparison of protocol behaviors.\n",
    "print(\"REST request JSON:\")\n",
    "print(rest_body)\n",
    "print(\"\\nREST predictions:\", rest_predictions.tolist())\n",
    "print(\"\\ngRPC style request keys:\", list(grpc_request.keys()))\n",
    "print(\"gRPC predictions:\", grpc_predictions.tolist())\n",
    "print(\"\\nBoth protocols map to same model inputs.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f109f00",
   "metadata": {},
   "source": [
    "## **2. Designing Prediction APIs**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4a632f",
   "metadata": {},
   "source": [
    "### **2.1. JSON Request Design**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2eee2d",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_09/Lecture_B/image_02_01.jpg?v=1769621105\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Define a clear, stable contract for requests\n",
    ">* Use descriptive names and unambiguous data types\n",
    "\n",
    ">* Map client-friendly instances to model features\n",
    ">* Service layer converts JSON objects into tensors\n",
    "\n",
    ">* Include optional parameters, versions, and configuration cleanly\n",
    ">* Separate options from features for stability, reliability\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fd7022",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - JSON Request Design\n",
    "\n",
    "# This script shows JSON request design basics.\n",
    "# We simulate a tiny prediction API service.\n",
    "# Focus is on clear JSON request structures.\n",
    "\n",
    "# Required library is tensorflow already available.\n",
    "# No extra installations are needed here.\n",
    "\n",
    "# Import standard libraries for JSON handling.\n",
    "import json\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "random.seed(7)\n",
    "tf.random.set_seed(7)\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Define a simple toy model for demonstration.\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(3,)),\n",
    "    tf.keras.layers.Dense(4, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "])\n",
    "\n",
    "# Compile the model with basic configuration.\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")\n",
    "\n",
    "# Create tiny dummy training data for the model.\n",
    "train_x = tf.constant([[0.1, 0.2, 0.3], [0.9, 0.8, 0.7]], dtype=tf.float32)\n",
    "train_y = tf.constant([[0.0], [1.0]], dtype=tf.float32)\n",
    "\n",
    "# Train briefly with silent output for speed.\n",
    "model.fit(train_x, train_y, epochs=5, verbose=0)\n",
    "\n",
    "# Define a function to build a JSON request body.\n",
    "def build_request(instances, options=None):\n",
    "    # Create base request with instances list.\n",
    "    request = {\"instances\": instances}\n",
    "\n",
    "    # Attach optional config section if provided.\n",
    "    if options is not None:\n",
    "        request[\"config\"] = options\n",
    "\n",
    "    # Return JSON string with sorted keys.\n",
    "    return json.dumps(request, sort_keys=True)\n",
    "\n",
    "# Define a function to parse and validate JSON.\n",
    "def parse_request(json_body):\n",
    "    # Parse JSON string into Python dictionary.\n",
    "    data = json.loads(json_body)\n",
    "\n",
    "    # Ensure instances key exists and is a list.\n",
    "    if \"instances\" not in data or not isinstance(\n",
    "        data[\"instances\"], list\n",
    "    ):\n",
    "        raise ValueError(\"Request must contain list field 'instances'.\")\n",
    "\n",
    "    # Extract instances and optional config safely.\n",
    "    instances = data[\"instances\"]\n",
    "    config = data.get(\"config\", {})\n",
    "\n",
    "    # Validate each instance has required fields.\n",
    "    features = []\n",
    "    for item in instances:\n",
    "        if not isinstance(item, dict):\n",
    "            raise ValueError(\"Each instance must be a JSON object.\")\n",
    "        if not {\"f1\", \"f2\", \"f3\"}.issubset(item.keys()):\n",
    "            raise ValueError(\"Each instance needs f1, f2, f3 fields.\")\n",
    "        features.append([item[\"f1\"], item[\"f2\"], item[\"f3\"]])\n",
    "\n",
    "    # Convert features list into float32 tensor.\n",
    "    features_tensor = tf.convert_to_tensor(features, dtype=tf.float32)\n",
    "\n",
    "    # Return tensor and config dictionary.\n",
    "    return features_tensor, config\n",
    "\n",
    "# Define a function that simulates prediction endpoint.\n",
    "def predict_endpoint(json_body):\n",
    "    # Parse and validate incoming JSON request.\n",
    "    features_tensor, config = parse_request(json_body)\n",
    "\n",
    "    # Optionally limit batch size from config.\n",
    "    max_batch = int(config.get(\"max_batch\", 16))\n",
    "    if features_tensor.shape[0] > max_batch:\n",
    "        features_tensor = features_tensor[:max_batch]\n",
    "\n",
    "    # Run model prediction on validated tensor.\n",
    "    preds = model.predict(features_tensor, verbose=0)\n",
    "\n",
    "    # Build response with predictions as simple list.\n",
    "    response = {\"predictions\": preds[:, 0].tolist()}\n",
    "\n",
    "    # Return JSON string response to the caller.\n",
    "    return json.dumps(response, sort_keys=True)\n",
    "\n",
    "# Build a client friendly JSON request example.\n",
    "client_instances = [\n",
    "    {\"f1\": 0.2, \"f2\": 0.1, \"f3\": 0.4, \"user_id\": \"u1\"},\n",
    "    {\"f1\": 0.9, \"f2\": 0.7, \"f3\": 0.3, \"user_id\": \"u2\"},\n",
    "]\n",
    "\n",
    "# Define optional configuration for the request.\n",
    "client_config = {\"max_batch\": 4, \"explain\": False}\n",
    "\n",
    "# Create JSON body using the helper function.\n",
    "json_request = build_request(client_instances, client_config)\n",
    "\n",
    "# Send JSON body into simulated prediction endpoint.\n",
    "json_response = predict_endpoint(json_request)\n",
    "\n",
    "# Print request and response to show clear contract.\n",
    "print(\"Request JSON:\", json_request)\n",
    "print(\"Response JSON:\", json_response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c118feb0",
   "metadata": {},
   "source": [
    "### **2.2. Service Layer Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbf0bc0",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_09/Lecture_B/image_02_02.jpg?v=1769621220\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Service layer cleans and transforms raw client requests\n",
    ">* It validates, handles errors, and stabilizes model inputs\n",
    "\n",
    ">* Service layer repeats training-time feature transformations consistently\n",
    ">* Builds final feature vectors using shared routines\n",
    "\n",
    ">* Service layer enforces limits, privacy, and defaults\n",
    ">* Adds logging, validation, and keeps responsibilities separated\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e794f582",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Service Layer Preprocessing\n",
    "\n",
    "# This script shows service layer preprocessing.\n",
    "# We simulate a simple JSON prediction request.\n",
    "# Focus on cleaning inputs before model prediction.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Import TensorFlow for a tiny demo model.\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "random.seed(7)\n",
    "np.random.seed(7)\n",
    "tf.random.set_seed(7)\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Define a tiny numeric regression model.\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(3,)),\n",
    "    tf.keras.layers.Dense(4, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1, activation=\"linear\"),\n",
    "])\n",
    "\n",
    "# Compile model with simple optimizer and loss.\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "\n",
    "# Create tiny synthetic training data.\n",
    "x_train = np.array([\n",
    "    [0.0, 0.0, 0.0],\n",
    "    [1.0, 0.0, 0.0],\n",
    "    [0.0, 1.0, 0.0],\n",
    "    [0.0, 0.0, 1.0],\n",
    "    [1.0, 1.0, 1.0],\n",
    "], dtype=\"float32\")\n",
    "\n",
    "# Create simple target values for training.\n",
    "y_train = np.array([[0.0], [1.0], [1.0], [1.0], [3.0]], dtype=\"float32\")\n",
    "\n",
    "# Train briefly with silent verbose setting.\n",
    "model.fit(x_train, y_train, epochs=50, verbose=0)\n",
    "\n",
    "# Define expected feature configuration for service.\n",
    "EXPECTED_FEATURES = {\n",
    "    \"age\": {\"min\": 0, \"max\": 120},\n",
    "    \"income\": {\"min\": 0, \"max\": 1_000_000},\n",
    "    \"is_premium\": {\"min\": 0, \"max\": 1},\n",
    "}\n",
    "\n",
    "# Define simple normalization helper function.\n",
    "def normalize_feature(value, min_value, max_value):\n",
    "    # Clip value into allowed range.\n",
    "    clipped = max(min_value, min(value, max_value))\n",
    "    # Avoid division by zero safely.\n",
    "    if max_value == min_value:\n",
    "        return 0.0\n",
    "    # Scale value into zero one range.\n",
    "    return (clipped - min_value) / (max_value - min_value)\n",
    "\n",
    "# Define service layer preprocessing function.\n",
    "def preprocess_request(json_payload):\n",
    "    # Parse JSON string into Python dictionary.\n",
    "    try:\n",
    "        data = json.loads(json_payload)\n",
    "    except json.JSONDecodeError:\n",
    "        raise ValueError(\"Invalid JSON payload format.\")\n",
    "\n",
    "    # Validate that payload is a dictionary.\n",
    "    if not isinstance(data, dict):\n",
    "        raise ValueError(\"Top level JSON must be object.\")\n",
    "\n",
    "    # Prepare list for normalized feature values.\n",
    "    features = []\n",
    "\n",
    "    # Iterate over expected features configuration.\n",
    "    for name, cfg in EXPECTED_FEATURES.items():\n",
    "        # Get raw value or default fallback.\n",
    "        raw_value = data.get(name, None)\n",
    "        if raw_value is None:\n",
    "            # Use safe default when missing.\n",
    "            raw_value = cfg[\"min\"]\n",
    "        # Ensure numeric type for model.\n",
    "        try:\n",
    "            numeric = float(raw_value)\n",
    "        except (TypeError, ValueError):\n",
    "            raise ValueError(f\"Feature {name} must be numeric.\")\n",
    "        # Normalize using helper function.\n",
    "        norm = normalize_feature(numeric, cfg[\"min\"], cfg[\"max\"])\n",
    "        features.append(norm)\n",
    "\n",
    "    # Convert list into proper tensor shape.\n",
    "    features_array = np.array(features, dtype=\"float32\").reshape(1, -1)\n",
    "\n",
    "    # Validate final shape before prediction.\n",
    "    if features_array.shape[1] != 3:\n",
    "        raise ValueError(\"Preprocessed features must have length three.\")\n",
    "\n",
    "    # Return model ready tensor for prediction.\n",
    "    return features_array\n",
    "\n",
    "# Define service layer prediction wrapper.\n",
    "def predict_from_service(json_payload):\n",
    "    # Preprocess raw JSON into numeric tensor.\n",
    "    inputs = preprocess_request(json_payload)\n",
    "    # Run model prediction without extra logs.\n",
    "    outputs = model.predict(inputs, verbose=0)\n",
    "    # Convert prediction to plain Python float.\n",
    "    score = float(outputs[0, 0])\n",
    "    # Build clean response dictionary.\n",
    "    response = {\"prediction\": round(score, 3)}\n",
    "    return response\n",
    "\n",
    "# Create example client JSON with mixed issues.\n",
    "example_request = json.dumps({\n",
    "    \"age\": 35,\n",
    "    \"income\": 50_000,\n",
    "    \"is_premium\": \"1\",\n",
    "})\n",
    "\n",
    "# Run service layer prediction on example.\n",
    "response = predict_from_service(example_request)\n",
    "\n",
    "# Print original JSON and cleaned prediction.\n",
    "print(\"Raw request JSON:\", example_request)\n",
    "print(\"Service response:\", response)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3e1258",
   "metadata": {},
   "source": [
    "### **2.3. Interpreting Model Responses**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321e0675",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_09/Lecture_B/image_02_03.jpg?v=1769621323\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Convert raw model tensors into readable fields\n",
    ">* Structure responses consistently for easy client integration\n",
    "\n",
    ">* Choose what predictions, alternatives, and probabilities to expose\n",
    ">* Design responses to show uncertainty and support oversight\n",
    "\n",
    ">* Clearly encode technical and model-level issues\n",
    ">* Include context flags to guide safe decisions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb98662f",
   "metadata": {},
   "source": [
    "## **3. Serving Performance Basics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06246c9c",
   "metadata": {},
   "source": [
    "### **3.1. Efficient Request Batching**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bafcd3",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_09/Lecture_B/image_03_01.jpg?v=1769621341\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Batching groups many requests into one pass\n",
    ">* Better uses accelerators, boosting throughput without hardware\n",
    "\n",
    ">* Batch size trades off throughput against user latency\n",
    ">* Use max batch size and wait time limits\n",
    "\n",
    ">* Tune batch size using real traffic patterns\n",
    ">* Use adaptive batching and monitor latency, throughput\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b0de25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Efficient Request Batching\n",
    "\n",
    "# This script demonstrates efficient request batching concepts.\n",
    "# It simulates batched versus unbatched prediction latency simply.\n",
    "# Use it to understand latency throughput tradeoffs clearly.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import time\n",
    "import random\n",
    "import statistics\n",
    "\n",
    "# Import tensorflow and check version.\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set deterministic random seeds for reproducibility.\n",
    "random.seed(42)\n",
    "\n",
    "# Print tensorflow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Define a simple dense model for demonstration.\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(16,)),\n",
    "    tf.keras.layers.Dense(32, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(8, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "])\n",
    "\n",
    "# Compile model with minimal configuration.\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")\n",
    "\n",
    "# Create small dummy input batch for warmup.\n",
    "warmup_inputs = tf.random.uniform(shape=(8, 16))\n",
    "\n",
    "# Run one warmup prediction to initialize model.\n",
    "_ = model.predict(warmup_inputs, verbose=0)\n",
    "\n",
    "# Define a helper to simulate single request prediction.\n",
    "def predict_single(sample_tensor: tf.Tensor) -> tf.Tensor:\n",
    "    # Ensure input shape is correct for single sample.\n",
    "    assert sample_tensor.shape == (1, 16)\n",
    "    return model.predict(sample_tensor, verbose=0)\n",
    "\n",
    "\n",
    "# Define a helper to simulate batched prediction.\n",
    "def predict_batch(batch_tensor: tf.Tensor) -> tf.Tensor:\n",
    "    # Ensure batch has correct feature dimension.\n",
    "    assert batch_tensor.shape[1] == 16\n",
    "    return model.predict(batch_tensor, verbose=0)\n",
    "\n",
    "\n",
    "# Define a function to measure latency for many calls.\n",
    "def measure_latency(num_requests: int, batch_size: int) -> float:\n",
    "    # Collect random inputs for all synthetic requests.\n",
    "    inputs = [\n",
    "        tf.random.uniform(shape=(1, 16)) for _ in range(num_requests)\n",
    "    ]\n",
    "\n",
    "    # Start timing for the whole scenario.\n",
    "    start = time.perf_counter()\n",
    "\n",
    "    # If batch size is one, call predict_single repeatedly.\n",
    "    if batch_size == 1:\n",
    "        for sample in inputs:\n",
    "            _ = predict_single(sample)\n",
    "\n",
    "    # Otherwise group requests into batches for prediction.\n",
    "    else:\n",
    "        current_batch = []\n",
    "        for sample in inputs:\n",
    "            current_batch.append(sample)\n",
    "            if len(current_batch) == batch_size:\n",
    "                batch_tensor = tf.concat(current_batch, axis=0)\n",
    "                _ = predict_batch(batch_tensor)\n",
    "                current_batch = []\n",
    "\n",
    "        # Handle any remaining partial batch at the end.\n",
    "        if current_batch:\n",
    "            batch_tensor = tf.concat(current_batch, axis=0)\n",
    "            _ = predict_batch(batch_tensor)\n",
    "\n",
    "    # Compute average latency per request in milliseconds.\n",
    "    total_time = time.perf_counter() - start\n",
    "    avg_latency_ms = (total_time / num_requests) * 1000.0\n",
    "    return avg_latency_ms\n",
    "\n",
    "\n",
    "# Configure experiment parameters for synthetic traffic.\n",
    "num_requests = 64\n",
    "batch_sizes = [1, 4, 16, 32]\n",
    "\n",
    "# Measure latency for each batch size configuration.\n",
    "results = []\n",
    "for bsize in batch_sizes:\n",
    "    avg_ms = measure_latency(num_requests=num_requests, batch_size=bsize)\n",
    "    throughput = num_requests / (avg_ms / 1000.0)\n",
    "    results.append((bsize, avg_ms, throughput))\n",
    "\n",
    "# Print concise summary of latency and throughput tradeoffs.\n",
    "print(\"BatchSize  AvgLatency(ms)  RequestsPerSecond\")\n",
    "for bsize, avg_ms, rps in results:\n",
    "    print(f\"{bsize:8d}  {avg_ms:14.3f}  {rps:17.1f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727e1598",
   "metadata": {},
   "source": [
    "### **3.2. Latency and Throughput Metrics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd75639a",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_09/Lecture_B/image_03_02.jpg?v=1769621440\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Latency measures one request’s end-to-end time\n",
    ">* Throughput measures total requests handled under load\n",
    "\n",
    ">* Check full latency distribution, especially tail percentiles\n",
    ">* Measure latency under realistic, end-to-end production conditions\n",
    "\n",
    ">* Test throughput as concurrent traffic steadily increases\n",
    ">* Use results to set limits and scaling strategies\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6f55d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Latency and Throughput Metrics\n",
    "\n",
    "# This script explores serving latency and throughput.\n",
    "# It uses a tiny TensorFlow model and dummy requests.\n",
    "# Focus is on simple timing not advanced deployment.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import time\n",
    "import statistics\n",
    "\n",
    "# Import TensorFlow and print version.\n",
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Set deterministic random seeds.\n",
    "os.environ[\"PYTHONHASHSEED\"] = \"0\"\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "# Create a tiny Sequential model.\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(4,)),\n",
    "    tf.keras.layers.Dense(4, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1, activation=\"linear\"),\n",
    "])\n",
    "\n",
    "# Compile the model with simple settings.\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "\n",
    "# Create a small dummy dataset.\n",
    "inputs = tf.random.uniform(shape=(32, 4))\n",
    "labels = tf.reduce_sum(inputs, axis=1, keepdims=True)\n",
    "\n",
    "# Train briefly with silent output.\n",
    "model.fit(inputs, labels, epochs=3, verbose=0)\n",
    "\n",
    "# Prepare a single dummy request tensor.\n",
    "request_example = tf.random.uniform(shape=(1, 4))\n",
    "\n",
    "# Warm up the model before timing.\n",
    "_ = model(request_example, training=False)\n",
    "\n",
    "# Define a helper to measure latency.\n",
    "def measure_latency(model, sample, runs):\n",
    "    latencies = []\n",
    "    for _ in range(runs):\n",
    "        start = time.perf_counter()\n",
    "        _ = model(sample, training=False)\n",
    "        end = time.perf_counter()\n",
    "        latencies.append((end - start) * 1000.0)\n",
    "    return latencies\n",
    "\n",
    "# Measure latency over multiple runs.\n",
    "latency_ms = measure_latency(model, request_example, runs=50)\n",
    "\n",
    "# Compute basic latency statistics.\n",
    "median_latency = statistics.median(latency_ms)\n",
    "mean_latency = statistics.mean(latency_ms)\n",
    "\n",
    "# Compute simple percentile estimates.\n",
    "sorted_lat = sorted(latency_ms)\n",
    "index_95 = int(0.95 * len(sorted_lat)) - 1\n",
    "index_99 = int(0.99 * len(sorted_lat)) - 1\n",
    "p95_latency = sorted_lat[max(index_95, 0)]\n",
    "p99_latency = sorted_lat[max(index_99, 0)]\n",
    "\n",
    "# Estimate throughput from median latency.\n",
    "throughput_rps = 1000.0 / median_latency if median_latency > 0 else 0.0\n",
    "\n",
    "# Print latency metrics in milliseconds.\n",
    "print(\"Median latency ms:\", round(median_latency, 4))\n",
    "print(\"Mean latency ms:\", round(mean_latency, 4))\n",
    "print(\"P95 latency ms:\", round(p95_latency, 4))\n",
    "print(\"P99 latency ms:\", round(p99_latency, 4))\n",
    "\n",
    "# Print simple throughput estimate.\n",
    "print(\"Estimated single worker throughput rps:\", round(throughput_rps, 2))\n",
    "\n",
    "# Simulate batched requests for throughput.\n",
    "batch_size = 16\n",
    "batched_example = tf.random.uniform(shape=(batch_size, 4))\n",
    "\n",
    "# Time one batched inference call.\n",
    "start_batch = time.perf_counter()\n",
    "_ = model(batched_example, training=False)\n",
    "end_batch = time.perf_counter()\n",
    "\n",
    "# Compute batched latency and throughput.\n",
    "batch_latency_ms = (end_batch - start_batch) * 1000.0\n",
    "per_request_ms = batch_latency_ms / float(batch_size)\n",
    "throughput_batched = 1000.0 * float(batch_size) / batch_latency_ms\n",
    "\n",
    "# Print batched performance metrics.\n",
    "print(\"Batch latency ms:\", round(batch_latency_ms, 4))\n",
    "print(\"Per request latency ms:\", round(per_request_ms, 4))\n",
    "print(\"Estimated batched throughput rps:\", round(throughput_batched, 2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4499f37e",
   "metadata": {},
   "source": [
    "### **3.3. Scaling Model Replicas**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bed99a9",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_09/Lecture_B/image_03_03.jpg?v=1769621533\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Run many identical servers to share load\n",
    ">* More replicas improve real-world latency and throughput\n",
    "\n",
    ">* More replicas increase throughput and reduce queuing\n",
    ">* They don’t speed single requests; monitor per-replica latency\n",
    "\n",
    ">* Continuously measure latency and throughput while scaling\n",
    ">* Find bottlenecks and balance the entire pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fb32aa",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Serving and APIs**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4bef9e",
   "metadata": {},
   "source": [
    "\n",
    "In this lecture, you learned to:\n",
    "- Deploy a SavedModel using TensorFlow Serving or a similar serving stack. \n",
    "- Expose a prediction API endpoint that accepts JSON inputs and returns model outputs. \n",
    "- Evaluate basic performance characteristics of a served model, including latency and throughput. \n",
    "\n",
    "In the next Module (Module 10), we will go over 'Advanced Topics'"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
