{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfcadc65",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Serving and APIs**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ec97ad",
   "metadata": {},
   "source": [
    ">Last update: 20260121.\n",
    "    \n",
    "By the end of this Lecture, you will be able to:\n",
    "- Deploy a SavedModel using TensorFlow Serving or a similar serving stack. \n",
    "- Expose a prediction API endpoint that accepts JSON inputs and returns model outputs. \n",
    "- Evaluate basic performance characteristics of a served model, including latency and throughput. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21afee06",
   "metadata": {},
   "source": [
    "## **1. TensorFlow Serving Essentials**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cc81b9",
   "metadata": {},
   "source": [
    "### **1.1. Dockerized TF Serving**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed875016",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_09/Lecture_B/image_01_01.jpg?v=1768987301\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Run TensorFlow Serving inside reusable Docker containers\n",
    ">* Ensures consistent, conflict-free environments across all machines\n",
    "\n",
    ">* Container mounts model directory and serves requests\n",
    ">* Isolation controls resources and prevents team interference\n",
    "\n",
    ">* Containers integrate with Kubernetes for scaling, reliability\n",
    ">* Enable easy rolling updates and high-volume production serving\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0f98b4",
   "metadata": {},
   "source": [
    "### **1.2. Model Versioning Basics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7eaab72",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_09/Lecture_B/image_01_02.jpg?v=1768987348\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Production needs multiple clearly labeled model versions\n",
    ">* TensorFlow Serving organizes SavedModel versions in subdirectories\n",
    "\n",
    ">* Versioning enables safe rollouts and instant rollbacks\n",
    ">* Supports A/B tests, canaries, and reliable baselines\n",
    "\n",
    ">* Versioning links models to data, code, config\n",
    ">* Enables traceability, compliance, and continuous production improvement\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34ed3fe",
   "metadata": {},
   "source": [
    "### **1.3. Configuring REST and gRPC**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f115807",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_09/Lecture_B/image_01_03.jpg?v=1768987371\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Choose REST or gRPC for client communication\n",
    ">* Both expose same predictions; differ in usability, performance\n",
    "\n",
    ">* Match ports, protocols, and routing to clients\n",
    ">* Document model I/O clearly to prevent integration errors\n",
    "\n",
    ">* Use gateways, security, health checks, and monitoring\n",
    ">* Build scalable, reliable endpoints for many clients\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f64f938",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Configuring REST and gRPC\n",
    "\n",
    "# This script compares REST and gRPC style configuration concepts practically.\n",
    "# It simulates REST JSON requests and gRPC style binary payloads locally.\n",
    "# It helps beginners understand endpoint configuration and payload structure.\n",
    "\n",
    "# !pip install tensorflow\n",
    "\n",
    "# Import required standard library modules for networking demonstration.\n",
    "import http.server as http_server\n",
    "import socketserver as socket_server\n",
    "import threading as threading_module\n",
    "\n",
    "# Import json module for encoding and decoding REST style payloads.\n",
    "import json as json_module\n",
    "\n",
    "# Import time module for simple latency measurement calculations.\n",
    "import time as time_module\n",
    "\n",
    "# Define a simple prediction function simulating TensorFlow Serving behavior.\n",
    "def simple_predict_function(input_value):\n",
    "    squared_value = input_value * input_value\n",
    "    return squared_value\n",
    "\n",
    "# Define a custom HTTP handler class for REST style JSON prediction requests.\n",
    "class RestPredictionHandler(http_server.BaseHTTPRequestHandler):\n",
    "\n",
    "    # Handle POST requests representing REST prediction calls.\n",
    "    def do_POST(self):\n",
    "        content_length = int(self.headers.get(\"Content-Length\", 0))\n",
    "        request_body = self.rfile.read(content_length)\n",
    "        parsed_body = json_module.loads(request_body.decode(\"utf-8\"))\n",
    "\n",
    "        input_value = float(parsed_body.get(\"input_value\", 0.0))\n",
    "        prediction_value = simple_predict_function(input_value)\n",
    "        response_body = {\"prediction_value\": prediction_value}\n",
    "\n",
    "        response_bytes = json_module.dumps(response_body).encode(\"utf-8\")\n",
    "        self.send_response(200)\n",
    "        self.send_header(\"Content-Type\", \"application/json\")\n",
    "        self.send_header(\"Content-Length\", str(len(response_bytes)))\n",
    "        self.end_headers()\n",
    "        self.wfile.write(response_bytes)\n",
    "\n",
    "    # Suppress default logging for cleaner beginner friendly output.\n",
    "    def log_message(self, format, *args):\n",
    "        return\n",
    "\n",
    "# Define a function that starts a REST server on a dedicated port.\n",
    "def start_rest_server(server_port, server_ready_event):\n",
    "    handler_class = RestPredictionHandler\n",
    "    httpd_server = socket_server.TCPServer((\"localhost\", server_port), handler_class)\n",
    "    server_ready_event.set()\n",
    "    httpd_server.serve_forever()\n",
    "\n",
    "# Define a function that simulates a REST client sending JSON payloads.\n",
    "def rest_client_request(server_port, input_value):\n",
    "    import urllib.request as urllib_request\n",
    "    url_string = f\"http://localhost:{server_port}/predict\"\n",
    "    request_body = {\"input_value\": float(input_value)}\n",
    "\n",
    "    encoded_body = json_module.dumps(request_body).encode(\"utf-8\")\n",
    "    request_object = urllib_request.Request(url_string, data=encoded_body)\n",
    "    request_object.add_header(\"Content-Type\", \"application/json\")\n",
    "    start_time = time_module.time()\n",
    "\n",
    "    with urllib_request.urlopen(request_object) as response_object:\n",
    "        response_data = response_object.read().decode(\"utf-8\")\n",
    "        elapsed_time = time_module.time() - start_time\n",
    "        parsed_response = json_module.loads(response_data)\n",
    "\n",
    "    return parsed_response[\"prediction_value\"], elapsed_time\n",
    "\n",
    "# Define a function that simulates gRPC style binary payload handling.\n",
    "def grpc_style_call_simulation(input_value):\n",
    "    import struct as struct_module\n",
    "    packed_bytes = struct_module.pack(\"!f\", float(input_value))\n",
    "    unpacked_value = struct_module.unpack(\"!f\", packed_bytes)[0]\n",
    "\n",
    "    start_time = time_module.time()\n",
    "    prediction_value = simple_predict_function(unpacked_value)\n",
    "    elapsed_time = time_module.time() - start_time\n",
    "    return prediction_value, elapsed_time\n",
    "\n",
    "# Define the main demonstration function orchestrating both communication styles.\n",
    "def main_demonstration_function():\n",
    "    rest_port = 8501\n",
    "    server_ready_event = threading_module.Event()\n",
    "    server_thread = threading_module.Thread(target=start_rest_server, args=(rest_port, server_ready_event))\n",
    "\n",
    "    server_thread.daemon = True\n",
    "    server_thread.start()\n",
    "    server_ready_event.wait(timeout=5.0)\n",
    "\n",
    "    test_input_value = 3.0\n",
    "    rest_prediction, rest_latency = rest_client_request(rest_port, test_input_value)\n",
    "    grpc_prediction, grpc_latency = grpc_style_call_simulation(test_input_value)\n",
    "\n",
    "    print(\"REST endpoint configured on port\", rest_port)\n",
    "    print(\"REST request input value\", test_input_value)\n",
    "    print(\"REST response prediction value\", rest_prediction)\n",
    "    print(\"REST measured latency seconds\", round(rest_latency, 6))\n",
    "\n",
    "    print(\"gRPC style simulated call used binary payloads\")\n",
    "    print(\"gRPC style input value\", test_input_value)\n",
    "    print(\"gRPC style prediction value\", grpc_prediction)\n",
    "    print(\"gRPC style latency seconds\", round(grpc_latency, 6))\n",
    "\n",
    "    print(\"Both interfaces share prediction logic and configuration concepts\")\n",
    "\n",
    "# Execute the main demonstration function when script is run directly.\n",
    "main_demonstration_function()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef795c18",
   "metadata": {},
   "source": [
    "## **2. Designing Prediction APIs**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f934c80b",
   "metadata": {},
   "source": [
    "### **2.1. JSON Request Design**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfa76c1",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_09/Lecture_B/image_02_01.jpg?v=1768987430\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Define clear, stable JSON fields and structure\n",
    ">* Separate metadata from features to reduce integration errors\n",
    "\n",
    ">* Clearly represent single or batched JSON inputs\n",
    ">* Consistently encode types, missing data, and nesting\n",
    "\n",
    ">* Design JSON for future changes and versions\n",
    ">* Use optional fields, versioning, and clear documentation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a09a48",
   "metadata": {},
   "source": [
    "### **2.2. Service Layer Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffaa19f",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_09/Lecture_B/image_02_02.jpg?v=1768987452\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Service layer cleans and validates incoming JSON\n",
    ">* Centralized preprocessing shields model and standardizes inputs\n",
    "\n",
    ">* Service layer converts client JSON into model features\n",
    ">* Mirrors training preprocessing to prevent distribution shifts\n",
    "\n",
    ">* Service layer enforces rules, privacy, and enrichment\n",
    ">* Creates canonical, auditable interface while isolating model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6ed6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Service Layer Preprocessing\n",
    "\n",
    "# This script demonstrates simple service layer preprocessing for JSON prediction requests.\n",
    "# It shows validation, default handling, and conversion into model ready tensors.\n",
    "# It mimics a small API layer sitting between clients and a TensorFlow model.\n",
    "\n",
    "# !pip install tensorflow\n",
    "\n",
    "# Import required standard libraries and TensorFlow framework for tensor handling.\n",
    "import json, os, random, numpy as np, tensorflow as tf\n",
    "\n",
    "# Set deterministic random seeds for reproducible behavior across different runtime sessions.\n",
    "random.seed(42); np.random.seed(42); tf.random.set_seed(42)\n",
    "\n",
    "# Print TensorFlow version information for clarity about the used deep learning framework.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Define a simple fake model function that expects numeric tensors as model inputs.\n",
    "def fake_model_predict(feature_tensor: tf.Tensor) -> tf.Tensor:\n",
    "    weights = tf.constant([[0.5], [1.0], [-0.25], [0.1]], dtype=tf.float32)\n",
    "    bias = tf.constant([0.2], dtype=tf.float32)\n",
    "    logits = tf.matmul(feature_tensor, weights) + bias\n",
    "    return tf.sigmoid(logits)\n",
    "\n",
    "# Define a preprocessing function that validates and transforms incoming JSON like payloads.\n",
    "def preprocess_request(json_payload: dict) -> tf.Tensor:\n",
    "    required_fields = [\"age_years\", \"miles_driven_weekly\", \"vehicle_type\", \"temperature_fahrenheit\"]\n",
    "    for field in required_fields:\n",
    "        if field not in json_payload:\n",
    "            raise ValueError(f\"Missing required field: {field}\")\n",
    "\n",
    "    age = float(json_payload.get(\"age_years\", 0.0))\n",
    "    miles = float(json_payload.get(\"miles_driven_weekly\", 0.0))\n",
    "    temp_f = float(json_payload.get(\"temperature_fahrenheit\", 70.0))\n",
    "\n",
    "    vehicle_raw = str(json_payload.get(\"vehicle_type\", \"car\")).lower()\n",
    "    vehicle_map = {\"car\": 0, \"truck\": 1, \"motorcycle\": 2}\n",
    "    vehicle_index = vehicle_map.get(vehicle_raw, 0)\n",
    "\n",
    "    if age < 16 or age > 100:\n",
    "        raise ValueError(\"Age outside allowed driving range bounds\")\n",
    "\n",
    "    miles_clipped = max(0.0, min(miles, 3000.0))\n",
    "    temp_celsius = (temp_f - 32.0) * (5.0 / 9.0)\n",
    "\n",
    "    age_scaled = age / 100.0\n",
    "    miles_scaled = miles_clipped / 3000.0\n",
    "    temp_scaled = (temp_celsius + 40.0) / 80.0\n",
    "\n",
    "    feature_list = [age_scaled, miles_scaled, float(vehicle_index), temp_scaled]\n",
    "    feature_array = np.array([feature_list], dtype=np.float32)\n",
    "\n",
    "    if feature_array.shape != (1, 4):\n",
    "        raise ValueError(f\"Unexpected feature shape: {feature_array.shape}\")\n",
    "\n",
    "    return tf.convert_to_tensor(feature_array, dtype=tf.float32)\n",
    "\n",
    "# Define a helper function that simulates a prediction API endpoint handler behavior.\n",
    "def handle_prediction_request(raw_json_string: str) -> float:\n",
    "    parsed = json.loads(raw_json_string)\n",
    "    features_tensor = preprocess_request(parsed)\n",
    "    prediction_tensor = fake_model_predict(features_tensor)\n",
    "    prediction_value = float(prediction_tensor.numpy()[0][0])\n",
    "    return prediction_value\n",
    "\n",
    "# Create two example JSON payloads representing realistic but simple driving related requests.\n",
    "example_request_valid = json.dumps({\"age_years\": 30, \"miles_driven_weekly\": 250, \"vehicle_type\": \"truck\", \"temperature_fahrenheit\": 86})\n",
    "\n",
    "# Create another payload with slightly different values to show preprocessing transformations.\n",
    "example_request_second = json.dumps({\"age_years\": 45, \"miles_driven_weekly\": 1200, \"vehicle_type\": \"car\", \"temperature_fahrenheit\": 50})\n",
    "\n",
    "# Run the handler on the first request and print the preprocessed features and prediction.\n",
    "features_one = preprocess_request(json.loads(example_request_valid))\n",
    "\n",
    "# Run the handler on the second request and print the preprocessed features and prediction.\n",
    "features_two = preprocess_request(json.loads(example_request_second))\n",
    "\n",
    "# Compute predictions using the fake model for both preprocessed feature tensors.\n",
    "prediction_one = fake_model_predict(features_one).numpy()[0][0]\n",
    "\n",
    "# Compute the second prediction value and convert it into a regular Python float.\n",
    "prediction_two = float(fake_model_predict(features_two).numpy()[0][0])\n",
    "\n",
    "# Print concise information about preprocessing results and final prediction outputs.\n",
    "print(\"First request features:\", features_one.numpy())\n",
    "\n",
    "# Print the second feature tensor and both prediction values for comparison demonstration.\n",
    "print(\"Second request features:\", features_two.numpy())\n",
    "\n",
    "# Print final prediction probabilities which represent model outputs after preprocessing.\n",
    "print(\"Prediction one probability:\", round(float(prediction_one), 4))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817373de",
   "metadata": {},
   "source": [
    "### **2.3. Postprocessing Model Outputs**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0db3ff7",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_09/Lecture_B/image_02_03.jpg?v=1768987501\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Convert raw model outputs into human-friendly results\n",
    ">* Service layer maps outputs to client-ready responses\n",
    "\n",
    ">* Postprocessing applies business, policy, and privacy rules\n",
    ">* Centralized rules simplify clients and improve API safety\n",
    "\n",
    ">* Design postprocessing for speed, clarity, and scale\n",
    ">* Make steps observable, testable, and pipeline-first\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9481b496",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Postprocessing Model Outputs\n",
    "\n",
    "# This script demonstrates simple model output postprocessing for a prediction API response.\n",
    "# It converts raw logits into probabilities and human readable labels for clients.\n",
    "# It shows how a service layer shapes safe structured JSON style outputs.\n",
    "\n",
    "# !pip install tensorflow==2.20.0\n",
    "\n",
    "# Import required standard libraries and TensorFlow framework.\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set deterministic seeds for reproducible random behavior in this example.\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Print TensorFlow version information for environment transparency and debugging.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Define fixed label names that a classification service might expose to clients.\n",
    "label_names = [\"cat\", \"dog\", \"car\", \"tree\"]\n",
    "\n",
    "# Validate that label names length matches expected number of classes here.\n",
    "num_classes = 4\n",
    "assert len(label_names) == num_classes\n",
    "\n",
    "# Simulate raw model logits output for a single prediction request.\n",
    "logits = tf.constant([[1.2, 0.3, -0.5, 2.0]], dtype=tf.float32)\n",
    "\n",
    "# Convert logits into probabilities using softmax for interpretable confidences.\n",
    "probabilities = tf.nn.softmax(logits, axis=-1)\n",
    "\n",
    "# Ensure probabilities shape matches batch size and class count expectations.\n",
    "assert probabilities.shape == (1, num_classes)\n",
    "\n",
    "# Convert probabilities tensor into a flat NumPy array for easier processing.\n",
    "prob_array = probabilities.numpy()[0]\n",
    "\n",
    "# Select top K predictions indices sorted by probability descending order.\n",
    "top_k = 2\n",
    "sorted_indices = np.argsort(prob_array)[::-1][:top_k]\n",
    "\n",
    "# Build structured prediction entries with labels and confidence scores.\n",
    "predictions_list = []\n",
    "for index in sorted_indices:\n",
    "    label = label_names[index]\n",
    "    confidence = float(prob_array[index])\n",
    "    predictions_list.append({\"label\": label, \"confidence\": round(confidence, 4)})\n",
    "\n",
    "# Create a mock postprocessed response payload similar to JSON API output.\n",
    "response_payload = {\n",
    "    \"model_name\": \"toy_classifier_v1\",\n",
    "    \"model_version\": \"1.0.0\",\n",
    "    \"predictions\": predictions_list,\n",
    "}\n",
    "\n",
    "# Serialize payload to JSON string for logging or HTTP response body usage.\n",
    "response_json = json.dumps(response_payload, indent=2)\n",
    "\n",
    "# Print a short explanation header and the final postprocessed JSON payload.\n",
    "print(\"\\nPostprocessed prediction response payload:\")\n",
    "print(response_json)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c59b70",
   "metadata": {},
   "source": [
    "## **3. Serving Performance Essentials**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d193b6",
   "metadata": {},
   "source": [
    "### **3.1. Efficient Request Batching**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fa86b1",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_09/Lecture_B/image_03_01.jpg?v=1768987541\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Batching groups many requests for one inference\n",
    ">* Larger batches boost hardware use and throughput\n",
    "\n",
    ">* Batching trades higher throughput for added waiting time\n",
    ">* Tune batch size and wait by use case\n",
    "\n",
    ">* Use dynamic batching with size and time limits\n",
    ">* Monitor metrics and tune queues for traffic\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a927f432",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Efficient Request Batching\n",
    "\n",
    "# This script compares batched and unbatched inference latency clearly.\n",
    "# It simulates a simple model server using TensorFlow dense layers.\n",
    "# It measures throughput and average latency for different batch sizes.\n",
    "\n",
    "# !pip install tensorflow\n",
    "\n",
    "# Import required modules including TensorFlow and time measurement.\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Set deterministic random seeds for reproducible behavior always.\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Print TensorFlow version information for educational clarity here.\n",
    "print(\"Using TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Define simple dense model representing a served prediction model.\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(32,)),\n",
    "    tf.keras.layers.Dense(16, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(8, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1, activation=\"linear\"),\n",
    "])\n",
    "\n",
    "# Compile model with mean squared error loss and adam optimizer.\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "\n",
    "# Create tiny random training data for quick dummy training.\n",
    "train_features = np.random.rand(256, 32).astype(\"float32\")\n",
    "train_targets = np.random.rand(256, 1).astype(\"float32\")\n",
    "\n",
    "# Train model briefly to initialize weights realistically and deterministically.\n",
    "model.fit(train_features, train_targets, epochs=2, batch_size=32, verbose=0)\n",
    "\n",
    "# Define function measuring latency and throughput for given batch size.\n",
    "def measure_performance(batch_size, total_requests, warmup_batches):\n",
    "    input_shape = (batch_size, 32)\n",
    "    dummy_input = np.random.rand(*input_shape).astype(\"float32\")\n",
    "    assert dummy_input.shape[1] == 32\n",
    "\n",
    "    for _ in range(warmup_batches):\n",
    "        _ = model.predict(dummy_input, verbose=0)\n",
    "\n",
    "    batches_needed = total_requests // batch_size\n",
    "    remaining_requests = total_requests % batch_size\n",
    "\n",
    "    latencies = []\n",
    "    processed_requests = 0\n",
    "\n",
    "    for _ in range(batches_needed):\n",
    "        start_time = time.time()\n",
    "        _ = model.predict(dummy_input, verbose=0)\n",
    "        end_time = time.time()\n",
    "        latencies.append(end_time - start_time)\n",
    "        processed_requests += batch_size\n",
    "\n",
    "    if remaining_requests > 0:\n",
    "        small_input = dummy_input[:remaining_requests]\n",
    "        start_time = time.time()\n",
    "        _ = model.predict(small_input, verbose=0)\n",
    "        end_time = time.time()\n",
    "        latencies.append(end_time - start_time)\n",
    "        processed_requests += remaining_requests\n",
    "\n",
    "    total_time = sum(latencies)\n",
    "    average_latency = total_time / len(latencies)\n",
    "    throughput = processed_requests / total_time\n",
    "\n",
    "    return average_latency, throughput\n",
    "\n",
    "# Configure experiment parameters for total requests and warmup batches.\n",
    "total_requests = 1024\n",
    "warmup_batches = 3\n",
    "\n",
    "# Define batch sizes list including unbatched and larger batched cases.\n",
    "batch_sizes = [1, 8, 32, 128]\n",
    "\n",
    "# Print header describing columns for latency and throughput results.\n",
    "print(\"BatchSize  AvgLatencySeconds  ThroughputRequestsPerSecond\")\n",
    "\n",
    "# Loop over batch sizes and measure performance metrics for each.\n",
    "for batch_size in batch_sizes:\n",
    "    avg_latency, throughput = measure_performance(batch_size, total_requests, warmup_batches)\n",
    "    print(batch_size, round(avg_latency, 5), round(throughput, 2))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7c3c11",
   "metadata": {},
   "source": [
    "### **3.2. Latency and Throughput Metrics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fa51bf",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_09/Lecture_B/image_03_02.jpg?v=1768987616\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Latency and throughput define speed and capacity\n",
    ">* These metrics guide UX, hardware, and scaling decisions\n",
    "\n",
    ">* Latency is a distribution; percentiles reveal tails\n",
    ">* Different applications care about different latency percentiles\n",
    "\n",
    ">* Throughput is sustained request rate under latency limits\n",
    ">* Load testing reveals safe capacity and scaling needs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3703863a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Latency and Throughput Metrics\n",
    "\n",
    "import time\n",
    "import statistics\n",
    "import random\n",
    "\n",
    "# Define a function to simulate request latency measurements.\n",
    "# This function returns a list of simulated latencies.\n",
    "# Latencies are measured in milliseconds units.\n",
    "# We use random variations for realism.\n",
    "# Seeded randomness ensures reproducible behavior.\n",
    "\n",
    "def simulate_latencies(request_count, base_latency_ms, jitter_ms):\n",
    "    latencies = []\n",
    "    for _ in range(request_count):\n",
    "        jitter = random.uniform(-jitter_ms, jitter_ms)\n",
    "        latency = max(1.0, base_latency_ms + jitter)\n",
    "        latencies.append(latency)\n",
    "    return latencies\n",
    "\n",
    "# Define a function to compute latency distribution metrics.\n",
    "# We compute average and median latencies.\n",
    "# We also compute p95 and p99 tail latencies.\n",
    "# Percentiles help understand slowest requests.\n",
    "# We return metrics inside a dictionary.\n",
    "\n",
    "def compute_latency_metrics(latencies):\n",
    "    sorted_latencies = sorted(latencies)\n",
    "    count = len(sorted_latencies)\n",
    "    average_latency = statistics.mean(sorted_latencies)\n",
    "    median_latency = statistics.median(sorted_latencies)\n",
    "    index_p95 = int(0.95 * (count - 1))\n",
    "    index_p99 = int(0.99 * (count - 1))\n",
    "    p95_latency = sorted_latencies[index_p95]\n",
    "    p99_latency = sorted_latencies[index_p99]\n",
    "    metrics = {\n",
    "        \"average_ms\": average_latency,\n",
    "        \"median_ms\": median_latency,\n",
    "        \"p95_ms\": p95_latency,\n",
    "        \"p99_ms\": p99_latency,\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "# Define a function to simulate throughput under increasing load.\n",
    "# We increase request counts to mimic higher traffic.\n",
    "# We measure total time for processing requests.\n",
    "# Throughput is requests per second metric.\n",
    "# We also track latency metrics for each load.\n",
    "\n",
    "def evaluate_throughput_scenarios():\n",
    "    random.seed(42)\n",
    "    base_latency_ms = 80.0\n",
    "    jitter_ms = 40.0\n",
    "    request_scenarios = [50, 200, 800, 2000]\n",
    "    results = []\n",
    "    for request_count in request_scenarios:\n",
    "        start_time = time.time()\n",
    "        latencies = simulate_latencies(request_count, base_latency_ms, jitter_ms)\n",
    "        end_time = time.time()\n",
    "        elapsed_seconds = max(1e-6, end_time - start_time)\n",
    "        throughput_rps = request_count / elapsed_seconds\n",
    "        metrics = compute_latency_metrics(latencies)\n",
    "        scenario_result = {\n",
    "            \"requests\": request_count,\n",
    "            \"throughput_rps\": throughput_rps,\n",
    "            \"average_ms\": metrics[\"average_ms\"],\n",
    "            \"median_ms\": metrics[\"median_ms\"],\n",
    "            \"p95_ms\": metrics[\"p95_ms\"],\n",
    "            \"p99_ms\": metrics[\"p99_ms\"],\n",
    "        }\n",
    "        results.append(scenario_result)\n",
    "    return results\n",
    "\n",
    "# Run the throughput evaluation and print concise results.\n",
    "# This demonstrates latency distribution behavior.\n",
    "# It also shows throughput changes with load.\n",
    "# We keep printed lines under fifteen.\n",
    "# Values are rounded for readability.\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scenarios = evaluate_throughput_scenarios()\n",
    "    print(\"Requests  Throughput_rps  Avg_ms  Median_ms  P95_ms  P99_ms\")\n",
    "    for scenario in scenarios:\n",
    "        print(\n",
    "            f\"{scenario['requests']:7d}  \"\n",
    "            f\"{scenario['throughput_rps']:13.1f}  \"\n",
    "            f\"{scenario['average_ms']:6.1f}  \"\n",
    "            f\"{scenario['median_ms']:9.1f}  \"\n",
    "            f\"{scenario['p95_ms']:6.1f}  \"\n",
    "            f\"{scenario['p99_ms']:6.1f}\"\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7950e00d",
   "metadata": {},
   "source": [
    "### **3.3. Scaling Model Replicas**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0ad6c4",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_09/Lecture_B/image_03_03.jpg?v=1768987653\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Run multiple replicas behind a load balancer\n",
    ">* More replicas increase capacity and keep latency low\n",
    "\n",
    ">* More replicas hit hardware and data bottlenecks\n",
    ">* Continuously measure latency and throughput when scaling\n",
    "\n",
    ">* Autoscaling adjusts replicas using real-time performance signals\n",
    ">* Tune replica counts to balance latency and cost\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3864f41b",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Serving and APIs**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d15cb86",
   "metadata": {},
   "source": [
    "\n",
    "In this lecture, you learned to:\n",
    "- Deploy a SavedModel using TensorFlow Serving or a similar serving stack. \n",
    "- Expose a prediction API endpoint that accepts JSON inputs and returns model outputs. \n",
    "- Evaluate basic performance characteristics of a served model, including latency and throughput. \n",
    "\n",
    "In the next Module (Module 10), we will go over 'Advanced Topics'"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
