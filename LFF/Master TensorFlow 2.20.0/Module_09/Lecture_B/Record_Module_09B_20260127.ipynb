{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62af0be8",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Serving and APIs**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8bc7eb",
   "metadata": {},
   "source": [
    ">Last update: 20260127.\n",
    "    \n",
    "By the end of this Lecture, you will be able to:\n",
    "- Deploy a SavedModel using TensorFlow Serving or a similar serving stack. \n",
    "- Expose a prediction API endpoint that accepts JSON inputs and returns model outputs. \n",
    "- Evaluate basic performance characteristics of a served model, including latency and throughput. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034005e1",
   "metadata": {},
   "source": [
    "## **1. TensorFlow Serving Essentials**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad90e493",
   "metadata": {},
   "source": [
    "### **1.1. Dockerized TF Serving**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2cdeae",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_09/Lecture_B/image_01_01.jpg?v=1769540523\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Prebuilt containers run models consistently across environments\n",
    ">* They load SavedModels and expose simple network endpoints\n",
    "\n",
    ">* Containers simplify scaling and managing model servers\n",
    ">* Reuse existing orchestration tools, improving reliability and collaboration\n",
    "\n",
    ">* Separates models, serving config, and applications cleanly\n",
    ">* Enables independent team workflows with predictable runtime\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d030b4a",
   "metadata": {},
   "source": [
    "### **1.2. Model Versioning Basics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde344b6",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_09/Lecture_B/image_01_02.jpg?v=1769540538\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Versioning production models ensures stability and safety\n",
    ">* Numbered directories let Serving switch versions transparently\n",
    "\n",
    ">* Each version folder stores a full SavedModel\n",
    ">* Supports safe upgrades, rollbacks, and validation runs\n",
    "\n",
    ">* Versioning enables A/B tests and gradual rollouts\n",
    ">* Policies ensure traceability, compliance, and reproducibility\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00a711c",
   "metadata": {},
   "source": [
    "### **1.3. REST and gRPC Setup**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd817e49",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_09/Lecture_B/image_01_03.jpg?v=1769540567\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* TensorFlow Serving exposes models via REST or gRPC\n",
    ">* REST favors simplicity; gRPC favors performance efficiency\n",
    "\n",
    ">* REST is easiest for early testing, collaboration\n",
    ">* JSON overhead can hurt performance at scale\n",
    "\n",
    ">* gRPC adds complexity but strong, efficient contracts\n",
    ">* Use REST for simplicity, gRPC for performance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c08fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - REST and gRPC Setup\n",
    "\n",
    "# This script shows simple REST and gRPC ideas.\n",
    "# We simulate TensorFlow Serving style prediction calls.\n",
    "# Focus is on beginner friendly client side usage.\n",
    "\n",
    "# Required external libraries would be installed like this.\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import standard libraries for HTTP and environment handling.\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "\n",
    "# Import TensorFlow to show version and simple model.\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set deterministic seeds for reproducible behavior.\n",
    "random.seed(7)\n",
    "tf.random.set_seed(7)\n",
    "\n",
    "# Print TensorFlow version in one concise line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Build a tiny example model for demonstration.\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(3,)),\n",
    "    tf.keras.layers.Dense(2, activation=\"linear\"),\n",
    "])\n",
    "\n",
    "# Compile the model with simple configuration.\n",
    "model.compile(optimizer=\"sgd\", loss=\"mse\")\n",
    "\n",
    "# Create a tiny deterministic training dataset.\n",
    "x_train = tf.constant([[0.0, 1.0, 2.0]], dtype=tf.float32)\n",
    "y_train = tf.constant([[1.0, 0.0]], dtype=tf.float32)\n",
    "\n",
    "# Train briefly with silent output to avoid logs.\n",
    "model.fit(x_train, y_train, epochs=5, verbose=0)\n",
    "\n",
    "# Save the model in SavedModel format to disk.\n",
    "saved_path = \"demo_saved_model\"\n",
    "model.export(saved_path)\n",
    "\n",
    "# Prepare a small example input for prediction.\n",
    "example_input = [0.5, 1.5, 2.5]\n",
    "\n",
    "# Build a JSON body similar to REST predict requests.\n",
    "rest_body = {\n",
    "    \"signature_name\": \"serving_default\",\n",
    "    \"instances\": [example_input],\n",
    "}\n",
    "\n",
    "# Serialize the REST body to a JSON string.\n",
    "rest_json = json.dumps(rest_body)\n",
    "\n",
    "# Show how a REST request might look conceptually.\n",
    "print(\"REST POST /v1/models/demo:predict body:\")\n",
    "print(rest_json)\n",
    "\n",
    "# Run a local prediction to compare with server output.\n",
    "local_pred = model.predict(\n",
    "    tf.constant([example_input], dtype=tf.float32), verbose=0\n",
    ")\n",
    "\n",
    "# Convert prediction to a regular Python list.\n",
    "local_pred_list = local_pred.tolist()\n",
    "\n",
    "# Show the local prediction that REST or gRPC would return.\n",
    "print(\"Local prediction result (simulating server):\")\n",
    "print(local_pred_list)\n",
    "\n",
    "# Build a simple dictionary similar to gRPC protobuf message.\n",
    "grpc_request = {\n",
    "    \"model_spec\": {\"name\": \"demo\", \"signature_name\": \"serving_default\"},\n",
    "    \"inputs\": {\"input_1\": example_input},\n",
    "}\n",
    "\n",
    "# Show the conceptual gRPC style request payload.\n",
    "print(\"gRPC style request dictionary (conceptual):\")\n",
    "print(grpc_request)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7085bb",
   "metadata": {},
   "source": [
    "## **2. Designing Prediction APIs**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6eea7e1",
   "metadata": {},
   "source": [
    "### **2.1. JSON Request Design**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a7fdb3",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_09/Lecture_B/image_02_01.jpg?v=1769540628\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Define clear, stable JSON fields for features\n",
    ">* Consistent structure reduces ambiguity and integration bugs\n",
    "\n",
    ">* Support clear single and batched prediction requests\n",
    ">* Separate optional from required fields and validate inputs\n",
    "\n",
    ">* Design extensible requests with versions and metadata\n",
    ">* Keep structure simple, consistent, and language-agnostic\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37831d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - JSON Request Design\n",
    "\n",
    "# This script teaches JSON request design basics.\n",
    "# We simulate a tiny prediction API using Flask.\n",
    "# Focus is on clear request structure and validation.\n",
    "\n",
    "# !pip install flask.\n",
    "\n",
    "# Import standard libraries for JSON handling.\n",
    "import json\n",
    "import random\n",
    "import textwrap\n",
    "\n",
    "# Import Flask for a simple local style API.\n",
    "from flask import Flask, request, jsonify\n",
    "\n",
    "# Create a Flask app instance for demonstration.\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Set a deterministic seed for reproducible behavior.\n",
    "random.seed(42)\n",
    "\n",
    "# Define required feature names for our fake model.\n",
    "REQUIRED_FEATURES = [\"transaction_amount\", \"merchant_category\", \"timestamp\"]\n",
    "\n",
    "# Define optional feature names for extra context.\n",
    "OPTIONAL_FEATURES = [\"user_id\", \"device_type\"]\n",
    "\n",
    "# Define supported API version identifiers for requests.\n",
    "SUPPORTED_VERSIONS = [\"v1\"]\n",
    "\n",
    "# Build a helper function to validate incoming JSON.\n",
    "def validate_request(payload: dict) -> tuple[bool, str]:\n",
    "    # Check that payload is a dictionary like JSON object.\n",
    "    if not isinstance(payload, dict):\n",
    "        return False, \"Payload must be a JSON object.\"\n",
    "\n",
    "    # Extract version field with a safe default value.\n",
    "    version = payload.get(\"version\", \"v1\")\n",
    "\n",
    "    # Ensure the version is supported by this service.\n",
    "    if version not in SUPPORTED_VERSIONS:\n",
    "        return False, \"Unsupported version in request payload.\"\n",
    "\n",
    "    # Extract records list which holds feature dictionaries.\n",
    "    records = payload.get(\"records\")\n",
    "\n",
    "    # Ensure records is a non empty list of items.\n",
    "    if not isinstance(records, list) or len(records) == 0:\n",
    "        return False, \"Field 'records' must be a non empty list.\"\n",
    "\n",
    "    # Validate each record for required feature presence.\n",
    "    for index, record in enumerate(records):\n",
    "        if not isinstance(record, dict):\n",
    "            return False, f\"Record {index} must be an object.\"\n",
    "        for feature in REQUIRED_FEATURES:\n",
    "            if feature not in record:\n",
    "                return False, f\"Missing required feature: {feature}.\"\n",
    "\n",
    "    # Return success flag and empty message when valid.\n",
    "    return True, \"\"\n",
    "\n",
    "# Define a fake model prediction using simple logic.\n",
    "def fake_model_predict(records: list[dict]) -> list[float]:\n",
    "    # Compute a simple score using transaction amount feature.\n",
    "    scores = []\n",
    "    for record in records:\n",
    "        amount = float(record[\"transaction_amount\"])\n",
    "        base_score = min(amount / 100.0, 1.0)\n",
    "        noise = 0.01 * random.random()\n",
    "        scores.append(round(base_score + noise, 3))\n",
    "    return scores\n",
    "\n",
    "# Define the prediction endpoint to handle JSON requests.\n",
    "@app.route(\"/predict\", methods=[\"POST\"])\n",
    "def predict_endpoint():\n",
    "    # Parse JSON payload safely from the incoming request.\n",
    "    payload = request.get_json(silent=True)\n",
    "\n",
    "    # Validate payload structure and required fields.\n",
    "    is_valid, message = validate_request(payload)\n",
    "\n",
    "    # Return error response when validation fails early.\n",
    "    if not is_valid:\n",
    "        return jsonify({\"error\": message}), 400\n",
    "\n",
    "    # Extract records and optional metadata from payload.\n",
    "    records = payload[\"records\"]\n",
    "    metadata = payload.get(\"metadata\", {})\n",
    "\n",
    "    # Run fake model prediction on provided records.\n",
    "    scores = fake_model_predict(records)\n",
    "\n",
    "    # Build response with predictions and echoed metadata.\n",
    "    response = {\"predictions\": scores, \"metadata\": metadata}\n",
    "    return jsonify(response), 200\n",
    "\n",
    "# Build a helper to pretty print example JSON payloads.\n",
    "def show_example_request() -> None:\n",
    "    # Create a single record with required and optional fields.\n",
    "    example_record = {\n",
    "        \"transaction_amount\": 42.5,\n",
    "        \"merchant_category\": \"grocery\",\n",
    "        \"timestamp\": \"2024-01-01T12:00:00Z\",\n",
    "        \"user_id\": \"user_123\",\n",
    "        \"device_type\": \"mobile\",\n",
    "    }\n",
    "\n",
    "    # Wrap record in a batched style request structure.\n",
    "    example_request = {\n",
    "        \"version\": \"v1\",\n",
    "        \"metadata\": {\"request_id\": \"demo_001\", \"locale\": \"en_US\"},\n",
    "        \"records\": [example_record],\n",
    "    }\n",
    "\n",
    "    # Convert dictionary to a formatted JSON string.\n",
    "    json_text = json.dumps(example_request, indent=2)\n",
    "\n",
    "    # Wrap text to keep printed lines reasonably short.\n",
    "    wrapped = textwrap.indent(json_text, prefix=\"  \")\n",
    "\n",
    "    # Print a short explanation and the JSON example.\n",
    "    print(\"Example JSON request body for /predict endpoint:\")\n",
    "    print(wrapped)\n",
    "\n",
    "# Only run demonstration code when executed directly.\n",
    "if __name__ == \"__main__\":\n",
    "    # Print a short description of the API contract.\n",
    "    print(\"This demo shows a clear JSON request design.\")\n",
    "\n",
    "    # Show the example request structure for beginners.\n",
    "    show_example_request()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f55c3f2",
   "metadata": {},
   "source": [
    "### **2.2. Service Layer Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505b8c27",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_09/Lecture_B/image_02_02.jpg?v=1769540659\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Service layer cleans and standardizes messy inputs\n",
    ">* It protects the model from malformed, inconsistent data\n",
    "\n",
    ">* Service layer converts raw JSON into features\n",
    ">* Reapplies training transformations to match model expectations\n",
    "\n",
    ">* Service layer adds business rules and context\n",
    ">* Handles logging, rate limits, routing, and evolution\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326ddc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Service Layer Preprocessing\n",
    "\n",
    "# This script shows simple service layer preprocessing.\n",
    "# We simulate a JSON request and clean its fields.\n",
    "# Focus on mapping raw inputs to model features.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Import tensorflow for tensor creation.\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set deterministic random seeds.\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Print TensorFlow version once.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Define a simple feature specification dictionary.\n",
    "FEATURE_SPEC = {\n",
    "    \"income\": {\"required\": True, \"default\": 0.0},\n",
    "    \"employment_years\": {\"required\": True, \"default\": 0.0},\n",
    "    \"requested_amount\": {\"required\": True, \"default\": 0.0},\n",
    "}\n",
    "\n",
    "# Define a simple currency conversion mapping.\n",
    "CURRENCY_RATES = {\"USD\": 1.0, \"EUR\": 1.1, \"GBP\": 1.3}\n",
    "\n",
    "# Define a helper to safely get numeric values.\n",
    "def _safe_float(value, default):\n",
    "    try:\n",
    "        return float(value)\n",
    "    except (TypeError, ValueError):\n",
    "        return float(default)\n",
    "\n",
    "# Define a function to normalize numeric features.\n",
    "def _normalize(value, mean, std):\n",
    "    if std == 0.0:\n",
    "        return 0.0\n",
    "    return (value - mean) / std\n",
    "\n",
    "# Define training time statistics for normalization.\n",
    "TRAIN_STATS = {\n",
    "    \"income\": {\"mean\": 50000.0, \"std\": 20000.0},\n",
    "    \"employment_years\": {\"mean\": 5.0, \"std\": 3.0},\n",
    "    \"requested_amount\": {\"mean\": 10000.0, \"std\": 5000.0},\n",
    "}\n",
    "\n",
    "# Define the core service layer preprocessing function.\n",
    "def preprocess_request(raw_json):\n",
    "    if isinstance(raw_json, str):\n",
    "        parsed = json.loads(raw_json)\n",
    "    else:\n",
    "        parsed = dict(raw_json)\n",
    "    features = {}\n",
    "    for name, spec in FEATURE_SPEC.items():\n",
    "        raw_value = parsed.get(name, spec[\"default\"])\n",
    "        numeric = _safe_float(raw_value, spec[\"default\"])\n",
    "        stats = TRAIN_STATS[name]\n",
    "        normalized = _normalize(numeric, stats[\"mean\"], stats[\"std\"])\n",
    "        features[name] = normalized\n",
    "    currency = parsed.get(\"currency\", \"USD\")\n",
    "    rate = CURRENCY_RATES.get(currency, CURRENCY_RATES[\"USD\"])\n",
    "    amount_raw = parsed.get(\"requested_amount\", 0.0)\n",
    "    amount_value = _safe_float(amount_raw, 0.0)\n",
    "    amount_usd = amount_value * rate\n",
    "    stats_amount = TRAIN_STATS[\"requested_amount\"]\n",
    "    features[\"requested_amount\"] = _normalize(\n",
    "        amount_usd, stats_amount[\"mean\"], stats_amount[\"std\"]\n",
    "    )\n",
    "    feature_order = [\"income\", \"employment_years\", \"requested_amount\"]\n",
    "    feature_vector = [features[name] for name in feature_order]\n",
    "    tensor = tf.convert_to_tensor([feature_vector], dtype=tf.float32)\n",
    "    if tensor.shape != (1, 3):\n",
    "        raise ValueError(\"Unexpected tensor shape in preprocessing.\")\n",
    "    return tensor\n",
    "\n",
    "# Define a dummy model that expects three input features.\n",
    "class DummyLoanModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dense = tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.dense(inputs)\n",
    "\n",
    "\n",
    "# Instantiate the dummy model once.\n",
    "model = DummyLoanModel()\n",
    "\n",
    "# Build the model by calling it with a sample tensor.\n",
    "sample_input = tf.zeros((1, 3), dtype=tf.float32)\n",
    "_ = model(sample_input)\n",
    "\n",
    "# Define a function that simulates the prediction endpoint.\n",
    "def predict_from_json(request_json):\n",
    "    features_tensor = preprocess_request(request_json)\n",
    "    prediction = model(features_tensor)\n",
    "    return float(prediction.numpy()[0, 0])\n",
    "\n",
    "# Create a few example JSON like requests.\n",
    "example_requests = [\n",
    "    {\n",
    "        \"income\": 60000,\n",
    "        \"employment_years\": 4,\n",
    "        \"requested_amount\": 8000,\n",
    "        \"currency\": \"USD\",\n",
    "    },\n",
    "    {\n",
    "        \"income\": 45000,\n",
    "        \"employment_years\": 2,\n",
    "        \"requested_amount\": 7000,\n",
    "        \"currency\": \"EUR\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# Loop through examples and show preprocessing and prediction.\n",
    "for idx, req in enumerate(example_requests, start=1):\n",
    "    tensor = preprocess_request(req)\n",
    "    pred = predict_from_json(req)\n",
    "    print(\"Request\", idx, \"features tensor:\", tensor.numpy())\n",
    "    print(\"Request\", idx, \"predicted approval score:\", round(pred, 4))\n",
    "\n",
    "# Print a short message summarizing the lesson.\n",
    "print(\"Service layer converted JSON into stable model inputs.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed2ded5",
   "metadata": {},
   "source": [
    "### **2.3. Interpreting Model Responses**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6112755f",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_09/Lecture_B/image_02_03.jpg?v=1769540736\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Convert raw model numbers into structured information\n",
    ">* Include prediction, alternatives, metadata for context\n",
    "\n",
    ">* Expose uncertainty with probabilities and risk levels\n",
    ">* Balance detail with client needs and usability\n",
    "\n",
    ">* Separate model scores from business rule decisions\n",
    ">* Enable auditing, reproducibility, and flexible downstream logic\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac0e9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Interpreting Model Responses\n",
    "\n",
    "# This script shows interpreting model responses.\n",
    "# We simulate a served classification prediction response.\n",
    "# Focus is on turning scores into clear decisions.\n",
    "\n",
    "# Required TensorFlow version is available by default.\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import standard libraries for typing and math.\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "\n",
    "# Import TensorFlow to align with course context.\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "random.seed(7)\n",
    "tf.random.set_seed(7)\n",
    "\n",
    "# Print TensorFlow version in one concise line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Define class labels for a simple classifier.\n",
    "CLASS_LABELS = [\"cat\", \"dog\", \"rabbit\"]\n",
    "\n",
    "# Define a helper to simulate raw model scores.\n",
    "def simulate_model_scores(num_classes: int) -> list[float]:\n",
    "    # Create small positive scores using random values.\n",
    "    scores = [random.random() + 0.1 for _ in range(num_classes)]\n",
    "    # Validate that we have at least one score.\n",
    "    if len(scores) == 0:\n",
    "        raise ValueError(\"No scores generated for prediction\")\n",
    "    return scores\n",
    "\n",
    "# Define a softmax function to convert scores to probabilities.\n",
    "def softmax(scores: list[float]) -> list[float]:\n",
    "    # Subtract max for numerical stability in exponent.\n",
    "    max_score = max(scores)\n",
    "    exps = [math.exp(s - max_score) for s in scores]\n",
    "    # Compute denominator and avoid division by zero.\n",
    "    denom = sum(exps)\n",
    "    if denom <= 0.0:\n",
    "        raise ValueError(\"Softmax denominator is not positive\")\n",
    "    return [v / denom for v in exps]\n",
    "\n",
    "# Define a function to build an interpretable API style response.\n",
    "def build_prediction_response(probabilities: list[float]) -> dict:\n",
    "    # Validate probability vector length matches labels.\n",
    "    if len(probabilities) != len(CLASS_LABELS):\n",
    "        raise ValueError(\"Probability length does not match labels\")\n",
    "    # Pair labels with probabilities for ranking.\n",
    "    label_probs = list(zip(CLASS_LABELS, probabilities))\n",
    "    # Sort by probability descending for top predictions.\n",
    "    label_probs.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Extract top prediction and confidence score.\n",
    "    top_label, top_prob = label_probs[0]\n",
    "    # Build a ranked list of alternative predictions.\n",
    "    alternatives = [\n",
    "        {\"label\": label, \"probability\": round(prob, 4)}\n",
    "        for label, prob in label_probs\n",
    "    ]\n",
    "\n",
    "    # Map probability into a simple risk style bucket.\n",
    "    if top_prob >= 0.8:\n",
    "        confidence_level = \"high\"\n",
    "    elif top_prob >= 0.5:\n",
    "        confidence_level = \"medium\"\n",
    "    else:\n",
    "        confidence_level = \"low\"\n",
    "\n",
    "    # Build metadata to give context for the decision.\n",
    "    metadata = {\n",
    "        \"model_version\": \"v1.0.0\",\n",
    "        \"request_id\": \"req_12345\",\n",
    "        \"confidence_level\": confidence_level,\n",
    "    }\n",
    "\n",
    "    # Build final response separating model and service fields.\n",
    "    response = {\n",
    "        \"top_prediction\": {\n",
    "            \"label\": top_label,\n",
    "            \"probability\": round(top_prob, 4),\n",
    "        },\n",
    "        \"alternatives\": alternatives,\n",
    "        \"metadata\": metadata,\n",
    "    }\n",
    "    return response\n",
    "\n",
    "# Simulate a client request body with simple JSON.\n",
    "request_body = {\"image_id\": \"img_001\", \"features\": \"dummy\"}\n",
    "\n",
    "# Simulate raw model scores as a serving system might return.\n",
    "raw_scores = simulate_model_scores(num_classes=len(CLASS_LABELS))\n",
    "\n",
    "# Convert raw scores into probabilities using softmax.\n",
    "probabilities = softmax(raw_scores)\n",
    "\n",
    "# Build an interpretable response for the API client.\n",
    "api_response = build_prediction_response(probabilities=probabilities)\n",
    "\n",
    "# Pretty print the request and interpreted response as JSON.\n",
    "print(\"Request JSON:\")\n",
    "print(json.dumps(request_body, indent=2))\n",
    "print(\"Interpreted response JSON:\")\n",
    "print(json.dumps(api_response, indent=2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8011b96a",
   "metadata": {},
   "source": [
    "## **3. Serving Performance Basics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e9e114",
   "metadata": {},
   "source": [
    "### **3.1. Batching for Higher Throughput**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb76cb0",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_09/Lecture_B/image_03_01.jpg?v=1769540770\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Batching groups many requests for parallel processing\n",
    ">* Improves hardware use, throughput, and cost efficiency\n",
    "\n",
    ">* Batching boosts throughput but can increase latency\n",
    ">* Tune batch size and wait time to balance\n",
    "\n",
    ">* Batching benefits depend on model and workload\n",
    ">* Use dynamic batching and monitoring to balance tradeoffs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56b68aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Batching for Higher Throughput\n",
    "\n",
    "# This script demonstrates batching for higher throughput.\n",
    "# We simulate a served model and measure simple performance.\n",
    "# Focus is on latency and throughput with batching.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Import tensorflow and check version.\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "random.seed(42)\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Select device string based on GPU availability.\n",
    "physical_gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "\n",
    "# Choose CPU if no GPU is available.\n",
    "if physical_gpus:\n",
    "    device_name = \"/GPU:0\"\n",
    "else:\n",
    "    device_name = \"/CPU:0\"\n",
    "\n",
    "# Define a tiny dense model to simulate serving.\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(32,)),\n",
    "    tf.keras.layers.Dense(32, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(16, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(4, activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "# Build the model by calling once on dummy data.\n",
    "dummy_input = tf.zeros((1, 32), dtype=tf.float32)\n",
    "\n",
    "# Run a single forward pass to initialize weights.\n",
    "_ = model(dummy_input)\n",
    "\n",
    "# Function to create random request tensors.\n",
    "def make_requests(num_requests, feature_size):\n",
    "    # Create random normal features for each request.\n",
    "    data = tf.random.normal((num_requests, feature_size))\n",
    "    return data\n",
    "\n",
    "# Function to measure latency and throughput.\n",
    "def measure_performance(batch_size, total_requests):\n",
    "    # Validate that total_requests is divisible by batch_size.\n",
    "    if total_requests % batch_size != 0:\n",
    "        raise ValueError(\"total_requests must be divisible by batch_size\")\n",
    "\n",
    "    # Prepare all synthetic requests once.\n",
    "    all_requests = make_requests(total_requests, 32)\n",
    "\n",
    "    # Ensure tensor has expected shape before batching.\n",
    "    if all_requests.shape[1] != 32:\n",
    "        raise ValueError(\"Unexpected feature size for requests\")\n",
    "\n",
    "    # Compute number of batches to process.\n",
    "    num_batches = total_requests // batch_size\n",
    "\n",
    "    # Warm up model with one batch to stabilize.\n",
    "    _ = model(all_requests[:batch_size])\n",
    "\n",
    "    # Measure wall clock time for all batches.\n",
    "    start = time.perf_counter()\n",
    "\n",
    "    # Loop over batches and run predictions.\n",
    "    for i in range(num_batches):\n",
    "        # Slice the current batch from all_requests.\n",
    "        batch = all_requests[i * batch_size:(i + 1) * batch_size]\n",
    "        # Run model prediction inside selected device.\n",
    "        with tf.device(device_name):\n",
    "            _ = model(batch, training=False)\n",
    "\n",
    "    # Compute total elapsed time in seconds.\n",
    "    elapsed = time.perf_counter() - start\n",
    "\n",
    "    # Compute average latency per request in milliseconds.\n",
    "    avg_latency_ms = (elapsed / total_requests) * 1000.0\n",
    "\n",
    "    # Compute throughput as requests per second.\n",
    "    throughput_rps = total_requests / elapsed\n",
    "\n",
    "    # Return metrics as a simple dictionary.\n",
    "    return {\n",
    "        \"batch_size\": batch_size,\n",
    "        \"total_requests\": total_requests,\n",
    "        \"elapsed_sec\": elapsed,\n",
    "        \"avg_latency_ms\": avg_latency_ms,\n",
    "        \"throughput_rps\": throughput_rps,\n",
    "    }\n",
    "\n",
    "# Define total number of synthetic requests.\n",
    "TOTAL_REQUESTS = 512\n",
    "\n",
    "# Define batch sizes to compare for throughput.\n",
    "batch_sizes = [1, 8, 32, 128]\n",
    "\n",
    "# Collect results for each batch size configuration.\n",
    "results = []\n",
    "\n",
    "# Measure performance for each batch size choice.\n",
    "for b in batch_sizes:\n",
    "    metrics = measure_performance(b, TOTAL_REQUESTS)\n",
    "    results.append(metrics)\n",
    "\n",
    "# Print a short header explaining the metrics.\n",
    "print(\"Batch size, avg latency (ms), throughput (req/s)\")\n",
    "\n",
    "# Print one summary line per batch size configuration.\n",
    "for m in results:\n",
    "    print(\n",
    "        f\"{m['batch_size']:>9}, \"\n",
    "        f\"{m['avg_latency_ms']:.3f}, \"\n",
    "        f\"{m['throughput_rps']:.1f}\"\n",
    "    )\n",
    "\n",
    "# Final line prints a brief interpretation hint.\n",
    "print(\"Larger batches usually reduce latency per request and increase throughput.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fcee9c",
   "metadata": {},
   "source": [
    "### **3.2. Measuring Latency and Throughput**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2325ec3d",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_09/Lecture_B/image_03_02.jpg?v=1769540803\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Latency and throughput describe speed and capacity\n",
    ">* They impact user experience and guide scaling decisions\n",
    "\n",
    ">* Simulate realistic concurrent traffic and record timestamps\n",
    ">* Compute latency percentiles and throughput as load increases\n",
    "\n",
    ">* Consider full request path and all dependencies\n",
    ">* Measure metrics, correlate with resources, locate bottlenecks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c417c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Measuring Latency and Throughput\n",
    "\n",
    "# This script demonstrates measuring latency and throughput.\n",
    "# It uses a tiny TensorFlow model for predictions.\n",
    "# Focus is on simple timing not advanced serving.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import time\n",
    "import statistics\n",
    "import random\n",
    "\n",
    "# Import TensorFlow and print version.\n",
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Set deterministic random seeds.\n",
    "random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Define a small dense model.\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(4,)),\n",
    "    tf.keras.layers.Dense(8, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1, activation=\"linear\"),\n",
    "])\n",
    "\n",
    "# Compile the model with mean squared error.\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "\n",
    "# Create tiny synthetic training data.\n",
    "x_train = tf.random.normal(shape=(64, 4))\n",
    "y_train = tf.reduce_sum(x_train, axis=1, keepdims=True)\n",
    "\n",
    "# Train briefly with silent output.\n",
    "model.fit(x_train, y_train, epochs=3, verbose=0)\n",
    "\n",
    "# Create a single dummy input example.\n",
    "sample_input = tf.random.normal(shape=(1, 4))\n",
    "\n",
    "# Validate input shape before timing.\n",
    "if sample_input.shape != (1, 4):\n",
    "    raise ValueError(\"Unexpected input shape for sample_input\")\n",
    "\n",
    "# Define a helper function for one prediction.\n",
    "def run_single_prediction(model_obj, input_tensor):\n",
    "    # Run one prediction and return elapsed seconds.\n",
    "    start = time.perf_counter()\n",
    "    _ = model_obj(input_tensor, training=False)\n",
    "    end = time.perf_counter()\n",
    "    return end - start\n",
    "\n",
    "# Measure latency for many single requests.\n",
    "num_requests = 50\n",
    "latencies = []\n",
    "for _ in range(num_requests):\n",
    "    elapsed = run_single_prediction(model, sample_input)\n",
    "    latencies.append(elapsed)\n",
    "\n",
    "# Compute basic latency statistics.\n",
    "avg_latency = statistics.mean(latencies)\n",
    "median_latency = statistics.median(latencies)\n",
    "max_latency = max(latencies)\n",
    "\n",
    "# Compute simple throughput estimate.\n",
    "throughput_rps = num_requests / sum(latencies)\n",
    "\n",
    "# Print latency and throughput summary.\n",
    "print(\"Requests:\", num_requests)\n",
    "print(\"Average latency seconds:\", round(avg_latency, 6))\n",
    "print(\"Median latency seconds:\", round(median_latency, 6))\n",
    "print(\"Max latency seconds:\", round(max_latency, 6))\n",
    "print(\"Throughput requests_per_second:\", round(throughput_rps, 2))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01ba631",
   "metadata": {},
   "source": [
    "### **3.3. Scaling Model Replicas**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f26003c",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_09/Lecture_B/image_03_03.jpg?v=1769540828\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Multiple replicas behind load balancer boost throughput\n",
    ">* Benefits depend on actual system bottlenecks and resources\n",
    "\n",
    ">* Per-replica latency stays similar as replicas increase\n",
    ">* More replicas boost throughput and improve tail latency\n",
    "\n",
    ">* More replicas need good load balancing and coordination\n",
    ">* Monitor full pipeline metrics to guide scaling\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a25fb2",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Serving and APIs**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344cc453",
   "metadata": {},
   "source": [
    "\n",
    "In this lecture, you learned to:\n",
    "- Deploy a SavedModel using TensorFlow Serving or a similar serving stack. \n",
    "- Expose a prediction API endpoint that accepts JSON inputs and returns model outputs. \n",
    "- Evaluate basic performance characteristics of a served model, including latency and throughput. \n",
    "\n",
    "In the next Module (Module 10), we will go over 'Advanced Topics'"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
