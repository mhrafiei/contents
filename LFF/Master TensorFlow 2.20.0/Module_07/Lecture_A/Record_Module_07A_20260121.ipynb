{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea1697f1",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Text Preprocessing**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9119348",
   "metadata": {},
   "source": [
    ">Last update: 20260121.\n",
    "    \n",
    "By the end of this Lecture, you will be able to:\n",
    "- Tokenize raw text into integer sequences using Keras text preprocessing tools. \n",
    "- Build tf.data pipelines that batch and pad text sequences for model input. \n",
    "- Create and configure embedding layers to represent tokens as dense vectors. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e8e3c5",
   "metadata": {},
   "source": [
    "## **1. Keras Text Tokenization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f32754",
   "metadata": {},
   "source": [
    "### **1.1. Using TextVectorization Layer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b95829",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_07/Lecture_A/image_01_01.jpg?v=1768976388\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Layer turns raw text into integer sequences\n",
    ">* Integrates preprocessing into Keras models for deployment\n",
    "\n",
    ">* Configure output type, vocabulary size, sequence length\n",
    ">* Choose word or character tokens for different tasks\n",
    "\n",
    ">* Adapt layer to data to learn vocabulary\n",
    ">* Ensures consistent token mapping and simpler deployment\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423c8c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Using TextVectorization Layer\n",
    "\n",
    "# This script demonstrates basic Keras TextVectorization layer usage clearly.\n",
    "# It shows how raw text becomes padded integer sequences for neural networks.\n",
    "# It keeps the example small simple and fully runnable in Google Colab.\n",
    "\n",
    "# !pip install tensorflow==2.20.0\n",
    "\n",
    "# Import required libraries including TensorFlow and NumPy.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Print TensorFlow version information for reproducibility reference.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Set deterministic random seeds for reproducible vectorization behavior.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Prepare a small list of example review sentences for demonstration.\n",
    "raw_text_data = [\n",
    "    \"I loved this movie so much\",\n",
    "    \"This movie was okay not great\",\n",
    "    \"I really disliked this boring movie\",\n",
    "    \"Amazing acting and great story overall\",\n",
    "]\n",
    "\n",
    "# Convert the raw text list into a TensorFlow constant tensor.\n",
    "text_tensor = tf.constant(raw_text_data)\n",
    "\n",
    "# Define maximum vocabulary size and sequence length hyperparameters.\n",
    "max_vocabulary_size = 20\n",
    "max_sequence_length = 8\n",
    "\n",
    "# Create a TextVectorization layer configured for integer sequence output.\n",
    "vectorize_layer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=max_vocabulary_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=max_sequence_length,\n",
    ")\n",
    "\n",
    "# Adapt the TextVectorization layer vocabulary using the example text data.\n",
    "vectorize_layer.adapt(text_tensor)\n",
    "\n",
    "# Apply the vectorization layer to the raw text tensor to obtain sequences.\n",
    "vectorized_sequences = vectorize_layer(text_tensor)\n",
    "\n",
    "# Retrieve the learned vocabulary list from the TextVectorization layer.\n",
    "vocabulary_list = vectorize_layer.get_vocabulary()\n",
    "\n",
    "# Print the original sentences and their corresponding integer sequences.\n",
    "for original_sentence, sequence in zip(raw_text_data, vectorized_sequences.numpy()):\n",
    "    print(\"Sentence:\", original_sentence)\n",
    "    print(\"Sequence:\", sequence)\n",
    "\n",
    "# Print a small slice of the learned vocabulary for quick inspection.\n",
    "print(\"Vocabulary sample:\", vocabulary_list[:10])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45909ac",
   "metadata": {},
   "source": [
    "### **1.2. Handling out of vocabulary tokens**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9f4cc1",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_07/Lecture_A/image_01_02.jpg?v=1768976419\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Real text includes unseen, out-of-vocabulary words\n",
    ">* Keras maps unknown words to special reserved indices\n",
    "\n",
    ">* Unknown words become one shared placeholder token\n",
    ">* Model learns to use this token contextually\n",
    "\n",
    ">* Adjust vocabulary size to control rare words\n",
    ">* Balance memory, model complexity, and information loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba8685b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Handling out of vocabulary tokens\n",
    "\n",
    "# This script demonstrates Keras handling unknown out of vocabulary tokens.\n",
    "# It shows how unseen words become a shared unknown token index.\n",
    "# It compares training vocabulary words with new unseen review words.\n",
    "\n",
    "# !pip install tensorflow==2.20.0\n",
    "\n",
    "# Import required modules from TensorFlow and Python.\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "import numpy as np\n",
    "\n",
    "# Print TensorFlow version for environment clarity.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Set deterministic random seeds for reproducible behavior.\n",
    "seed_value = 42\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Define small training sentences with restaurant review style text.\n",
    "train_sentences = [\n",
    "    \"The burger was delicious and very juicy\",\n",
    "    \"Service was slow but the food tasted great\",\n",
    "    \"I loved the crispy fries and friendly staff\",\n",
    "]\n",
    "\n",
    "# Define new sentences containing unseen restaurant and cuisine words.\n",
    "new_sentences = [\n",
    "    \"The sushi at Moonlight Bistro was incredible\",\n",
    "    \"I tried the new dragonfire burger and galaxy shake\",\n",
    "]\n",
    "\n",
    "# Create TextVectorization layer with limited vocabulary size.\n",
    "max_tokens = 15\n",
    "output_sequence_length = 10\n",
    "text_vectorizer = TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=output_sequence_length,\n",
    ")\n",
    "\n",
    "# Adapt vectorizer vocabulary using training sentences dataset.\n",
    "text_vectorizer.adapt(tf.data.Dataset.from_tensor_slices(train_sentences))\n",
    "\n",
    "# Obtain and print learned vocabulary including unknown token entry.\n",
    "vocab = text_vectorizer.get_vocabulary()\n",
    "print(\"\\nLearned vocabulary tokens:\")\n",
    "print(vocab)\n",
    "\n",
    "# Vectorize new sentences that contain unseen out of vocabulary words.\n",
    "new_sequences = text_vectorizer(new_sentences)\n",
    "\n",
    "# Convert sequences to numpy arrays for easier inspection.\n",
    "new_sequences_np = new_sequences.numpy()\n",
    "\n",
    "# Print vectorized sequences to show unknown token indices.\n",
    "print(\"\\nVectorized new sentences with OOV handling:\")\n",
    "print(new_sequences_np)\n",
    "\n",
    "# Identify unknown token index from vocabulary list position.\n",
    "unknown_token = vocab[1]\n",
    "unknown_index = 1\n",
    "\n",
    "# Print explanation line describing unknown token mapping behavior.\n",
    "print(\"\\nUnknown token:\", unknown_token, \"mapped at index\", unknown_index)\n",
    "\n",
    "# Verify that at least one sequence element equals unknown index value.\n",
    "contains_unknown = bool((new_sequences_np == unknown_index).any())\n",
    "print(\"Sequences contain unknown index:\", contains_unknown)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0125fc9",
   "metadata": {},
   "source": [
    "### **1.3. Building and saving vocabularies**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c16c43",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_07/Lecture_A/image_01_03.jpg?v=1768976453\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Vocabulary maps text tokens to integer indices\n",
    ">* Adapt layer on text to build ordered mapping\n",
    "\n",
    ">* Saved vocabularies keep word-to-index mapping consistent\n",
    ">* Export, store, and reload vocabularies with models\n",
    "\n",
    ">* Versioned vocabularies support collaboration, experiments, and audits\n",
    ">* Treat vocabularies as stable, reusable project artifacts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c2f621",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Building and saving vocabularies\n",
    "\n",
    "# This script builds a vocabulary using Keras TextVectorization layer.\n",
    "# It then saves the learned vocabulary into a small text file.\n",
    "# Finally it reloads the vocabulary and recreates an identical layer.\n",
    "\n",
    "# !pip install tensorflow==2.20.0\n",
    "\n",
    "# Import required standard libraries for paths and randomness.\n",
    "import os\n",
    "import random\n",
    "import pathlib\n",
    "\n",
    "# Import TensorFlow and Keras preprocessing layers.\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "# Set deterministic seeds for reproducible vocabulary building.\n",
    "random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Print TensorFlow version for environment clarity and debugging.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Create a tiny example corpus with short movie style reviews.\n",
    "corpus = [\n",
    "    \"This movie was excellent and very enjoyable\",\n",
    "    \"The movie was terrible and extremely boring\",\n",
    "    \"Excellent acting but the story was average\",\n",
    "]\n",
    "\n",
    "# Convert the Python list into a TensorFlow dataset object.\n",
    "ds_text = tf.data.Dataset.from_tensor_slices(corpus)\n",
    "\n",
    "# Create a TextVectorization layer with limited vocabulary size.\n",
    "vectorizer = TextVectorization(max_tokens=20, output_mode=\"int\")\n",
    "\n",
    "# Adapt the vectorizer on the dataset to build its vocabulary.\n",
    "vectorizer.adapt(ds_text)\n",
    "\n",
    "# Get the learned vocabulary list from the vectorizer layer.\n",
    "vocab_list = vectorizer.get_vocabulary()\n",
    "\n",
    "# Print a few vocabulary entries to inspect token to index mapping.\n",
    "print(\"First vocabulary tokens:\", vocab_list[:10])\n",
    "\n",
    "# Define a small file path for saving the vocabulary tokens.\n",
    "vocab_path = pathlib.Path(\"saved_vocabulary.txt\")\n",
    "\n",
    "# Save the vocabulary tokens into a plain text file safely.\n",
    "with vocab_path.open(\"w\", encoding=\"utf-8\") as vocab_file:\n",
    "    for token in vocab_list:\n",
    "        vocab_file.write(f\"{token}\\n\")\n",
    "\n",
    "# Confirm that the vocabulary file now exists on disk.\n",
    "print(\"Vocabulary file exists:\", vocab_path.exists())\n",
    "\n",
    "# Load the saved vocabulary tokens back from the text file.\n",
    "with vocab_path.open(\"r\", encoding=\"utf-8\") as vocab_file:\n",
    "    loaded_vocab = [line.strip() for line in vocab_file.readlines()]\n",
    "\n",
    "# Create a new TextVectorization layer using loaded vocabulary.\n",
    "new_vectorizer = TextVectorization(max_tokens=len(loaded_vocab), output_mode=\"int\", vocabulary=loaded_vocab)\n",
    "\n",
    "# Define a sample sentence to compare original and new vectorizers.\n",
    "sample_sentence = tf.constant([\"This movie was excellent but slightly boring overall\"])\n",
    "\n",
    "# Vectorize the sample sentence using the original vectorizer.\n",
    "original_tokens = vectorizer(sample_sentence)\n",
    "\n",
    "# Vectorize the same sentence using the recreated vectorizer.\n",
    "reloaded_tokens = new_vectorizer(sample_sentence)\n",
    "\n",
    "# Print both token sequences to verify identical integer mappings.\n",
    "print(\"Original token sequence:\", original_tokens.numpy())\n",
    "print(\"Reloaded token sequence:\", reloaded_tokens.numpy())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374d11e6",
   "metadata": {},
   "source": [
    "## **2. Batching Text Sequences**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a9779c",
   "metadata": {},
   "source": [
    "### **2.1. Sequence Padding Basics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ee85f1",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_07/Lecture_A/image_02_01.jpg?v=1768976492\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Padding makes all text sequences equal length\n",
    ">* Special padding tokens enable efficient batch processing\n",
    "\n",
    ">* Choose a special padding token and value\n",
    ">* Decide between pre-padding or post-padding consistently\n",
    "\n",
    ">* Choose a max sequence length to reduce computation\n",
    ">* Truncate long texts while preserving key information\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea604f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Sequence Padding Basics\n",
    "\n",
    "# This script shows basic sequence padding with simple integer token sequences.\n",
    "# It demonstrates pre padding and post padding using TensorFlow Keras utilities.\n",
    "# It prints padded sequences and shapes to clarify batch friendly dimensions.\n",
    "\n",
    "# !pip install tensorflow\n",
    "\n",
    "# Import required modules from TensorFlow and standard Python libraries.\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Set deterministic random seeds for reproducible padding demonstration.\n",
    "np.random.seed(7)\n",
    "tf.random.set_seed(7)\n",
    "\n",
    "# Print TensorFlow version information for environment transparency and reproducibility.\n",
    "print(\"Using TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Create small example sequences representing tokenized short text sentences.\n",
    "sequences = [[4, 7, 2], [10, 3], [6, 9, 1, 5]]\n",
    "\n",
    "# Convert sequences into a TensorFlow ragged tensor for flexible length handling.\n",
    "ragged_sequences = tf.ragged.constant(sequences)\n",
    "\n",
    "# Print original ragged sequences and their varying lengths for comparison.\n",
    "print(\"Original ragged sequences:\", ragged_sequences)\n",
    "\n",
    "# Choose a maximum sequence length for padding to create uniform batch shapes.\n",
    "max_length = 6\n",
    "\n",
    "# Apply post padding using pad_sequences with padding tokens appended at sequence ends.\n",
    "post_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    sequences=sequences,\n",
    "    maxlen=max_length,\n",
    "    dtype=\"int32\",\n",
    "    padding=\"post\",\n",
    "    truncating=\"post\",\n",
    "    value=0,\n",
    ")\n",
    "\n",
    "# Apply pre padding using pad_sequences with padding tokens added at sequence beginnings.\n",
    "pre_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    sequences=sequences,\n",
    "    maxlen=max_length,\n",
    "    dtype=\"int32\",\n",
    "    padding=\"pre\",\n",
    "    truncating=\"pre\",\n",
    "    value=0,\n",
    ")\n",
    "\n",
    "# Print post padded sequences and their shapes to show uniform batch friendly dimensions.\n",
    "print(\"Post padded sequences:\\n\", post_padded, \"\\nShape:\", post_padded.shape)\n",
    "\n",
    "# Print pre padded sequences and their shapes to compare padding placement strategies.\n",
    "print(\"Pre padded sequences:\\n\", pre_padded, \"\\nShape:\", pre_padded.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21ec75f",
   "metadata": {},
   "source": [
    "### **2.2. Handling Padding Masks**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521d1068",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_07/Lecture_A/image_02_02.jpg?v=1768976523\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Padding masks mark real tokens versus padding\n",
    ">* They prevent models learning from meaningless padded positions\n",
    "\n",
    ">* Create masks during padding and pass along\n",
    ">* Use masks so models ignore padded tokens\n",
    "\n",
    ">* Use one padding mask across all model layers\n",
    ">* Treat masks as core data to preserve meaning\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103cb4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Handling Padding Masks\n",
    "\n",
    "# This script shows padded sequences and corresponding padding masks clearly.\n",
    "# It uses TensorFlow to create masks and pass them into a simple model.\n",
    "# It helps beginners see how models ignore padded time steps using masks.\n",
    "\n",
    "# !pip install tensorflow\n",
    "\n",
    "# Import required modules including TensorFlow and NumPy libraries.\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Print TensorFlow version information for reproducibility and clarity.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Set deterministic random seeds for reproducible behavior and outputs.\n",
    "seed_value = 42\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Define a tiny corpus with variable length example sentences.\n",
    "texts = [\n",
    "    \"I love this movie\",\n",
    "    \"This film was okay\",\n",
    "    \"Bad\",\n",
    "    \"Absolutely fantastic acting today\",\n",
    "]\n",
    "\n",
    "# Create a Keras Tokenizer and fit it on the example texts.\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=1000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "# Convert texts into integer sequences using the fitted tokenizer.\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "print(\"Original sequences:\", sequences)\n",
    "\n",
    "# Pad sequences to equal length using post padding with zeros values.\n",
    "padded = tf.keras.preprocessing.sequence.pad_sequences(sequences, padding=\"post\", value=0)\n",
    "print(\"Padded sequences shape:\", padded.shape)\n",
    "\n",
    "# Create a boolean padding mask where True means real token positions.\n",
    "mask = tf.cast(tf.math.not_equal(padded, 0), dtype=tf.float32)\n",
    "print(\"Mask shape:\", mask.shape)\n",
    "\n",
    "# Validate that padded and mask shapes match exactly for safety.\n",
    "assert padded.shape == mask.shape\n",
    "\n",
    "# Build a simple model that accepts mask information from inputs.\n",
    "inputs = tf.keras.Input(shape=(padded.shape[1],), dtype=\"int32\")\n",
    "embedding = tf.keras.layers.Embedding(input_dim=1000, output_dim=8, mask_zero=True)(inputs)\n",
    "pooled = tf.keras.layers.GlobalAveragePooling1D()(embedding)\n",
    "outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(pooled)\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Compile the model with binary crossentropy loss and Adam optimizer.\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")\n",
    "\n",
    "# Create dummy labels for demonstration of forward pass behavior.\n",
    "labels = np.array([1, 1, 0, 1], dtype=\"float32\")\n",
    "\n",
    "# Run one training epoch to show model using internal masking.\n",
    "history = model.fit(padded, labels, epochs=1, batch_size=2, verbose=0)\n",
    "\n",
    "# Show model predictions to confirm successful masked forward pass.\n",
    "predictions = model.predict(padded, verbose=0)\n",
    "print(\"Predictions shape:\", predictions.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78033ce2",
   "metadata": {},
   "source": [
    "### **2.3. tf data text batching**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f97daa",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_07/Lecture_A/image_02_03.jpg?v=1768976563\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Use tf.data to batch and pad sequences\n",
    ">* Get uniform batches, better performance, easier training\n",
    "\n",
    ">* Dataset pairs token sequences with their labels\n",
    ">* Pipeline batches, pads, and tracks consistent tensor shapes\n",
    "\n",
    ">* Combine tokenize, filter, batch, and pad steps\n",
    ">* Shuffle, repeat, prefetch to train efficiently\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba3f566",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - tf data text batching\n",
    "\n",
    "# Demonstrate TensorFlow text dataset batching and padding pipeline usage.\n",
    "# Show how sequences become padded batches for model ready tensors.\n",
    "# Keep example simple, deterministic, and beginner friendly throughout.\n",
    "\n",
    "# !pip install tensorflow==2.20.0\n",
    "\n",
    "# Import required TensorFlow and operating system modules.\n",
    "import tensorflow as tf\n",
    "import os as os_module\n",
    "\n",
    "# Set deterministic random seeds for reproducible dataset behavior.\n",
    "tf.random.set_seed(7)\n",
    "\n",
    "# Print TensorFlow version information for environment transparency.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Define small toy tokenized sequences representing short text examples.\n",
    "sequences = tf.ragged.constant([[1, 2, 3], [4, 5], [6], [7, 8, 9, 10]])\n",
    "\n",
    "# Define simple integer labels for each corresponding tokenized sequence.\n",
    "labels = [0, 1, 0, 1]\n",
    "\n",
    "# Create TensorFlow dataset from sequences and labels using from_tensor_slices.\n",
    "base_dataset = tf.data.Dataset.from_tensor_slices((sequences.to_tensor(), labels))\n",
    "\n",
    "# Shuffle dataset examples with fixed buffer size and deterministic seed.\n",
    "shuffled_dataset = base_dataset.shuffle(buffer_size=4, seed=7, reshuffle_each_iteration=False)\n",
    "\n",
    "# Define batch size and padded sequence length hyperparameters for batching.\n",
    "batch_size = 2\n",
    "\n",
    "# Define padded shapes for sequences and labels within each dataset batch.\n",
    "padded_shapes = ([None], [])\n",
    "\n",
    "# Define padding values for sequences and labels within padded batches.\n",
    "padding_values = (0, tf.constant(-1, dtype=tf.int32))\n",
    "\n",
    "# Apply padded_batch transformation to dataset using shapes and padding values.\n",
    "batched_dataset = shuffled_dataset.padded_batch(batch_size=batch_size, padded_shapes=padded_shapes, padding_values=padding_values)\n",
    "\n",
    "# Prefetch one batch to overlap data preparation with potential model execution.\n",
    "prefetched_dataset = batched_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "# Iterate over dataset batches and print shapes and contents for inspection.\n",
    "for batch_index, (batch_sequences, batch_labels) in enumerate(prefetched_dataset):\n",
    "\n",
    "    # Print batch index and tensor shapes for sequences and labels.\n",
    "    print(\"Batch\", batch_index, \"shapes\", batch_sequences.shape, batch_labels.shape)\n",
    "\n",
    "    # Print padded batch sequences tensor to observe padding behavior.\n",
    "    print(\"Padded sequences batch:\", batch_sequences.numpy())\n",
    "\n",
    "    # Print batch labels tensor to confirm correct alignment with sequences.\n",
    "    print(\"Batch labels:\", batch_labels.numpy())\n",
    "\n",
    "# Confirm script completed without errors by printing final confirmation message.\n",
    "print(\"Dataset batching and padding demonstration finished successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71be90f",
   "metadata": {},
   "source": [
    "## **3. Configuring Embedding Layers**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5bedfd",
   "metadata": {},
   "source": [
    "### **3.1. Embedding Layer Basics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883577d3",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_07/Lecture_A/image_03_01.jpg?v=1768976627\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Embedding layers turn token IDs into vectors\n",
    ">* They act as a learned lookup matrix\n",
    "\n",
    ">* Embeddings learn token relationships from training data\n",
    ">* Similar contexts cluster; vectors encode meaning and sentiment\n",
    "\n",
    ">* Embeddings turn token sequences into uniform vectors\n",
    ">* They reduce dimensionality and share parameters efficiently\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84df9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Embedding Layer Basics\n",
    "\n",
    "# This script shows how embedding layers map token indices to dense vectors.\n",
    "# It builds a tiny Keras model using an Embedding layer for toy sentences.\n",
    "# It prints token indices and corresponding embedding vectors for clear understanding.\n",
    "\n",
    "# !pip install tensorflow==2.20.0\n",
    "\n",
    "# Import required modules including TensorFlow and NumPy for this example.\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Set deterministic seeds for reproducible random behavior in this simple script.\n",
    "np.random.seed(7)\n",
    "tf.random.set_seed(7)\n",
    "\n",
    "# Print TensorFlow version information for environment clarity and reproducibility.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Define a tiny corpus of example sentences for demonstrating token embeddings.\n",
    "texts = [\n",
    "    \"this movie was great\",\n",
    "    \"this movie was awful\",\n",
    "]\n",
    "\n",
    "# Create a Keras Tokenizer to map words into integer indices for the vocabulary.\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=20, oov_token=\"<OOV>\")\n",
    "\n",
    "# Fit the tokenizer on the example texts to build the word index mapping.\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "# Convert the texts into sequences of integer token indices using the tokenizer.\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "# Pad the sequences so they share equal length for batch processing convenience.\n",
    "padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(sequences, padding=\"post\")\n",
    "\n",
    "# Retrieve vocabulary size from tokenizer plus one for reserved padding index.\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Define embedding dimension size which controls vector length for each token.\n",
    "embedding_dim = 4\n",
    "\n",
    "# Build a simple Sequential model containing only a single Embedding layer.\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=padded_sequences.shape[1]),\n",
    "])\n",
    "\n",
    "# Run the padded integer sequences through the embedding layer to obtain vectors.\n",
    "embeddings = model(padded_sequences)\n",
    "\n",
    "# Convert embeddings to NumPy arrays for easier printing and shape inspection.\n",
    "embeddings_array = embeddings.numpy()\n",
    "\n",
    "# Print token index sequences and corresponding embedding shapes for both sentences.\n",
    "print(\"Token index sequences:\", padded_sequences)\n",
    "print(\"Embeddings shape:\", embeddings_array.shape)\n",
    "\n",
    "# Print embeddings for first sentence tokens to show lookup table behavior.\n",
    "print(\"First sentence embeddings:\")\n",
    "print(embeddings_array[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ebf8f2",
   "metadata": {},
   "source": [
    "### **3.2. Understanding Embedding Size**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f8f1fb",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_07/Lecture_A/image_03_02.jpg?v=1768976662\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Embedding size controls how much meaning fits\n",
    ">* Small compresses information; large captures finer nuances\n",
    "\n",
    ">* Balance size with data and compute limits\n",
    ">* Start with defaults, tune using validation performance\n",
    "\n",
    ">* Complex tasks and diverse vocabularies need larger embeddings\n",
    ">* Match embedding dimension to task complexity and scope\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cfb811",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Understanding Embedding Size\n",
    "\n",
    "# This script compares different embedding sizes visually and numerically for beginners.\n",
    "# It builds tiny models with various embedding dimensions and prints parameter counts.\n",
    "# It helps understand how embedding size affects model capacity and memory usage.\n",
    "\n",
    "# !pip install tensorflow==2.20.0\n",
    "\n",
    "# Import required modules including TensorFlow and NumPy for computations.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set deterministic seeds for reproducible random behavior across different runs.\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Print TensorFlow version information for environment transparency and reproducibility.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Define a small example vocabulary size for our toy text dataset.\n",
    "vocab_size = 50\n",
    "\n",
    "# Define several embedding dimensions to compare model size and behavior.\n",
    "embedding_dims = [4, 8, 16]\n",
    "\n",
    "# Create a tiny batch of token id sequences representing short example sentences.\n",
    "example_sequences = np.array([[1, 5, 9, 2], [3, 7, 0, 0]], dtype=np.int32)\n",
    "\n",
    "# Loop through each embedding dimension and build a simple embedding model.\n",
    "for dim in embedding_dims:\n",
    "\n",
    "    # Create a simple Sequential model containing only an Embedding layer.\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=dim, input_length=4)\n",
    "    ])\n",
    "\n",
    "    # Build the model explicitly to ensure weights are created before inspection.\n",
    "    model.build(input_shape=(None, 4))\n",
    "\n",
    "    # Compute total trainable parameters which equals vocab size times embedding dimension.\n",
    "    total_params = model.count_params()\n",
    "\n",
    "    # Run the example sequences through the embedding layer to obtain dense vectors.\n",
    "    embedded_output = model(example_sequences)\n",
    "\n",
    "    # Convert embedded output to NumPy array and inspect resulting shape dimensions.\n",
    "    output_array = embedded_output.numpy()\n",
    "\n",
    "    # Print concise information about current embedding dimension and parameter count.\n",
    "    print(\"Embedding dim:\", dim, \"Total params:\", total_params)\n",
    "\n",
    "    # Print the output shape to show how embedding dimension changes last axis size.\n",
    "    print(\"Output shape:\", output_array.shape)\n",
    "\n",
    "# Confirm script finished successfully without errors or unexpected behavior.\n",
    "print(\"Finished comparing embedding sizes.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32aec3d5",
   "metadata": {},
   "source": [
    "### **3.3. Initializing from pretrained vectors**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23221116",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_07/Lecture_A/image_03_03.jpg?v=1768976694\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Use pretrained embeddings instead of learning from scratch\n",
    ">* They encode word similarities and boost performance quickly\n",
    "\n",
    ">* Align your tokens with pretrained embedding vocabulary\n",
    ">* Build embedding matrix, handle unknown words, initialize layer\n",
    "\n",
    ">* Choose to freeze or train embeddings\n",
    ">* Often freeze first, then unfreeze for specialization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a2d728",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Initializing from pretrained vectors\n",
    "\n",
    "# This script shows initializing an embedding layer from pretrained vectors.\n",
    "# It uses a tiny fake pretrained embedding file for demonstration purposes.\n",
    "# It builds a simple Keras model using the loaded embedding matrix.\n",
    "\n",
    "# !pip install tensorflow==2.20.0\n",
    "\n",
    "# Import required standard libraries and TensorFlow framework.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set deterministic seeds for reproducible random behavior everywhere.\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Print TensorFlow version information for environment verification purposes.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Define a small example vocabulary that our tokenizer might have produced.\n",
    "vocab = [\"<pad>\", \"<unk>\", \"refund\", \"return\", \"exchange\", \"banana\"]\n",
    "\n",
    "# Define embedding dimensionality matching our pretend pretrained vectors.\n",
    "embedding_dim = 4\n",
    "\n",
    "# Create a fake pretrained embedding dictionary with simple numeric patterns.\n",
    "pretrained_vectors = {\n",
    "    \"refund\": np.array([0.9, 0.1, 0.0, 0.0], dtype=np.float32),\n",
    "    \"return\": np.array([0.88, 0.12, 0.0, 0.0], dtype=np.float32),\n",
    "}\n",
    "\n",
    "# Initialize an empty embedding matrix with zeros for every vocabulary token.\n",
    "embedding_matrix = np.zeros((len(vocab), embedding_dim), dtype=np.float32)\n",
    "\n",
    "# Fill the embedding matrix using pretrained vectors or random fallbacks.\n",
    "for index, token in enumerate(vocab):\n",
    "    if token in pretrained_vectors:\n",
    "        embedding_matrix[index] = pretrained_vectors[token]\n",
    "    elif token == \"<pad>\":\n",
    "        embedding_matrix[index] = np.zeros(embedding_dim, dtype=np.float32)\n",
    "    else:\n",
    "        embedding_matrix[index] = np.random.uniform(-0.05, 0.05, embedding_dim)\n",
    "\n",
    "# Convert the numpy embedding matrix into a TensorFlow constant tensor.\n",
    "embedding_initializer = tf.constant_initializer(embedding_matrix)\n",
    "\n",
    "# Define a simple Keras Sequential model using the initialized embedding layer.\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=len(vocab),\n",
    "        output_dim=embedding_dim,\n",
    "        embeddings_initializer=embedding_initializer,\n",
    "        trainable=False,\n",
    "        mask_zero=True,\n",
    "        input_length=3,\n",
    "    ),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "])\n",
    "\n",
    "# Compile the model with a simple optimizer and binary crossentropy loss.\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")\n",
    "\n",
    "# Create a tiny batch of token index sequences representing example sentences.\n",
    "example_sequences = np.array([[2, 3, 4], [4, 5, 0]], dtype=np.int32)\n",
    "\n",
    "# Run a forward pass to obtain predictions and verify everything works.\n",
    "predictions = model.predict(example_sequences, verbose=0)\n",
    "\n",
    "# Print the embedding matrix rows for refund and banana tokens for comparison.\n",
    "print(\"Embedding for 'refund':\", embedding_matrix[vocab.index(\"refund\")])\n",
    "print(\"Embedding for 'banana':\", embedding_matrix[vocab.index(\"banana\")])\n",
    "\n",
    "# Print the model predictions to show the initialized embeddings are usable.\n",
    "print(\"Model predictions shape:\", predictions.shape)\n",
    "print(\"Model predictions values:\", predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8a3bdc",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Text Preprocessing**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420c7f6d",
   "metadata": {},
   "source": [
    "\n",
    "In this lecture, you learned to:\n",
    "- Tokenize raw text into integer sequences using Keras text preprocessing tools. \n",
    "- Build tf.data pipelines that batch and pad text sequences for model input. \n",
    "- Create and configure embedding layers to represent tokens as dense vectors. \n",
    "\n",
    "In the next Lecture (Lecture B), we will go over 'Sequence Models'"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
