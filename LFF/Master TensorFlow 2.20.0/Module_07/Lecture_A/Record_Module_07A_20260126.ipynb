{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71ed832d",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Text Preprocessing**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d899ff",
   "metadata": {},
   "source": [
    ">Last update: 20260126.\n",
    "    \n",
    "By the end of this Lecture, you will be able to:\n",
    "- Tokenize raw text into integer sequences using Keras text preprocessing tools. \n",
    "- Build tf.data pipelines that batch and pad text sequences for model input. \n",
    "- Create and configure embedding layers to represent tokens as dense vectors. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4d5905",
   "metadata": {},
   "source": [
    "## **1. Keras Text Tokenization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef98aed",
   "metadata": {},
   "source": [
    "### **1.1. Using TextVectorization Layer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c938f70d",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_07/Lecture_A/image_01_01.jpg?v=1769419323\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* TextVectorization turns raw text into integer sequences\n",
    ">* Handles preprocessing and plugs directly into Keras models\n",
    "\n",
    ">* Configure vocabulary size and output token format\n",
    ">* Adapt on sample texts to produce consistent integers\n",
    "\n",
    ">* Stores rules and vocabulary for consistent preprocessing\n",
    ">* Ensures reliable deployment, collaboration, and model updates\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2634dca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Using TextVectorization Layer\n",
    "\n",
    "# This script demonstrates Keras TextVectorization usage.\n",
    "# It shows how raw text becomes integer token sequences.\n",
    "# Run cells to observe simple deterministic preprocessing behavior.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Import TensorFlow and Keras preprocessing tools.\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "# Set deterministic random seeds for reproducibility.\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# Print TensorFlow version in one concise line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Create a tiny corpus of example text sentences.\n",
    "raw_texts = [\n",
    "    \"I loved this movie so much\",\n",
    "    \"This movie was terrible and boring\",\n",
    "    \"Absolutely fantastic acting and great story\",\n",
    "    \"I would not recommend this movie\",\n",
    "]\n",
    "\n",
    "# Convert list to TensorFlow constant tensor.\n",
    "text_ds = tf.constant(raw_texts)\n",
    "\n",
    "# Wrap tensor in a small Dataset for adaptation.\n",
    "text_dataset = tf.data.Dataset.from_tensor_slices(text_ds)\n",
    "\n",
    "# Define maximum vocabulary size and sequence length.\n",
    "max_tokens = 20\n",
    "sequence_length = 8\n",
    "\n",
    "# Create a TextVectorization layer with basic settings.\n",
    "vectorizer = TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length,\n",
    ")\n",
    "\n",
    "# Adapt the layer on the small text dataset.\n",
    "vectorizer.adapt(text_dataset)\n",
    "\n",
    "# Retrieve the learned vocabulary list from the layer.\n",
    "vocab = vectorizer.get_vocabulary()\n",
    "\n",
    "# Print a few vocabulary entries with their indices.\n",
    "print(\"Vocabulary size:\", len(vocab))\n",
    "print(\"First ten tokens:\", vocab[:10])\n",
    "\n",
    "# Apply the vectorizer to the original raw texts.\n",
    "vectorized_texts = vectorizer(text_ds)\n",
    "\n",
    "# Confirm the resulting tensor shape is as expected.\n",
    "print(\"Vectorized shape:\", vectorized_texts.shape)\n",
    "\n",
    "# Convert vectorized tensor to a NumPy array for inspection.\n",
    "vectorized_array = vectorized_texts.numpy()\n",
    "\n",
    "# Print each original sentence with its integer sequence.\n",
    "for i, sentence in enumerate(raw_texts):\n",
    "    print(\"Sentence:\", sentence)\n",
    "    print(\"Sequence:\", vectorized_array[i])\n",
    "\n",
    "# Build a simple tf.data pipeline with batching and padding.\n",
    "text_pipeline = (\n",
    "    text_dataset\n",
    "    .batch(2)\n",
    "    .map(vectorizer, num_parallel_calls=tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "# Take one batch from the pipeline and inspect its shape.\n",
    "for batch in text_pipeline.take(1):\n",
    "    print(\"Batch shape from pipeline:\", batch.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f26702f",
   "metadata": {},
   "source": [
    "### **1.2. Handling Unknown Tokens**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b7f60e",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_07/Lecture_A/image_01_02.jpg?v=1769419363\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Limited vocabularies create unknown or rare tokens\n",
    ">* Keras maps unknown words to special indices\n",
    "\n",
    ">* Unknown token handling affects model generalization strongly\n",
    ">* Mapping unseen words to one index preserves meaning\n",
    "\n",
    ">* Balance vocab size, coverage, and model capacity\n",
    ">* Use Keras controls and multiple unknown token indices\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac42ecdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Handling Unknown Tokens\n",
    "\n",
    "# This script shows Keras unknown token handling.\n",
    "# It focuses on simple text tokenization concepts.\n",
    "# Run each part to observe unknown token behavior.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import TensorFlow and Keras preprocessing utilities.\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Print TensorFlow version for reproducibility.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Define a tiny training corpus with simple sentences.\n",
    "train_sentences = [\n",
    "    \"this phone is great\",\n",
    "    \"this phone is bad\",\n",
    "    \"this camera is great\",\n",
    "]\n",
    "\n",
    "# Define some new sentences containing unknown words.\n",
    "new_sentences = [\n",
    "    \"this phone is absolutely fire\",\n",
    "    \"this tablet is great\",\n",
    "]\n",
    "\n",
    "# Create a tokenizer with limited vocabulary size.\n",
    "max_words = 5\n",
    "\n",
    "# Reserve index for out of vocabulary tokens.\n",
    "oov_token = \"<OOV>\"\n",
    "\n",
    "# Initialize tokenizer with max words and oov token.\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token=oov_token)\n",
    "\n",
    "# Fit tokenizer on training sentences only.\n",
    "tokenizer.fit_on_texts(train_sentences)\n",
    "\n",
    "# Show the learned word index dictionary.\n",
    "print(\"Word index:\", tokenizer.word_index)\n",
    "\n",
    "# Convert training sentences to integer sequences.\n",
    "train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
    "\n",
    "# Print tokenized training sequences for inspection.\n",
    "print(\"Training sequences:\", train_sequences)\n",
    "\n",
    "# Convert new sentences containing unknown words.\n",
    "new_sequences = tokenizer.texts_to_sequences(new_sentences)\n",
    "\n",
    "# Print tokenized new sequences with unknown handling.\n",
    "print(\"New sequences:\", new_sequences)\n",
    "\n",
    "# Extract the index used for unknown tokens.\n",
    "unknown_index = tokenizer.word_index.get(oov_token, None)\n",
    "\n",
    "# Safely print the unknown token index value.\n",
    "print(\"Unknown token index:\", unknown_index)\n",
    "\n",
    "# Count how many unknown tokens appear in new sequences.\n",
    "unknown_count = sum(\n",
    "    1 for seq in new_sequences for token in seq if token == unknown_index\n",
    ")\n",
    "\n",
    "# Print how many tokens were mapped as unknown.\n",
    "print(\"Number of unknown tokens:\", unknown_count)\n",
    "\n",
    "# Demonstrate that sequence lengths are still preserved.\n",
    "lengths = [len(seq) for seq in new_sequences]\n",
    "\n",
    "# Print the lengths of new sequences after tokenization.\n",
    "print(\"New sequence lengths:\", lengths)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ed2459",
   "metadata": {},
   "source": [
    "### **1.3. Building and saving vocabularies**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b43b6c",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_07/Lecture_A/image_01_03.jpg?v=1769419400\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Vocabulary maps unique tokens to integer indices\n",
    ">* Control vocab size and frequency to balance coverage\n",
    "\n",
    ">* Keep token-to-index mapping stable across runs\n",
    ">* Save and reload vocab files for consistency\n",
    "\n",
    ">* Update vocabularies as language and data change\n",
    ">* Version, reuse, and align vocabularies for compatibility\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac51a004",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Building and saving vocabularies\n",
    "\n",
    "# This script shows basic Keras text tokenization.\n",
    "# It focuses on building and saving vocabularies.\n",
    "# Run cells in order inside Google Colab.\n",
    "\n",
    "# Install TensorFlow if not already available.\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "\n",
    "# Import TensorFlow and Keras preprocessing tools.\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "\n",
    "# Set TensorFlow random seed for reproducibility.\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Create a tiny sample corpus of sentences.\n",
    "corpus = [\n",
    "    \"This movie was great and very fun\",\n",
    "    \"The movie was okay but a bit long\",\n",
    "    \"I did not enjoy this movie at all\",\n",
    "]\n",
    "\n",
    "# Show the raw corpus to the learner.\n",
    "print(\"Sample corpus:\")\n",
    "print(corpus)\n",
    "\n",
    "# Define basic TextVectorization configuration.\n",
    "max_tokens = 20\n",
    "output_sequence_length = 8\n",
    "\n",
    "# Create the TextVectorization layer instance.\n",
    "vectorizer = TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=output_sequence_length,\n",
    ")\n",
    "\n",
    "# Convert corpus to a TensorFlow Dataset.\n",
    "text_ds = tf.data.Dataset.from_tensor_slices(corpus)\n",
    "\n",
    "# Adapt the vectorizer to build the vocabulary.\n",
    "vectorizer.adapt(text_ds)\n",
    "\n",
    "# Get the learned vocabulary as a Python list.\n",
    "vocab = vectorizer.get_vocabulary()\n",
    "\n",
    "# Show the vocabulary and its size.\n",
    "print(\"\\nLearned vocabulary:\")\n",
    "print(vocab)\n",
    "\n",
    "# Build a mapping from token to index for clarity.\n",
    "word_to_index = {word: index for index, word in enumerate(vocab)}\n",
    "\n",
    "# Show a few example token to index pairs.\n",
    "print(\"\\nExample token to index mapping:\")\n",
    "for word in [\"\", \"[UNK]\", \"movie\", \"great\"]:\n",
    "    if word in word_to_index:\n",
    "        print(word, \"->\", word_to_index[word])\n",
    "\n",
    "# Choose a small filename for saving vocabulary.\n",
    "vocab_filename = \"vocab_example.json\"\n",
    "\n",
    "# Save the vocabulary list as a JSON file.\n",
    "with open(vocab_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(vocab, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# Confirm that the file now exists on disk.\n",
    "file_exists = os.path.exists(vocab_filename)\n",
    "print(\"\\nVocabulary file saved:\", file_exists)\n",
    "\n",
    "# Load the vocabulary back from the JSON file.\n",
    "with open(vocab_filename, \"r\", encoding=\"utf-8\") as f:\n",
    "    loaded_vocab = json.load(f)\n",
    "\n",
    "# Verify that loaded vocabulary matches the original.\n",
    "print(\"Loaded vocabulary matches:\", loaded_vocab == vocab)\n",
    "\n",
    "# Create a new TextVectorization layer using loaded vocabulary.\n",
    "new_vectorizer = TextVectorization(\n",
    "    max_tokens=len(loaded_vocab),\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=output_sequence_length,\n",
    ")\n",
    "\n",
    "# Set the vocabulary directly without adapting again.\n",
    "new_vectorizer.set_vocabulary(loaded_vocab)\n",
    "\n",
    "# Pick a test sentence to compare encodings.\n",
    "test_sentence = tf.constant([\"This movie was great\"])\n",
    "\n",
    "# Vectorize with the original adapted layer.\n",
    "original_encoded = vectorizer(test_sentence)\n",
    "\n",
    "# Vectorize with the new layer using loaded vocabulary.\n",
    "reloaded_encoded = new_vectorizer(test_sentence)\n",
    "\n",
    "# Show that both encodings are identical.\n",
    "print(\"\\nOriginal encoding:\", original_encoded.numpy())\n",
    "print(\"Reloaded encoding:\", reloaded_encoded.numpy())\n",
    "\n",
    "# Final check that shapes are as expected.\n",
    "print(\"Encoding shape:\", original_encoded.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a337eb",
   "metadata": {},
   "source": [
    "## **2. Batching Text Sequences**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be4c9bf",
   "metadata": {},
   "source": [
    "### **2.1. Padding to Fixed Length**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a647f7f1",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_07/Lecture_A/image_02_01.jpg?v=1769419440\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Text sequences vary; models need uniform lengths\n",
    ">* Pad shorter sequences with special tokens for batching\n",
    "\n",
    ">* Balance truncation risk against padding overhead\n",
    ">* Choose max length from dataset length distribution\n",
    "\n",
    ">* Choose padding start or end by model\n",
    ">* Use unique pad token and thoughtful length\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7d4cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Padding to Fixed Length\n",
    "\n",
    "# This script shows padding text sequences clearly.\n",
    "# It focuses on batching sequences to fixed length.\n",
    "# Run each part to observe shapes and padding.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required modules from TensorFlow.\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set a deterministic random seed value.\n",
    "tf.random.set_seed(7)\n",
    "\n",
    "# Print TensorFlow version briefly.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Create a tiny list of example sentences.\n",
    "raw_texts = [\n",
    "    \"I love TensorFlow\",\n",
    "    \"Padding makes batches rectangular\",\n",
    "    \"Short\",\n",
    "    \"This sentence is a bit longer than others\",\n",
    "]\n",
    "\n",
    "# Show the raw texts to understand input.\n",
    "print(\"Raw texts:\")\n",
    "print(raw_texts)\n",
    "\n",
    "# Create a TextVectorization layer for tokenization.\n",
    "vectorizer = tf.keras.layers.TextVectorization(\n",
    "    standardize=\"lower_and_strip_punctuation\",\n",
    "    split=\"whitespace\",\n",
    "    output_mode=\"int\",\n",
    ")\n",
    "\n",
    "# Adapt the vectorizer on the raw texts.\n",
    "vectorizer.adapt(raw_texts)\n",
    "\n",
    "# Convert raw texts to integer sequences.\n",
    "int_sequences = vectorizer(tf.constant(raw_texts))\n",
    "\n",
    "# Print unpadded integer sequences and shapes.\n",
    "print(\"\\nInteger sequences before padding:\")\n",
    "for seq in int_sequences:\n",
    "    print(seq.numpy(), \"shape:\", seq.shape)\n",
    "\n",
    "# Decide a fixed maximum sequence length.\n",
    "max_length = 8\n",
    "\n",
    "# Create a small tf.data Dataset from sequences.\n",
    "seq_ds = tf.data.Dataset.from_tensor_slices(int_sequences)\n",
    "\n",
    "# Define a function to pad each sequence.\n",
    "def pad_to_fixed_length(seq):\n",
    "    seq = seq[:max_length]\n",
    "    padded = tf.pad(\n",
    "        seq,\n",
    "        paddings=[[0, max_length - tf.shape(seq)[0]]],\n",
    "        constant_values=0,\n",
    "    )\n",
    "    return padded\n",
    "\n",
    "# Map padding function and batch the dataset.\n",
    "batched_ds = (\n",
    "    seq_ds.map(pad_to_fixed_length)\n",
    "    .batch(2)\n",
    ")\n",
    "\n",
    "# Take one batch and inspect its shape.\n",
    "for batch in batched_ds.take(2):\n",
    "    print(\"\\nBatch shape:\", batch.shape)\n",
    "    print(\"Batch contents:\\n\", batch.numpy())\n",
    "\n",
    "# Create an Embedding layer for padded tokens.\n",
    "vocab_size = vectorizer.vocabulary_size()\n",
    "embedding_dim = 4\n",
    "embedding_layer = tf.keras.layers.Embedding(\n",
    "    input_dim=vocab_size,\n",
    "    output_dim=embedding_dim,\n",
    "    mask_zero=True,\n",
    ")\n",
    "\n",
    "# Pass one padded batch through the embedding layer.\n",
    "for batch in batched_ds.take(1):\n",
    "    embedded = embedding_layer(batch)\n",
    "    print(\"\\nEmbedded batch shape:\", embedded.shape)\n",
    "    print(\"Mask from embedding:\", embedding_layer.compute_mask(batch))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6788e54",
   "metadata": {},
   "source": [
    "### **2.2. Handling Padding Masks**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70df1c2d",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_07/Lecture_A/image_02_02.jpg?v=1769419474\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Padding masks mark real tokens versus padding\n",
    ">* They prevent models learning from meaningless padded positions\n",
    "\n",
    ">* Padding masks mirror batch and sequence shapes\n",
    ">* They flag real tokens and ignore padded positions\n",
    "\n",
    ">* Create masks from padded batches and propagate\n",
    ">* Use separate masks for each padding level\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e40606",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Handling Padding Masks\n",
    "\n",
    "# This script shows padding masks with text batches.\n",
    "# It uses TensorFlow text preprocessing and masking.\n",
    "# Focus on batching variable length sequences safely.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required TensorFlow modules.\n",
    "import tensorflow as tf\n",
    "\n",
    "# Print TensorFlow version briefly.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Create a tiny toy corpus of short sentences.\n",
    "texts = [\n",
    "    \"I love TensorFlow\",\n",
    "    \"Masks ignore padding\",\n",
    "    \"Short\",\n",
    "    \"Variable length text sequences\",\n",
    "]\n",
    "\n",
    "# Build a TextVectorization layer for tokenization.\n",
    "vectorizer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=50,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=None,\n",
    ")\n",
    "\n",
    "# Convert Python list to a TensorFlow dataset.\n",
    "text_ds = tf.data.Dataset.from_tensor_slices(texts)\n",
    "\n",
    "# Adapt vectorizer vocabulary using the dataset.\n",
    "vectorizer.adapt(text_ds.batch(4))\n",
    "\n",
    "# Map raw text to integer sequences using vectorizer.\n",
    "int_ds = text_ds.map(lambda x: vectorizer(x))\n",
    "\n",
    "# Define a small maximum sequence length for padding.\n",
    "max_len = 6\n",
    "\n",
    "# Function to pad sequences to fixed length.\n",
    "def pad_to_length(x):\n",
    "    x = x[:max_len]\n",
    "    pad_amount = max_len - tf.shape(x)[0]\n",
    "    paddings = tf.stack([[0, pad_amount]])\n",
    "    return tf.pad(x, paddings, constant_values=0)\n",
    "\n",
    "# Apply padding and batch the sequences.\n",
    "batched_ds = int_ds.map(pad_to_length).batch(2)\n",
    "\n",
    "# Function to create a boolean padding mask.\n",
    "def make_mask(batch):\n",
    "    mask = tf.not_equal(batch, 0)\n",
    "    return batch, mask\n",
    "\n",
    "# Attach masks to each padded batch.\n",
    "masked_ds = batched_ds.map(make_mask)\n",
    "\n",
    "# Take one example batch and mask from dataset.\n",
    "for batch_tokens, batch_mask in masked_ds.take(1):\n",
    "    example_tokens = batch_tokens\n",
    "    example_mask = batch_mask\n",
    "\n",
    "# Validate shapes before printing them.\n",
    "print(\"Batch tokens shape:\", example_tokens.shape)\n",
    "print(\"Batch mask shape:\", example_mask.shape)\n",
    "\n",
    "# Show the padded integer token sequences.\n",
    "print(\"Padded token batch:\\n\", example_tokens.numpy())\n",
    "\n",
    "# Show the corresponding boolean padding mask.\n",
    "print(\"Padding mask batch:\\n\", example_mask.numpy())\n",
    "\n",
    "# Demonstrate how mask can zero out padded positions.\n",
    "example_tokens = tf.cast(example_tokens, tf.int32)\n",
    "masked_values = tf.cast(example_mask, tf.int32) * example_tokens\n",
    "\n",
    "# Print masked values where padding positions become zeros.\n",
    "print(\"Tokens after applying mask:\\n\", masked_values.numpy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4c4660",
   "metadata": {},
   "source": [
    "### **2.3. tf data batching**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d664e8b",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_07/Lecture_A/image_02_03.jpg?v=1769419527\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Use data pipelines to batch many text sequences\n",
    ">* Pipelines output padded batches for scalable NLP training\n",
    "\n",
    ">* Pipelines separate data handling from model logic\n",
    ">* They shuffle, batch, pad, prefetch, and optimize\n",
    "\n",
    ">* Pipelines flexibly handle complex, varied real-world text\n",
    ">* Composable steps keep batches consistent and scalable\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb5020c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - tf data batching\n",
    "\n",
    "# This script demonstrates tf.data text batching.\n",
    "# It focuses on padding and batching sequences.\n",
    "# Run it in Colab to follow along.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import TensorFlow and NumPy for this demo.\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "tf.random.set_seed(7)\n",
    "np.random.seed(7)\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Define a few short example text sentences.\n",
    "raw_texts = [\n",
    "    \"I love TensorFlow for NLP\",\n",
    "    \"Batching sequences keeps training efficient\",\n",
    "    \"Padding makes shapes consistent\",\n",
    "    \"tf data pipelines are powerful\",\n",
    "]\n",
    "\n",
    "# Create a simple TextVectorization layer.\n",
    "vectorizer = tf.keras.layers.TextVectorization(\n",
    "    standardize=\"lower_and_strip_punctuation\",\n",
    "    split=\"whitespace\",\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=0,\n",
    ")\n",
    "\n",
    "# Adapt the vectorizer on the raw texts.\n",
    "text_ds = tf.data.Dataset.from_tensor_slices(raw_texts)\n",
    "vectorizer.adapt(text_ds)\n",
    "\n",
    "# Vectorize texts to get ragged integer sequences.\n",
    "int_sequences = vectorizer(tf.constant(raw_texts))\n",
    "print(\"Vectorized shape:\", int_sequences.shape)\n",
    "\n",
    "# Convert ragged sequences to a Dataset of examples.\n",
    "seq_ds = tf.data.Dataset.from_tensor_slices(int_sequences)\n",
    "\n",
    "# Inspect one example sequence length safely.\n",
    "for one_seq in seq_ds.take(1):\n",
    "    print(\"One sequence length:\", one_seq.shape[0])\n",
    "\n",
    "# Define a small batch size for demonstration.\n",
    "batch_size = 2\n",
    "\n",
    "# Shuffle the dataset to randomize order.\n",
    "shuffled_ds = seq_ds.shuffle(buffer_size=len(raw_texts))\n",
    "\n",
    "# Batch sequences and let dataset handle padding.\n",
    "batched_padded_ds = shuffled_ds.padded_batch(\n",
    "    batch_size=batch_size,\n",
    "    padded_shapes=(None,),\n",
    "    padding_values=tf.constant(0, dtype=tf.int64),\n",
    ")\n",
    "\n",
    "# Prefetch to overlap data preparation and usage.\n",
    "final_ds = batched_padded_ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "# Iterate over a few batches and print shapes.\n",
    "for batch_index, batch in enumerate(final_ds):\n",
    "    print(\"Batch\", batch_index, \"shape:\", batch.shape)\n",
    "\n",
    "# Show one batch content to visualize padding.\n",
    "for example_batch in final_ds.take(1):\n",
    "    print(\"Example batch tensor:\\n\", example_batch.numpy())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767f0d0b",
   "metadata": {},
   "source": [
    "## **3. Configuring Embedding Layers**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b989453c",
   "metadata": {},
   "source": [
    "### **3.1. Embedding Layer Basics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a1afc7",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_07/Lecture_A/image_03_01.jpg?v=1769419562\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Embedding layers turn token IDs into dense vectors\n",
    ">* Similar tokens learn nearby vectors, unlike one-hot\n",
    "\n",
    ">* Embedding layer is a trainable lookup matrix\n",
    ">* Learns compact vectors that preserve task-specific meaning\n",
    "\n",
    ">* Embeddings turn token IDs into rich vectors\n",
    ">* They reveal patterns and power advanced text models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafb6d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Embedding Layer Basics\n",
    "\n",
    "# This script explains basic embedding layers.\n",
    "# It uses tiny text data for clarity.\n",
    "# Run each part to observe printed outputs.\n",
    "\n",
    "# Install TensorFlow if missing in your environment.\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import TensorFlow and NumPy modules.\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Set deterministic random seeds for reproducibility.\n",
    "tf.random.set_seed(7)\n",
    "np.random.seed(7)\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Define a tiny toy corpus of short sentences.\n",
    "corpus = [\n",
    "    \"this movie was great\",\n",
    "    \"this movie was terrible\",\n",
    "    \"the film was fantastic\",\n",
    "]\n",
    "\n",
    "# Create a simple TextVectorization layer.\n",
    "vectorizer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=20,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=5,\n",
    ")\n",
    "\n",
    "# Adapt the vectorizer on the small corpus.\n",
    "vectorizer.adapt(corpus)\n",
    "\n",
    "# Vectorize the corpus into integer token sequences.\n",
    "int_sequences = vectorizer(corpus)\n",
    "\n",
    "# Convert sequences to NumPy for easy inspection.\n",
    "int_sequences_np = int_sequences.numpy()\n",
    "\n",
    "# Print the integer sequences for each sentence.\n",
    "print(\"Integer sequences for corpus:\")\n",
    "print(int_sequences_np)\n",
    "\n",
    "# Get vocabulary size from the vectorizer.\n",
    "vocab_size = len(vectorizer.get_vocabulary())\n",
    "\n",
    "# Define embedding dimension for dense token vectors.\n",
    "embedding_dim = 4\n",
    "\n",
    "# Create a basic Embedding layer instance.\n",
    "embedding_layer = tf.keras.layers.Embedding(\n",
    "    input_dim=vocab_size,\n",
    "    output_dim=embedding_dim,\n",
    "    mask_zero=True,\n",
    ")\n",
    "\n",
    "# Pass integer sequences through the embedding layer.\n",
    "embedded_sequences = embedding_layer(int_sequences)\n",
    "\n",
    "# Validate the embedded tensor shape safely.\n",
    "print(\"Embedded tensor shape:\", embedded_sequences.shape)\n",
    "\n",
    "# Select first sentence embeddings for inspection.\n",
    "first_sentence_embeddings = embedded_sequences[0]\n",
    "\n",
    "# Convert first sentence embeddings to NumPy array.\n",
    "first_sentence_np = first_sentence_embeddings.numpy()\n",
    "\n",
    "# Print embeddings for the first sentence tokens.\n",
    "print(\"Embeddings for first sentence tokens:\")\n",
    "print(first_sentence_np)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6d9b33",
   "metadata": {},
   "source": [
    "### **3.2. Understanding Embedding Size**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396e7525",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_07/Lecture_A/image_03_02.jpg?v=1769419597\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Embedding size sets features per vocabulary token\n",
    ">* Too small loses nuance; larger captures richer relationships\n",
    "\n",
    ">* Choose size based on task and data\n",
    ">* Too small underfits; too large overfits, costly\n",
    "\n",
    ">* Embedding size affects parameters, speed, and memory\n",
    ">* Start with defaults, tune size for each task\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a98a55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Understanding Embedding Size\n",
    "\n",
    "# This script explores embedding size choices.\n",
    "# It uses tiny text data for clarity.\n",
    "# Run all cells together in Google Colab.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Import TensorFlow and Keras layers.\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Create a tiny corpus of short sentences.\n",
    "corpus = [\n",
    "    \"I love this movie\",\n",
    "    \"This movie is terrible\",\n",
    "    \"Amazing acting and great story\",\n",
    "    \"The plot was boring\",\n",
    "]\n",
    "\n",
    "# Build a simple word index manually.\n",
    "word_index = {\"<pad>\": 0}\n",
    "for sentence in corpus:\n",
    "    for word in sentence.lower().split():\n",
    "        if word not in word_index:\n",
    "            word_index[word] = len(word_index)\n",
    "\n",
    "# Show the vocabulary size including padding.\n",
    "vocab_size = len(word_index)\n",
    "print(\"Vocabulary size including pad:\", vocab_size)\n",
    "\n",
    "# Convert sentences to integer sequences.\n",
    "sequences = []\n",
    "for sentence in corpus:\n",
    "    tokens = []\n",
    "    for word in sentence.lower().split():\n",
    "        tokens.append(word_index[word])\n",
    "    sequences.append(tokens)\n",
    "\n",
    "# Pad sequences to the same length.\n",
    "max_len = max(len(seq) for seq in sequences)\n",
    "padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    sequences, maxlen=max_len, padding=\"post\"\n",
    ")\n",
    "\n",
    "# Wrap padded data in a small tf.data Dataset.\n",
    "dataset = tf.data.Dataset.from_tensor_slices(padded)\n",
    "dataset = dataset.batch(2)\n",
    "\n",
    "# Define two different embedding sizes.\n",
    "embedding_dim_small = 4\n",
    "embedding_dim_large = 16\n",
    "\n",
    "# Create a small embedding layer instance.\n",
    "small_embedding = layers.Embedding(\n",
    "    input_dim=vocab_size, output_dim=embedding_dim_small\n",
    ")\n",
    "\n",
    "# Create a large embedding layer instance.\n",
    "large_embedding = layers.Embedding(\n",
    "    input_dim=vocab_size, output_dim=embedding_dim_large\n",
    ")\n",
    "\n",
    "# Take one batch of token ids from dataset.\n",
    "for batch_tokens in dataset.take(1):\n",
    "    example_tokens = batch_tokens\n",
    "\n",
    "# Validate the batch shape before embedding.\n",
    "print(\"Token batch shape:\", example_tokens.shape)\n",
    "\n",
    "# Compute small embedding representations.\n",
    "small_emb_output = small_embedding(example_tokens)\n",
    "\n",
    "# Compute large embedding representations.\n",
    "large_emb_output = large_embedding(example_tokens)\n",
    "\n",
    "# Print shapes to compare embedding sizes.\n",
    "print(\"Small embedding output shape:\", small_emb_output.shape)\n",
    "print(\"Large embedding output shape:\", large_emb_output.shape)\n",
    "\n",
    "# Show a single token id and its small embedding.\n",
    "first_token_id = int(example_tokens[0, 0].numpy())\n",
    "print(\"First token id:\", first_token_id)\n",
    "print(\"Small embedding vector:\", small_emb_output[0, 0].numpy())\n",
    "\n",
    "# Show the same token id and its large embedding.\n",
    "print(\"Large embedding vector:\", large_emb_output[0, 0].numpy())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a6f3da",
   "metadata": {},
   "source": [
    "### **3.3. Using Pretrained Embeddings**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e37be5",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_07/Lecture_A/image_03_03.jpg?v=1769419637\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Pretrained embeddings give models rich language knowledge\n",
    ">* They speed training and boost performance on NLP tasks\n",
    "\n",
    ">* Match vocabulary, load known pretrained word vectors\n",
    ">* Initialize missing words, model starts with semantics\n",
    "\n",
    ">* Choose to freeze or fine-tune embeddings\n",
    ">* Balance stability with domain adaptation for performance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4ee9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Using Pretrained Embeddings\n",
    "\n",
    "# This script shows pretrained embeddings usage simply.\n",
    "# It builds a tiny model with a frozen embedding.\n",
    "# Focus is on configuring the Keras Embedding layer.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Import TensorFlow and Keras layers.\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Define a tiny example vocabulary list.\n",
    "vocab = [\"<pad>\", \"<unk>\", \"i\", \"love\", \"nlp\", \"and\", \"tensorflow\"]\n",
    "\n",
    "# Build a mapping from token to integer index.\n",
    "word_index = {word: idx for idx, word in enumerate(vocab)}\n",
    "\n",
    "# Set vocabulary size and embedding dimension.\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 4\n",
    "\n",
    "# Create a fake pretrained embedding matrix.\n",
    "pretrained_matrix = np.zeros((vocab_size, embedding_dim), dtype=\"float32\")\n",
    "\n",
    "# Fill non special tokens with simple deterministic vectors.\n",
    "for word, idx in word_index.items():\n",
    "    if word in (\"<pad>\", \"<unk>\"):\n",
    "        continue\n",
    "    base = float(idx)\n",
    "    pretrained_matrix[idx] = np.array([\n",
    "        base * 0.1,\n",
    "        base * 0.1 + 0.1,\n",
    "        base * 0.1 + 0.2,\n",
    "        base * 0.1 + 0.3,\n",
    "    ], dtype=\"float32\")\n",
    "\n",
    "# Verify the pretrained matrix shape safely.\n",
    "assert pretrained_matrix.shape == (vocab_size, embedding_dim)\n",
    "\n",
    "# Build a simple Keras model using the matrix.\n",
    "inputs = layers.Input(shape=(3,), dtype=\"int32\", name=\"token_ids\")\n",
    "\n",
    "# Configure an Embedding layer with pretrained weights.\n",
    "embedding_layer = layers.Embedding(\n",
    "    input_dim=vocab_size,\n",
    "    output_dim=embedding_dim,\n",
    "    embeddings_initializer=tf.keras.initializers.Constant(pretrained_matrix),\n",
    "    trainable=False,\n",
    "    mask_zero=True,\n",
    "    name=\"pretrained_embedding\",\n",
    ")\n",
    "\n",
    "# Apply the embedding layer to the inputs.\n",
    "embedded = embedding_layer(inputs)\n",
    "\n",
    "# Pool token embeddings into a single vector.\n",
    "pooled = layers.GlobalAveragePooling1D()(embedded)\n",
    "\n",
    "# Add a tiny dense output layer for demonstration.\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(pooled)\n",
    "\n",
    "# Create the final model object.\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Compile the model with simple settings.\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")\n",
    "\n",
    "# Show a short model summary line count safely.\n",
    "model.summary(print_fn=lambda x: None)\n",
    "\n",
    "# Prepare two tiny example sentences as token ids.\n",
    "example_sentences = np.array([\n",
    "    [word_index[\"i\"], word_index[\"love\"], word_index[\"nlp\"]],\n",
    "    [word_index[\"i\"], word_index[\"love\"], word_index[\"tensorflow\"]],\n",
    "], dtype=\"int32\")\n",
    "\n",
    "# Run a forward pass to get predictions.\n",
    "preds = model.predict(example_sentences, verbose=0)\n",
    "\n",
    "# Fetch the embedding weights from the layer.\n",
    "emb_weights = embedding_layer.get_weights()[0]\n",
    "\n",
    "# Print a few key results for inspection.\n",
    "print(\"Vocabulary:\", vocab)\n",
    "print(\"Embedding matrix shape:\", emb_weights.shape)\n",
    "print(\"Embedding for 'love':\", emb_weights[word_index[\"love\"]])\n",
    "print(\"Embedding for 'tensorflow':\", emb_weights[word_index[\"tensorflow\"]])\n",
    "print(\"Model predictions shape:\", preds.shape)\n",
    "print(\"Predictions sample:\", preds[0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbc14b2",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Text Preprocessing**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d1770a",
   "metadata": {},
   "source": [
    "\n",
    "In this lecture, you learned to:\n",
    "- Tokenize raw text into integer sequences using Keras text preprocessing tools. \n",
    "- Build tf.data pipelines that batch and pad text sequences for model input. \n",
    "- Create and configure embedding layers to represent tokens as dense vectors. \n",
    "\n",
    "In the next Lecture (Lecture B), we will go over 'Sequence Models'"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
