{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a7dd1de",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Sequence Models**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b9039c",
   "metadata": {},
   "source": [
    ">Last update: 20260121.\n",
    "    \n",
    "By the end of this Lecture, you will be able to:\n",
    "- Build LSTM or GRU-based sequence models for text classification using tf.keras. \n",
    "- Implement a simple transformer-style encoder block using Keras layers for sequence modeling. \n",
    "- Evaluate and compare the performance of different sequence architectures on an NLP task. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b0f4b2",
   "metadata": {},
   "source": [
    "## **1. LSTM and GRU Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8bad6d",
   "metadata": {},
   "source": [
    "### **1.1. Recurrent Layer Basics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43e1296",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_07/Lecture_B/image_01_01.jpg?v=1768979976\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Recurrent layers process sequences step by step\n",
    ">* Internal state summarizes past tokens for classification\n",
    "\n",
    ">* LSTM or GRU reuses weights across timesteps\n",
    ">* Gates build a summary state for classification\n",
    "\n",
    ">* Choose final state or full state sequence\n",
    ">* Final state summarizes all tokens for classification\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd776d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Recurrent Layer Basics\n",
    "\n",
    "# This script shows basic LSTM sequence processing for text classification.\n",
    "# It contrasts final state outputs with full sequence outputs clearly.\n",
    "# It uses a tiny sentiment dataset for quick demonstration.\n",
    "\n",
    "# !pip install tensorflow==2.20.0\n",
    "\n",
    "# Import required libraries including TensorFlow and NumPy.\n",
    "import os, random, numpy as np, tensorflow as tf\n",
    "\n",
    "# Set deterministic seeds for reproducible training behavior.\n",
    "seed_value = 7\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version information for environment clarity.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Detect available device preference using TensorFlow configuration utilities.\n",
    "physical_gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "if physical_gpus:\n",
    "    print(\"Using GPU device for training operations.\")\n",
    "else:\n",
    "    print(\"Using CPU device for training operations.\")\n",
    "\n",
    "# Create a tiny in memory sentiment dataset with short sentences.\n",
    "texts = [\n",
    "    \"I love this movie so much\",\n",
    "    \"This film was absolutely terrible\",\n",
    "    \"What a fantastic and enjoyable story\",\n",
    "    \"I hated every single minute\",\n",
    "    \"The plot was boring and slow\",\n",
    "    \"Actors did an amazing job\",\n",
    "]\n",
    "\n",
    "# Create corresponding sentiment labels where one indicates positive.\n",
    "labels = np.array([1, 0, 1, 0, 0, 1], dtype=np.int32)\n",
    "\n",
    "# Build a simple tokenizer to convert words into integer indices.\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=1000, oov_token=\"<OOV>\")\n",
    "\n",
    "# Fit tokenizer on the small training texts for vocabulary creation.\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "# Convert texts into padded integer sequences with equal lengths.\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "max_len = max(len(seq) for seq in sequences)\n",
    "\n",
    "# Pad sequences so that all sequences share identical temporal length.\n",
    "X = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_len, padding=\"post\")\n",
    "\n",
    "# Validate resulting input shape to ensure correct dimensions.\n",
    "print(\"Input batch shape:\", X.shape)\n",
    "\n",
    "# Define embedding dimension and recurrent units for the LSTM layer.\n",
    "embedding_dim = 8\n",
    "lstm_units = 4\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Build a model that returns only the final LSTM hidden state.\n",
    "inputs_final = tf.keras.Input(shape=(max_len,), name=\"input_final\")\n",
    "\n",
    "# Add an embedding layer to map tokens into dense vector representations.\n",
    "embedded_final = tf.keras.layers.Embedding(vocab_size, embedding_dim)(inputs_final)\n",
    "\n",
    "# Add LSTM configured to return only final hidden state representation.\n",
    "lstm_final = tf.keras.layers.LSTM(lstm_units, return_sequences=False, name=\"lstm_final\")(embedded_final)\n",
    "\n",
    "# Add dense output layer for binary sentiment classification predictions.\n",
    "outputs_final = tf.keras.layers.Dense(1, activation=\"sigmoid\")(lstm_final)\n",
    "\n",
    "# Create and compile the final state model using binary crossentropy loss.\n",
    "model_final = tf.keras.Model(inputs_final, outputs_final, name=\"final_state_model\")\n",
    "model_final.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the final state model briefly on the tiny dataset.\n",
    "model_final.fit(X, labels, epochs=5, batch_size=2, verbose=0)\n",
    "\n",
    "# Evaluate the final state model performance on the same dataset.\n",
    "loss_final, acc_final = model_final.evaluate(X, labels, verbose=0)\n",
    "print(\"Final state model accuracy:\", round(float(acc_final), 3))\n",
    "\n",
    "# Build a model that returns full sequence of LSTM hidden states.\n",
    "inputs_seq = tf.keras.Input(shape=(max_len,), name=\"input_seq\")\n",
    "embedded_seq = tf.keras.layers.Embedding(vocab_size, embedding_dim)(inputs_seq)\n",
    "\n",
    "# Configure LSTM to return full sequence of hidden states for each timestep.\n",
    "lstm_seq = tf.keras.layers.LSTM(lstm_units, return_sequences=True, name=\"lstm_seq\")(embedded_seq)\n",
    "\n",
    "# Pool sequence outputs by averaging across timesteps for classification.\n",
    "pooled_seq = tf.keras.layers.GlobalAveragePooling1D()(lstm_seq)\n",
    "\n",
    "# Add dense output layer for binary sentiment classification predictions.\n",
    "outputs_seq = tf.keras.layers.Dense(1, activation=\"sigmoid\")(pooled_seq)\n",
    "\n",
    "# Create and compile the sequence output model with identical settings.\n",
    "model_seq = tf.keras.Model(inputs_seq, outputs_seq, name=\"sequence_output_model\")\n",
    "model_seq.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the sequence output model briefly on the same tiny dataset.\n",
    "model_seq.fit(X, labels, epochs=5, batch_size=2, verbose=0)\n",
    "\n",
    "# Evaluate the sequence output model performance on the same dataset.\n",
    "loss_seq, acc_seq = model_seq.evaluate(X, labels, verbose=0)\n",
    "print(\"Sequence output model accuracy:\", round(float(acc_seq), 3))\n",
    "\n",
    "# Inspect raw LSTM outputs for a single example from the dataset.\n",
    "sample_input = X[0:1]\n",
    "full_sequence_output = tf.keras.Model(inputs_seq, lstm_seq)(sample_input)\n",
    "\n",
    "# Print shapes to highlight difference between final and sequence outputs.\n",
    "print(\"Single example input shape:\", sample_input.shape)\n",
    "print(\"Full sequence output shape:\", full_sequence_output.shape)\n",
    "print(\"Final state vector length:\", lstm_units)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9caa2b09",
   "metadata": {},
   "source": [
    "### **1.2. Bidirectional RNN Wrappers**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64739a25",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_07/Lecture_B/image_01_02.jpg?v=1768980026\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Bidirectional RNNs read sequences forward and backward\n",
    ">* Combined context improves understanding and text classification\n",
    "\n",
    ">* Bidirectional RNNs see both earlier and later words\n",
    ">* They capture negation, contrast, and long-range context\n",
    "\n",
    ">* Bidirectional outputs increase representation size and power\n",
    ">* Balance accuracy gains with cost and overfitting\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e99a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Bidirectional RNN Wrappers\n",
    "\n",
    "# This script demonstrates bidirectional LSTM sequence classification basics.\n",
    "# It compares unidirectional and bidirectional recurrent models on toy sentences.\n",
    "# It prints validation accuracy to show bidirectional wrapper performance.\n",
    "\n",
    "# !pip install tensorflow==2.20.0\n",
    "\n",
    "# Import required libraries including TensorFlow and NumPy.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set deterministic seeds for reproducible training behavior.\n",
    "seed_value = 7\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version information for environment clarity.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Select computation device preferring GPU when available safely.\n",
    "physical_gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "if physical_gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(physical_gpus[0], True)\n",
    "    except Exception as device_error:\n",
    "        print(\"GPU configuration warning:\", device_error)\n",
    "else:\n",
    "    print(\"GPU device not available, using CPU.\")\n",
    "\n",
    "# Define a tiny sentiment style text dataset with labels.\n",
    "texts = [\n",
    "    \"I loved this movie so much\",\n",
    "    \"This film was terrible and boring\",\n",
    "    \"Absolutely fantastic acting and story\",\n",
    "    \"I would never watch this again\",\n",
    "    \"The plot was great and exciting\",\n",
    "    \"I hated the ending of this\",\n",
    "    \"What a wonderful and touching film\",\n",
    "    \"The movie was bad and disappointing\",\n",
    "]\n",
    "\n",
    "# Define binary sentiment labels aligned with dataset sentences.\n",
    "labels = np.array([1, 0, 1, 0, 1, 0, 1, 0], dtype=np.int32)\n",
    "\n",
    "# Create a simple Keras tokenizer for integer encoding.\n",
    "vocab_size = 1000\n",
    "max_length = 12\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
    "\n",
    "# Fit tokenizer on the small text corpus examples.\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "# Pad sequences to fixed length for batch processing.\n",
    "padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_length, padding=\"post\", truncating=\"post\")\n",
    "\n",
    "# Split dataset into simple training and validation subsets.\n",
    "train_sequences = padded_sequences[:6]\n",
    "train_labels = labels[:6]\n",
    "val_sequences = padded_sequences[6:]\n",
    "val_labels = labels[6:]\n",
    "\n",
    "# Define a function building unidirectional LSTM classification model.\n",
    "def build_unidirectional_model(vocab_size_value, max_length_value):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Embedding(vocab_size_value, 16, input_length=max_length_value))\n",
    "    model.add(tf.keras.layers.LSTM(16))\n",
    "    model.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "# Define a function building bidirectional LSTM classification model.\n",
    "def build_bidirectional_model(vocab_size_value, max_length_value):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Embedding(vocab_size_value, 16, input_length=max_length_value))\n",
    "    model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16)))\n",
    "    model.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "# Build both models for comparison of architectures.\n",
    "uni_model = build_unidirectional_model(vocab_size, max_length)\n",
    "bi_model = build_bidirectional_model(vocab_size, max_length)\n",
    "\n",
    "# Train unidirectional model briefly on training data.\n",
    "uni_history = uni_model.fit(train_sequences, train_labels, epochs=10, batch_size=2, verbose=0, validation_data=(val_sequences, val_labels))\n",
    "\n",
    "# Train bidirectional model briefly on same training data.\n",
    "bi_history = bi_model.fit(train_sequences, train_labels, epochs=10, batch_size=2, verbose=0, validation_data=(val_sequences, val_labels))\n",
    "\n",
    "# Evaluate both models on validation data for accuracy comparison.\n",
    "uni_loss, uni_acc = uni_model.evaluate(val_sequences, val_labels, verbose=0)\n",
    "bi_loss, bi_acc = bi_model.evaluate(val_sequences, val_labels, verbose=0)\n",
    "\n",
    "# Print concise accuracy results highlighting bidirectional wrapper effect.\n",
    "print(\"Unidirectional LSTM validation accuracy:\", round(float(uni_acc), 3))\n",
    "print(\"Bidirectional LSTM validation accuracy:\", round(float(bi_acc), 3))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97da0b6",
   "metadata": {},
   "source": [
    "### **1.3. Handling variable sequences**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9a865b",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_07/Lecture_B/image_01_03.jpg?v=1768980079\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Real text sequences vary widely in length\n",
    ">* We pad and track tokens to batch efficiently\n",
    "\n",
    ">* Masking tells RNNs to ignore padded tokens\n",
    ">* Prevents padding noise, improves text classification learning\n",
    "\n",
    ">* Choose and enforce a sensible maximum sequence length\n",
    ">* Balance speed, memory, and information for long texts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658823e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Handling variable sequences\n",
    "\n",
    "# This script shows handling variable length sequences with padding and masking.\n",
    "# It builds a tiny LSTM classifier using padded movie review style sentences.\n",
    "# It demonstrates masking behavior by comparing padded and unpadded predictions.\n",
    "\n",
    "# !pip install tensorflow==2.20.0\n",
    "\n",
    "# Import required libraries including TensorFlow and NumPy.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Print TensorFlow version for reproducibility information.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Set deterministic random seeds for reproducible behavior.\n",
    "random.seed(7)\n",
    "np.random.seed(7)\n",
    "tf.random.set_seed(7)\n",
    "\n",
    "# Define a tiny toy text dataset with variable length sentences.\n",
    "texts = [\n",
    "    \"bad movie\",\n",
    "    \"really bad boring movie\",\n",
    "    \"great film\",\n",
    "    \"really great amazing film\",\n",
    "    \"terrible and slow\",\n",
    "    \"excellent and fun\",\n",
    "]\n",
    "\n",
    "# Define binary sentiment labels aligned with the toy sentences.\n",
    "labels = np.array([0, 0, 1, 1, 0, 1], dtype=np.int32)\n",
    "\n",
    "# Create a TextVectorization layer for integer tokenization.\n",
    "vectorizer = tf.keras.layers.TextVectorization(output_sequence_length=6)\n",
    "\n",
    "# Adapt the vectorizer vocabulary using the toy texts.\n",
    "vectorizer.adapt(tf.constant(texts))\n",
    "\n",
    "# Vectorize the texts into fixed length integer sequences.\n",
    "int_sequences = vectorizer(tf.constant(texts))\n",
    "\n",
    "# Verify the resulting integer sequence tensor shape.\n",
    "print(\"Vectorized shape:\", int_sequences.shape)\n",
    "\n",
    "# Build a simple model with masking and an LSTM classifier.\n",
    "inputs = tf.keras.Input(shape=(6,), dtype=\"int32\")\n",
    "\n",
    "# Use an Embedding layer with mask_zero enabled for padding handling.\n",
    "embedding = tf.keras.layers.Embedding(input_dim=100, output_dim=8, mask_zero=True)(inputs)\n",
    "\n",
    "# Add a small LSTM layer that respects the computed mask.\n",
    "lstm_output = tf.keras.layers.LSTM(units=8)(embedding)\n",
    "\n",
    "# Add a dense output layer for binary sentiment classification.\n",
    "outputs = tf.keras.layers.Dense(units=1, activation=\"sigmoid\")(lstm_output)\n",
    "\n",
    "# Create the Keras model object from inputs and outputs.\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Compile the model with binary crossentropy loss and accuracy metric.\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model briefly on the tiny dataset for demonstration.\n",
    "model.fit(int_sequences, labels, epochs=10, batch_size=2, verbose=0)\n",
    "\n",
    "# Choose a short sentence and create padded and unpadded versions.\n",
    "short_text = tf.constant([\"great film\"])\n",
    "\n",
    "# Vectorize the short sentence using the same vectorizer.\n",
    "short_vector = vectorizer(short_text)\n",
    "\n",
    "# Manually create a longer padded version by appending zeros.\n",
    "short_vector_padded = tf.concat([short_vector, tf.zeros_like(short_vector)], axis=1)\n",
    "\n",
    "# Ensure shapes are correct for both padded and unpadded sequences.\n",
    "print(\"Unpadded shape:\", short_vector.shape)\n",
    "print(\"Padded shape:\", short_vector_padded.shape)\n",
    "\n",
    "# Get prediction for the unpadded sequence using the trained model.\n",
    "pred_unpadded = model(short_vector).numpy()[0, 0]\n",
    "\n",
    "# Get prediction for the padded sequence which includes extra zeros.\n",
    "pred_padded = model(short_vector_padded[:, :6]).numpy()[0, 0]\n",
    "\n",
    "# Print both predictions to show masking ignores padding zeros.\n",
    "print(\"Prediction unpadded:\", float(pred_unpadded))\n",
    "print(\"Prediction padded:\", float(pred_padded))\n",
    "\n",
    "# Confirm that predictions are numerically very close despite extra padding.\n",
    "print(\"Difference between predictions:\", float(abs(pred_unpadded - pred_padded)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8faabc2",
   "metadata": {},
   "source": [
    "## **2. Transformer Encoder Block**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f180d92f",
   "metadata": {},
   "source": [
    "### **2.1. Multihead Attention Basics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8ac459",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_07/Lecture_B/image_02_01.jpg?v=1768980122\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Attention lets each token weight all others\n",
    ">* Multiple heads capture diverse relationships in parallel\n",
    "\n",
    ">* Queries compare to keys, weighting value vectors\n",
    ">* Multiple heads specialize, then outputs are combined\n",
    "\n",
    ">* Multiple heads capture local and global context\n",
    ">* Combined heads give stronger features for NLP tasks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba88e93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Multihead Attention Basics\n",
    "\n",
    "# This script demonstrates basic multihead attention using TensorFlow Keras layers.\n",
    "# It creates fake token embeddings and applies MultiHeadAttention to them.\n",
    "# It prints shapes and small values to explain multihead attention behavior.\n",
    "\n",
    "# !pip install tensorflow==2.20.0\n",
    "\n",
    "# Import required standard libraries and TensorFlow framework.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set deterministic random seeds for reproducible attention outputs.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version information for environment transparency.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Define simple configuration values for sequence and embedding dimensions.\n",
    "batch_size = 2\n",
    "sequence_length = 5\n",
    "embedding_dim = 8\n",
    "num_heads = 2\n",
    "\n",
    "# Validate that embedding dimension divides evenly by number of heads.\n",
    "assert embedding_dim % num_heads == 0\n",
    "\n",
    "# Create random input embeddings representing token vectors in sentences.\n",
    "input_embeddings = tf.random.normal(shape=(batch_size, sequence_length, embedding_dim))\n",
    "\n",
    "# Print input shape to confirm expected batch and sequence dimensions.\n",
    "print(\"Input embeddings shape:\", input_embeddings.shape)\n",
    "\n",
    "# Create a MultiHeadAttention layer using Keras functional style.\n",
    "attention_layer = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embedding_dim // num_heads)\n",
    "\n",
    "# Apply self attention where queries, keys, values share same tensor.\n",
    "attention_output, attention_scores = attention_layer(\n",
    "    query=input_embeddings,\n",
    "    value=input_embeddings,\n",
    "    key=input_embeddings,\n",
    "    return_attention_scores=True,\n",
    ")\n",
    "\n",
    "# Print output shape to show same sequence length and embedding dimension.\n",
    "print(\"Attention output shape:\", attention_output.shape)\n",
    "\n",
    "# Print attention scores shape to reveal heads and sequence relationships.\n",
    "print(\"Attention scores shape:\", attention_scores.shape)\n",
    "\n",
    "# Select first batch and first head attention matrix for inspection.\n",
    "first_head_scores = attention_scores[0, 0]\n",
    "\n",
    "# Print small rounded attention matrix to visualize token relationships.\n",
    "print(\"First head attention matrix (rounded):\")\n",
    "print(tf.round(first_head_scores * 100) / 100)\n",
    "\n",
    "# Verify that each row of scores approximately sums to one probability.\n",
    "row_sums = tf.reduce_sum(first_head_scores, axis=-1)\n",
    "\n",
    "# Print row sums to confirm attention distribution normalization behavior.\n",
    "print(\"Row sums for first head:\")\n",
    "print(tf.round(row_sums * 100) / 100)\n",
    "\n",
    "# Show that attention output remains same shape as original embeddings.\n",
    "print(\"Output equals input shape:\", attention_output.shape == input_embeddings.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02e9766",
   "metadata": {},
   "source": [
    "### **2.2. Residual Connections and Normalization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92d64f7",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_07/Lecture_B/image_02_02.jpg?v=1768980154\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Residual connections stabilize deep transformer encoder training\n",
    ">* They refine representations and help capture subtle cues\n",
    "\n",
    ">* Layer normalization stabilizes activations after residual addition\n",
    ">* Same normalize pattern used after attention, feedforward\n",
    "\n",
    ">* Residuals and normalization stabilize and support learning\n",
    ">* They refine sequence understanding and improve NLP robustness\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627e56ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Residual Connections and Normalization\n",
    "\n",
    "# This script shows residual connections and layer normalization in a tiny encoder block.\n",
    "# It builds a simple Keras model that compares with and without normalization behavior.\n",
    "# It prints tensor statistics to illustrate stability from residual connections and normalization.\n",
    "\n",
    "# !pip install tensorflow==2.20.0\n",
    "\n",
    "# Import required libraries including TensorFlow and NumPy for computations.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set deterministic seeds for reproducible behavior across different hardware environments.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version information for clarity about used deep learning framework.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Detect available device preference using TensorFlow configuration utilities for runtime placement.\n",
    "physical_gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "if physical_gpus:\n",
    "    print(\"Using GPU device count:\", len(physical_gpus))\n",
    "else:\n",
    "    print(\"Using CPU because no GPU was detected\")\n",
    "\n",
    "# Define simple function that builds encoder block with residual and normalization layers.\n",
    "def build_encoder_block(hidden_dim, num_heads, ff_dim, use_layer_norm):\n",
    "    inputs = tf.keras.Input(shape=(None, hidden_dim))\n",
    "    attention_layer = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=hidden_dim)\n",
    "    attention_output = attention_layer(inputs, inputs)\n",
    "\n",
    "    # Add residual connection around attention output using Keras Add layer operation.\n",
    "    attention_residual = tf.keras.layers.Add()([inputs, attention_output])\n",
    "\n",
    "    # Optionally apply layer normalization after residual connection for stability.\n",
    "    if use_layer_norm:\n",
    "        attention_norm = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attention_residual)\n",
    "    else:\n",
    "        attention_norm = attention_residual\n",
    "\n",
    "    # Build small feedforward network with residual connection and optional normalization.\n",
    "    ff_dense_one = tf.keras.layers.Dense(ff_dim, activation=\"relu\")\n",
    "    ff_dense_two = tf.keras.layers.Dense(hidden_dim)\n",
    "    ff_output = ff_dense_two(ff_dense_one(attention_norm))\n",
    "\n",
    "    # Add residual connection around feedforward sublayer using Add layer operation.\n",
    "    ff_residual = tf.keras.layers.Add()([attention_norm, ff_output])\n",
    "\n",
    "    # Optionally apply layer normalization after feedforward residual connection.\n",
    "    if use_layer_norm:\n",
    "        ff_norm = tf.keras.layers.LayerNormalization(epsilon=1e-6)(ff_residual)\n",
    "    else:\n",
    "        ff_norm = ff_residual\n",
    "\n",
    "    # Create Keras model object representing single encoder block transformation.\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=ff_norm)\n",
    "    return model\n",
    "\n",
    "# Define small batch of fake token embeddings representing short text sequences.\n",
    "batch_size = 2\n",
    "sequence_length = 5\n",
    "hidden_dimension = 16\n",
    "fake_inputs = tf.random.normal(shape=(batch_size, sequence_length, hidden_dimension))\n",
    "\n",
    "# Build encoder block without layer normalization to compare activation statistics.\n",
    "encoder_without_norm = build_encoder_block(hidden_dim=hidden_dimension, num_heads=2, ff_dim=32, use_layer_norm=False)\n",
    "\n",
    "# Build encoder block with layer normalization to observe stabilized activations.\n",
    "encoder_with_norm = build_encoder_block(hidden_dim=hidden_dimension, num_heads=2, ff_dim=32, use_layer_norm=True)\n",
    "\n",
    "# Run both encoder variants on same fake inputs to compare output distributions.\n",
    "outputs_without_norm = encoder_without_norm(fake_inputs, training=False)\n",
    "outputs_with_norm = encoder_with_norm(fake_inputs, training=False)\n",
    "\n",
    "# Compute simple statistics for outputs without normalization to inspect scale behavior.\n",
    "mean_without = tf.reduce_mean(outputs_without_norm).numpy()\n",
    "std_without = tf.math.reduce_std(outputs_without_norm).numpy()\n",
    "\n",
    "# Compute simple statistics for outputs with normalization to inspect stabilized scale.\n",
    "mean_with = tf.reduce_mean(outputs_with_norm).numpy()\n",
    "std_with = tf.math.reduce_std(outputs_with_norm).numpy()\n",
    "\n",
    "# Print summary statistics showing effect of residual connections and normalization.\n",
    "print(\"Without layer normalization mean:\", float(mean_without))\n",
    "print(\"Without layer normalization std:\", float(std_without))\n",
    "print(\"With layer normalization mean:\", float(mean_with))\n",
    "print(\"With layer normalization std:\", float(std_with))\n",
    "\n",
    "# Verify shapes remain unchanged thanks to residual connections preserving representation dimensions.\n",
    "print(\"Input shape:\", fake_inputs.shape)\n",
    "print(\"Output shape without normalization:\", outputs_without_norm.shape)\n",
    "print(\"Output shape with normalization:\", outputs_with_norm.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbc16ad",
   "metadata": {},
   "source": [
    "### **2.3. Feedforward Encoder Layers**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463a1cd1",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_07/Lecture_B/image_02_03.jpg?v=1768980193\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Position-wise feedforward network refines each token\n",
    ">* Two dense layers expand then shrink dimensions\n",
    "\n",
    ">* Feedforward layers add nonlinear, higher-level token features\n",
    ">* Shared feedforward network generalizes efficiently across positions\n",
    "\n",
    ">* Hidden size and activations trade capacity and cost\n",
    ">* Careful tuning yields robust, task-specific sequence representations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf724256",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Feedforward Encoder Layers\n",
    "\n",
    "# This script demonstrates transformer feedforward encoder layers conceptually using Keras Dense layers.\n",
    "# It builds a tiny encoder block with attention and feedforward sublayers for sequences.\n",
    "# It shows how feedforward expands and compresses token representations after attention.\n",
    "\n",
    "# !pip install tensorflow==2.20.0\n",
    "\n",
    "# Import required libraries including TensorFlow and NumPy for computations.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set deterministic seeds for reproducible behavior across different runtime sessions.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version information for environment transparency and reproducibility.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Detect available device preference using TensorFlow configuration utilities for safety.\n",
    "physical_gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "use_gpu = bool(physical_gpus)\n",
    "print(\"Using GPU:\", use_gpu)\n",
    "\n",
    "# Define simple parameters for sequence length and model dimensionality sizes.\n",
    "sequence_length = 5\n",
    "model_dimension = 8\n",
    "feedforward_hidden = 16\n",
    "\n",
    "# Create a small batch of token indices representing toy sentences for demonstration.\n",
    "vocab_size = 50\n",
    "batch_size = 4\n",
    "sample_token_indices = np.random.randint(vocab_size, size=(batch_size, sequence_length))\n",
    "\n",
    "# Build an embedding layer to convert token indices into dense vector representations.\n",
    "embedding_layer = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=model_dimension)\n",
    "embedded_tokens = embedding_layer(sample_token_indices)\n",
    "\n",
    "# Define a simple multihead attention layer for contextual token mixing operations.\n",
    "attention_layer = tf.keras.layers.MultiHeadAttention(num_heads=2, key_dim=model_dimension // 2)\n",
    "attention_output = attention_layer(embedded_tokens, embedded_tokens)\n",
    "\n",
    "# Add residual connection around attention output and apply layer normalization operation.\n",
    "attention_residual = embedded_tokens + attention_output\n",
    "norm_layer_one = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "normalized_attention = norm_layer_one(attention_residual)\n",
    "\n",
    "# Define first feedforward Dense layer expanding dimensionality with ReLU activation.\n",
    "ffn_dense_one = tf.keras.layers.Dense(units=feedforward_hidden, activation=\"relu\")\n",
    "expanded_representation = ffn_dense_one(normalized_attention)\n",
    "\n",
    "# Define second feedforward Dense layer projecting back to original model dimension.\n",
    "ffn_dense_two = tf.keras.layers.Dense(units=model_dimension, activation=None)\n",
    "projected_representation = ffn_dense_two(expanded_representation)\n",
    "\n",
    "# Add residual connection around feedforward network and apply second normalization.\n",
    "feedforward_residual = normalized_attention + projected_representation\n",
    "norm_layer_two = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "encoder_output = norm_layer_two(feedforward_residual)\n",
    "\n",
    "# Print shapes before and after feedforward to highlight expansion and compression behavior.\n",
    "print(\"Embedded tokens shape:\", embedded_tokens.shape)\n",
    "print(\"Expanded representation shape:\", expanded_representation.shape)\n",
    "print(\"Encoder output shape:\", encoder_output.shape)\n",
    "\n",
    "# Show a small slice of one token vector before and after feedforward transformation.\n",
    "print(\"First token before feedforward:\", normalized_attention[0, 0].numpy())\n",
    "print(\"First token after feedforward:\", encoder_output[0, 0].numpy())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18805a21",
   "metadata": {},
   "source": [
    "## **3. Comparing Sequence Architectures**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f446bf5",
   "metadata": {},
   "source": [
    "### **3.1. Training Stability and Speed**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ff642b",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_07/Lecture_B/image_03_01.jpg?v=1768980229\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Compare models by training stability and speed\n",
    ">* Prefer slightly less accurate but reliably trainable models\n",
    "\n",
    ">* RNNs train slower per step but steadily\n",
    ">* Transformers train faster yet need careful tuning\n",
    "\n",
    ">* Measure stability, speed, and training curve behavior\n",
    ">* Prefer models that converge reliably faster with consistency\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1c7209",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Training Stability and Speed\n",
    "\n",
    "# This script compares training stability and speed for two simple sequence models.\n",
    "# It trains a GRU and a Transformer encoder on a tiny text dataset.\n",
    "# It prints loss histories and timing to illustrate stability and speed.\n",
    "\n",
    "# !pip install tensorflow\n",
    "\n",
    "# Import required standard libraries and TensorFlow framework.\n",
    "import os, time, random, numpy as np, tensorflow as tf\n",
    "\n",
    "# Set deterministic seeds for reproducible and stable training behavior.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version information for reproducibility and environment clarity.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Select appropriate device based on GPU availability for fair speed comparison.\n",
    "physical_gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "if physical_gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(physical_gpus[0], True)\n",
    "    except Exception as device_error:\n",
    "        print(\"GPU configuration issue, using CPU instead\")\n",
    "\n",
    "# Create a tiny synthetic sentiment dataset with short text examples.\n",
    "texts = [\n",
    "    \"I love this movie so much\",\n",
    "    \"This film was terrible and boring\",\n",
    "    \"Absolutely fantastic experience and great acting\",\n",
    "    \"I hated every minute of it\",\n",
    "    \"What a wonderful and inspiring story\",\n",
    "    \"The plot was dull and predictable\",\n",
    "    \"Brilliant direction and superb cast\",\n",
    "    \"Worst movie I have ever seen\",\n",
    "]\n",
    "\n",
    "# Create corresponding binary labels for positive and negative sentiment examples.\n",
    "labels = [1, 0, 1, 0, 1, 0, 1, 0]\n",
    "\n",
    "# Convert lists into TensorFlow dataset friendly NumPy arrays.\n",
    "texts_array = np.array(texts, dtype=object)\n",
    "labels_array = np.array(labels, dtype=np.int32)\n",
    "\n",
    "# Tokenize text into integer sequences with limited vocabulary size.\n",
    "max_words = 1000\n",
    "max_len = 12\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
    "\n",
    "# Fit tokenizer on the small text corpus for word index creation.\n",
    "tokenizer.fit_on_texts(texts_array.tolist())\n",
    "sequences = tokenizer.texts_to_sequences(texts_array.tolist())\n",
    "\n",
    "# Pad sequences to fixed length for batch processing compatibility.\n",
    "padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_len, padding=\"post\")\n",
    "\n",
    "# Validate shapes to ensure correct dimensions for model inputs and labels.\n",
    "assert padded_sequences.shape[0] == labels_array.shape[0]\n",
    "assert padded_sequences.shape[1] == max_len\n",
    "\n",
    "# Split data into simple training and validation sets for evaluation.\n",
    "train_size = 6\n",
    "x_train, x_val = padded_sequences[:train_size], padded_sequences[train_size:]\n",
    "y_train, y_val = labels_array[:train_size], labels_array[train_size:]\n",
    "\n",
    "# Build a simple GRU based sequence model for baseline comparison.\n",
    "embedding_dim = 16\n",
    "gru_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(max_words, embedding_dim, input_length=max_len),\n",
    "    tf.keras.layers.GRU(16, return_sequences=False),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "])\n",
    "\n",
    "# Compile GRU model with binary crossentropy loss and Adam optimizer.\n",
    "gru_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "                  loss=\"binary_crossentropy\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "\n",
    "# Build a tiny Transformer style encoder model for comparison.\n",
    "inputs = tf.keras.Input(shape=(max_len,), dtype=\"int32\")\n",
    "embedding_layer = tf.keras.layers.Embedding(max_words, embedding_dim)(inputs)\n",
    "\n",
    "# Add simple self attention using MultiHeadAttention layer.\n",
    "attention_output = tf.keras.layers.MultiHeadAttention(num_heads=2, key_dim=8)(\n",
    "    embedding_layer, embedding_layer\n",
    ")\n",
    "\n",
    "# Add residual connection and layer normalization for stability.\n",
    "attention_output = tf.keras.layers.LayerNormalization(epsilon=1e-6)(\n",
    "    embedding_layer + attention_output\n",
    ")\n",
    "\n",
    "# Add small feedforward network with dropout for regularization.\n",
    "ffn = tf.keras.layers.Dense(16, activation=\"relu\")(attention_output)\n",
    "ffn = tf.keras.layers.Dropout(0.1)(ffn)\n",
    "\n",
    "# Add second residual connection and layer normalization.\n",
    "encoder_output = tf.keras.layers.LayerNormalization(epsilon=1e-6)(\n",
    "    attention_output + ffn\n",
    ")\n",
    "\n",
    "# Pool sequence outputs using global average pooling for classification.\n",
    "pooled_output = tf.keras.layers.GlobalAveragePooling1D()(encoder_output)\n",
    "outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(pooled_output)\n",
    "\n",
    "# Create Transformer encoder model using functional API.\n",
    "transformer_model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Compile Transformer model with same optimizer and loss for fairness.\n",
    "transformer_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "                          loss=\"binary_crossentropy\",\n",
    "                          metrics=[\"accuracy\"])\n",
    "\n",
    "# Define small training parameters to keep runtime and memory usage safe.\n",
    "batch_size = 2\n",
    "epochs = 8\n",
    "\n",
    "# Train GRU model while measuring wall clock training time.\n",
    "start_time_gru = time.time()\n",
    "history_gru = gru_model.fit(x_train, y_train,\n",
    "                            validation_data=(x_val, y_val),\n",
    "                            batch_size=batch_size,\n",
    "                            epochs=epochs,\n",
    "                            verbose=0)\n",
    "end_time_gru = time.time()\n",
    "\n",
    "# Train Transformer model while measuring wall clock training time.\n",
    "start_time_trans = time.time()\n",
    "history_trans = transformer_model.fit(x_train, y_train,\n",
    "                                      validation_data=(x_val, y_val),\n",
    "                                      batch_size=batch_size,\n",
    "                                      epochs=epochs,\n",
    "                                      verbose=0)\n",
    "end_time_trans = time.time()\n",
    "\n",
    "# Extract training losses for both models to inspect stability behavior.\n",
    "gru_losses = history_gru.history[\"loss\"]\n",
    "trans_losses = history_trans.history[\"loss\"]\n",
    "\n",
    "# Print concise comparison of training stability and speed metrics.\n",
    "print(\"GRU training losses:\", np.round(gru_losses, 3))\n",
    "print(\"Transformer training losses:\", np.round(trans_losses, 3))\n",
    "print(\"GRU training time seconds:\", round(end_time_gru - start_time_gru, 3))\n",
    "print(\"Transformer training time seconds:\", round(end_time_trans - start_time_trans, 3))\n",
    "print(\"GRU final validation accuracy:\", round(history_gru.history[\"val_accuracy\"][-1], 3))\n",
    "print(\"Transformer final validation accuracy:\", round(history_trans.history[\"val_accuracy\"][-1], 3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacd6b21",
   "metadata": {},
   "source": [
    "### **3.2. Overfitting Across Architectures**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ce845a",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_07/Lecture_B/image_03_02.jpg?v=1768980300\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* RNNs compress sequence history into hidden states\n",
    ">* Large recurrent models can still severely overfit\n",
    "\n",
    ">* Transformers memorize patterns easily through powerful self-attention\n",
    ">* They overfit spurious cues, needing careful validation monitoring\n",
    "\n",
    ">* Compare models under distribution shifts and difficulty\n",
    ">* Use insights to choose architecture-specific regularization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc38e40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Overfitting Across Architectures\n",
    "\n",
    "# This script compares overfitting between LSTM and Transformer encoders on text data.\n",
    "# It trains tiny models briefly and prints training versus validation accuracies.\n",
    "# The goal is illustrating how different architectures can overfit with limited data.\n",
    "\n",
    "# !pip install tensorflow==2.20.0\n",
    "\n",
    "# Import required standard libraries and TensorFlow framework modules.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Print TensorFlow version information for reproducibility and environment clarity.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Set deterministic random seeds for reproducible training and evaluation behavior.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Select computation device preferring GPU when available otherwise defaulting to CPU.\n",
    "physical_gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "if physical_gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(physical_gpus[0], True)\n",
    "    except Exception as device_error:\n",
    "        print(\"GPU configuration issue, using default settings.\")\n",
    "\n",
    "# Load IMDB dataset with subwords tokenizer for compact text representation.\n",
    "imdb_data, imdb_info = tf.keras.datasets.imdb, tf.keras.datasets.imdb.get_word_index()\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=8000)\n",
    "\n",
    "# Restrict dataset size to keep training fast and highlight overfitting behavior.\n",
    "train_samples = 4000\n",
    "test_samples = 2000\n",
    "x_train_small = x_train[:train_samples]\n",
    "y_train_small = y_train[:train_samples]\n",
    "x_test_small = x_test[:test_samples]\n",
    "y_test_small = y_test[:test_samples]\n",
    "\n",
    "# Pad sequences to fixed length for batching compatibility across both architectures.\n",
    "max_length = 150\n",
    "x_train_padded = tf.keras.preprocessing.sequence.pad_sequences(x_train_small, maxlen=max_length)\n",
    "x_test_padded = tf.keras.preprocessing.sequence.pad_sequences(x_test_small, maxlen=max_length)\n",
    "\n",
    "# Verify padded shapes to ensure correct dimensions for subsequent model definitions.\n",
    "print(\"Train shape:\", x_train_padded.shape)\n",
    "print(\"Test shape:\", x_test_padded.shape)\n",
    "\n",
    "# Build a simple LSTM based sequence classifier with modest capacity.\n",
    "embedding_dim = 32\n",
    "lstm_units = 32\n",
    "inputs_lstm = tf.keras.Input(shape=(max_length,))\n",
    "embedding_lstm = tf.keras.layers.Embedding(8000, embedding_dim)(inputs_lstm)\n",
    "encoded_lstm = tf.keras.layers.LSTM(lstm_units)(embedding_lstm)\n",
    "outputs_lstm = tf.keras.layers.Dense(1, activation=\"sigmoid\")(encoded_lstm)\n",
    "model_lstm = tf.keras.Model(inputs=inputs_lstm, outputs=outputs_lstm)\n",
    "\n",
    "# Build a simple Transformer style encoder classifier using MultiHeadAttention.\n",
    "inputs_trans = tf.keras.Input(shape=(max_length,))\n",
    "embedding_trans = tf.keras.layers.Embedding(8000, embedding_dim)(inputs_trans)\n",
    "attention_layer = tf.keras.layers.MultiHeadAttention(num_heads=2, key_dim=16)\n",
    "attended_output = attention_layer(embedding_trans, embedding_trans)\n",
    "pooled_output = tf.keras.layers.GlobalAveragePooling1D()(attended_output)\n",
    "outputs_trans = tf.keras.layers.Dense(1, activation=\"sigmoid\")(pooled_output)\n",
    "model_trans = tf.keras.Model(inputs=inputs_trans, outputs=outputs_trans)\n",
    "\n",
    "# Compile both models with identical optimizer and loss for fair comparison.\n",
    "optimizer_lstm = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "optimizer_trans = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "model_lstm.compile(optimizer=optimizer_lstm, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "model_trans.compile(optimizer=optimizer_trans, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train LSTM model for several epochs to encourage visible overfitting behavior.\n",
    "history_lstm = model_lstm.fit(x_train_padded, y_train_small, validation_split=0.3, epochs=5, batch_size=64, verbose=0)\n",
    "\n",
    "# Train Transformer model similarly and observe different overfitting characteristics.\n",
    "history_trans = model_trans.fit(x_train_padded, y_train_small, validation_split=0.3, epochs=5, batch_size=64, verbose=0)\n",
    "\n",
    "# Evaluate both models on held out test data for generalization performance comparison.\n",
    "loss_lstm, acc_lstm = model_lstm.evaluate(x_test_padded, y_test_small, verbose=0)\n",
    "loss_trans, acc_trans = model_trans.evaluate(x_test_padded, y_test_small, verbose=0)\n",
    "\n",
    "# Helper function prints final epoch metrics summarizing overfitting indicators.\n",
    "def summarize_history(label, history_object, test_accuracy_value):\n",
    "    train_acc_values = history_object.history[\"accuracy\"]\n",
    "    val_acc_values = history_object.history[\"val_accuracy\"]\n",
    "    final_train = float(train_acc_values[-1])\n",
    "    final_val = float(val_acc_values[-1])\n",
    "    print(f\"{label} train accuracy: {final_train:.3f}, validation accuracy: {final_val:.3f}, test accuracy: {test_accuracy_value:.3f}\")\n",
    "\n",
    "# Print concise comparison showing training versus validation and test accuracies.\n",
    "summarize_history(\"LSTM\", history_lstm, acc_lstm)\n",
    "summarize_history(\"Transformer\", history_trans, acc_trans)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447b5ed9",
   "metadata": {},
   "source": [
    "### **3.3. Selecting Sequence Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd8abcd",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_07/Lecture_B/image_03_03.jpg?v=1768980414\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Match model choice to task constraints, metrics\n",
    ">* Prefer simpler models when performance is similar\n",
    "\n",
    ">* Compare architectures across lengths, data, and noise\n",
    ">* Use RNNs for short texts, transformers for long\n",
    "\n",
    ">* Consider lifecycle, transfer learning, and explainability needs\n",
    ">* Use error analysis to balance performance and fairness\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc04045",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Sequence Models**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149a65eb",
   "metadata": {},
   "source": [
    "\n",
    "In this lecture, you learned to:\n",
    "- Build LSTM or GRU-based sequence models for text classification using tf.keras. \n",
    "- Implement a simple transformer-style encoder block using Keras layers for sequence modeling. \n",
    "- Evaluate and compare the performance of different sequence architectures on an NLP task. \n",
    "\n",
    "In the next Module (Module 8), we will go over 'Distributed Training'"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
