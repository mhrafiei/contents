{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bea04fb",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Sequence Models**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023857cc",
   "metadata": {},
   "source": [
    ">Last update: 20260126.\n",
    "    \n",
    "By the end of this Lecture, you will be able to:\n",
    "- Build LSTM or GRU-based sequence models for text classification using tf.keras. \n",
    "- Implement a simple transformer-style encoder block using Keras layers for sequence modeling. \n",
    "- Evaluate and compare the performance of different sequence architectures on an NLP task. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da67d70f",
   "metadata": {},
   "source": [
    "## **1. Recurrent Neural Networks**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ee75db",
   "metadata": {},
   "source": [
    "### **1.1. LSTM and GRU Essentials**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846a1d1f",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_07/Lecture_B/image_01_01.jpg?v=1769440720\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* LSTMs and GRUs handle long text dependencies\n",
    ">* Gates manage context for accurate text classification\n",
    "\n",
    ">* LSTMs use gates to manage long and short memory\n",
    ">* GRUs simplify gating yet match LSTM text performance\n",
    "\n",
    ">* Choose LSTMs or GRUs based on data\n",
    ">* Both maintain context over time for classification\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5256473",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - LSTM and GRU Essentials\n",
    "\n",
    "# This script shows simple LSTM and GRU usage.\n",
    "# It builds tiny text classifiers with TensorFlow.\n",
    "# Focus on essentials for recurrent sequence models.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Import TensorFlow and Keras layers.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Set deterministic random seeds.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version briefly.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Prepare a tiny toy text dataset.\n",
    "texts = [\n",
    "    \"I love this movie it is great\",\n",
    "    \"This film was terrible and boring\",\n",
    "    \"Amazing acting and wonderful story\",\n",
    "    \"I hate this movie it is awful\",\n",
    "    \"What a fantastic and enjoyable film\",\n",
    "    \"The plot was dull and predictable\",\n",
    "]\n",
    "\n",
    "# Create binary sentiment labels.\n",
    "labels = np.array([1, 0, 1, 0, 1, 0], dtype=np.int32)\n",
    "\n",
    "# Use Keras TextVectorization for tokenization.\n",
    "max_tokens = 1000\n",
    "max_length = 10\n",
    "vectorizer = layers.TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=max_length,\n",
    ")\n",
    "\n",
    "# Adapt vectorizer on the small corpus.\n",
    "vectorizer.adapt(texts)\n",
    "\n",
    "# Vectorize the raw text data.\n",
    "text_ds = tf.constant(texts)\n",
    "sequences = vectorizer(text_ds)\n",
    "\n",
    "# Validate resulting tensor shape.\n",
    "print(\"Vectorized shape:\", sequences.shape)\n",
    "\n",
    "# Split data into train and validation.\n",
    "train_sequences = sequences[:4]\n",
    "train_labels = labels[:4]\n",
    "val_sequences = sequences[4:]\n",
    "val_labels = labels[4:]\n",
    "\n",
    "# Define a function building an LSTM model.\n",
    "def build_lstm_model(vocab_size, sequence_length):\n",
    "    inputs = keras.Input(shape=(sequence_length,))\n",
    "    x = layers.Embedding(vocab_size, 16)(inputs)\n",
    "    x = layers.LSTM(16)(x)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "# Define a function building a GRU model.\n",
    "def build_gru_model(vocab_size, sequence_length):\n",
    "    inputs = keras.Input(shape=(sequence_length,))\n",
    "    x = layers.Embedding(vocab_size, 16)(inputs)\n",
    "    x = layers.GRU(16)(x)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Build LSTM and GRU models.\n",
    "lstm_model = build_lstm_model(max_tokens, max_length)\n",
    "gru_model = build_gru_model(max_tokens, max_length)\n",
    "\n",
    "# Train LSTM model briefly.\n",
    "history_lstm = lstm_model.fit(\n",
    "    train_sequences,\n",
    "    train_labels,\n",
    "    epochs=10,\n",
    "    batch_size=2,\n",
    "    validation_data=(val_sequences, val_labels),\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Train GRU model briefly.\n",
    "history_gru = gru_model.fit(\n",
    "    train_sequences,\n",
    "    train_labels,\n",
    "    epochs=10,\n",
    "    batch_size=2,\n",
    "    validation_data=(val_sequences, val_labels),\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Evaluate both models on validation.\n",
    "val_loss_lstm, val_acc_lstm = lstm_model.evaluate(\n",
    "    val_sequences,\n",
    "    val_labels,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Evaluate GRU model similarly.\n",
    "val_loss_gru, val_acc_gru = gru_model.evaluate(\n",
    "    val_sequences,\n",
    "    val_labels,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Print concise comparison of accuracies.\n",
    "print(\"LSTM validation accuracy:\", round(float(val_acc_lstm), 3))\n",
    "print(\"GRU validation accuracy:\", round(float(val_acc_gru), 3))\n",
    "\n",
    "# Show predictions for a few new sentences.\n",
    "new_texts = [\n",
    "    \"I really enjoyed this wonderful movie\",\n",
    "    \"This was a horrible and dull film\",\n",
    "]\n",
    "\n",
    "# Vectorize new sentences.\n",
    "new_seq = vectorizer(tf.constant(new_texts))\n",
    "\n",
    "# Get LSTM and GRU predictions.\n",
    "pred_lstm = lstm_model.predict(new_seq, verbose=0)\n",
    "pred_gru = gru_model.predict(new_seq, verbose=0)\n",
    "\n",
    "# Print predictions rounded for clarity.\n",
    "for i, txt in enumerate(new_texts):\n",
    "    print(\"Text:\", txt)\n",
    "    print(\n",
    "        \"LSTM prob:\", round(float(pred_lstm[i][0]), 3),\n",
    "        \"GRU prob:\", round(float(pred_gru[i][0]), 3),\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df00b72d",
   "metadata": {},
   "source": [
    "### **1.2. Bidirectional RNN Layers**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa1ce5d",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_07/Lecture_B/image_01_02.jpg?v=1769440818\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Bidirectional RNNs read sequences forward and backward\n",
    ">* Each token uses both past and future context\n",
    "\n",
    ">* Forward and backward passes give full sentence context\n",
    ">* Merged states capture long-range patterns and nuances\n",
    "\n",
    ">* Place bidirectional layer after word embeddings\n",
    ">* Summarize outputs using pooling, states, or attention\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a15113c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Bidirectional RNN Layers\n",
    "\n",
    "# This script shows bidirectional RNN layers.\n",
    "# It builds a tiny text classifier example.\n",
    "# It runs quickly and prints concise results.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Import TensorFlow and Keras utilities.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Set deterministic random seeds.\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Set TensorFlow random seed.\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# Print TensorFlow version once.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Prepare a tiny toy text dataset.\n",
    "texts = [\n",
    "    \"I love this movie so much\",\n",
    "    \"This film was terrible and boring\",\n",
    "    \"Absolutely fantastic acting and story\",\n",
    "    \"I hated the plot and characters\",\n",
    "    \"What a great and inspiring film\",\n",
    "    \"The movie was bad and disappointing\",\n",
    "]\n",
    "\n",
    "# Define binary sentiment labels.\n",
    "labels = np.array([1, 0, 1, 0, 1, 0], dtype=np.int32)\n",
    "\n",
    "# Create a simple TextVectorization layer.\n",
    "max_tokens = 1000\n",
    "sequence_length = 10\n",
    "vectorizer = layers.TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length,\n",
    ")\n",
    "\n",
    "# Adapt vectorizer on the small corpus.\n",
    "vectorizer.adapt(texts)\n",
    "\n",
    "# Vectorize the raw text data.\n",
    "text_ds = tf.constant(texts)\n",
    "vectorized_texts = vectorizer(text_ds)\n",
    "\n",
    "# Confirm shapes before building the model.\n",
    "print(\"Vectorized shape:\", vectorized_texts.shape)\n",
    "\n",
    "# Build a simple bidirectional LSTM classifier.\n",
    "embedding_dim = 16\n",
    "inputs = keras.Input(shape=(sequence_length,))\n",
    "\n",
    "# Add an embedding layer after inputs.\n",
    "x = layers.Embedding(max_tokens, embedding_dim)(inputs)\n",
    "\n",
    "# Add a bidirectional LSTM layer.\n",
    "x = layers.Bidirectional(\n",
    "    layers.LSTM(16, return_sequences=False)\n",
    ")(x)\n",
    "\n",
    "# Add a small dense layer for features.\n",
    "x = layers.Dense(16, activation=\"relu\")(x)\n",
    "\n",
    "# Add the final classification output layer.\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "# Create the Keras model object.\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Compile the model with binary crossentropy.\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Train briefly with silent verbose setting.\n",
    "history = model.fit(\n",
    "    vectorized_texts,\n",
    "    labels,\n",
    "    epochs=10,\n",
    "    batch_size=2,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Evaluate the model on the same tiny data.\n",
    "loss, acc = model.evaluate(\n",
    "    vectorized_texts,\n",
    "    labels,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Print a short summary of performance.\n",
    "print(\"Bidirectional model accuracy:\", round(float(acc), 3))\n",
    "\n",
    "# Demonstrate prediction on a new sentence.\n",
    "new_texts = tf.constant([\n",
    "    \"The movie was surprisingly good overall\",\n",
    "])\n",
    "\n",
    "# Vectorize the new sentence.\n",
    "new_vec = vectorizer(new_texts)\n",
    "\n",
    "# Get prediction probability from the model.\n",
    "prob = model.predict(new_vec, verbose=0)[0, 0]\n",
    "\n",
    "# Print the predicted sentiment probability.\n",
    "print(\"Predicted positive sentiment probability:\", round(float(prob), 3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad039af",
   "metadata": {},
   "source": [
    "### **1.3. Handling variable sequences**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067fa5d0",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_07/Lecture_B/image_01_03.jpg?v=1769440861\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Real text sequences vary widely in length\n",
    ">* We pad and mask to batch efficiently, accurately\n",
    "\n",
    ">* Pad or truncate token sequences to length\n",
    ">* Use masks so RNN ignores padding tokens\n",
    "\n",
    ">* Use final states aligned with real tokens\n",
    ">* Batch similar lengths; tune padding and masking\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd09fc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Handling variable sequences\n",
    "\n",
    "# This script shows padding and masking usage.\n",
    "# It builds a tiny LSTM text classifier.\n",
    "# It highlights handling variable length sequences.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Import TensorFlow and Keras utilities.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Prepare a tiny toy text dataset.\n",
    "texts = [\n",
    "    \"bad\",\n",
    "    \"very bad\",\n",
    "    \"really bad movie\",\n",
    "    \"good\",\n",
    "    \"very good\",\n",
    "    \"really good movie\",\n",
    "]\n",
    "\n",
    "# Create matching sentiment labels for texts.\n",
    "labels = np.array([0, 0, 0, 1, 1, 1], dtype=\"int32\")\n",
    "\n",
    "# Build a TextVectorization layer with padding.\n",
    "max_tokens = 100\n",
    "max_length = 5\n",
    "vectorizer = layers.TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=max_length,\n",
    ")\n",
    "\n",
    "# Adapt vectorizer on the small text dataset.\n",
    "vectorizer.adapt(texts)\n",
    "\n",
    "# Vectorize texts into padded integer sequences.\n",
    "text_ds = tf.data.Dataset.from_tensor_slices(texts)\n",
    "seq_ds = text_ds.map(vectorizer)\n",
    "sequences = np.stack(list(seq_ds.as_numpy_iterator()))\n",
    "\n",
    "# Show original texts and padded sequences.\n",
    "for t, s in zip(texts, sequences):\n",
    "    print(\"Text:\", repr(t), \"->\", s)\n",
    "\n",
    "# Confirm padded sequence shape is as expected.\n",
    "print(\"Sequence batch shape:\", sequences.shape)\n",
    "\n",
    "# Build a simple LSTM model with masking.\n",
    "embedding_dim = 8\n",
    "model = keras.Sequential([\n",
    "    layers.Embedding(\n",
    "        input_dim=max_tokens,\n",
    "        output_dim=embedding_dim,\n",
    "        mask_zero=True,\n",
    "        input_length=max_length,\n",
    "    ),\n",
    "    layers.LSTM(16),\n",
    "    layers.Dense(1, activation=\"sigmoid\"),\n",
    "])\n",
    "\n",
    "# Compile model with binary crossentropy loss.\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Train briefly with very small epochs.\n",
    "history = model.fit(\n",
    "    sequences,\n",
    "    labels,\n",
    "    batch_size=2,\n",
    "    epochs=10,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Evaluate model performance on same tiny data.\n",
    "loss, acc = model.evaluate(\n",
    "    sequences,\n",
    "    labels,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Print evaluation metrics in a compact form.\n",
    "print(\"Loss:\", round(float(loss), 4), \"Accuracy:\", round(float(acc), 4))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daefbd8f",
   "metadata": {},
   "source": [
    "## **2. Transformer Encoder Block**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ea4ef5",
   "metadata": {},
   "source": [
    "### **2.1. Multihead Attention Basics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d5bd88",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_07/Lecture_B/image_02_01.jpg?v=1769440929\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Attention lets each token focus on others\n",
    ">* Weighted combinations capture long-range dependencies without recurrence\n",
    "\n",
    ">* Multiple heads learn different sequence relationships simultaneously\n",
    ">* Their combined views give richer token representations\n",
    "\n",
    ">* Parallel heads link any tokens, capturing long dependencies\n",
    ">* Enables context-aware representations for many NLP tasks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2d4b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Multihead Attention Basics\n",
    "\n",
    "# This script explains basic multihead attention concepts.\n",
    "# It uses TensorFlow to build a tiny example.\n",
    "# Run cells in order to follow the explanation.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required modules from TensorFlow.\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "tf.random.set_seed(7)\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Define tiny toy token id sequences for a batch.\n",
    "toy_token_ids = tf.constant([[1, 2, 3, 0], [4, 5, 0, 0]])\n",
    "\n",
    "# Show the toy token id tensor shape.\n",
    "print(\"Token ids shape:\", toy_token_ids.shape)\n",
    "\n",
    "# Define basic configuration hyperparameters for the demo.\n",
    "vocab_size, embed_dim, num_heads = 20, 8, 2\n",
    "\n",
    "# Create a simple embedding layer for token ids.\n",
    "embedding_layer = tf.keras.layers.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "# Embed the toy token ids into dense vectors.\n",
    "embedded_tokens = embedding_layer(toy_token_ids)\n",
    "\n",
    "# Confirm the embedded tensor has expected shape.\n",
    "print(\"Embedded shape:\", embedded_tokens.shape)\n",
    "\n",
    "# Create a padding mask where zeros mark padded positions.\n",
    "padding_mask = tf.cast(tf.math.not_equal(toy_token_ids, 0), tf.float32)\n",
    "\n",
    "# Reshape mask to match attention expected dimensions.\n",
    "attention_mask = padding_mask[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "# Show the attention mask shape for verification.\n",
    "print(\"Attention mask shape:\", attention_mask.shape)\n",
    "\n",
    "# Create a MultiHeadAttention layer for self attention.\n",
    "mha_layer = tf.keras.layers.MultiHeadAttention(num_heads, embed_dim)\n",
    "\n",
    "# Use the same tensor for queries, keys, and values.\n",
    "query_tensor = embedded_tokens\n",
    "\n",
    "# Apply multihead self attention with the padding mask.\n",
    "attended_output, attention_scores = mha_layer(\n",
    "    query=query_tensor,\n",
    "    value=embedded_tokens,\n",
    "    key=embedded_tokens,\n",
    "    attention_mask=attention_mask,\n",
    "    return_attention_scores=True,\n",
    ")\n",
    "\n",
    "# Confirm the attended output tensor shape.\n",
    "print(\"Attended output shape:\", attended_output.shape)\n",
    "\n",
    "# Confirm the attention scores tensor shape.\n",
    "print(\"Attention scores shape:\", attention_scores.shape)\n",
    "\n",
    "# Take mean attention over heads for easier inspection.\n",
    "mean_scores = tf.reduce_mean(attention_scores, axis=1)\n",
    "\n",
    "# Round scores for compact printing demonstration.\n",
    "rounded_scores = tf.round(mean_scores * 10.0) / 10.0\n",
    "\n",
    "# Print the compact attention scores for the first sequence.\n",
    "print(\"Mean attention scores sample:\")\n",
    "\n",
    "# Print only the first sequence attention matrix.\n",
    "print(rounded_scores[0].numpy())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68eb1e0",
   "metadata": {},
   "source": [
    "### **2.2. Residual Connections and Normalization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b559ec",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_07/Lecture_B/image_02_02.jpg?v=1769440963\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Residual connections add inputs back after attention\n",
    ">* They ease gradient flow and stabilize deep training\n",
    "\n",
    ">* Layer normalization rescales features per individual example\n",
    ">* Applied after residual add to stabilize training\n",
    "\n",
    ">* Residual plus normalization form repeated encoder patterns\n",
    ">* They enable deep, stable, easily stackable transformers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6901cb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Residual Connections and Normalization\n",
    "\n",
    "# This script shows residual connections and normalization.\n",
    "# It builds a tiny transformer encoder using Keras layers.\n",
    "# It trains briefly on dummy text classification data.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Import TensorFlow and Keras submodules.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Define tiny dummy text dataset as integer sequences.\n",
    "num_samples = 64\n",
    "sequence_length = 10\n",
    "vocab_size = 50\n",
    "num_classes = 2\n",
    "\n",
    "# Create random integer token sequences.\n",
    "X = np.random.randint(1, vocab_size, size=(num_samples, sequence_length))\n",
    "\n",
    "# Create random binary labels for classification.\n",
    "y = np.random.randint(0, num_classes, size=(num_samples,))\n",
    "\n",
    "# Split into small train and validation sets.\n",
    "train_size = 48\n",
    "X_train, X_val = X[:train_size], X[train_size:]\n",
    "y_train, y_val = y[:train_size], y[train_size:]\n",
    "\n",
    "# Define a simple transformer encoder block class.\n",
    "class SimpleTransformerEncoder(layers.Layer):\n",
    "    # Initialize attention, dense, and normalization layers.\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.att = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.ffn = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(ff_dim, activation=\"relu\"),\n",
    "                layers.Dense(embed_dim),\n",
    "            ]\n",
    "        )\n",
    "        self.norm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    # Apply attention, residuals, and normalization.\n",
    "    def call(self, inputs, training=False):\n",
    "        attn_output = self.att(inputs, inputs, training=training)\n",
    "        res1 = inputs + attn_output\n",
    "        out1 = self.norm1(res1, training=training)\n",
    "        ffn_output = self.ffn(out1, training=training)\n",
    "        res2 = out1 + ffn_output\n",
    "        return self.norm2(res2, training=training)\n",
    "\n",
    "\n",
    "# Define model hyperparameters for embeddings.\n",
    "embed_dim = 16\n",
    "num_heads = 2\n",
    "ff_dim = 32\n",
    "\n",
    "# Build the Keras model using the encoder block.\n",
    "inputs = keras.Input(shape=(sequence_length,), dtype=\"int32\")\n",
    "\n",
    "# Embed tokens into dense vectors.\n",
    "embedding_layer = layers.Embedding(\n",
    "    input_dim=vocab_size, output_dim=embed_dim\n",
    ")\n",
    "embedded = embedding_layer(inputs)\n",
    "\n",
    "# Apply the custom transformer encoder block.\n",
    "encoder = SimpleTransformerEncoder(\n",
    "    embed_dim=embed_dim, num_heads=num_heads, ff_dim=ff_dim\n",
    ")\n",
    "encoded = encoder(embedded)\n",
    "\n",
    "# Pool sequence representations with global average.\n",
    "pooled = layers.GlobalAveragePooling1D()(encoded)\n",
    "\n",
    "# Add a small dense layer for classification.\n",
    "outputs = layers.Dense(num_classes, activation=\"softmax\")(pooled)\n",
    "\n",
    "# Create and compile the final model.\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(\n",
    "    optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Validate model output shape for safety.\n",
    "sample_out = model(X_train[:2])\n",
    "print(\"Sample output shape:\", sample_out.shape)\n",
    "\n",
    "# Train briefly with silent verbose setting.\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=3,\n",
    "    batch_size=8,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Evaluate model performance on validation set.\n",
    "val_loss, val_acc = model.evaluate(X_val, y_val, verbose=0)\n",
    "\n",
    "# Print concise results about residual encoder performance.\n",
    "print(\"Validation loss:\", float(val_loss))\n",
    "print(\"Validation accuracy:\", float(val_acc))\n",
    "print(\"Encoder block uses residuals and layer normalization.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b399c2",
   "metadata": {},
   "source": [
    "### **2.3. Feedforward Layers Explained**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c39dec9",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_07/Lecture_B/image_02_03.jpg?v=1769441054\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Feedforward layer transforms attention outputs into richer features\n",
    ">* Applies same two-layer MLP to each token\n",
    "\n",
    ">* Shared feedforward network balances power and efficiency\n",
    ">* Expanded dimension and activations learn rich token features\n",
    "\n",
    ">* Feedforward uses dense layers per time step\n",
    ">* Design choices and residuals affect generalization strength\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9c31e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Feedforward Layers Explained\n",
    "\n",
    "# This script explains transformer feedforward layers simply.\n",
    "# It builds a tiny encoder block using Keras layers.\n",
    "# It trains briefly on toy text data for clarity.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries safely.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "random.seed(7)\n",
    "np.random.seed(7)\n",
    "\n",
    "# Import TensorFlow and Keras submodules.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Select device preference based on availability.\n",
    "physical_gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "if physical_gpus:\n",
    "    device_name = \"GPU\"\n",
    "else:\n",
    "    device_name = \"CPU\"\n",
    "\n",
    "# Print which device type will likely be used.\n",
    "print(\"Using device type:\", device_name)\n",
    "\n",
    "# Define tiny toy sentences and labels.\n",
    "texts = [\n",
    "    \"good movie nice story\",\n",
    "    \"bad movie boring plot\",\n",
    "    \"great acting and good direction\",\n",
    "    \"terrible acting and bad script\",\n",
    "]\n",
    "\n",
    "# Define binary sentiment labels for sentences.\n",
    "labels = np.array([1, 0, 1, 0], dtype=\"int32\")\n",
    "\n",
    "# Create a simple Keras tokenizer object.\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(num_words=50)\n",
    "\n",
    "# Fit tokenizer on the tiny text corpus.\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "# Convert texts to integer sequences.\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "# Pad sequences to equal fixed length.\n",
    "max_len = 6\n",
    "padded = keras.preprocessing.sequence.pad_sequences(\n",
    "    sequences, maxlen=max_len, padding=\"post\"\n",
    ")\n",
    "\n",
    "# Convert padded sequences to int32 tensor.\n",
    "inputs_array = np.array(padded, dtype=\"int32\")\n",
    "\n",
    "# Validate shapes before building model.\n",
    "print(\"Input shape:\", inputs_array.shape)\n",
    "\n",
    "# Define model hyperparameters for encoder.\n",
    "vocab_size = 50\n",
    "model_dim = 16\n",
    "ff_dim = 32\n",
    "num_heads = 2\n",
    "\n",
    "# Create model input layer for token ids.\n",
    "inputs = keras.layers.Input(shape=(max_len,), dtype=\"int32\")\n",
    "\n",
    "# Embed tokens into dense vectors.\n",
    "embedding_layer = keras.layers.Embedding(\n",
    "    input_dim=vocab_size, output_dim=model_dim\n",
    ")\n",
    "\n",
    "# Apply embedding to input token ids.\n",
    "embedded = embedding_layer(inputs)\n",
    "\n",
    "# Add simple positional encoding via indices.\n",
    "positions = tf.range(start=0, limit=max_len, delta=1)\n",
    "\n",
    "# Embed positions into same dimensional space.\n",
    "pos_embedding_layer = keras.layers.Embedding(\n",
    "    input_dim=max_len, output_dim=model_dim\n",
    ")\n",
    "\n",
    "# Broadcast positional embeddings across batch.\n",
    "positional = pos_embedding_layer(positions)\n",
    "\n",
    "# Add token and positional embeddings together.\n",
    "encoded_tokens = embedded + positional\n",
    "\n",
    "# Apply multihead self attention layer.\n",
    "attention_layer = keras.layers.MultiHeadAttention(\n",
    "    num_heads=num_heads, key_dim=model_dim\n",
    ")\n",
    "\n",
    "# Use self attention with query key value same.\n",
    "attn_output = attention_layer(\n",
    "    query=encoded_tokens, value=encoded_tokens, key=encoded_tokens\n",
    ")\n",
    "\n",
    "# Add residual connection around attention.\n",
    "attn_residual = keras.layers.Add()([encoded_tokens, attn_output])\n",
    "\n",
    "# Normalize attention output for stability.\n",
    "attn_norm = keras.layers.LayerNormalization(epsilon=1e-6)(attn_residual)\n",
    "\n",
    "# First dense layer expands dimensionality.\n",
    "ffn_dense1 = keras.layers.Dense(ff_dim, activation=\"relu\")\n",
    "\n",
    "# Second dense layer projects back down.\n",
    "ffn_dense2 = keras.layers.Dense(model_dim)\n",
    "\n",
    "# Apply first dense layer position wise.\n",
    "ffn_intermediate = ffn_dense1(attn_norm)\n",
    "\n",
    "# Optionally apply dropout for regularization.\n",
    "ffn_intermediate = keras.layers.Dropout(0.1)(ffn_intermediate)\n",
    "\n",
    "# Apply second dense layer position wise.\n",
    "ffn_output = ffn_dense2(ffn_intermediate)\n",
    "\n",
    "# Add residual connection around feedforward.\n",
    "ffn_residual = keras.layers.Add()([attn_norm, ffn_output])\n",
    "\n",
    "# Normalize feedforward output for stability.\n",
    "ffn_norm = keras.layers.LayerNormalization(epsilon=1e-6)(ffn_residual)\n",
    "\n",
    "# Pool sequence by averaging token representations.\n",
    "pooled = keras.layers.GlobalAveragePooling1D()(ffn_norm)\n",
    "\n",
    "# Final dense layer for binary classification.\n",
    "logits = keras.layers.Dense(1, activation=\"sigmoid\")(pooled)\n",
    "\n",
    "# Build Keras model object from inputs outputs.\n",
    "model = keras.Model(inputs=inputs, outputs=logits)\n",
    "\n",
    "# Compile model with simple optimizer and loss.\n",
    "model.compile(\n",
    "    optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Train briefly with silent verbose setting.\n",
    "history = model.fit(\n",
    "    inputs_array,\n",
    "    labels,\n",
    "    epochs=10,\n",
    "    batch_size=2,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Evaluate model performance on same tiny data.\n",
    "loss, acc = model.evaluate(inputs_array, labels, verbose=0)\n",
    "\n",
    "# Print accuracy to show model is learning.\n",
    "print(\"Training accuracy on tiny set:\", round(float(acc), 3))\n",
    "\n",
    "# Show example prediction before and after feedforward.\n",
    "example_input = inputs_array[:1]\n",
    "\n",
    "# Build intermediate model to inspect tensors.\n",
    "intermediate_model = keras.Model(\n",
    "    inputs=inputs, outputs=[attn_norm, ffn_norm]\n",
    ")\n",
    "\n",
    "# Get attention and feedforward outputs.\n",
    "attn_out_val, ffn_out_val = intermediate_model.predict(\n",
    "    example_input, verbose=0\n",
    ")\n",
    "\n",
    "# Print shapes to highlight position wise behavior.\n",
    "print(\"Attention output shape:\", attn_out_val.shape)\n",
    "print(\"Feedforward output shape:\", ffn_out_val.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d4bd9a",
   "metadata": {},
   "source": [
    "## **3. Comparing Sequence Architectures**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c678366",
   "metadata": {},
   "source": [
    "### **3.1. Training Stability and Speed**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5fa95a",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_07/Lecture_B/image_03_01.jpg?v=1769441097\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Compare models by training stability and speed\n",
    ">* Prefer reliably converging models under real resource limits\n",
    "\n",
    ">* LSTMs and GRUs train steadily but slowly\n",
    ">* Gates aid stability, yet gradients need monitoring\n",
    "\n",
    ">* Transformers train faster using parallel self-attention operations\n",
    ">* They need careful tuning to stay stable and reliable\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69de3193",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Training Stability and Speed\n",
    "\n",
    "# This script compares training stability and speed.\n",
    "# It uses tiny models on simple text data.\n",
    "# Focus on LSTM and Transformer style encoders.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required libraries safely.\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version briefly.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Prepare a tiny synthetic text dataset.\n",
    "texts = [\n",
    "    \"good movie and great acting\",\n",
    "    \"bad movie and terrible acting\",\n",
    "    \"excellent film with nice story\",\n",
    "    \"awful film with boring story\",\n",
    "    \"loved the plot and characters\",\n",
    "    \"hated the plot and characters\",\n",
    "    \"wonderful experience and fun scenes\",\n",
    "    \"horrible experience and dull scenes\",\n",
    "]\n",
    "\n",
    "# Create simple binary sentiment labels.\n",
    "labels = [1, 0, 1, 0, 1, 0, 1, 0]\n",
    "\n",
    "# Convert lists to numpy arrays.\n",
    "labels = np.array(labels, dtype=np.int32)\n",
    "\n",
    "# Define a small TextVectorization layer.\n",
    "max_tokens = 100\n",
    "max_len = 8\n",
    "vectorizer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=max_len,\n",
    ")\n",
    "\n",
    "# Adapt vectorizer on the tiny corpus.\n",
    "vectorizer.adapt(texts)\n",
    "\n",
    "# Vectorize the text data.\n",
    "text_ds = tf.constant(texts)\n",
    "int_sequences = vectorizer(text_ds)\n",
    "\n",
    "# Validate shapes before building datasets.\n",
    "assert int_sequences.shape[0] == labels.shape[0]\n",
    "\n",
    "# Build a small tf.data.Dataset.\n",
    "batch_size = 4\n",
    "dataset = tf.data.Dataset.from_tensor_slices((int_sequences, labels))\n",
    "dataset = dataset.shuffle(buffer_size=8, seed=seed_value)\n",
    "dataset = dataset.batch(batch_size)\n",
    "\n",
    "# Define a function to build an LSTM model.\n",
    "def build_lstm_model(vocab_size, sequence_length):\n",
    "    inputs = tf.keras.Input(shape=(sequence_length,))\n",
    "    x = tf.keras.layers.Embedding(vocab_size, 16)(inputs)\n",
    "    x = tf.keras.layers.LSTM(16)(x)\n",
    "    outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Define a simple Transformer style encoder block.\n",
    "def transformer_encoder_block(inputs, num_heads, key_dim):\n",
    "    attn_output = tf.keras.layers.MultiHeadAttention(\n",
    "        num_heads=num_heads,\n",
    "        key_dim=key_dim,\n",
    "    )(inputs, inputs)\n",
    "    attn_output = tf.keras.layers.Dropout(0.1)(attn_output)\n",
    "    out1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(\n",
    "        inputs + attn_output\n",
    "    )\n",
    "    ffn = tf.keras.Sequential(\n",
    "        [\n",
    "            tf.keras.layers.Dense(32, activation=\"relu\"),\n",
    "            tf.keras.layers.Dense(inputs.shape[-1]),\n",
    "        ]\n",
    "    )\n",
    "    ffn_output = ffn(out1)\n",
    "    ffn_output = tf.keras.layers.Dropout(0.1)(ffn_output)\n",
    "    return tf.keras.layers.LayerNormalization(epsilon=1e-6)(\n",
    "        out1 + ffn_output\n",
    "    )\n",
    "\n",
    "# Define a function to build a Transformer encoder model.\n",
    "def build_transformer_model(vocab_size, sequence_length):\n",
    "    inputs = tf.keras.Input(shape=(sequence_length,))\n",
    "    x = tf.keras.layers.Embedding(vocab_size, 16)(inputs)\n",
    "    x = transformer_encoder_block(x, num_heads=2, key_dim=8)\n",
    "    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "    outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Get vocabulary size and sequence length.\n",
    "vocab_size = max_tokens\n",
    "sequence_length = max_len\n",
    "\n",
    "# Build both models for comparison.\n",
    "lstm_model = build_lstm_model(vocab_size, sequence_length)\n",
    "transformer_model = build_transformer_model(vocab_size, sequence_length)\n",
    "\n",
    "# Define a helper to train and time models.\n",
    "def train_and_time(model, dataset, epochs, name):\n",
    "    start = time.time()\n",
    "    history = model.fit(\n",
    "        dataset,\n",
    "        epochs=epochs,\n",
    "        verbose=0,\n",
    "    )\n",
    "    end = time.time()\n",
    "    losses = history.history[\"loss\"]\n",
    "    accs = history.history[\"accuracy\"]\n",
    "    return {\n",
    "        \"name\": name,\n",
    "        \"time\": end - start,\n",
    "        \"losses\": losses,\n",
    "        \"accs\": accs,\n",
    "    }\n",
    "\n",
    "# Train both models briefly for comparison.\n",
    "results_lstm = train_and_time(lstm_model, dataset, epochs=5, name=\"LSTM\")\n",
    "results_trans = train_and_time(\n",
    "    transformer_model,\n",
    "    dataset,\n",
    "    epochs=5,\n",
    "    name=\"Transformer\",\n",
    ")\n",
    "\n",
    "# Print a compact comparison of stability and speed.\n",
    "print(\"\\nModel comparison on tiny dataset:\")\n",
    "print(\"LSTM time seconds:\", round(results_lstm[\"time\"], 4))\n",
    "print(\"Transformer time seconds:\", round(results_trans[\"time\"], 4))\n",
    "print(\"LSTM losses:\", [round(x, 4) for x in results_lstm[\"losses\"]])\n",
    "print(\"Transformer losses:\", [round(x, 4) for x in results_trans[\"losses\"]])\n",
    "print(\"LSTM accuracies:\", [round(x, 3) for x in results_lstm[\"accs\"]])\n",
    "print(\"Transformer accuracies:\", [round(x, 3) for x in results_trans[\"accs\"]])\n",
    "\n",
    "# Show which model trained faster on this run.\n",
    "faster = (\n",
    "    \"LSTM\" if results_lstm[\"time\"] < results_trans[\"time\"] else \"Transformer\"\n",
    ")\n",
    "print(\"Faster model on this tiny example:\", faster)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c03d656",
   "metadata": {},
   "source": [
    "### **3.2. Overfitting Across Architectures**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b15182",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_07/Lecture_B/image_03_02.jpg?v=1769441214\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* RNNs compress sequences, slightly limiting overfitting capacity\n",
    ">* Transformers overfit faster due to higher capacity\n",
    "\n",
    ">* Overfitting patterns depend on task and length\n",
    ">* Compare train versus validation curves across architectures\n",
    "\n",
    ">* Compare models while varying size, regularization, training\n",
    ">* Use validation gaps to match capacity to data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8805ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Overfitting Across Architectures\n",
    "\n",
    "# This script compares overfitting across architectures.\n",
    "# It trains tiny GRU and Transformer text classifiers.\n",
    "# It prints training and validation accuracy for both.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Import TensorFlow and Keras submodules.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Load IMDB dataset with limited vocabulary.\n",
    "(vocab_train, labels_train), (vocab_test, labels_test) = (\n",
    "    keras.datasets.imdb.load_data(num_words=5000)\n",
    ")\n",
    "\n",
    "# Use small subsets for quick demonstration.\n",
    "train_samples = 4000\n",
    "test_samples = 2000\n",
    "x_train = vocab_train[:train_samples]\n",
    "y_train = labels_train[:train_samples]\n",
    "\n",
    "# Slice test data subset.\n",
    "x_test = vocab_test[:test_samples]\n",
    "y_test = labels_test[:test_samples]\n",
    "\n",
    "# Pad sequences to fixed maximum length.\n",
    "max_len = 150\n",
    "x_train = keras.preprocessing.sequence.pad_sequences(\n",
    "    x_train, maxlen=max_len\n",
    ")\n",
    "\n",
    "# Pad test sequences similarly.\n",
    "x_test = keras.preprocessing.sequence.pad_sequences(\n",
    "    x_test, maxlen=max_len\n",
    ")\n",
    "\n",
    "# Validate shapes before building models.\n",
    "assert x_train.shape[0] == train_samples\n",
    "assert x_test.shape[0] == test_samples\n",
    "\n",
    "# Create a simple GRU based classifier.\n",
    "def build_gru_model(vocab_size, embed_dim, units):\n",
    "    inputs = keras.Input(shape=(max_len,))\n",
    "    x = layers.Embedding(vocab_size, embed_dim)(inputs)\n",
    "    x = layers.GRU(units, dropout=0.2)(x)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Create a tiny Transformer style encoder classifier.\n",
    "def build_transformer_model(vocab_size, embed_dim, heads):\n",
    "    inputs = keras.Input(shape=(max_len,))\n",
    "    x = layers.Embedding(vocab_size, embed_dim)(inputs)\n",
    "    attn_output = layers.MultiHeadAttention(\n",
    "        num_heads=heads, key_dim=embed_dim\n",
    "    )(x, x)\n",
    "    x = layers.Add()([x, attn_output])\n",
    "    x = layers.LayerNormalization()(x)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Define shared training configuration.\n",
    "batch_size = 64\n",
    "epochs = 4\n",
    "\n",
    "# Build GRU model with modest capacity.\n",
    "gru_model = build_gru_model(vocab_size=5000, embed_dim=32, units=32)\n",
    "\n",
    "# Train GRU model silently with validation split.\n",
    "hist_gru = gru_model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_split=0.2,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Build Transformer style model with higher capacity.\n",
    "transformer_model = build_transformer_model(\n",
    "    vocab_size=5000, embed_dim=64, heads=2\n",
    ")\n",
    "\n",
    "# Train Transformer model silently with validation split.\n",
    "hist_trans = transformer_model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_split=0.2,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Helper function to fetch last epoch metrics.\n",
    "def last_metrics(history):\n",
    "    acc = history.history[\"accuracy\"][-1]\n",
    "    val_acc = history.history[\"val_accuracy\"][-1]\n",
    "    return float(acc), float(val_acc)\n",
    "\n",
    "# Compute final training and validation accuracy.\n",
    "gru_acc, gru_val_acc = last_metrics(hist_gru)\n",
    "trans_acc, trans_val_acc = last_metrics(hist_trans)\n",
    "\n",
    "# Print concise comparison of overfitting behavior.\n",
    "print(\"GRU train acc:\", round(gru_acc, 3))\n",
    "print(\"GRU val acc:\", round(gru_val_acc, 3))\n",
    "print(\"Transformer train acc:\", round(trans_acc, 3))\n",
    "print(\"Transformer val acc:\", round(trans_val_acc, 3))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7e17ff",
   "metadata": {},
   "source": [
    "### **3.3. Choosing architectures for tasks**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838121a5",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_07/Lecture_B/image_03_03.jpg?v=1769441333\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Match model choice to text length, outputs\n",
    ">* Balance transformer benefits against compute and resources\n",
    "\n",
    ">* Local cues suit either recurrent or transformers\n",
    ">* Long-range or streaming needs favor different architectures\n",
    "\n",
    ">* Compare models using metrics and practical constraints\n",
    ">* Balance accuracy with latency, resources, and deployment needs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a7efbd",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Sequence Models**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15893a89",
   "metadata": {},
   "source": [
    "\n",
    "In this lecture, you learned to:\n",
    "- Build LSTM or GRU-based sequence models for text classification using tf.keras. \n",
    "- Implement a simple transformer-style encoder block using Keras layers for sequence modeling. \n",
    "- Evaluate and compare the performance of different sequence architectures on an NLP task. \n",
    "\n",
    "In the next Module (Module 8), we will go over 'Distributed Training'"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
