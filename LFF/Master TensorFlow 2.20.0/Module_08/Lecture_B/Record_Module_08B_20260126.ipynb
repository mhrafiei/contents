{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16bf97e3",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Implementing Strategies**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0794f01",
   "metadata": {},
   "source": [
    ">Last update: 20260126.\n",
    "    \n",
    "By the end of this Lecture, you will be able to:\n",
    "- Configure and run model.fit under a tf.distribute strategy for multi-GPU or multi-worker setups. \n",
    "- Implement a custom training loop that uses strategy.run and distributed datasets. \n",
    "- Troubleshoot common distributed training errors related to shapes, batch sizes, and variable placement. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c2e3fd",
   "metadata": {},
   "source": [
    "## **1. Using fit with Strategies**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ae523b",
   "metadata": {},
   "source": [
    "### **1.1. Working With strategy scope**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f2b1dc",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_08/Lecture_B/image_01_01.jpg?v=1769459157\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Strategy scope controls model and variable creation\n",
    ">* Enables automatic multi-device placement using model.fit\n",
    "\n",
    ">* Create strategy, enter scope, then build components\n",
    ">* Building outside scope breaks distribution and devices\n",
    "\n",
    ">* Strategy scope controls variable placement and communication\n",
    ">* Ensures consistent training, checkpoints, and scaling behavior\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ff6bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Working With strategy scope\n",
    "\n",
    "# This script shows using strategy scope correctly.\n",
    "# It demonstrates model creation inside strategy scope.\n",
    "# It keeps training simple with minimal printed output.\n",
    "\n",
    "# Install TensorFlow if needed in some environments.\n",
    "# pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Import TensorFlow and Keras utilities.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Detect available GPUs for potential distribution.\n",
    "physical_gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "print(\"GPUs detected:\", len(physical_gpus))\n",
    "\n",
    "# Choose a simple strategy based on available devices.\n",
    "if len(physical_gpus) > 1:\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "else:\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "\n",
    "# Print strategy type to confirm selection.\n",
    "print(\"Using strategy:\", type(strategy).__name__)\n",
    "\n",
    "# Load a small subset of MNIST dataset.\n",
    "(x_train, y_train), _ = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Reduce dataset size for quick demonstration.\n",
    "x_train = x_train[:4000]\n",
    "y_train = y_train[:4000]\n",
    "\n",
    "# Normalize images and add channel dimension.\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_train = np.expand_dims(x_train, axis=-1)\n",
    "\n",
    "# Validate shapes before building the model.\n",
    "print(\"Train shape:\", x_train.shape, \"Labels:\", y_train.shape)\n",
    "\n",
    "# Define global batch size for distributed training.\n",
    "num_replicas = strategy.num_replicas_in_sync\n",
    "global_batch_size = 128 * max(1, num_replicas)\n",
    "\n",
    "# Create a tf.data dataset with batching.\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_ds = train_ds.shuffle(4000, seed=seed_value)\n",
    "train_ds = train_ds.batch(global_batch_size)\n",
    "\n",
    "# Prefetch for better pipeline performance.\n",
    "train_ds = train_ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Enter strategy scope before creating model and optimizer.\n",
    "with strategy.scope():\n",
    "    # Define a simple sequential CNN model.\n",
    "    model = keras.Sequential([\n",
    "        layers.Input(shape=(28, 28, 1)),\n",
    "        layers.Conv2D(16, 3, activation=\"relu\"),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(32, activation=\"relu\"),\n",
    "        layers.Dense(10, activation=\"softmax\"),\n",
    "    ])\n",
    "\n",
    "    # Compile model with optimizer and loss.\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "# Show that variables are created under the strategy.\n",
    "print(\"Replicas in sync:\", strategy.num_replicas_in_sync)\n",
    "\n",
    "# Train the model briefly with silent logs.\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=2,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Extract final loss and accuracy from history.\n",
    "final_loss = history.history[\"loss\"][-1]\n",
    "final_acc = history.history[\"accuracy\"][-1]\n",
    "\n",
    "# Print concise training results for inspection.\n",
    "print(\"Final loss:\", round(float(final_loss), 4))\n",
    "print(\"Final accuracy:\", round(float(final_acc), 4))\n",
    "\n",
    "# Confirm that training finished without distribution errors.\n",
    "print(\"Training completed successfully under strategy scope.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65049eb9",
   "metadata": {},
   "source": [
    "### **1.2. Global vs per replica**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a3cea7",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_08/Lecture_B/image_01_02.jpg?v=1769459244\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Per replica values come from each individual device\n",
    ">* Global values aggregate results across all replicas\n",
    "\n",
    ">* Global batch splits into per-replica mini-batches\n",
    ">* Metrics aggregate per-replica results; misreading causes confusion\n",
    "\n",
    ">* Design datasets using global, not per-replica batches\n",
    ">* Know which tensors are local versus globally synced\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709ba43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Global vs per replica\n",
    "\n",
    "# This script shows global versus per replica concepts.\n",
    "# It uses TensorFlow distribution strategies with small data.\n",
    "# Run in Colab to explore multi device batch behavior.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required modules safely.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Choose a distribution strategy based on available devices.\n",
    "if len(tf.config.list_logical_devices(\"GPU\")) > 1:\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "else:\n",
    "    strategy = tf.distribute.OneDeviceStrategy(\"/cpu:0\")\n",
    "\n",
    "# Show how many replicas are in sync.\n",
    "num_replicas = strategy.num_replicas_in_sync\n",
    "print(\"Replicas in sync:\", num_replicas)\n",
    "\n",
    "# Define a small global batch size for the example.\n",
    "global_batch_size = 8\n",
    "per_replica_batch_size = max(global_batch_size // num_replicas, 1)\n",
    "print(\"Global batch size:\", global_batch_size)\n",
    "\n",
    "# Print the computed per replica batch size.\n",
    "print(\"Per replica batch size:\", per_replica_batch_size)\n",
    "\n",
    "# Create a tiny synthetic dataset with known size.\n",
    "num_samples = 32\n",
    "features = np.random.randn(num_samples, 4).astype(\"float32\")\n",
    "labels = np.random.randint(0, 2, size=(num_samples, 1)).astype(\"float32\")\n",
    "\n",
    "# Build a tf.data.Dataset with global batch size.\n",
    "ds = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "ds = ds.batch(global_batch_size, drop_remainder=True)\n",
    "\n",
    "# Distribute the dataset using the chosen strategy.\n",
    "dist_ds = strategy.experimental_distribute_dataset(ds)\n",
    "\n",
    "# Define a simple model building function.\n",
    "def create_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(4,)),\n",
    "        tf.keras.layers.Dense(4, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Build and compile the model inside strategy scope.\n",
    "with strategy.scope():\n",
    "    model = create_model()\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.SGD(learning_rate=0.1),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "# Run a single epoch with silent training logs.\n",
    "history = model.fit(\n",
    "    dist_ds,\n",
    "    epochs=1,\n",
    "    steps_per_epoch=2,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Print global metrics from model.fit history.\n",
    "print(\"Global loss after epoch:\", float(history.history[\"loss\"][0]))\n",
    "print(\"Global accuracy after epoch:\", float(history.history[\"accuracy\"][0]))\n",
    "\n",
    "# Take one distributed batch to inspect shapes.\n",
    "for batch_features, batch_labels in iter(dist_ds):\n",
    "    per_replica_x = batch_features\n",
    "    per_replica_y = batch_labels\n",
    "    break\n",
    "\n",
    "# Show the type of per replica objects.\n",
    "print(\"Type of per_replica_x:\", type(per_replica_x).__name__)\n",
    "\n",
    "# Define a function to inspect per replica shapes.\n",
    "def inspect_per_replica(tensor_per_replica, name):\n",
    "    if isinstance(tensor_per_replica, tf.distribute.DistributedValues):\n",
    "        shapes = []\n",
    "        for replica_id in range(num_replicas):\n",
    "            value = tensor_per_replica.values[replica_id]\n",
    "            shapes.append(tuple(value.shape.as_list()))\n",
    "        print(name, \"per replica shapes:\", shapes)\n",
    "    else:\n",
    "        print(name, \"shape:\", tuple(tensor_per_replica.shape.as_list()))\n",
    "\n",
    "# Call the inspection function for features and labels.\n",
    "inspect_per_replica(per_replica_x, \"Features\")\n",
    "inspect_per_replica(per_replica_y, \"Labels\")\n",
    "\n",
    "# Define a simple step that returns per replica loss.\n",
    "loss_obj = tf.keras.losses.BinaryCrossentropy(\n",
    "    reduction=tf.keras.losses.Reduction.NONE\n",
    ")\n",
    "\n",
    "# Create a distributed train step using strategy.run.\n",
    "@tf.function\n",
    "def distributed_step(inputs):\n",
    "    def replica_step(x, y):\n",
    "        logits = model(x, training=True)\n",
    "        per_example_loss = loss_obj(y, logits)\n",
    "        return per_example_loss\n",
    "\n",
    "    per_replica_losses = strategy.run(replica_step, args=inputs)\n",
    "    return per_replica_losses\n",
    "\n",
    "# Prepare one batch tuple for the distributed step.\n",
    "inputs = (per_replica_x, per_replica_y)\n",
    "per_replica_losses = distributed_step(inputs)\n",
    "\n",
    "# Inspect per replica loss shapes before reduction.\n",
    "inspect_per_replica(per_replica_losses, \"Per example loss\")\n",
    "\n",
    "# Reduce per replica losses to a single global mean.\n",
    "global_loss = strategy.reduce(\n",
    "    tf.distribute.ReduceOp.MEAN,\n",
    "    per_replica_losses,\n",
    "    axis=None,\n",
    ")\n",
    "\n",
    "# Print the final global loss value for this batch.\n",
    "print(\"Global mean loss for one batch:\", float(global_loss.numpy().mean()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a27b99",
   "metadata": {},
   "source": [
    "### **1.3. Distributed Checkpointing Basics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53150f9",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_08/Lecture_B/image_01_03.jpg?v=1769459380\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Distributed checkpointing coordinates state across all workers\n",
    ">* Strategy syncs model, optimizer, schedules, and variables\n",
    "\n",
    ">* Checkpoints store logical model state, not replicas\n",
    ">* Same checkpoint works across changing devices and workers\n",
    "\n",
    ">* Choose smart save intervals and responsible writer\n",
    ">* Use portable checkpoints to build robust pipelines\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bb0390",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Distributed Checkpointing Basics\n",
    "\n",
    "# This script shows distributed checkpointing basics.\n",
    "# It uses MirroredStrategy with model.fit safely.\n",
    "# It keeps output short and beginner friendly.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required modules from TensorFlow.\n",
    "import os\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "np.random.seed(7)\n",
    "tf.random.set_seed(7)\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Detect available GPUs for potential distribution.\n",
    "physical_gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "print(\"GPUs detected:\", len(physical_gpus))\n",
    "\n",
    "# Choose strategy based on available GPUs.\n",
    "if len(physical_gpus) > 1:\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "else:\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "\n",
    "# Show which strategy class is being used.\n",
    "print(\"Using strategy:\", strategy.__class__.__name__)\n",
    "\n",
    "# Create a small directory for checkpoints.\n",
    "base_dir = pathlib.Path(\"distributed_ckpt_demo\")\n",
    "base_dir.mkdir(exist_ok=True)\n",
    "ckpt_dir = base_dir / \"checkpoints\"\n",
    "ckpt_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Prepare a tiny synthetic dataset for classification.\n",
    "num_samples = 256\n",
    "num_features = 20\n",
    "num_classes = 3\n",
    "\n",
    "# Create random features and integer labels.\n",
    "features = np.random.randn(num_samples, num_features).astype(\"float32\")\n",
    "labels = np.random.randint(num_classes, size=(num_samples,)).astype(\"int32\")\n",
    "\n",
    "# Validate shapes before building the dataset.\n",
    "assert features.shape[0] == labels.shape[0]\n",
    "assert features.shape[1] == num_features\n",
    "\n",
    "# Build a tf.data.Dataset with small batch size.\n",
    "batch_size = 32\n",
    "dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "\n",
    "# Shuffle and batch the dataset deterministically.\n",
    "dataset = dataset.shuffle(buffer_size=num_samples, seed=7)\n",
    "dataset = dataset.batch(batch_size)\n",
    "\n",
    "# Define a simple model inside the strategy scope.\n",
    "with strategy.scope():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(num_features,)),\n",
    "        tf.keras.layers.Dense(16, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ])\n",
    "\n",
    "# Compile the model with optimizer and loss.\n",
    "with strategy.scope():\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "# Create a checkpoint callback that saves every epoch.\n",
    "ckpt_path = str(ckpt_dir / \"weights.epoch{epoch:02d}.keras\")\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=ckpt_path,\n",
    "    save_weights_only=False,\n",
    "    save_freq=\"epoch\",\n",
    "    monitor=\"loss\",\n",
    "    save_best_only=False,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Train briefly with model.fit under the strategy.\n",
    "history = model.fit(\n",
    "    dataset,\n",
    "    epochs=2,\n",
    "    verbose=0,\n",
    "    callbacks=[checkpoint_cb],\n",
    ")\n",
    "\n",
    "# List checkpoint files to show what was saved.\n",
    "ckpt_files = sorted(ckpt_dir.glob(\"*.keras\"))\n",
    "print(\"Checkpoint files saved:\")\n",
    "for path in ckpt_files:\n",
    "    print(\"-\", path.name)\n",
    "\n",
    "# Load the last checkpoint into a new model instance.\n",
    "with strategy.scope():\n",
    "    restored_model = tf.keras.models.load_model(ckpt_files[-1])\n",
    "\n",
    "# Evaluate original and restored models on one batch.\n",
    "for batch_features, batch_labels in dataset.take(1):\n",
    "    original_eval = model.evaluate(\n",
    "        batch_features,\n",
    "        batch_labels,\n",
    "        verbose=0,\n",
    "    )\n",
    "    restored_eval = restored_model.evaluate(\n",
    "        batch_features,\n",
    "        batch_labels,\n",
    "        verbose=0,\n",
    "    )\n",
    "\n",
    "# Print a short comparison of evaluation results.\n",
    "print(\"Original model loss, accuracy:\", original_eval)\n",
    "print(\"Restored model loss, accuracy:\", restored_eval)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d297bcca",
   "metadata": {},
   "source": [
    "## **2. Custom Distributed Loops**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ed1857",
   "metadata": {},
   "source": [
    "### **2.1. Writing Strategy Run Steps**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ee9e70",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_08/Lecture_B/image_02_01.jpg?v=1769459468\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Step function runs per replica, processing batch slice\n",
    ">* You define computation; strategy runs and aggregates updates\n",
    "\n",
    ">* Put per-replica forward pass and loss inside\n",
    ">* Share variables, then compute, reduce, apply gradients\n",
    "\n",
    ">* Make step functions efficient, deterministic, and side‑effect free\n",
    ">* Scale losses consistently so aggregated gradients train stably\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f06c96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Writing Strategy Run Steps\n",
    "\n",
    "# This script shows a simple distributed training step.\n",
    "# It focuses on strategy.run with a custom step.\n",
    "# Use it to understand replica step functions.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required modules from TensorFlow.\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "tf.random.set_seed(7)\n",
    "\n",
    "# Detect available GPUs and choose a strategy.\n",
    "physical_gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "\n",
    "# Select MirroredStrategy for multi GPU or fallback CPU.\n",
    "if physical_gpus:\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "else:\n",
    "    strategy = tf.distribute.OneDeviceStrategy(\"/cpu:0\")\n",
    "\n",
    "# Print TensorFlow version and strategy type.\n",
    "print(\"TF\", tf.__version__, \"Strategy\", type(strategy).__name__)\n",
    "\n",
    "# Create a tiny synthetic dataset for demonstration.\n",
    "features = tf.random.normal(shape=(64, 4))\n",
    "\n",
    "# Create simple labels as a linear function.\n",
    "labels = tf.reduce_sum(features, axis=1, keepdims=True)\n",
    "\n",
    "# Build a tf.data.Dataset from tensors.\n",
    "base_ds = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "\n",
    "# Batch the dataset with a small global batch size.\n",
    "base_ds = base_ds.batch(8, drop_remainder=True)\n",
    "\n",
    "# Distribute the dataset using the chosen strategy.\n",
    "train_ds = strategy.experimental_distribute_dataset(base_ds)\n",
    "\n",
    "# Define global batch size for loss scaling.\n",
    "GLOBAL_BATCH_SIZE = 8\n",
    "\n",
    "# Create model and optimizer inside strategy scope.\n",
    "with strategy.scope():\n",
    "    # Build a tiny sequential regression model.\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(8, activation=\"relu\", input_shape=(4,)),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    # Use a simple optimizer for gradient updates.\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=0.05)\n",
    "\n",
    "    # Define a mean squared error loss object.\n",
    "    loss_obj = tf.keras.losses.MeanSquaredError(\n",
    "        reduction=tf.keras.losses.Reduction.NONE\n",
    "    )\n",
    "\n",
    "# Define a function to compute per replica loss.\n",
    "def compute_loss(labels, predictions):\n",
    "    # Compute unscaled per example loss values.\n",
    "    per_example_loss = loss_obj(labels, predictions)\n",
    "\n",
    "    # Scale loss by global batch size for correctness.\n",
    "    return tf.nn.compute_average_loss(\n",
    "        per_example_loss, global_batch_size=GLOBAL_BATCH_SIZE\n",
    "    )\n",
    "\n",
    "# Define one training step to run on each replica.\n",
    "@tf.function\n",
    "def train_step(dist_inputs):\n",
    "    # Unpack distributed features and labels.\n",
    "    dist_features, dist_labels = dist_inputs\n",
    "\n",
    "    # Record operations for automatic differentiation.\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Forward pass through the model on each replica.\n",
    "        predictions = model(dist_features, training=True)\n",
    "\n",
    "        # Compute scaled loss for this replica.\n",
    "        loss = compute_loss(dist_labels, predictions)\n",
    "\n",
    "    # Compute gradients of loss with respect to variables.\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "    # Apply gradients to update shared model weights.\n",
    "    optimizer.apply_gradients(\n",
    "        zip(gradients, model.trainable_variables)\n",
    "    )\n",
    "\n",
    "    # Return the replica loss for later reduction.\n",
    "    return loss\n",
    "\n",
    "# Define a function that calls strategy.run on the step.\n",
    "@tf.function\n",
    "def distributed_train_step(dist_inputs):\n",
    "    # Run train_step on each replica in parallel.\n",
    "    per_replica_losses = strategy.run(train_step, args=(dist_inputs,))\n",
    "\n",
    "    # Reduce losses to get a single scalar value.\n",
    "    return strategy.reduce(\n",
    "        tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None\n",
    "    )\n",
    "\n",
    "# Run a few epochs over the small dataset.\n",
    "for epoch in range(3):\n",
    "    # Initialize metric to track average loss.\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    # Initialize counter for number of batches.\n",
    "    num_batches = 0\n",
    "\n",
    "    # Iterate over distributed batches from dataset.\n",
    "    for batch in train_ds:\n",
    "        # Call the distributed training step.\n",
    "        loss_value = distributed_train_step(batch)\n",
    "\n",
    "        # Accumulate loss and batch count.\n",
    "        epoch_loss += loss_value\n",
    "        num_batches += 1\n",
    "\n",
    "    # Compute mean loss for this epoch.\n",
    "    mean_loss = epoch_loss / tf.cast(num_batches, tf.float32)\n",
    "\n",
    "    # Print a short summary line for the epoch.\n",
    "    print(\"Epoch\", epoch, \"mean loss\", float(mean_loss))\n",
    "\n",
    "# Run one final batch to inspect shapes and loss.\n",
    "for batch in iter(train_ds):\n",
    "    final_loss = distributed_train_step(batch)\n",
    "    break\n",
    "\n",
    "# Print final loss value to confirm training behavior.\n",
    "print(\"Final distributed step loss\", float(final_loss))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5bc9c9",
   "metadata": {},
   "source": [
    "### **2.2. Building Distributed Datasets**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0625092b",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_08/Lecture_B/image_02_02.jpg?v=1769459541\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Think in global batches split across replicas\n",
    ">* Design pipelines for consistent, non-overlapping replica data\n",
    "\n",
    ">* Build a simple pipeline that outputs global batches\n",
    ">* Strategy splits batches so replicas get balanced shards\n",
    "\n",
    ">* Shard data so each worker gets unique subsets\n",
    ">* Combine sharding, shuffling, batching for efficient pipelines\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b533ae20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Building Distributed Datasets\n",
    "\n",
    "# This script shows distributed dataset basics.\n",
    "# It uses TensorFlow strategy with simple data.\n",
    "# Focus is on global and per replica batches.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required modules for TensorFlow training.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set deterministic seeds for reproducible behavior.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Choose a distribution strategy based on hardware.\n",
    "if len(tf.config.list_logical_devices(\"GPU\")) > 1:\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "else:\n",
    "    strategy = tf.distribute.OneDeviceStrategy(\"/cpu:0\")\n",
    "\n",
    "# Show which strategy class is being used.\n",
    "print(\"Using strategy:\", strategy.__class__.__name__)\n",
    "\n",
    "# Define small synthetic dataset parameters clearly.\n",
    "num_examples = 32\n",
    "feature_dim = 4\n",
    "num_classes = 3\n",
    "\n",
    "# Create simple numeric features and integer labels.\n",
    "features = np.random.randn(num_examples, feature_dim).astype(\"float32\")\n",
    "labels = np.random.randint(num_classes, size=(num_examples,))\n",
    "\n",
    "# Wrap arrays in a tf.data Dataset object.\n",
    "base_ds = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "\n",
    "# Define a small global batch size for training.\n",
    "global_batch_size = 8\n",
    "\n",
    "# Shuffle, repeat, and batch to form global batches.\n",
    "train_ds = (base_ds.shuffle(buffer_size=num_examples)\n",
    "            .repeat(1)\n",
    "            .batch(global_batch_size))\n",
    "\n",
    "# Validate that batched shapes match expectations.\n",
    "for batch_x, batch_y in train_ds.take(1):\n",
    "    print(\"Global batch shape:\", batch_x.shape)\n",
    "\n",
    "# Create a distributed dataset from the global dataset.\n",
    "dist_train_ds = strategy.experimental_distribute_dataset(train_ds)\n",
    "\n",
    "# Define a simple model building function.\n",
    "def build_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(feature_dim,)),\n",
    "        tf.keras.layers.Dense(8, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(num_classes)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Build model, optimizer, and loss inside strategy scope.\n",
    "with strategy.scope():\n",
    "    model = build_model()\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)\n",
    "    loss_obj = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True,\n",
    "        reduction=tf.keras.losses.Reduction.NONE\n",
    "    )\n",
    "\n",
    "# Define a function to compute per example loss.\n",
    "def compute_loss(labels, logits):\n",
    "    per_example_loss = loss_obj(labels, logits)\n",
    "    return tf.nn.compute_average_loss(\n",
    "        per_example_loss,\n",
    "        global_batch_size=global_batch_size\n",
    "    )\n",
    "\n",
    "# Define one training step run on each replica.\n",
    "@tf.function\n",
    "def train_step(dist_inputs):\n",
    "    def replica_step(inputs):\n",
    "        x, y = inputs\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x, training=True)\n",
    "            loss = compute_loss(y, logits)\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        return loss\n",
    "\n",
    "    per_replica_losses = strategy.run(replica_step, args=(dist_inputs,))\n",
    "    mean_loss = strategy.reduce(\n",
    "        tf.distribute.ReduceOp.SUM,\n",
    "        per_replica_losses,\n",
    "        axis=None\n",
    "    )\n",
    "    return mean_loss\n",
    "\n",
    "# Run a short custom loop over distributed dataset.\n",
    "step = 0\n",
    "for dist_batch in dist_train_ds:\n",
    "    step += 1\n",
    "    loss_value = train_step(dist_batch)\n",
    "    print(\"Step\", step, \"loss:\", float(loss_value))\n",
    "\n",
    "# Confirm that training finished without shape issues.\n",
    "print(\"Finished custom distributed loop successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b05da4",
   "metadata": {},
   "source": [
    "### **2.3. Replica Metrics Reduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06b140c",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_08/Lecture_B/image_02_03.jpg?v=1769459665\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Each replica computes its own local metrics\n",
    ">* We must aggregate replicas’ metrics for trustworthy monitoring\n",
    "\n",
    ">* Compute per-replica metrics, then reduce centrally\n",
    ">* Use mean for normalized losses, sum for counts\n",
    "\n",
    ">* Consistent reduction keeps metrics stable when scaling\n",
    ">* Correct reductions ensure trustworthy gradients and alerts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3fbe17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Replica Metrics Reduction\n",
    "\n",
    "# This script shows replica metrics reduction simply.\n",
    "# It uses TensorFlow strategy with a tiny dataset.\n",
    "# Focus is on custom loop and metric aggregation.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries safely.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Import TensorFlow and check version.\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Choose strategy based on available GPUs.\n",
    "if len(tf.config.list_physical_devices(\"GPU\")) > 0:\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "else:\n",
    "    strategy = tf.distribute.OneDeviceStrategy(\"/cpu:0\")\n",
    "\n",
    "# Create a tiny synthetic classification dataset.\n",
    "num_samples = 64\n",
    "num_features = 8\n",
    "num_classes = 3\n",
    "\n",
    "# Generate random features and integer labels.\n",
    "x_data = np.random.randn(num_samples, num_features).astype(\"float32\")\n",
    "y_data = np.random.randint(num_classes, size=(num_samples,)).astype(\"int32\")\n",
    "\n",
    "# Validate shapes before building dataset.\n",
    "assert x_data.shape[0] == y_data.shape[0]\n",
    "\n",
    "# Create a tf.data.Dataset with small batch size.\n",
    "batch_size = 16\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x_data, y_data))\n",
    "\n",
    "# Shuffle and batch the dataset deterministically.\n",
    "dataset = dataset.shuffle(num_samples, seed=SEED).batch(batch_size)\n",
    "\n",
    "# Distribute the dataset using the chosen strategy.\n",
    "dist_dataset = strategy.experimental_distribute_dataset(dataset)\n",
    "\n",
    "# Build a simple model inside strategy scope.\n",
    "with strategy.scope():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(num_features,)),\n",
    "        tf.keras.layers.Dense(16, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(num_classes)\n",
    "    ])\n",
    "\n",
    "# Define loss object and optimizer inside scope.\n",
    "with strategy.scope():\n",
    "    loss_obj = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True, reduction=tf.keras.losses.Reduction.NONE\n",
    "    )\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "# Define a function to compute per replica loss.\n",
    "@tf.function\n",
    "def compute_loss(labels, predictions):\n",
    "    per_example_loss = loss_obj(labels, predictions)\n",
    "    return tf.nn.compute_average_loss(\n",
    "        per_example_loss, global_batch_size=batch_size\n",
    "    )\n",
    "\n",
    "# Define one training step run on each replica.\n",
    "@tf.function\n",
    "def train_step(dist_inputs):\n",
    "    def replica_step(inputs):\n",
    "        features, labels = inputs\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(features, training=True)\n",
    "            loss = compute_loss(labels, logits)\n",
    "            preds = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
    "            correct = tf.cast(tf.equal(preds, labels), tf.float32)\n",
    "            correct_count = tf.reduce_sum(correct)\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        return loss, correct_count\n",
    "\n",
    "    per_replica_loss, per_replica_correct = strategy.run(\n",
    "        replica_step, args=(dist_inputs,)\n",
    "    )\n",
    "\n",
    "    mean_loss = strategy.reduce(\n",
    "        tf.distribute.ReduceOp.MEAN, per_replica_loss, axis=None\n",
    "    )\n",
    "    total_correct = strategy.reduce(\n",
    "        tf.distribute.ReduceOp.SUM, per_replica_correct, axis=None\n",
    "    )\n",
    "    return mean_loss, total_correct\n",
    "\n",
    "# Run a single epoch over the tiny distributed dataset.\n",
    "num_epochs = 1\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    epoch_correct = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    for batch in dist_dataset:\n",
    "        mean_loss, total_correct = train_step(batch)\n",
    "        epoch_loss += mean_loss.numpy()\n",
    "        epoch_correct += total_correct.numpy()\n",
    "        num_batches += 1\n",
    "\n",
    "    avg_loss = epoch_loss / float(num_batches)\n",
    "    total_examples = float(num_batches * batch_size)\n",
    "    accuracy = epoch_correct / total_examples\n",
    "\n",
    "# Print final reduced metrics from all replicas.\n",
    "print(\"Reduced mean loss over epoch:\", float(avg_loss))\n",
    "print(\"Reduced accuracy over epoch:\", float(accuracy))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3a4be0",
   "metadata": {},
   "source": [
    "## **3. Distributed Debugging Essentials**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8081ab2",
   "metadata": {},
   "source": [
    "### **3.1. Replica Shape Mismatches**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864e9dd9",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_08/Lecture_B/image_03_01.jpg?v=1769459765\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Replicas must see tensors with identical shapes\n",
    ">* Uneven batches or preprocessing differences cause mismatches\n",
    "\n",
    ">* Align global and per-replica batch sizes\n",
    ">* Normalize input shapes to avoid replica mismatches\n",
    "\n",
    ">* Data-dependent conditionals can desynchronize replica tensor shapes\n",
    ">* Use deterministic logic, preprocessing, and shape logging\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b38e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Replica Shape Mismatches\n",
    "\n",
    "# This script demonstrates replica shape mismatches.\n",
    "# It uses TensorFlow distribution strategy with simple data.\n",
    "# Focus is on debugging shapes across replicas.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required modules safely.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version and device information.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"GPUs:\", len(tf.config.list_physical_devices(\"GPU\")))\n",
    "\n",
    "# Choose strategy based on available GPUs.\n",
    "if len(tf.config.list_physical_devices(\"GPU\")) > 1:\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "else:\n",
    "    strategy = tf.distribute.OneDeviceStrategy(\"/cpu:0\")\n",
    "\n",
    "# Show chosen strategy for clarity.\n",
    "print(\"Using strategy:\", type(strategy).__name__)\n",
    "\n",
    "# Define global and per replica batch sizes.\n",
    "num_replicas = strategy.num_replicas_in_sync\n",
    "global_batch_size = 8\n",
    "per_replica_batch = global_batch_size // num_replicas\n",
    "\n",
    "# Print basic batch configuration.\n",
    "print(\"Replicas:\", num_replicas, \"Global batch:\", global_batch_size)\n",
    "print(\"Per replica batch:\", per_replica_batch)\n",
    "\n",
    "# Create toy features and labels with odd size.\n",
    "num_samples = global_batch_size + 3\n",
    "x_data = tf.random.normal((num_samples, 4))\n",
    "y_data = tf.random.normal((num_samples, 1))\n",
    "\n",
    "# Build dataset without dropping remainder first.\n",
    "ds_bad = tf.data.Dataset.from_tensor_slices((x_data, y_data))\n",
    "ds_bad = ds_bad.batch(global_batch_size, drop_remainder=False)\n",
    "\n",
    "# Build dataset with drop_remainder to fix shapes.\n",
    "ds_good = tf.data.Dataset.from_tensor_slices((x_data, y_data))\n",
    "ds_good = ds_good.batch(global_batch_size, drop_remainder=True)\n",
    "\n",
    "# Distribute both datasets using the strategy.\n",
    "dist_bad = strategy.experimental_distribute_dataset(ds_bad)\n",
    "dist_good = strategy.experimental_distribute_dataset(ds_good)\n",
    "\n",
    "# Define a simple model inside strategy scope.\n",
    "with strategy.scope():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(4,)),\n",
    "        tf.keras.layers.Dense(4, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(1),\n",
    "    ])\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "    loss_fn = tf.keras.losses.MeanSquaredError(\n",
    "        reduction=tf.keras.losses.Reduction.NONE\n",
    "    )\n",
    "\n",
    "\n",
    "# Define one training step using strategy.run.\n",
    "@tf.function\n",
    "def train_step(dist_inputs):\n",
    "    def step_fn(inputs):\n",
    "        x_batch, y_batch = inputs\n",
    "        with tf.GradientTape() as tape:\n",
    "            preds = model(x_batch, training=True)\n",
    "            per_example_loss = loss_fn(y_batch, preds)\n",
    "            loss = tf.nn.compute_average_loss(\n",
    "                per_example_loss, global_batch_size=global_batch_size\n",
    "            )\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        return loss, tf.shape(x_batch)[0]\n",
    "\n",
    "    per_replica_loss, per_replica_size = strategy.run(step_fn, (dist_inputs,))\n",
    "    mean_loss = strategy.reduce(\n",
    "        tf.distribute.ReduceOp.MEAN, per_replica_loss, axis=None\n",
    "    )\n",
    "    total_size = strategy.reduce(\n",
    "        tf.distribute.ReduceOp.SUM, per_replica_size, axis=None\n",
    "    )\n",
    "    return mean_loss, total_size\n",
    "\n",
    "\n",
    "# Helper to inspect one distributed batch shapes.\n",
    "def inspect_batch(dist_batch, label):\n",
    "    print(\"\\nInspecting\", label)\n",
    "\n",
    "    def show_fn(inputs):\n",
    "        x_batch, y_batch = inputs\n",
    "        print(\"Replica x shape:\", x_batch.shape)\n",
    "        print(\"Replica y shape:\", y_batch.shape)\n",
    "        return 0\n",
    "\n",
    "    _ = strategy.run(show_fn, (dist_batch,))\n",
    "\n",
    "\n",
    "# Take first batch from bad and good datasets.\n",
    "for bad_batch in iter(dist_bad):\n",
    "    inspect_batch(bad_batch, \"batch without drop_remainder\")\n",
    "    break\n",
    "\n",
    "for good_batch in iter(dist_good):\n",
    "    inspect_batch(good_batch, \"batch with drop_remainder\")\n",
    "    break\n",
    "\n",
    "# Run one safe training step on the good batch.\n",
    "loss_value, seen_size = train_step(good_batch)\n",
    "print(\"\\nTrain step finished. Loss:\", float(loss_value))\n",
    "print(\"Examples seen in step:\", int(seen_size))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a073807e",
   "metadata": {},
   "source": [
    "### **3.2. Managing Local Variables**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c92b3c4",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_08/Lecture_B/image_03_02.jpg?v=1769459896\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Local variables keep separate values on each replica\n",
    ">* Confusing local versus global variables breaks training\n",
    "\n",
    ">* Create variables in correct strategy and replica scopes\n",
    ">* Avoid unintended per-replica copies and metric counters\n",
    "\n",
    ">* Clarify which variables must be local or shared\n",
    ">* Trace creation, updates, aggregation to spot issues\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d320cfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Managing Local Variables\n",
    "\n",
    "# This script shows managing local variables.\n",
    "# It uses TensorFlow distribution strategies safely.\n",
    "# Focus on debugging variable placement issues.\n",
    "\n",
    "# Install TensorFlow in some environments if needed.\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Import TensorFlow and distribution strategies.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# Set TensorFlow random seed deterministically.\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Choose a simple distribution strategy for demo.\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "# Create tiny synthetic dataset for quick training.\n",
    "features = np.random.randn(64, 4).astype(\"float32\")\n",
    "labels = np.random.randint(0, 2, size=(64, 1)).astype(\"float32\")\n",
    "\n",
    "# Validate shapes before building dataset.\n",
    "print(\"Features shape:\", features.shape)\n",
    "print(\"Labels shape:\", labels.shape)\n",
    "\n",
    "# Build a tf.data dataset with global batch size.\n",
    "batch_size = 8\n",
    "dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "\n",
    "# Shuffle and batch the dataset safely.\n",
    "dataset = dataset.shuffle(64, seed=seed_value).batch(batch_size)\n",
    "\n",
    "# Distribute the dataset using the chosen strategy.\n",
    "dist_dataset = strategy.experimental_distribute_dataset(dataset)\n",
    "\n",
    "# Define a simple model building function.\n",
    "def build_model():\n",
    "    # Create a tiny sequential model.\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Input(shape=(4,)),\n",
    "        keras.layers.Dense(4, activation=\"relu\"),\n",
    "        keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "    ])\n",
    "\n",
    "    # Compile model with simple optimizer.\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.SGD(learning_rate=0.1),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Create model inside strategy scope for correct placement.\n",
    "with strategy.scope():\n",
    "    model = build_model()\n",
    "\n",
    "# Define a metric created once, not per replica.\n",
    "with strategy.scope():\n",
    "    train_loss = keras.metrics.Mean(name=\"train_loss\")\n",
    "\n",
    "# Show that metric is a MirroredVariable backed object.\n",
    "print(\"Metric variable type:\", type(train_loss.variables[0]).__name__)\n",
    "\n",
    "# Define loss object for custom training loop.\n",
    "loss_obj = keras.losses.BinaryCrossentropy(reduction=tf.keras.losses.Reduction.NONE)\n",
    "\n",
    "# Define per replica loss computation function.\n",
    "def compute_loss(labels_replica, preds_replica):\n",
    "    # Compute unreduced per example loss.\n",
    "    per_example_loss = loss_obj(labels_replica, preds_replica)\n",
    "\n",
    "    # Scale loss by global batch size.\n",
    "    return tf.nn.compute_average_loss(\n",
    "        per_example_loss,\n",
    "        global_batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "# Create optimizer inside strategy scope for safety.\n",
    "with strategy.scope():\n",
    "    optimizer = keras.optimizers.SGD(learning_rate=0.1)\n",
    "\n",
    "# Define one training step run on each replica.\n",
    "@tf.function\n",
    "def train_step(dist_inputs):\n",
    "    # Unpack distributed features and labels.\n",
    "    def step_fn(inputs):\n",
    "        x_batch, y_batch = inputs\n",
    "\n",
    "        # Use GradientTape for custom training.\n",
    "        with tf.GradientTape() as tape:\n",
    "            preds = model(x_batch, training=True)\n",
    "            loss = compute_loss(y_batch, preds)\n",
    "\n",
    "        # Compute gradients and apply updates.\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        # Update global metric once per replica.\n",
    "        train_loss.update_state(loss)\n",
    "        return loss\n",
    "\n",
    "    # Run step function on each replica.\n",
    "    per_replica_losses = strategy.run(step_fn, args=(dist_inputs,))\n",
    "\n",
    "    # Reduce losses to get mean across replicas.\n",
    "    return strategy.reduce(\n",
    "        tf.distribute.ReduceOp.SUM,\n",
    "        per_replica_losses,\n",
    "        axis=None,\n",
    "    )\n",
    "\n",
    "# Run a short custom training loop.\n",
    "for epoch in range(2):\n",
    "    # Reset metric at the start of each epoch.\n",
    "    train_loss.reset_state()\n",
    "\n",
    "    # Iterate over distributed batches.\n",
    "    for batch_inputs in dist_dataset:\n",
    "        loss_value = train_step(batch_inputs)\n",
    "\n",
    "    # Print epoch summary with metric result.\n",
    "    print(\n",
    "        \"Epoch\",\n",
    "        epoch,\n",
    "        \"mean loss:\",\n",
    "        float(train_loss.result()),\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c56d66",
   "metadata": {},
   "source": [
    "### **3.3. Monitoring Distributed Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7deb9a45",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_08/Lecture_B/image_03_03.jpg?v=1769459946\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Monitor replica behavior and resource use continuously\n",
    ">* Track per-replica metrics to catch hidden issues\n",
    "\n",
    ">* Track both global and per-worker training metrics\n",
    ">* Use detailed logs to reveal hidden replica issues\n",
    "\n",
    ">* Log key steps to catch rare errors\n",
    ">* Use dashboards and profiling to trace replicas\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c799c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Monitoring Distributed Training\n",
    "\n",
    "# This script shows basic distributed monitoring.\n",
    "# It uses MirroredStrategy with simple metrics.\n",
    "# Focus is on safe concise debugging ideas.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Import TensorFlow and check version.\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "\n",
    "# Set numpy and tensorflow seeds deterministically.\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Choose devices automatically for strategy.\n",
    "if tf.config.list_logical_devices(\"GPU\"):\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "else:\n",
    "    strategy = tf.distribute.OneDeviceStrategy(\"/cpu:0\")\n",
    "\n",
    "# Print number of replicas for quick check.\n",
    "print(\"Replicas in sync:\", strategy.num_replicas_in_sync)\n",
    "\n",
    "# Create a tiny synthetic dataset safely.\n",
    "num_samples = 256\n",
    "features = np.random.randn(num_samples, 8).astype(\"float32\")\n",
    "\n",
    "# Create simple binary labels from features.\n",
    "labels = (np.sum(features, axis=1) > 0).astype(\"float32\")\n",
    "\n",
    "# Validate shapes before building dataset.\n",
    "assert features.shape[0] == labels.shape[0]\n",
    "\n",
    "# Define global batch size divisible by replicas.\n",
    "per_replica_batch = 8\n",
    "global_batch_size = per_replica_batch * strategy.num_replicas_in_sync\n",
    "\n",
    "# Build tf.data dataset with batching.\n",
    "base_ds = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "base_ds = base_ds.shuffle(256, seed=seed_value)\n",
    "\n",
    "# Batch and repeat for a few steps.\n",
    "base_ds = base_ds.batch(global_batch_size).repeat(3)\n",
    "\n",
    "# Distribute dataset for the chosen strategy.\n",
    "dist_ds = strategy.experimental_distribute_dataset(base_ds)\n",
    "\n",
    "# Define a simple model building function.\n",
    "def create_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(8,)),\n",
    "        tf.keras.layers.Dense(16, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Create optimizer and loss objects.\n",
    "with strategy.scope():\n",
    "    model = create_model()\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "# Use BinaryCrossentropy with reduction NONE.\n",
    "with strategy.scope():\n",
    "    loss_obj = tf.keras.losses.BinaryCrossentropy(\n",
    "        from_logits=False,\n",
    "        reduction=tf.keras.losses.Reduction.NONE,\n",
    "    )\n",
    "\n",
    "# Define a function to compute per replica loss.\n",
    "def compute_loss(labels, predictions):\n",
    "    per_example_loss = loss_obj(labels, predictions)\n",
    "    per_example_loss = tf.reshape(per_example_loss, [-1])\n",
    "    return tf.nn.compute_average_loss(\n",
    "        per_example_loss,\n",
    "        global_batch_size=global_batch_size,\n",
    "    )\n",
    "\n",
    "# Create metrics for monitoring training.\n",
    "with strategy.scope():\n",
    "    train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
    "    train_acc = tf.keras.metrics.BinaryAccuracy(name=\"train_accuracy\")\n",
    "\n",
    "# Define the per replica train step function.\n",
    "def train_step(inputs):\n",
    "    features_batch, labels_batch = inputs\n",
    "    tf.debugging.assert_shapes(\n",
    "        [(features_batch, (None, 8)), (labels_batch, (None,))]\n",
    "    )\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(features_batch, training=True)\n",
    "        predictions = tf.squeeze(predictions, axis=-1)\n",
    "        loss = compute_loss(labels_batch, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    train_loss.update_state(loss)\n",
    "    train_acc.update_state(labels_batch, predictions)\n",
    "    return loss\n",
    "\n",
    "# Define one distributed training step with monitoring.\n",
    "@tf.function\n",
    "def distributed_train_step(dist_inputs):\n",
    "    per_replica_losses = strategy.run(train_step, args=(dist_inputs,))\n",
    "    mean_loss = strategy.reduce(\n",
    "        tf.distribute.ReduceOp.SUM,\n",
    "        per_replica_losses,\n",
    "        axis=None,\n",
    "    )\n",
    "    return mean_loss\n",
    "\n",
    "# Run a short custom training loop with logs.\n",
    "step_times = []\n",
    "for step, batch in enumerate(dist_ds):\n",
    "    if step >= 5:\n",
    "        break\n",
    "    start = tf.timestamp()\n",
    "    mean_loss = distributed_train_step(batch)\n",
    "    end = tf.timestamp()\n",
    "    step_times.append(float(end - start))\n",
    "    if (step + 1) % 2 == 0:\n",
    "        print(\n",
    "            \"Step\",\n",
    "            step + 1,\n",
    "            \"loss=\",\n",
    "            float(train_loss.result()),\n",
    "            \"acc=\",\n",
    "            float(train_acc.result()),\n",
    "        )\n",
    "\n",
    "# Print simple monitoring summary at the end.\n",
    "avg_step_time = sum(step_times) / len(step_times)\n",
    "print(\"Average step time (seconds):\", round(avg_step_time, 4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c73f6e6",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Implementing Strategies**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0eebf39",
   "metadata": {},
   "source": [
    "\n",
    "In this lecture, you learned to:\n",
    "- Configure and run model.fit under a tf.distribute strategy for multi-GPU or multi-worker setups. \n",
    "- Implement a custom training loop that uses strategy.run and distributed datasets. \n",
    "- Troubleshoot common distributed training errors related to shapes, batch sizes, and variable placement. \n",
    "\n",
    "In the next Module (Module 9), we will go over 'Production and Serving'"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
