{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f200b825",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Sequential Models**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a967e1f",
   "metadata": {},
   "source": [
    ">Last update: 20260125.\n",
    "    \n",
    "By the end of this Lecture, you will be able to:\n",
    "- Construct Keras Sequential models with common layer types for supervised learning tasks. \n",
    "- Configure model compilation settings including loss functions, optimizers, and metrics. \n",
    "- Train and evaluate Sequential models using model.fit and model.evaluate. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b037e17f",
   "metadata": {},
   "source": [
    "## **1. Building Sequential Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7edea2",
   "metadata": {},
   "source": [
    "### **1.1. Layer List Definition**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36976e2d",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_03/Lecture_A/image_01_01.jpg?v=1769387639\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Sequential models are ordered pipelines of layers\n",
    ">* Data flows linearly, each layer transforms and passes\n",
    "\n",
    ">* Layer choices depend on data and prediction\n",
    ">* Sequential layer order encodes domain knowledge strategy\n",
    "\n",
    ">* Choose depth, width, and regularization to control complexity\n",
    ">* Iteratively refine layer sequences based on results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28a777d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Layer List Definition\n",
    "\n",
    "# This script shows a simple Sequential model pipeline.\n",
    "# It focuses on defining an ordered list of layers.\n",
    "# Run cells in order to follow the explanation.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import TensorFlow and Keras Sequential API.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Print TensorFlow version for reproducibility.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Set random seeds for deterministic behavior.\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Create small synthetic tabular style input data.\n",
    "inputs = tf.random.normal(shape=(32, 8))\n",
    "\n",
    "# Create small synthetic numeric targets for regression.\n",
    "targets = tf.random.normal(shape=(32, 1))\n",
    "\n",
    "# Check that input and target batch sizes match.\n",
    "assert inputs.shape[0] == targets.shape[0]\n",
    "\n",
    "# Define a Sequential model as an ordered layer list.\n",
    "model = keras.Sequential([\n",
    "    layers.Input(shape=(8,)),\n",
    "    layers.Dense(16, activation=\"relu\"),\n",
    "    layers.Dense(8, activation=\"relu\"),\n",
    "    layers.Dense(1, activation=\"linear\"),\n",
    "])\n",
    "\n",
    "# Print a short summary of the layer list.\n",
    "model.summary(expand_nested=False, show_trainable=True)\n",
    "\n",
    "# Compile the model with optimizer, loss, and metric.\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.01),\n",
    "    loss=\"mse\",\n",
    "    metrics=[\"mae\"],\n",
    ")\n",
    "\n",
    "# Train briefly with silent verbose setting.\n",
    "history = model.fit(\n",
    "    inputs,\n",
    "    targets,\n",
    "    epochs=5,\n",
    "    batch_size=8,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Evaluate the trained model on the same small data.\n",
    "loss, mae = model.evaluate(inputs, targets, verbose=0)\n",
    "\n",
    "# Print final loss and metric to see performance.\n",
    "print(\"Final MSE loss:\", float(loss))\n",
    "print(\"Final MAE metric:\", float(mae))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e7cf0a",
   "metadata": {},
   "source": [
    "### **1.2. Specifying Input Shapes**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8882affa",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_03/Lecture_A/image_01_02.jpg?v=1769387682\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Input shape describes one sampleâ€™s feature structure\n",
    ">* Clear input shapes let Keras wire layers correctly\n",
    "\n",
    ">* Specify shape for one example, not batch\n",
    ">* Separate batch size from features for flexibility\n",
    "\n",
    ">* Match input shapes to each layer type\n",
    ">* Design sample structure to avoid shape errors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd56953",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Specifying Input Shapes\n",
    "\n",
    "# This script shows Keras input shapes.\n",
    "# It focuses on simple Sequential models.\n",
    "# Run cells to see shapes and outputs.\n",
    "\n",
    "# TensorFlow is available in this Colab environment.\n",
    "# If needed elsewhere uncomment the install line.\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import TensorFlow and Keras utilities.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "tf.random.set_seed(7)\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Create tiny dummy tabular data with three features.\n",
    "import numpy as np\n",
    "num_samples = 8\n",
    "\n",
    "# Build feature matrix with shape (samples, features).\n",
    "X_tabular = np.random.rand(num_samples, 3).astype(\"float32\")\n",
    "\n",
    "# Build target values for a regression style task.\n",
    "y_tabular = np.random.rand(num_samples, 1).astype(\"float32\")\n",
    "\n",
    "# Show the shape of one example only.\n",
    "print(\"One tabular example shape:\", X_tabular[0].shape)\n",
    "\n",
    "# Define a Sequential model with explicit input shape.\n",
    "model_tabular = keras.Sequential([\n",
    "    layers.Input(shape=(3,)),\n",
    "    layers.Dense(4, activation=\"relu\"),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "# Display model input and output shapes clearly.\n",
    "print(\"Model input shape:\", model_tabular.input_shape)\n",
    "print(\"Model output shape:\", model_tabular.output_shape)\n",
    "\n",
    "# Compile model with simple optimizer and loss.\n",
    "model_tabular.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"mse\",\n",
    "    metrics=[\"mae\"]\n",
    ")\n",
    "\n",
    "# Train briefly with silent output to avoid logs.\n",
    "history = model_tabular.fit(\n",
    "    X_tabular,\n",
    "    y_tabular,\n",
    "    epochs=5,\n",
    "    batch_size=4,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Evaluate model once and print metric results.\n",
    "loss_value, mae_value = model_tabular.evaluate(\n",
    "    X_tabular,\n",
    "    y_tabular,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Show final loss and metric in one concise line.\n",
    "print(\"Final loss and MAE:\", float(loss_value), float(mae_value))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1416d0",
   "metadata": {},
   "source": [
    "### **1.3. Reading Model Summary**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27432875",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_03/Lecture_A/image_01_03.jpg?v=1769387715\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Model summary shows layer order and details\n",
    ">* Use it to verify shapes and catch mistakes\n",
    "\n",
    ">* Output shapes show data size after layers\n",
    ">* Compare shapes to expectations to catch mismatches\n",
    "\n",
    ">* Parameter counts show capacity, cost, overfitting risk\n",
    ">* Use counts to adjust architecture before training\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6aa584",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Reading Model Summary\n",
    "\n",
    "# This script shows a simple Sequential model summary.\n",
    "# It focuses on reading layer shapes and parameters.\n",
    "# Run all cells to see concise printed output.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import TensorFlow and Keras modules.\n",
    "import tensorflow as tf\n",
    "\n",
    "# Print TensorFlow version briefly.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Set deterministic random seeds for reproducibility.\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Prepare a tiny dummy dataset for demonstration.\n",
    "import numpy as np\n",
    "\n",
    "# Create small feature matrix with ten samples.\n",
    "X_dummy = np.random.rand(10, 8).astype(\"float32\")\n",
    "\n",
    "# Create small target vector for binary labels.\n",
    "y_dummy = np.random.randint(0, 2, size=(10, 1)).astype(\"float32\")\n",
    "\n",
    "# Validate input and target shapes before modeling.\n",
    "assert X_dummy.shape[0] == y_dummy.shape[0]\n",
    "\n",
    "# Build a simple Sequential model for classification.\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Define the Sequential model step by step.\n",
    "model = Sequential([\n",
    "    layers.Input(shape=(8,)),\n",
    "    layers.Dense(16, activation=\"relu\"),\n",
    "    layers.Dense(8, activation=\"relu\"),\n",
    "    layers.Dense(1, activation=\"sigmoid\"),\n",
    "])\n",
    "\n",
    "# Print a clear title before the summary.\n",
    "print(\"\\nModel summary for our Sequential network:\")\n",
    "\n",
    "# Show the model summary to inspect shapes.\n",
    "model.summary()\n",
    "\n",
    "# Compile the model with common supervised settings.\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Train briefly with silent logging to avoid clutter.\n",
    "history = model.fit(\n",
    "    X_dummy,\n",
    "    y_dummy,\n",
    "    epochs=5,\n",
    "    batch_size=2,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Evaluate the model once and print concise results.\n",
    "loss, acc = model.evaluate(X_dummy, y_dummy, verbose=0)\n",
    "\n",
    "# Print final loss and accuracy in one short line.\n",
    "print(\"Final loss and accuracy:\", float(loss), float(acc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e26cd9",
   "metadata": {},
   "source": [
    "## **2. Configuring Keras Layers**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4468c726",
   "metadata": {},
   "source": [
    "### **2.1. Designing Dense And Dropout Layers**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e770be3",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_03/Lecture_A/image_02_01.jpg?v=1769387753\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Dense layers learn complex feature combinations\n",
    ">* Balance layer size to avoid overfitting data\n",
    "\n",
    ">* Dropout randomly disables units to improve generalization\n",
    ">* Choose dropout rate to reduce overfitting without underlearning\n",
    "\n",
    ">* Iteratively tune dense size and dropout rate\n",
    ">* Dense adds capacity; dropout protects against overfitting\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91cc441",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Designing Dense And Dropout Layers\n",
    "\n",
    "# This script shows dense and dropout layers.\n",
    "# It builds a small Sequential model stepwise.\n",
    "# It focuses on configuration and compilation.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import TensorFlow and Keras utilities.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "import numpy as np\n",
    "np.random.seed(7)\n",
    "tf.random.set_seed(7)\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Create a tiny synthetic classification dataset.\n",
    "num_samples = 200\n",
    "num_features = 10\n",
    "num_classes = 2\n",
    "\n",
    "# Generate random input features safely.\n",
    "X = np.random.randn(num_samples, num_features).astype(\"float32\")\n",
    "\n",
    "# Generate random integer labels for classes.\n",
    "y = np.random.randint(num_classes, size=(num_samples,))\n",
    "\n",
    "# Validate feature and label shapes before modeling.\n",
    "assert X.shape == (num_samples, num_features)\n",
    "assert y.shape == (num_samples,)\n",
    "\n",
    "# Split data into simple train and test sets.\n",
    "train_size = 160\n",
    "test_size = num_samples - train_size\n",
    "\n",
    "# Slice arrays deterministically for training.\n",
    "X_train = X[:train_size]\n",
    "y_train = y[:train_size]\n",
    "\n",
    "# Slice arrays deterministically for testing.\n",
    "X_test = X[train_size:]\n",
    "y_test = y[train_size:]\n",
    "\n",
    "# Build a Sequential model step by step.\n",
    "model = keras.Sequential()\n",
    "\n",
    "# Add first dense layer with moderate units.\n",
    "model.add(layers.Dense(32, activation=\"relu\", input_shape=(num_features,)))\n",
    "\n",
    "# Add dropout to reduce overfitting risk.\n",
    "model.add(layers.Dropout(0.3))\n",
    "\n",
    "# Add second dense layer with fewer units.\n",
    "model.add(layers.Dense(16, activation=\"relu\"))\n",
    "\n",
    "# Add stronger dropout after deeper dense layer.\n",
    "model.add(layers.Dropout(0.5))\n",
    "\n",
    "# Add final dense layer for binary output.\n",
    "model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "# Choose binary crossentropy loss for classification.\n",
    "loss_fn = keras.losses.BinaryCrossentropy()\n",
    "\n",
    "# Choose Adam optimizer with safe learning rate.\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# Track accuracy metric during training and evaluation.\n",
    "metrics_list = [\"accuracy\"]\n",
    "\n",
    "# Compile model with chosen loss, optimizer, metrics.\n",
    "model.compile(optimizer=optimizer, loss=loss_fn, metrics=metrics_list)\n",
    "\n",
    "# Train model briefly with silent verbose setting.\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=16, verbose=0)\n",
    "\n",
    "# Evaluate model on held out test data.\n",
    "loss_value, accuracy_value = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "# Print concise summary of configuration choices.\n",
    "print(\"Loss function:\", loss_fn.name)\n",
    "print(\"Optimizer:\", optimizer.__class__.__name__)\n",
    "print(\"Metrics:\", metrics_list)\n",
    "\n",
    "# Print evaluation results in two short lines.\n",
    "print(\"Test loss:\", round(float(loss_value), 4))\n",
    "print(\"Test accuracy:\", round(float(accuracy_value), 4))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0a58b1",
   "metadata": {},
   "source": [
    "### **2.2. Keras Activation Functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b0ce6e",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_03/Lecture_A/image_02_02.jpg?v=1769387792\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Activations enable networks to learn nonlinear patterns\n",
    ">* Choose output activations to match task and loss\n",
    "\n",
    ">* Hidden-layer activations control learning speed and patterns\n",
    ">* Choosing ReLU variants affects optimization and performance\n",
    "\n",
    ">* Match output activations with task and losses\n",
    ">* Use sigmoid, softmax, or linear for correct metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba39975",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Keras Activation Functions\n",
    "\n",
    "# This script shows Keras activation functions.\n",
    "# It focuses on output activations and compilation.\n",
    "# It runs quickly with tiny synthetic datasets.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required libraries for TensorFlow usage.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set deterministic seeds for reproducible behavior.\n",
    "seed_value = 7\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version in one concise line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Select device preference based on GPU availability.\n",
    "physical_gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "use_gpu = bool(physical_gpus)\n",
    "device_type = \"GPU\" if use_gpu else \"CPU\"\n",
    "\n",
    "# Print which device type will likely be used.\n",
    "print(\"Running on device type:\", device_type)\n",
    "\n",
    "# Create tiny synthetic regression style dataset.\n",
    "num_samples = 200\n",
    "x_reg = np.linspace(-1.0, 1.0, num_samples).reshape(-1, 1)\n",
    "noise_reg = 0.1 * np.random.randn(num_samples, 1)\n",
    "\n",
    "# Define regression targets with simple linear pattern.\n",
    "y_reg = 3.0 * x_reg + 0.5 + noise_reg\n",
    "\n",
    "# Create tiny synthetic binary classification dataset.\n",
    "x_bin = np.linspace(-2.0, 2.0, num_samples).reshape(-1, 1)\n",
    "noise_bin = 0.3 * np.random.randn(num_samples, 1)\n",
    "\n",
    "# Define binary labels using a threshold rule.\n",
    "y_bin = (x_bin + noise_bin > 0.0).astype(\"float32\")\n",
    "\n",
    "# Validate shapes before building models.\n",
    "assert x_reg.shape == y_reg.shape\n",
    "assert x_bin.shape == y_bin.shape\n",
    "\n",
    "# Build Sequential model for regression output layer.\n",
    "reg_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(1,)),\n",
    "    tf.keras.layers.Dense(8, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1, activation=\"linear\"),\n",
    "])\n",
    "\n",
    "# Compile regression model with matching loss choice.\n",
    "reg_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "    loss=\"mse\",\n",
    "    metrics=[\"mae\"],\n",
    ")\n",
    "\n",
    "# Train regression model briefly with silent logging.\n",
    "reg_history = reg_model.fit(\n",
    "    x_reg,\n",
    "    y_reg,\n",
    "    epochs=40,\n",
    "    batch_size=16,\n",
    "    verbose=0,\n",
    "    validation_split=0.2,\n",
    ")\n",
    "\n",
    "# Evaluate regression model performance on full data.\n",
    "reg_loss, reg_mae = reg_model.evaluate(\n",
    "    x_reg,\n",
    "    y_reg,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Build Sequential model for binary classification.\n",
    "bin_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(1,)),\n",
    "    tf.keras.layers.Dense(8, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "])\n",
    "\n",
    "# Compile binary model with aligned loss and metric.\n",
    "bin_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Train binary model briefly with silent logging.\n",
    "bin_history = bin_model.fit(\n",
    "    x_bin,\n",
    "    y_bin,\n",
    "    epochs=40,\n",
    "    batch_size=16,\n",
    "    verbose=0,\n",
    "    validation_split=0.2,\n",
    ")\n",
    "\n",
    "# Evaluate binary model performance on full data.\n",
    "bin_loss, bin_acc = bin_model.evaluate(\n",
    "    x_bin,\n",
    "    y_bin,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Print concise summary of both compiled models.\n",
    "print(\"Regression output activation: linear, loss: mse.\")\n",
    "print(\"Regression evaluation - loss:\", float(reg_loss))\n",
    "print(\"Regression evaluation - mae:\", float(reg_mae))\n",
    "print(\"Binary output activation: sigmoid, loss: binary_crossentropy.\")\n",
    "print(\"Binary evaluation - loss:\", float(bin_loss))\n",
    "print(\"Binary evaluation - accuracy:\", float(bin_acc))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5a9793",
   "metadata": {},
   "source": [
    "### **2.3. Weight Initialization and Regularization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e61777f",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_03/Lecture_A/image_02_03.jpg?v=1769387839\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Weight initialization strongly affects learning speed and stability\n",
    ">* Good initial weights guide the optimizer toward solutions\n",
    "\n",
    ">* Match initializers to activations for stable training\n",
    ">* Use sensible defaults, adjust only for special cases\n",
    "\n",
    ">* Regularization limits model complexity and discourages large weights\n",
    ">* Layer-wise penalties improve generalization and deployment reliability\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec302c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Weight Initialization and Regularization\n",
    "\n",
    "# This script shows weight initialization and regularization.\n",
    "# It uses a tiny dataset for quick demonstration.\n",
    "# Focus on how layer settings affect training.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required libraries for TensorFlow and NumPy.\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Set deterministic seeds for reproducible behavior.\n",
    "tf.random.set_seed(7)\n",
    "np.random.seed(7)\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Create a tiny synthetic regression style dataset.\n",
    "num_samples = 200\n",
    "num_features = 10\n",
    "\n",
    "# Generate random input features with small values.\n",
    "X = np.random.randn(num_samples, num_features).astype(\"float32\")\n",
    "\n",
    "# Create targets as a simple linear combination.\n",
    "true_w = np.arange(1, num_features + 1, dtype=\"float32\")\n",
    "\n",
    "# Compute targets with small noise added.\n",
    "y = X @ true_w + 0.1 * np.random.randn(num_samples).astype(\"float32\")\n",
    "\n",
    "# Split into train and test subsets safely.\n",
    "train_size = 160\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "\n",
    "# Split targets into train and test subsets.\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "# Confirm shapes are as expected before modeling.\n",
    "print(\"Train shape:\", X_train.shape, y_train.shape)\n",
    "\n",
    "# Confirm test shapes to avoid later shape errors.\n",
    "print(\"Test shape:\", X_test.shape, y_test.shape)\n",
    "\n",
    "# Build a model with default Glorot uniform initializer.\n",
    "def build_default_model():\n",
    "    # Create a simple Sequential model with Dense layers.\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(\n",
    "            16,\n",
    "            activation=\"relu\",\n",
    "            input_shape=(num_features,)\n",
    "        ),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    # Compile with mean squared error and Adam optimizer.\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "        loss=\"mse\",\n",
    "        metrics=[\"mae\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Build a model with He initialization and L2 regularization.\n",
    "def build_regularized_model():\n",
    "    # Use He normal initializer for ReLU activations.\n",
    "    he_init = tf.keras.initializers.HeNormal()\n",
    "\n",
    "    # Use L2 regularizer to penalize large weights.\n",
    "    l2_reg = tf.keras.regularizers.L2(0.01)\n",
    "\n",
    "    # Create Sequential model with custom settings.\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(\n",
    "            16,\n",
    "            activation=\"relu\",\n",
    "            kernel_initializer=he_init,\n",
    "            kernel_regularizer=l2_reg,\n",
    "            input_shape=(num_features,)\n",
    "        ),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    # Compile with same optimizer and loss for fairness.\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "        loss=\"mse\",\n",
    "        metrics=[\"mae\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Build both models for comparison experiments.\n",
    "default_model = build_default_model()\n",
    "regularized_model = build_regularized_model()\n",
    "\n",
    "# Train default model briefly with silent training logs.\n",
    "history_default = default_model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=30,\n",
    "    batch_size=16,\n",
    "    verbose=0,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "# Train regularized model with same settings.\n",
    "history_reg = regularized_model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=30,\n",
    "    batch_size=16,\n",
    "    verbose=0,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "# Evaluate both models on the held out test set.\n",
    "loss_default, mae_default = default_model.evaluate(\n",
    "    X_test,\n",
    "    y_test,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Evaluate regularized model on the same test set.\n",
    "loss_reg, mae_reg = regularized_model.evaluate(\n",
    "    X_test,\n",
    "    y_test,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Print a short summary comparing test performance.\n",
    "print(\"Default model - test MSE:\", round(loss_default, 4))\n",
    "\n",
    "# Print mean absolute error for default model.\n",
    "print(\"Default model - test MAE:\", round(mae_default, 4))\n",
    "\n",
    "# Print test metrics for regularized model.\n",
    "print(\"Regularized model - test MSE:\", round(loss_reg, 4))\n",
    "\n",
    "# Print mean absolute error for regularized model.\n",
    "print(\"Regularized model - test MAE:\", round(mae_reg, 4))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219bafda",
   "metadata": {},
   "source": [
    "## **3. Compile Train Evaluate**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceddb54a",
   "metadata": {},
   "source": [
    "### **3.1. Selecting Loss and Metrics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed1c60e",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_03/Lecture_A/image_03_01.jpg?v=1769387937\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Loss choice defines what training minimizes\n",
    ">* Match loss to task and error priorities\n",
    "\n",
    ">* Metrics summarize model performance for humans\n",
    ">* Choose metrics matching data balance and real costs\n",
    "\n",
    ">* Loss aids optimization; metrics reflect real impact\n",
    ">* Pair mathematically convenient loss with meaningful metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cdfdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Selecting Loss and Metrics\n",
    "\n",
    "# This script shows loss and metrics choices.\n",
    "# It uses a tiny dataset for quick training.\n",
    "# It focuses on Sequential compile train evaluate.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Import TensorFlow and Keras utilities.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Load MNIST dataset from Keras datasets.\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Use a small subset for faster training.\n",
    "train_samples = 2000\n",
    "test_samples = 500\n",
    "x_train = x_train[:train_samples]\n",
    "y_train = y_train[:train_samples]\n",
    "\n",
    "# Slice test data subset safely.\n",
    "x_test = x_test[:test_samples]\n",
    "y_test = y_test[:test_samples]\n",
    "\n",
    "# Validate shapes before preprocessing.\n",
    "assert x_train.ndim == 3 and x_test.ndim == 3\n",
    "assert y_train.shape[0] == train_samples\n",
    "\n",
    "# Normalize pixel values to range zero one.\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_test = x_test.astype(\"float32\") / 255.0\n",
    "\n",
    "# Flatten images to vectors for dense layers.\n",
    "x_train = x_train.reshape((train_samples, 28 * 28))\n",
    "x_test = x_test.reshape((test_samples, 28 * 28))\n",
    "\n",
    "# Build a simple Sequential classification model.\n",
    "model = keras.Sequential([\n",
    "    layers.Input(shape=(28 * 28,)),\n",
    "    layers.Dense(64, activation=\"relu\"),\n",
    "    layers.Dense(10, activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "# Choose loss for multi class classification.\n",
    "loss_fn = \"sparse_categorical_crossentropy\"\n",
    "\n",
    "# Choose optimizer with safe learning rate.\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# Choose accuracy metric for easy interpretation.\n",
    "metrics_list = [\"accuracy\"]\n",
    "\n",
    "# Compile model with chosen loss and metrics.\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=loss_fn,\n",
    "              metrics=metrics_list)\n",
    "\n",
    "# Train model briefly with silent verbose setting.\n",
    "history = model.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs=3,\n",
    "                    batch_size=64,\n",
    "                    verbose=0,\n",
    "                    validation_split=0.1)\n",
    "\n",
    "# Evaluate model on held out test data.\n",
    "test_loss, test_accuracy = model.evaluate(\n",
    "    x_test,\n",
    "    y_test,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Print selected loss name and metric name.\n",
    "print(\"Used loss:\", loss_fn, \"metrics:\", metrics_list)\n",
    "\n",
    "# Print final test loss and accuracy values.\n",
    "print(\"Test loss:\", round(float(test_loss), 4))\n",
    "print(\"Test accuracy:\", round(float(test_accuracy), 4))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d05eb3",
   "metadata": {},
   "source": [
    "### **3.2. Training with Callbacks**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76d0a3d",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_03/Lecture_A/image_03_02.jpg?v=1769387971\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Callbacks add custom control during model training\n",
    ">* They monitor progress and stop wasteful extra epochs\n",
    "\n",
    ">* Early stopping halts training when validation stops improving\n",
    ">* Checkpointing saves best model during long training runs\n",
    "\n",
    ">* Callbacks log metrics and help debug training\n",
    ">* They enable flexible control and systematic experiments\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ce4754",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Training with Callbacks\n",
    "\n",
    "# This script shows training with Keras callbacks.\n",
    "# We use a small MNIST subset for quick training.\n",
    "# Focus is on early stopping and checkpoints.\n",
    "\n",
    "# Install TensorFlow in some environments if needed.\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Import TensorFlow and Keras utilities.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Select device based on GPU availability.\n",
    "physical_gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "if physical_gpus:\n",
    "    device_name = \"GPU\"\n",
    "else:\n",
    "    device_name = \"CPU\"\n",
    "\n",
    "# Load MNIST dataset from Keras datasets.\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Confirm dataset shapes before preprocessing.\n",
    "print(\"Train shape:\", x_train.shape, \"Test shape:\", x_test.shape)\n",
    "\n",
    "# Normalize pixel values to range zero one.\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_test = x_test.astype(\"float32\") / 255.0\n",
    "\n",
    "# Flatten images into vectors for dense network.\n",
    "x_train = x_train.reshape((-1, 28 * 28))\n",
    "x_test = x_test.reshape((-1, 28 * 28))\n",
    "\n",
    "# Use a small subset for faster demonstration.\n",
    "train_samples = 8000\n",
    "test_samples = 2000\n",
    "\n",
    "# Slice the arrays to keep only subsets.\n",
    "x_train_small = x_train[:train_samples]\n",
    "y_train_small = y_train[:train_samples]\n",
    "\n",
    "# Slice test data similarly for quick evaluation.\n",
    "x_test_small = x_test[:test_samples]\n",
    "y_test_small = y_test[:test_samples]\n",
    "\n",
    "# Build a simple Sequential classification model.\n",
    "model = keras.Sequential([\n",
    "    layers.Input(shape=(28 * 28,)),\n",
    "    layers.Dense(64, activation=\"relu\"),\n",
    "    layers.Dense(10, activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "# Compile model with optimizer loss and metrics.\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Define early stopping callback on validation loss.\n",
    "early_stop = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=2,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "# Define model checkpoint callback saving best weights.\n",
    "checkpoint_path = \"best_mnist_model.keras\"\n",
    "model_ckpt = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    monitor=\"val_accuracy\",\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False,\n",
    ")\n",
    "\n",
    "# Train model with callbacks and silent verbose.\n",
    "history = model.fit(\n",
    "    x_train_small,\n",
    "    y_train_small,\n",
    "    validation_split=0.2,\n",
    "    epochs=20,\n",
    "    batch_size=64,\n",
    "    callbacks=[early_stop, model_ckpt],\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Evaluate model on held out test subset.\n",
    "loss, accuracy = model.evaluate(\n",
    "    x_test_small,\n",
    "    y_test_small,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Count how many epochs were actually run.\n",
    "trained_epochs = len(history.history[\"loss\"])\n",
    "\n",
    "# Print concise summary of training with callbacks.\n",
    "print(\"Device used:\", device_name)\n",
    "print(\"Epochs requested:\", 20, \"Epochs run:\", trained_epochs)\n",
    "print(\"Best validation loss:\", min(history.history[\"val_loss\"]))\n",
    "print(\"Test loss:\", round(float(loss), 4))\n",
    "print(\"Test accuracy:\", round(float(accuracy), 4))\n",
    "print(\"Checkpoint file exists:\", os.path.exists(checkpoint_path))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764f5a5e",
   "metadata": {},
   "source": [
    "### **3.3. Model Evaluation Basics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04486771",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_03/Lecture_A/image_03_03.jpg?v=1769388012\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Evaluate on unseen data using loss, metrics\n",
    ">* Keeps weights fixed, gives objective performance snapshot\n",
    "\n",
    ">* Use separate training, validation, and test datasets\n",
    ">* Held-out data reveals generalization and overfitting\n",
    "\n",
    ">* Relate loss and metrics to real goals\n",
    ">* Use domain metrics to guide model improvements\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af8540c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Model Evaluation Basics\n",
    "\n",
    "# This script shows basic model evaluation.\n",
    "# We train a tiny Sequential model safely.\n",
    "# Then we evaluate it on held out data.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "random.seed(7)\n",
    "np.random.seed(7)\n",
    "os.environ[\"PYTHONHASHSEED\"] = \"7\"\n",
    "\n",
    "# Import TensorFlow and Keras utilities.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Select device based on GPU availability.\n",
    "physical_gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "if physical_gpus:\n",
    "    device_name = \"GPU\"\n",
    "else:\n",
    "    device_name = \"CPU\"\n",
    "\n",
    "# Show which device type will be used.\n",
    "print(\"Using device type:\", device_name)\n",
    "\n",
    "# Load MNIST dataset from Keras datasets.\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Confirm shapes to avoid unexpected issues.\n",
    "print(\"Train shape:\", x_train.shape, y_train.shape)\n",
    "\n",
    "# Reduce dataset size for faster execution.\n",
    "x_train_small = x_train[:5000]\n",
    "y_train_small = y_train[:5000]\n",
    "\n",
    "# Create a small validation split from training.\n",
    "x_val_small = x_train_small[4000:5000]\n",
    "y_val_small = y_train_small[4000:5000]\n",
    "\n",
    "# Keep a smaller training subset for fitting.\n",
    "x_train_small = x_train_small[:4000]\n",
    "y_train_small = y_train_small[:4000]\n",
    "\n",
    "# Normalize pixel values to range zero one.\n",
    "x_train_small = x_train_small.astype(\"float32\") / 255.0\n",
    "x_val_small = x_val_small.astype(\"float32\") / 255.0\n",
    "x_test_small = x_test[:2000].astype(\"float32\") / 255.0\n",
    "\n",
    "# Flatten images to vectors for Dense layers.\n",
    "input_shape = (28 * 28,)\n",
    "x_train_small = x_train_small.reshape((-1,) + input_shape)\n",
    "\n",
    "# Reshape validation and test data similarly.\n",
    "x_val_small = x_val_small.reshape((-1,) + input_shape)\n",
    "x_test_small = x_test_small.reshape((-1,) + input_shape)\n",
    "\n",
    "# Verify final shapes before building model.\n",
    "print(\"Train small shape:\", x_train_small.shape)\n",
    "\n",
    "# Build a simple Sequential classification model.\n",
    "model = keras.Sequential([\n",
    "    layers.Input(shape=input_shape),\n",
    "    layers.Dense(64, activation=\"relu\"),\n",
    "    layers.Dense(10, activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "# Compile model with loss optimizer and metric.\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Train model quietly using small epochs.\n",
    "history = model.fit(\n",
    "    x_train_small,\n",
    "    y_train_small,\n",
    "    epochs=3,\n",
    "    batch_size=64,\n",
    "    validation_data=(x_val_small, y_val_small),\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Evaluate on validation data to monitor generalization.\n",
    "val_loss, val_acc = model.evaluate(\n",
    "    x_val_small,\n",
    "    y_val_small,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Evaluate once on held out test data.\n",
    "test_loss, test_acc = model.evaluate(\n",
    "    x_test_small,\n",
    "    y_test[:2000],\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Print concise summary of evaluation results.\n",
    "print(\"Validation loss and accuracy:\", round(val_loss, 4), round(val_acc, 4))\n",
    "print(\"Test loss and accuracy:\", round(test_loss, 4), round(test_acc, 4))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94e20b3",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Sequential Models**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabd374e",
   "metadata": {},
   "source": [
    "\n",
    "In this lecture, you learned to:\n",
    "- Construct Keras Sequential models with common layer types for supervised learning tasks. \n",
    "- Configure model compilation settings including loss functions, optimizers, and metrics. \n",
    "- Train and evaluate Sequential models using model.fit and model.evaluate. \n",
    "\n",
    "In the next Lecture (Lecture B), we will go over 'Functional API'"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
