{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20ff5ef1",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Customization**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971be587",
   "metadata": {},
   "source": [
    ">Last update: 20260127.\n",
    "    \n",
    "By the end of this Lecture, you will be able to:\n",
    "- Implement custom Keras layers by subclassing tf.keras.layers.Layer with build and call methods. \n",
    "- Define custom loss and metric functions or classes compatible with Keras training workflows. \n",
    "- Integrate custom components into models and verify they save and load correctly. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cbc7a4",
   "metadata": {},
   "source": [
    "## **1. Building Custom Layers**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6013155",
   "metadata": {},
   "source": [
    "### **1.1. Subclassing Keras Layers**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593759e2",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_10/Lecture_A/image_01_01.jpg?v=1769558974\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Custom layers capture problem-specific tensor operations\n",
    ">* They add structure, parameters, and reusable configurations\n",
    "\n",
    ">* Custom layers have a clear, repeatable lifecycle\n",
    ">* They initialize, manage weights, and transform inputs predictably\n",
    "\n",
    ">* Subclassing enables complex, custom layer behaviors and logic\n",
    ">* Keeps compatibility with Keras training, saving, deployment\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9da9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Subclassing Keras Layers\n",
    "\n",
    "# This script shows a simple custom layer.\n",
    "# It focuses on subclassing tf.keras.layers.Layer.\n",
    "# Run cells to see shapes and behavior.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required TensorFlow modules.\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set deterministic random seeds for reproducibility.\n",
    "tf.random.set_seed(7)\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Define a custom scaling layer using subclassing.\n",
    "class ScalingLayer(tf.keras.layers.Layer):\n",
    "\n",
    "    # Initialize layer and store configuration values.\n",
    "    def __init__(self, scale_init=1.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.scale_init = float(scale_init)\n",
    "\n",
    "    # Create trainable weights based on input shape.\n",
    "    def build(self, input_shape):\n",
    "        if len(input_shape) < 2:\n",
    "            raise ValueError(\"Input rank must be at least two.\")\n",
    "        last_dim = int(input_shape[-1])\n",
    "        if last_dim <= 0:\n",
    "            raise ValueError(\"Last dimension must be positive.\")\n",
    "        self.scale = self.add_weight(\n",
    "            name=\"scale\", shape=(last_dim,), initializer=tf.keras.initializers.Constant(\n",
    "                self.scale_init\n",
    "            ), trainable=True,\n",
    "        )\n",
    "        super().build(input_shape)\n",
    "\n",
    "    # Define the forward computation for the layer.\n",
    "    def call(self, inputs):\n",
    "        inputs = tf.convert_to_tensor(inputs)\n",
    "        if inputs.shape.rank is None:\n",
    "            raise ValueError(\"Inputs must have known rank.\")\n",
    "        return inputs * self.scale\n",
    "\n",
    "    # Enable serialization of configuration values.\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"scale_init\": self.scale_init})\n",
    "        return config\n",
    "\n",
    "# Create a tiny model that uses the custom layer.\n",
    "inputs = tf.keras.Input(shape=(3,), name=\"features\")\n",
    "\n",
    "# Apply the custom scaling layer to the inputs.\n",
    "x = ScalingLayer(scale_init=0.5, name=\"scaling_layer\")(inputs)\n",
    "\n",
    "# Add a small dense layer for demonstration.\n",
    "outputs = tf.keras.layers.Dense(1, activation=\"linear\")(x)\n",
    "\n",
    "# Build the Keras model object.\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs, name=\"demo_model\")\n",
    "\n",
    "# Compile the model with simple settings.\n",
    "model.compile(optimizer=\"sgd\", loss=\"mse\", metrics=[\"mae\"])\n",
    "\n",
    "# Create a tiny deterministic dataset for training.\n",
    "x_train = tf.constant([[1.0, 2.0, 3.0], [0.5, 0.0, -1.0]], dtype=tf.float32)\n",
    "\n",
    "y_train = tf.constant([[1.0], [0.0]], dtype=tf.float32)\n",
    "\n",
    "# Train briefly with silent output to avoid logs.\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=1, verbose=0)\n",
    "\n",
    "# Show model prediction before saving and loading.\n",
    "original_pred = model.predict(x_train, verbose=0)\n",
    "\n",
    "# Save the model including the custom layer.\n",
    "model_path = \"scaling_layer_demo.keras\"\n",
    "\n",
    "# Use the native Keras saving format.\n",
    "model.save(model_path)\n",
    "\n",
    "# Load the model with custom_objects mapping.\n",
    "loaded_model = tf.keras.models.load_model(\n",
    "    model_path, custom_objects={\"ScalingLayer\": ScalingLayer}\n",
    ")\n",
    "\n",
    "# Compute predictions with the loaded model.\n",
    "loaded_pred = loaded_model.predict(x_train, verbose=0)\n",
    "\n",
    "# Print shapes and a few prediction values.\n",
    "print(\"Input shape:\", x_train.shape)\n",
    "\n",
    "# Show the learned scale weights from original model.\n",
    "print(\"Original scale weights:\", model.get_layer(\"scaling_layer\").get_weights())\n",
    "\n",
    "# Show the learned scale weights from loaded model.\n",
    "print(\"Loaded scale weights:\", loaded_model.get_layer(\"scaling_layer\").get_weights())\n",
    "\n",
    "# Compare first prediction from original and loaded models.\n",
    "print(\"Original first prediction:\", float(original_pred[0, 0]))\n",
    "\n",
    "# Final line prints loaded first prediction for confirmation.\n",
    "print(\"Loaded first prediction:\", float(loaded_pred[0, 0]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3155766f",
   "metadata": {},
   "source": [
    "### **1.2. Managing Trainable Weights**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e48db8",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_10/Lecture_A/image_01_02.jpg?v=1769559073\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Custom layers must clearly declare trainable weights\n",
    ">* Clear weights enable training, saving, and reuse\n",
    "\n",
    ">* Choose which layer parameters are learnable\n",
    ">* Separate trainable, frozen parts to control training\n",
    "\n",
    ">* Choose weight shapes, initializations, and constraints carefully\n",
    ">* Good choices improve training stability, speed, and robustness\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907dc974",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Managing Trainable Weights\n",
    "\n",
    "# This script shows custom layer weights management.\n",
    "# It focuses on trainable and nontrainable TensorFlow weights.\n",
    "# Run cells sequentially to follow the simple example.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import TensorFlow and NumPy with short aliases.\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Print TensorFlow version in one concise line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Set deterministic seeds for reproducible behavior.\n",
    "tf.random.set_seed(7)\n",
    "np.random.seed(7)\n",
    "\n",
    "# Define a custom scaling layer with explicit weights.\n",
    "class ScalingLayer(tf.keras.layers.Layer):\n",
    "    # Initialize layer and choose trainable behavior.\n",
    "    def __init__(self, trainable_scale=True, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self._trainable_scale = bool(trainable_scale)\n",
    "\n",
    "    # Create weights once input shape is known.\n",
    "    def build(self, input_shape):\n",
    "        feature_dim = int(input_shape[-1])\n",
    "        if feature_dim <= 0:\n",
    "            raise ValueError(\"Feature dimension must be positive.\")\n",
    "        self.scale = self.add_weight(\n",
    "            name=\"scale\", shape=(feature_dim,), initializer=\"ones\",\n",
    "            trainable=self._trainable_scale,\n",
    "        )\n",
    "        self.bias = self.add_weight(\n",
    "            name=\"bias\", shape=(feature_dim,), initializer=\"zeros\",\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.running_mean = self.add_weight(\n",
    "            name=\"running_mean\", shape=(feature_dim,), initializer=\"zeros\",\n",
    "            trainable=False,\n",
    "        )\n",
    "        super().build(input_shape)\n",
    "\n",
    "    # Define the forward computation for the layer.\n",
    "    def call(self, inputs, training=False):\n",
    "        inputs = tf.convert_to_tensor(inputs)\n",
    "        if training:\n",
    "            batch_mean = tf.reduce_mean(inputs, axis=0)\n",
    "            self.running_mean.assign(batch_mean)\n",
    "        return inputs * self.scale + self.bias\n",
    "\n",
    "# Create a small model using the custom layer.\n",
    "inputs = tf.keras.Input(shape=(3,))\n",
    "outputs = ScalingLayer(trainable_scale=True)(inputs)\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Show how many trainable weights the model has.\n",
    "print(\"Trainable weights count:\", len(model.trainable_weights))\n",
    "\n",
    "# Show how many nontrainable weights the model has.\n",
    "print(\"Nontrainable weights count:\", len(model.non_trainable_weights))\n",
    "\n",
    "# Build a tiny dataset of simple numeric pairs.\n",
    "x_data = np.array([[1.0, 2.0, 3.0]], dtype=np.float32)\n",
    "y_data = np.array([[2.0, 4.0, 6.0]], dtype=np.float32)\n",
    "\n",
    "# Compile the model with a basic optimizer and loss.\n",
    "model.compile(optimizer=\"sgd\", loss=\"mse\")\n",
    "\n",
    "# Train briefly with silent output to update weights.\n",
    "model.fit(x_data, y_data, epochs=20, verbose=0)\n",
    "\n",
    "# Inspect the learned scale and bias values after training.\n",
    "scale_value = model.layers[1].scale.numpy()\n",
    "bias_value = model.layers[1].bias.numpy()\n",
    "\n",
    "# Print a short summary of learned parameters.\n",
    "print(\"Learned scale:\", np.round(scale_value, 3))\n",
    "print(\"Learned bias:\", np.round(bias_value, 3))\n",
    "\n",
    "# Demonstrate that running_mean stayed nontrainable during training.\n",
    "print(\"Running mean:\", np.round(model.layers[1].running_mean.numpy(), 3))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bc9626",
   "metadata": {},
   "source": [
    "### **1.3. Using build vs init**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b3724e",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_10/Lecture_A/image_01_03.jpg?v=1769559163\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Constructor sets hyperparameters independent of input shape\n",
    ">* Build creates input-shaped weights when data arrives\n",
    "\n",
    ">* build delays weight creation for flexible inputs\n",
    ">* constructor stores fixed choices; build creates variables\n",
    "\n",
    ">* Avoid creating input-shaped variables in constructors\n",
    ">* Create weights in build to ensure flexibility\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21f275d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Using build vs init\n",
    "\n",
    "# This script explains build versus init clearly.\n",
    "# It shows a custom dense layer example.\n",
    "# It compares behavior and prints useful shapes.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required modules from TensorFlow.\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set a deterministic random seed for reproducibility.\n",
    "tf.random.set_seed(7)\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Define a custom dense layer using build correctly.\n",
    "class MyDenseBuilt(tf.keras.layers.Layer):\n",
    "    # Store configuration only inside the constructor.\n",
    "    def __init__(self, units, use_bias=True, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "    # Create weights using the input shape inside build.\n",
    "    def build(self, input_shape):\n",
    "        last_dim = int(input_shape[-1])\n",
    "        self.w = self.add_weight(\n",
    "            name=\"kernel\", shape=(last_dim, self.units)\n",
    "        )\n",
    "        if self.use_bias:\n",
    "            self.b = self.add_weight(\n",
    "                name=\"bias\", shape=(self.units,)\n",
    "            )\n",
    "        else:\n",
    "            self.b = None\n",
    "        super().build(input_shape)\n",
    "\n",
    "    # Define the forward computation inside call.\n",
    "    def call(self, inputs):\n",
    "        outputs = tf.matmul(inputs, self.w)\n",
    "        if self.b is not None:\n",
    "            outputs = outputs + self.b\n",
    "        return outputs\n",
    "\n",
    "# Define a layer that incorrectly creates weights in __init__.\n",
    "class MyDenseInit(tf.keras.layers.Layer):\n",
    "    # Try to guess input dimension inside constructor.\n",
    "    def __init__(self, units, guessed_input_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.guessed_input_dim = guessed_input_dim\n",
    "        self.w = self.add_weight(\n",
    "            name=\"kernel_init\", shape=(guessed_input_dim, units)\n",
    "        )\n",
    "\n",
    "    # Keep build empty because weights already exist.\n",
    "    def build(self, input_shape):\n",
    "        super().build(input_shape)\n",
    "\n",
    "    # Forward pass uses the guessed kernel shape.\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w)\n",
    "\n",
    "# Create two small input tensors with different feature sizes.\n",
    "inputs_4 = tf.ones(shape=(2, 4), dtype=tf.float32)\n",
    "\n",
    "# Another input has three features instead of four.\n",
    "inputs_3 = tf.ones(shape=(2, 3), dtype=tf.float32)\n",
    "\n",
    "# Instantiate the correctly built layer with five units.\n",
    "layer_built = MyDenseBuilt(units=5, use_bias=True)\n",
    "\n",
    "# Call the layer on inputs_4 to trigger build.\n",
    "outputs_built_4 = layer_built(inputs_4)\n",
    "\n",
    "# Call the same layer on inputs_3 to test flexibility.\n",
    "try:\n",
    "    outputs_built_3 = layer_built(inputs_3)\n",
    "    error_message_built = None\n",
    "except Exception as exc:\n",
    "    outputs_built_3 = None\n",
    "    error_message_built = str(exc)\n",
    "\n",
    "# Instantiate the incorrect layer guessing four features.\n",
    "layer_init = MyDenseInit(units=5, guessed_input_dim=4)\n",
    "\n",
    "# Call the incorrect layer on matching shape inputs.\n",
    "outputs_init_4 = layer_init(inputs_4)\n",
    "\n",
    "# Try calling incorrect layer on mismatched shape safely.\n",
    "try:\n",
    "    outputs_init_3 = layer_init(inputs_3)\n",
    "    error_message = None\n",
    "except Exception as exc:\n",
    "    outputs_init_3 = None\n",
    "    error_message = str(exc)\n",
    "\n",
    "# Print shapes created by the correctly built layer.\n",
    "print(\"MyDenseBuilt kernel shape:\", layer_built.w.shape)\n",
    "\n",
    "# Show output shapes for both compatible input sizes.\n",
    "print(\"Output shape with inputs_4:\", outputs_built_4.shape)\n",
    "\n",
    "# The same layer adapts to inputs_3 automatically.\n",
    "print(\"Output shape with inputs_3:\", getattr(outputs_built_3, \"shape\", None))\n",
    "\n",
    "# Print the error message or a fallback string for built layer.\n",
    "print(\"Error when using inputs_3 with MyDenseBuilt:\")\n",
    "print(error_message_built if error_message_built else \"No error raised.\")\n",
    "\n",
    "# Print kernel shape for the incorrect initialization layer.\n",
    "print(\"MyDenseInit kernel shape:\", layer_init.w.shape)\n",
    "\n",
    "# Show output shape when guessed dimension matches input.\n",
    "print(\"Output shape init with inputs_4:\", outputs_init_4.shape)\n",
    "\n",
    "# Print whether calling with wrong shape produced an error.\n",
    "print(\"Error when using inputs_3 with MyDenseInit:\")\n",
    "\n",
    "# Print the error message or a fallback string.\n",
    "print(error_message if error_message else \"No error raised.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3caed98",
   "metadata": {},
   "source": [
    "## **2. Designing Custom Losses**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0496fab0",
   "metadata": {},
   "source": [
    "### **2.1. Function Based Losses**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32d26a2",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_10/Lecture_A/image_02_01.jpg?v=1769559294\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Loss functions map predictions and targets to error\n",
    ">* They encode which mistakes matter for the task\n",
    "\n",
    ">* Loss functions must be batched, differentiable, scalar\n",
    ">* Shape loss to emphasize domain-specific risk priorities\n",
    "\n",
    ">* Custom losses encode constraints and domain rules\n",
    ">* They integrate smoothly into existing training workflows\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c528ad5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Function Based Losses\n",
    "\n",
    "# This script shows custom function based losses.\n",
    "# It uses TensorFlow Keras with small dummy data.\n",
    "# Focus on clarity not model performance today.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required libraries for TensorFlow usage.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set seeds for reproducible behavior in training.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Select device preference based on GPU availability.\n",
    "physical_gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "if physical_gpus:\n",
    "    device_name = \"GPU\"\n",
    "else:\n",
    "    device_name = \"CPU\"\n",
    "\n",
    "# Print which device type will likely be used.\n",
    "print(\"Using device type:\", device_name)\n",
    "\n",
    "# Create small synthetic regression style dataset.\n",
    "num_samples = 64\n",
    "x_data = np.linspace(-1.0, 1.0, num_samples).reshape(-1, 1)\n",
    "noise = 0.1 * np.random.randn(num_samples, 1)\n",
    "\n",
    "# Generate targets with simple linear relationship.\n",
    "y_true = 2.0 * x_data + 0.5 + noise\n",
    "\n",
    "# Validate shapes before building the model.\n",
    "assert x_data.shape == (num_samples, 1)\n",
    "assert y_true.shape == (num_samples, 1)\n",
    "\n",
    "# Define a simple baseline mean squared error loss.\n",
    "def mse_loss(y_true_batch, y_pred_batch):\n",
    "    squared_error = tf.square(y_true_batch - y_pred_batch)\n",
    "    return tf.reduce_mean(squared_error)\n",
    "\n",
    "# Define custom loss penalizing underestimates more strongly.\n",
    "def asymmetric_underestimate_loss(y_true_batch, y_pred_batch):\n",
    "    diff = y_pred_batch - y_true_batch\n",
    "    under_mask = tf.cast(diff < 0.0, tf.float32)\n",
    "    over_mask = 1.0 - under_mask\n",
    "\n",
    "    # Apply larger weight when model underestimates targets.\n",
    "    under_weight = 3.0\n",
    "    over_weight = 1.0\n",
    "    weighted_error = (\n",
    "        under_weight * under_mask * tf.square(diff) +\n",
    "        over_weight * over_mask * tf.square(diff)\n",
    "    )\n",
    "\n",
    "    # Return mean loss value over the batch.\n",
    "    return tf.reduce_mean(weighted_error)\n",
    "\n",
    "# Build a tiny sequential regression model.\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(1,)),\n",
    "    tf.keras.layers.Dense(8, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# Compile model with custom asymmetric loss function.\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.05),\n",
    "    loss=asymmetric_underestimate_loss,\n",
    "    metrics=[mse_loss]\n",
    ")\n",
    "\n",
    "# Train briefly with silent verbose setting.\n",
    "history = model.fit(\n",
    "    x_data,\n",
    "    y_true,\n",
    "    epochs=40,\n",
    "    batch_size=16,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Evaluate model on same small dataset silently.\n",
    "loss_value, mse_value = model.evaluate(\n",
    "    x_data,\n",
    "    y_true,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Make a few predictions for inspection.\n",
    "x_test = np.array([[-0.5], [0.0], [0.5]], dtype=np.float32)\n",
    "y_pred = model.predict(x_test, verbose=0)\n",
    "\n",
    "# Print concise summary of training results.\n",
    "print(\"Custom asymmetric loss on training data:\", float(loss_value))\n",
    "print(\"Baseline MSE metric on training data:\", float(mse_value))\n",
    "\n",
    "# Print example predictions versus expected linear targets.\n",
    "print(\"Test inputs:\", x_test.reshape(-1))\n",
    "print(\"Model predictions:\", y_pred.reshape(-1))\n",
    "print(\"Ideal targets (approx):\", (2.0 * x_test + 0.5).reshape(-1))\n",
    "\n",
    "# Save and reload model to confirm loss compatibility.\n",
    "save_path = \"custom_loss_model.keras\"\n",
    "model.save(save_path, include_optimizer=False)\n",
    "reloaded_model = tf.keras.models.load_model(\n",
    "    save_path,\n",
    "    custom_objects={\"asymmetric_underestimate_loss\": asymmetric_underestimate_loss,\n",
    "                    \"mse_loss\": mse_loss}\n",
    ")\n",
    "\n",
    "# Evaluate reloaded model to verify consistent behavior.\n",
    "reloaded_loss, reloaded_mse = reloaded_model.evaluate(\n",
    "    x_data,\n",
    "    y_true,\n",
    "    verbose=0\n",
    ")\n",
    "print(\"Reloaded model loss and mse:\", float(reloaded_loss), float(reloaded_mse))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446af3d8",
   "metadata": {},
   "source": [
    "### **2.2. Loss classes with call**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02f8de1",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_10/Lecture_A/image_02_02.jpg?v=1769559342\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Loss classes add configuration, state, and helpers\n",
    ">* call computes loss using tunable, reusable behavior\n",
    "\n",
    ">* Class-based losses serialize cleanly with model configs\n",
    ">* They preserve behavior across environments, deployments, and audits\n",
    "\n",
    ">* Class losses can adapt using tracked statistics\n",
    ">* They keep training loops simple while adding sophistication\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07496a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Loss classes with call\n",
    "\n",
    "# This script shows a custom loss class.\n",
    "# It focuses on the call method usage.\n",
    "# It keeps training tiny and output short.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Import TensorFlow and Keras components.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Set deterministic random seeds.\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# Print TensorFlow version briefly.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Define a custom loss class with call.\n",
    "class ScaledMAELoss(keras.losses.Loss):\n",
    "    # Initialize with a scale hyperparameter.\n",
    "    def __init__(self, scale=1.0, name=\"scaled_mae\", reduction=keras.losses.Reduction.SUM_OVER_BATCH_SIZE):\n",
    "        super().__init__(name=name, reduction=reduction)\n",
    "        self.scale = tf.convert_to_tensor(scale, dtype=tf.float32)\n",
    "\n",
    "    # Implement the core loss computation.\n",
    "    def call(self, y_true, y_pred):\n",
    "        y_true = tf.convert_to_tensor(y_true, dtype=tf.float32)\n",
    "        y_pred = tf.convert_to_tensor(y_pred, dtype=tf.float32)\n",
    "        tf.debugging.assert_shapes([(y_true, (None, 1)), (y_pred, (None, 1))])\n",
    "        mae = tf.reduce_mean(tf.abs(y_true - y_pred))\n",
    "        return self.scale * mae\n",
    "\n",
    "# Create a tiny synthetic regression dataset.\n",
    "num_samples = 64\n",
    "x_values = np.linspace(-1.0, 1.0, num_samples).astype(\"float32\")\n",
    "noise = 0.05 * np.random.randn(num_samples).astype(\"float32\")\n",
    "\n",
    "# Compute targets with a simple linear rule.\n",
    "y_values = (2.0 * x_values + 0.5 + noise).reshape(-1, 1)\n",
    "\n",
    "# Reshape features for Keras dense input.\n",
    "x_values = x_values.reshape(-1, 1)\n",
    "\n",
    "# Build a simple sequential regression model.\n",
    "model = keras.Sequential([\n",
    "    layers.Input(shape=(1,)),\n",
    "    layers.Dense(8, activation=\"relu\"),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "# Instantiate the custom loss with a chosen scale.\n",
    "custom_loss = ScaledMAELoss(scale=2.0)\n",
    "\n",
    "# Compile the model using the custom loss.\n",
    "model.compile(optimizer=\"adam\", loss=custom_loss, metrics=[\"mae\"])\n",
    "\n",
    "# Train briefly with silent logging.\n",
    "history = model.fit(x_values, y_values, epochs=10, batch_size=16, verbose=0)\n",
    "\n",
    "# Evaluate the trained model once.\n",
    "loss_value, mae_value = model.evaluate(x_values, y_values, verbose=0)\n",
    "\n",
    "# Show the final loss and metric values.\n",
    "print(\"Final scaled loss:\", float(loss_value))\n",
    "print(\"Final MAE metric:\", float(mae_value))\n",
    "\n",
    "# Save the model including the custom loss configuration.\n",
    "model_path = \"custom_loss_model.keras\"\n",
    "model.save(model_path, include_optimizer=False)\n",
    "\n",
    "# Load the model with custom_objects mapping.\n",
    "loaded_model = keras.models.load_model(\n",
    "    model_path,\n",
    "    custom_objects={\"ScaledMAELoss\": ScaledMAELoss}\n",
    ")\n",
    "\n",
    "# Evaluate the loaded model to verify behavior.\n",
    "loaded_loss, loaded_mae = loaded_model.evaluate(\n",
    "    x_values,\n",
    "    y_values,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Print a short confirmation of consistency.\n",
    "print(\"Loaded model loss:\", float(loaded_loss))\n",
    "print(\"Loaded model MAE:\", float(loaded_mae))\n",
    "print(\"Loss class scale attribute:\", float(custom_loss.scale))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3aea3d",
   "metadata": {},
   "source": [
    "### **2.3. Loss Reduction and Masking**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29482125",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_10/Lecture_A/image_02_03.jpg?v=1769559417\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Loss reduction aggregates per-example errors into one\n",
    ">* Choose averaging, summing, or weighting to match priorities\n",
    "\n",
    ">* Masking selects which elements affect loss aggregation\n",
    ">* It ignores padding or missing data, focusing learning\n",
    "\n",
    ">* Align loss with masks and workflow expectations\n",
    ">* Careful design improves performance, stability, and reuse\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d896a4",
   "metadata": {},
   "source": [
    "## **3. Managing Custom Metrics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d951fb",
   "metadata": {},
   "source": [
    "### **3.1. Metric State Variables**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628833aa",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_10/Lecture_A/image_03_01.jpg?v=1769559589\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Metrics use state variables to accumulate information\n",
    ">* Stored sums and counts produce final averaged values\n",
    "\n",
    ">* Metric state variables integrate with TensorFlow systems\n",
    ">* They persist across saving, loading, and distributed training\n",
    "\n",
    ">* State variables control metric behavior across workflows\n",
    ">* They enable continuous tracking, export, and reliable restoration\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d539dc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Metric State Variables\n",
    "\n",
    "# This script shows metric state variables.\n",
    "# It uses a tiny model and custom metric.\n",
    "# Focus is saving and loading metric state.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required libraries safely.\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "pseed = 123\n",
    "np.random.seed(pseed)\n",
    "tf.random.set_seed(pseed)\n",
    "\n",
    "# Print TensorFlow version briefly.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Define a simple custom metric class.\n",
    "class RunningMeanAbsoluteError(tf.keras.metrics.Metric):\n",
    "    # Initialize metric state variables.\n",
    "    def __init__(self, name=\"running_mae\", **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.total_abs_error = self.add_weight(\n",
    "            name=\"total_abs_error\", initializer=\"zeros\"\n",
    "        )\n",
    "        self.count = self.add_weight(\n",
    "            name=\"count\", initializer=\"zeros\"\n",
    "        )\n",
    "\n",
    "    # Update state variables with new batch values.\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.cast(y_pred, tf.float32)\n",
    "        tf.debugging.assert_shapes(\n",
    "            [(y_true, (None, 1)), (y_pred, (None, 1))]\n",
    "        )\n",
    "        abs_error = tf.abs(y_true - y_pred)\n",
    "        batch_sum = tf.reduce_sum(abs_error)\n",
    "        batch_count = tf.cast(tf.size(abs_error), tf.float32)\n",
    "        self.total_abs_error.assign_add(batch_sum)\n",
    "        self.count.assign_add(batch_count)\n",
    "\n",
    "    # Compute final metric value from state.\n",
    "    def result(self):\n",
    "        return tf.math.divide_no_nan(self.total_abs_error, self.count)\n",
    "\n",
    "    # Reset state variables between epochs.\n",
    "    def reset_states(self):\n",
    "        self.total_abs_error.assign(0.0)\n",
    "        self.count.assign(0.0)\n",
    "\n",
    "# Create a tiny synthetic regression dataset.\n",
    "num_samples = 64\n",
    "x_data = np.linspace(-1.0, 1.0, num_samples).reshape(-1, 1)\n",
    "noise = 0.1 * np.random.randn(num_samples, 1)\n",
    "y_data = 2.0 * x_data + 0.5 + noise\n",
    "\n",
    "# Build a minimal Keras Sequential model.\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(1,)),\n",
    "    tf.keras.layers.Dense(4, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# Compile model with custom metric instance.\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"mse\",\n",
    "    metrics=[RunningMeanAbsoluteError()]\n",
    ")\n",
    "\n",
    "# Train briefly to update metric state variables.\n",
    "history = model.fit(\n",
    "    x_data,\n",
    "    y_data,\n",
    "    epochs=3,\n",
    "    batch_size=16,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Show metric result after training.\n",
    "metric_name = \"running_mae\"\n",
    "final_metric = history.history[metric_name][-1]\n",
    "print(\"Final training\", metric_name, \"before save:\", final_metric)\n",
    "\n",
    "# Save model including custom metric configuration.\n",
    "save_path = \"custom_metric_model.keras\"\n",
    "model.save(save_path)\n",
    "\n",
    "# Load model with custom_objects mapping.\n",
    "loaded_model = tf.keras.models.load_model(\n",
    "    save_path,\n",
    "    custom_objects={\"RunningMeanAbsoluteError\": RunningMeanAbsoluteError}\n",
    ")\n",
    "\n",
    "# Evaluate loaded model to confirm metric still works.\n",
    "loaded_results = loaded_model.evaluate(\n",
    "    x_data,\n",
    "    y_data,\n",
    "    batch_size=16,\n",
    "    verbose=0,\n",
    "    return_dict=True\n",
    ")\n",
    "\n",
    "# Print metric value after loading model.\n",
    "print(\"Metric from loaded model:\", loaded_results[metric_name])\n",
    "\n",
    "# Inspect internal state variables of loaded metric.\n",
    "loaded_metric = None\n",
    "for m in loaded_model.metrics:\n",
    "    if isinstance(m, RunningMeanAbsoluteError):\n",
    "        loaded_metric = m\n",
    "        break\n",
    "\n",
    "# Safely print metric state variable values.\n",
    "if loaded_metric is not None:\n",
    "    print(\"Loaded total_abs_error:\", float(loaded_metric.total_abs_error))\n",
    "    print(\"Loaded count:\", float(loaded_metric.count))\n",
    "else:\n",
    "    print(\"Custom metric instance not found in loaded model.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe34418",
   "metadata": {},
   "source": [
    "### **3.2. Updating And Reading Metrics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbcf527",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_10/Lecture_A/image_03_02.jpg?v=1769559682\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Metric update combines new batch with state\n",
    ">* Reading converts state into stable, logged value\n",
    "\n",
    ">* Metrics update each batch and log performance\n",
    ">* Domain metrics use cumulative stats for reporting\n",
    "\n",
    ">* Metric state must persist across saves and restarts\n",
    ">* Deterministic updates ensure consistent, trustworthy reported values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312b3377",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Updating And Reading Metrics\n",
    "\n",
    "# This script shows custom metric behavior.\n",
    "# It focuses on updating and reading metrics.\n",
    "# It also demonstrates saving and loading models.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Import TensorFlow and Keras components.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Define a simple custom metric class.\n",
    "class RunningMeanAbsoluteError(tf.keras.metrics.Metric):\n",
    "\n",
    "    # Initialize metric with two state variables.\n",
    "    def __init__(self, name=\"running_mae\", **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.total_abs_error = self.add_weight(\n",
    "            name=\"total_abs_error\", initializer=\"zeros\"\n",
    "        )\n",
    "        self.total_samples = self.add_weight(\n",
    "            name=\"total_samples\", initializer=\"zeros\"\n",
    "        )\n",
    "\n",
    "    # Update state using predictions and true targets.\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.cast(y_pred, tf.float32)\n",
    "        tf.debugging.assert_shapes([(y_true, (None, 1)), (y_pred, (None, 1))])\n",
    "        abs_error = tf.abs(y_true - y_pred)\n",
    "        batch_error = tf.reduce_sum(abs_error)\n",
    "        batch_size = tf.cast(tf.size(y_true), tf.float32)\n",
    "        self.total_abs_error.assign_add(batch_error)\n",
    "        self.total_samples.assign_add(batch_size)\n",
    "        return self.result()\n",
    "\n",
    "    # Read metric value from internal state.\n",
    "    def result(self):\n",
    "        return tf.math.divide_no_nan(self.total_abs_error, self.total_samples)\n",
    "\n",
    "    # Reset state between epochs or evaluations.\n",
    "    def reset_states(self):\n",
    "        self.total_abs_error.assign(0.0)\n",
    "        self.total_samples.assign(0.0)\n",
    "\n",
    "# Create a tiny synthetic regression dataset.\n",
    "num_samples = 128\n",
    "x_data = np.linspace(-1.0, 1.0, num_samples).astype(\"float32\")\n",
    "y_data = (2.0 * x_data + 0.3).reshape(-1, 1).astype(\"float32\")\n",
    "x_data = x_data.reshape(-1, 1)\n",
    "\n",
    "# Build a very small Keras model.\n",
    "inputs = keras.Input(shape=(1,))\n",
    "outputs = layers.Dense(1, activation=\"linear\")(inputs)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Instantiate the custom metric for training.\n",
    "running_mae = RunningMeanAbsoluteError()\n",
    "\n",
    "# Compile model with custom metric included.\n",
    "model.compile(\n",
    "    optimizer=\"sgd\",\n",
    "    loss=\"mse\",\n",
    "    metrics=[running_mae],\n",
    ")\n",
    "\n",
    "# Train briefly with silent logs to avoid spam.\n",
    "history = model.fit(\n",
    "    x_data,\n",
    "    y_data,\n",
    "    epochs=3,\n",
    "    batch_size=16,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Read metric value after training completes.\n",
    "final_metric_value = running_mae.result().numpy()\n",
    "print(\"Running MAE after training:\", float(final_metric_value))\n",
    "\n",
    "# Show internal state variables for clarity.\n",
    "print(\"Total absolute error state:\", float(running_mae.total_abs_error.numpy()))\n",
    "print(\"Total samples state:\", float(running_mae.total_samples.numpy()))\n",
    "\n",
    "# Save the model including custom metric configuration.\n",
    "save_path = \"custom_metric_model.keras\"\n",
    "model.save(save_path)\n",
    "\n",
    "# Load the model with custom_objects mapping.\n",
    "loaded_model = keras.models.load_model(\n",
    "    save_path,\n",
    "    custom_objects={\"RunningMeanAbsoluteError\": RunningMeanAbsoluteError},\n",
    ")\n",
    "\n",
    "# Evaluate loaded model to update its metric state.\n",
    "results = loaded_model.evaluate(\n",
    "    x_data,\n",
    "    y_data,\n",
    "    batch_size=16,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Read metric value from loaded model evaluation.\n",
    "loaded_metric_value = float(results[1])\n",
    "print(\"Running MAE after loading:\", loaded_metric_value)\n",
    "\n",
    "# Confirm shapes are correct before manual metric update.\n",
    "assert x_data.shape == y_data.shape\n",
    "\n",
    "# Manually call metric on a small batch to illustrate update.\n",
    "small_x = x_data[:4]\n",
    "small_y = y_data[:4]\n",
    "manual_metric = RunningMeanAbsoluteError()\n",
    "manual_metric.update_state(small_y, small_x)\n",
    "print(\"Manual metric value on small batch:\", float(manual_metric.result().numpy()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b939cfc",
   "metadata": {},
   "source": [
    "### **3.3. Resetting Metric State**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db26d08",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_10/Lecture_A/image_03_03.jpg?v=1769559806\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Metrics accumulate batch information over time\n",
    ">* Reset state to avoid mixed, misleading results\n",
    "\n",
    ">* Reset metrics between epochs and workflow phases\n",
    ">* Clean states keep comparisons fair and reliable\n",
    "\n",
    ">* Reset metrics after loading models for reuse\n",
    ">* Prevents stale state, keeps evaluations clean, trustworthy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe6c5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Resetting Metric State\n",
    "\n",
    "# This script shows resetting custom metric state.\n",
    "# It uses a tiny model and synthetic data.\n",
    "# Focus on clear metric behavior across phases.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required libraries safely.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version briefly.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Define a simple custom accuracy like metric.\n",
    "class SimpleAccuracy(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name=\"simple_accuracy\", **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.total_examples = self.add_weight(\n",
    "            name=\"total_examples\", initializer=\"zeros\", dtype=tf.float32\n",
    "        )\n",
    "        self.correct_examples = self.add_weight(\n",
    "            name=\"correct_examples\", initializer=\"zeros\", dtype=tf.float32\n",
    "        )\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.cast(tf.reshape(y_true, [-1]), tf.int32)\n",
    "        y_pred_labels = tf.argmax(y_pred, axis=-1, output_type=tf.int32)\n",
    "        matches = tf.cast(tf.equal(y_true, y_pred_labels), tf.float32)\n",
    "        batch_total = tf.cast(tf.size(y_true), tf.float32)\n",
    "        self.total_examples.assign_add(batch_total)\n",
    "        self.correct_examples.assign_add(tf.reduce_sum(matches))\n",
    "\n",
    "    def result(self):\n",
    "        return tf.math.divide_no_nan(self.correct_examples, self.total_examples)\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.total_examples.assign(0.0)\n",
    "        self.correct_examples.assign(0.0)\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config}\n",
    "\n",
    "# Create tiny synthetic classification data.\n",
    "num_classes = 3\n",
    "num_features = 4\n",
    "num_train = 64\n",
    "num_val = 32\n",
    "\n",
    "# Generate random features and integer labels.\n",
    "x_train = np.random.randn(num_train, num_features).astype(\"float32\")\n",
    "y_train = np.random.randint(num_classes, size=(num_train,)).astype(\"int32\")\n",
    "x_val = np.random.randn(num_val, num_features).astype(\"float32\")\n",
    "y_val = np.random.randint(num_classes, size=(num_val,)).astype(\"int32\")\n",
    "\n",
    "# Validate shapes before building the model.\n",
    "assert x_train.shape[1] == num_features\n",
    "assert x_val.shape[1] == num_features\n",
    "\n",
    "# Build a tiny dense model using Keras.\n",
    "inputs = tf.keras.Input(shape=(num_features,))\n",
    "hidden = tf.keras.layers.Dense(8, activation=\"relu\")(inputs)\n",
    "outputs = tf.keras.layers.Dense(num_classes, activation=\"softmax\")(hidden)\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Instantiate the custom metric for training.\n",
    "train_metric = SimpleAccuracy(name=\"train_simple_accuracy\")\n",
    "\n",
    "# Compile the model with custom metric included.\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[train_metric],\n",
    ")\n",
    "\n",
    "# Train briefly with silent logs to avoid spam.\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=2,\n",
    "    batch_size=16,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Show metric value after training phase.\n",
    "train_metric_value = float(train_metric.result().numpy())\n",
    "print(\"Metric after training phase:\", round(train_metric_value, 4))\n",
    "\n",
    "# Manually reset metric state before validation.\n",
    "train_metric.reset_state()\n",
    "print(\"Metric after manual reset:\", float(train_metric.result().numpy()))\n",
    "\n",
    "# Evaluate on validation data using a fresh metric.\n",
    "val_results = model.evaluate(\n",
    "    x_val,\n",
    "    y_val,\n",
    "    batch_size=16,\n",
    "    verbose=0,\n",
    "    return_dict=True,\n",
    ")\n",
    "\n",
    "# Print validation metric value clearly.\n",
    "print(\"Validation metric from evaluate:\", round(val_results[\"train_simple_accuracy\"], 4))\n",
    "\n",
    "# Save the model including custom metric configuration.\n",
    "save_path = \"custom_metric_model.keras\"\n",
    "model.save(save_path, include_optimizer=False)\n",
    "\n",
    "# Load the model with custom_objects mapping.\n",
    "loaded_model = tf.keras.models.load_model(\n",
    "    save_path,\n",
    "    custom_objects={\"SimpleAccuracy\": SimpleAccuracy},\n",
    ")\n",
    "\n",
    "# Evaluate loaded model to show fresh metric state.\n",
    "loaded_results = loaded_model.evaluate(\n",
    "    x_val,\n",
    "    y_val,\n",
    "    batch_size=16,\n",
    "    verbose=0,\n",
    "    return_dict=True,\n",
    ")\n",
    "\n",
    "# Print metric from loaded model evaluation.\n",
    "print(\n",
    "    \"Loaded model validation metric:\",\n",
    "    round(loaded_results[\"train_simple_accuracy\"], 4),\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680a8373",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Customization**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccde96d9",
   "metadata": {},
   "source": [
    "\n",
    "In this lecture, you learned to:\n",
    "- Implement custom Keras layers by subclassing tf.keras.layers.Layer with build and call methods. \n",
    "- Define custom loss and metric functions or classes compatible with Keras training workflows. \n",
    "- Integrate custom components into models and verify they save and load correctly. \n",
    "\n",
    "<font color='yellow'>Congratulations on completing this course!</font>"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
