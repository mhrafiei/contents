{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c24aee2",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Performance and Debug**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cee2575",
   "metadata": {},
   "source": [
    ">Last update: 20260128.\n",
    "    \n",
    "By the end of this Lecture, you will be able to:\n",
    "- Enable and configure mixed precision training in TensorFlow 2.20.0 to leverage modern GPUs and TPUs. \n",
    "- Use TensorFlow profiling tools to identify performance bottlenecks in models and input pipelines. \n",
    "- Diagnose and mitigate common numerical and stability issues such as NaNs and exploding gradients. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d1607e",
   "metadata": {},
   "source": [
    "## **1. Mixed Precision Setup**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0ebae3",
   "metadata": {},
   "source": [
    "### **1.1. Global Mixed Precision Policy**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29af320",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_10/Lecture_B/image_01_01.jpg?v=1769608039\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* One central setting controls model numeric precision\n",
    ">* Ensures consistent behavior and easy experimentation changes\n",
    "\n",
    ">* Global policy exploits fast low-precision GPU hardware\n",
    ">* Keeps sensitive values high precision for stability\n",
    "\n",
    ">* Global precision policies improve discipline and reproducibility\n",
    ">* They ease hardware adaptation and performance–quality tradeoffs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8c0cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Global Mixed Precision Policy\n",
    "\n",
    "# This script shows global mixed precision policy.\n",
    "# It uses a tiny model for quick demonstration.\n",
    "# Run in Colab with TensorFlow 2.20.0 installed.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Import TensorFlow and Keras utilities.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Set deterministic random seeds.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version and device info.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"GPU available:\", tf.config.list_physical_devices(\"GPU\"))\n",
    "\n",
    "# Check that mixed precision API exists.\n",
    "from tensorflow.keras import mixed_precision\n",
    "policy_names = [\"float32\", \"mixed_float16\"]\n",
    "print(\"Available policies:\", policy_names)\n",
    "\n",
    "# Create and set a global mixed precision policy.\n",
    "policy = mixed_precision.Policy(\"mixed_float16\")\n",
    "mixed_precision.set_global_policy(policy)\n",
    "print(\"Global policy:\", mixed_precision.global_policy())\n",
    "\n",
    "# Prepare a tiny subset of MNIST data.\n",
    "(x_train, y_train), _ = keras.datasets.mnist.load_data()\n",
    "x_train = x_train[:2048].astype(\"float32\") / 255.0\n",
    "y_train = y_train[:2048].astype(\"int32\")\n",
    "\n",
    "# Add a channel dimension for convolution.\n",
    "x_train = np.expand_dims(x_train, axis=-1)\n",
    "print(\"Train shape:\", x_train.shape)\n",
    "\n",
    "# Validate shapes before building the model.\n",
    "assert x_train.shape[1:] == (28, 28, 1)\n",
    "assert y_train.ndim == 1\n",
    "\n",
    "# Build a simple CNN model under global policy.\n",
    "inputs = keras.Input(shape=(28, 28, 1))\n",
    "x = layers.Conv2D(16, 3, activation=\"relu\")(inputs)\n",
    "x = layers.MaxPooling2D()(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(32, activation=\"relu\")(x)\n",
    "outputs = layers.Dense(10)(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "# Show layer dtypes to see policy effect.\n",
    "for layer in model.layers:\n",
    "    print(\"Layer\", layer.name, \"dtype:\", layer.dtype)\n",
    "\n",
    "# Use a loss scale optimizer for stability.\n",
    "base_opt = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "opt = mixed_precision.LossScaleOptimizer(base_opt)\n",
    "\n",
    "# Compile the model with sparse labels.\n",
    "model.compile(optimizer=opt,\n",
    "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# Train briefly with silent logs for speed.\n",
    "history = model.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs=1,\n",
    "                    batch_size=64,\n",
    "                    verbose=0)\n",
    "\n",
    "# Print final training metrics from history.\n",
    "final_loss = history.history[\"loss\"][-1]\n",
    "final_acc = history.history[\"accuracy\"][-1]\n",
    "print(\"Final loss (mixed):\", float(final_loss))\n",
    "print(\"Final accuracy (mixed):\", float(final_acc))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33c62dd",
   "metadata": {},
   "source": [
    "### **1.2. Loss Scaling Essentials**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e273bdcf",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_10/Lecture_B/image_01_02.jpg?v=1769608085\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Loss scaling prevents tiny gradients from underflowing\n",
    ">* Scale loss up for gradients, then scale back\n",
    "\n",
    ">* Static scaling uses one fixed carefully chosen factor\n",
    ">* Dynamic scaling auto-adjusts factor to avoid overflows\n",
    "\n",
    ">* Use loss scaling signals to debug training\n",
    ">* Tune scaling to match gradient ranges and hardware\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99d4247",
   "metadata": {},
   "source": [
    "### **1.3. Future Hardware Needs**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61201849",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_10/Lecture_B/image_01_03.jpg?v=1769608230\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Modern accelerators are built for mixed precision\n",
    ">* Choose hardware that fully supports mixed precision workflows\n",
    "\n",
    ">* Mixed precision changes memory needs and throughput tradeoffs\n",
    ">* Match GPU or TPU choices to workload characteristics\n",
    "\n",
    ">* Plan fast interconnects to avoid communication bottlenecks\n",
    ">* Use mixed precision to cut energy and scale\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6061cb4",
   "metadata": {},
   "source": [
    "## **2. TensorFlow Profiling Essentials**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e31b07",
   "metadata": {},
   "source": [
    "### **2.1. TensorBoard Profiler Setup**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9196db09",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_10/Lecture_B/image_02_01.jpg?v=1769608252\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* TensorBoard Profiler collects and visualizes training performance\n",
    ">* Use it to inspect detailed operation and resource usage\n",
    "\n",
    ">* Choose log directory and start TensorBoard access\n",
    ">* Profile a short, steady-state window of training\n",
    "\n",
    ">* Complex setups need coordinated logs and permissions\n",
    ">* Ensure clear path from training job to TensorBoard\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37409ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - TensorBoard Profiler Setup\n",
    "\n",
    "# This script shows TensorBoard profiler setup.\n",
    "# It uses a tiny Keras model example.\n",
    "# It is designed for Google Colab use.\n",
    "\n",
    "# Install TensorFlow only if needed here.\n",
    "# !pip install -q tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries safely.\n",
    "import os\n",
    "import pathlib\n",
    "import random\n",
    "\n",
    "# Import TensorFlow and TensorBoard utilities.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "\n",
    "# Set TensorFlow random seed for determinism.\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Detect available device type for information.\n",
    "physical_gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "\n",
    "# Choose device description string for printing.\n",
    "device_desc = \"GPU\" if physical_gpus else \"CPU\"\n",
    "\n",
    "# Print which device type will likely run.\n",
    "print(\"Running on device type:\", device_desc)\n",
    "\n",
    "# Define a small log directory for profiling.\n",
    "base_log_dir = pathlib.Path(\"logs_profiler_demo\")\n",
    "\n",
    "# Ensure the log directory exists on disk.\n",
    "base_log_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Create a unique run directory for this script.\n",
    "run_log_dir = base_log_dir / \"run_01\"\n",
    "\n",
    "# Ensure the run directory exists for summaries.\n",
    "run_log_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Print the log directory path for TensorBoard.\n",
    "print(\"Log directory:\", str(run_log_dir))\n",
    "\n",
    "# Load MNIST dataset with small subset only.\n",
    "(mnist_x_train, mnist_y_train), _ = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize images to float32 in range zero one.\n",
    "mnist_x_train = mnist_x_train.astype(\"float32\") / 255.0\n",
    "\n",
    "# Add channel dimension to images for Conv2D.\n",
    "mnist_x_train = mnist_x_train[..., tf.newaxis]\n",
    "\n",
    "# Select a small subset to keep runtime short.\n",
    "subset_size = 2000\n",
    "\n",
    "# Validate subset size does not exceed dataset.\n",
    "subset_size = min(subset_size, mnist_x_train.shape[0])\n",
    "\n",
    "# Slice the subset for features and labels.\n",
    "mnist_x_train = mnist_x_train[:subset_size]\n",
    "mnist_y_train = mnist_y_train[:subset_size]\n",
    "\n",
    "# Confirm shapes are as expected for training.\n",
    "print(\"Train subset shape:\", mnist_x_train.shape)\n",
    "\n",
    "# Build a simple sequential convolutional model.\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Input(shape=(28, 28, 1)),\n",
    "    keras.layers.Conv2D(16, (3, 3), activation=\"relu\"),\n",
    "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(32, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "# Compile the model with simple optimizer.\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Create a TensorBoard callback for scalar summaries.\n",
    "tb_callback = keras.callbacks.TensorBoard(\n",
    "    log_dir=str(run_log_dir),\n",
    "    histogram_freq=0,\n",
    "    write_graph=True,\n",
    "    write_images=False,\n",
    ")\n",
    "\n",
    "# Define a function to run a short training.\n",
    "def run_training_with_profiler():\n",
    "    # Create a profiler callback for specific steps.\n",
    "    profiler_callback = keras.callbacks.TensorBoard(\n",
    "        log_dir=str(run_log_dir),\n",
    "        histogram_freq=0,\n",
    "        profile_batch=(20, 30),\n",
    "    )\n",
    "\n",
    "    # Train the model briefly with callbacks.\n",
    "    history = model.fit(\n",
    "        mnist_x_train,\n",
    "        mnist_y_train,\n",
    "        epochs=2,\n",
    "        batch_size=64,\n",
    "        verbose=0,\n",
    "        callbacks=[tb_callback, profiler_callback],\n",
    "    )\n",
    "\n",
    "    # Return final training accuracy for confirmation.\n",
    "    return history.history[\"accuracy\"][-1]\n",
    "\n",
    "# Run the training function to generate traces.\n",
    "final_accuracy = run_training_with_profiler()\n",
    "\n",
    "# Print short instructions for launching TensorBoard.\n",
    "print(\"To view profiler, run in Colab cell:\")\n",
    "\n",
    "# Provide the exact TensorBoard magic command string.\n",
    "print(\"%load_ext tensorboard\")\n",
    "\n",
    "# Provide the TensorBoard start command with logdir.\n",
    "print(\"%tensorboard --logdir\", str(base_log_dir))\n",
    "\n",
    "# Print final accuracy to show training completed.\n",
    "print(\"Final training accuracy:\", round(float(final_accuracy), 4))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1adc45",
   "metadata": {},
   "source": [
    "### **2.2. Reading Trace Timelines**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37f38ce",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_10/Lecture_B/image_02_02.jpg?v=1769608298\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Trace timeline shows time-aligned model and hardware activity\n",
    ">* Tracks and colored blocks reveal operation order and efficiency\n",
    "\n",
    ">* Inspect one typical step’s CPU–GPU workflow\n",
    ">* Compare idle CPU or GPU to locate bottlenecks\n",
    "\n",
    ">* Spot gaps, tiny kernels, and poor overlap\n",
    ">* Link visual timeline patterns to concrete bottlenecks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8331b628",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Reading Trace Timelines\n",
    "\n",
    "# This script shows TensorFlow profiling basics.\n",
    "# You will capture and view a trace.\n",
    "# Focus on reading simple timeline patterns.\n",
    "\n",
    "# Install TensorFlow only if missing.\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Import TensorFlow and Keras utilities.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Select device string based on availability.\n",
    "if tf.config.list_physical_devices(\"GPU\"):\n",
    "    device_name = \"/GPU:0\"\n",
    "else:\n",
    "    device_name = \"/CPU:0\"\n",
    "\n",
    "# Print which device will run the model.\n",
    "print(\"Using device for demo:\", device_name)\n",
    "\n",
    "# Load MNIST dataset from Keras datasets.\n",
    "(x_train, y_train), _ = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Reduce dataset size for quick profiling.\n",
    "x_train = x_train[:2000]\n",
    "y_train = y_train[:2000]\n",
    "\n",
    "# Normalize images to float32 in range zero one.\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "\n",
    "# Add channel dimension for convolutional layers.\n",
    "x_train = np.expand_dims(x_train, axis=-1)\n",
    "\n",
    "# Validate shapes before building dataset.\n",
    "print(\"Train data shape:\", x_train.shape)\n",
    "\n",
    "# Build a simple tf.data pipeline.\n",
    "ds_train = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "\n",
    "# Shuffle and batch with small batch size.\n",
    "ds_train = ds_train.shuffle(2000, seed=seed_value).batch(64)\n",
    "\n",
    "# Prefetch to overlap input and compute.\n",
    "ds_train = ds_train.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Define a small CNN model for the demo.\n",
    "def create_model():\n",
    "    model = keras.Sequential([\n",
    "        layers.Conv2D(16, (3, 3), activation=\"relu\", input_shape=(28, 28, 1)),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(32, activation=\"relu\"),\n",
    "        layers.Dense(10, activation=\"softmax\"),\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "# Create log directory for TensorBoard traces.\n",
    "log_dir = \"logs_profile_demo\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Explain where to open TensorBoard later.\n",
    "print(\"Trace logs will be in:\", log_dir)\n",
    "\n",
    "# Create a TensorBoard profiler callback.\n",
    "profiler_callback = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=log_dir,\n",
    "    histogram_freq=0,\n",
    "    write_graph=False,\n",
    "    write_images=False,\n",
    "    profile_batch=\"2,4\",\n",
    ")\n",
    "\n",
    "# Train inside selected device context.\n",
    "with tf.device(device_name):\n",
    "    model = create_model()\n",
    "    model.fit(\n",
    "        ds_train,\n",
    "        epochs=1,\n",
    "        steps_per_epoch=10,\n",
    "        verbose=0,\n",
    "        callbacks=[profiler_callback],\n",
    "    )\n",
    "\n",
    "# Print short instructions for reading timelines.\n",
    "print(\"Open TensorBoard with: tensorboard --logdir=logs_profile_demo\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d3b926",
   "metadata": {},
   "source": [
    "### **2.3. Finding Performance Bottlenecks**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac59cf6",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_10/Lecture_B/image_02_03.jpg?v=1769608373\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Compare step time across compute and input stages\n",
    ">* Use summaries to locate input or model bottlenecks\n",
    "\n",
    ">* Use trace timelines to locate slow operations\n",
    ">* Spot idle gaps or many tiny kernels\n",
    "\n",
    ">* Use profiler patterns to spot memory and hardware issues\n",
    ">* Iteratively tweak data, model, and settings to remove bottlenecks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4de334",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Finding Performance Bottlenecks\n",
    "\n",
    "# This script shows TensorFlow profiling basics.\n",
    "# It compares slow and fast input pipelines visually.\n",
    "# Use it to spot simple performance bottlenecks.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Import TensorFlow and check version.\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "\n",
    "# Set TensorFlow random seed deterministically.\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Detect available device type for information.\n",
    "physical_gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "\n",
    "# Choose device string based on availability.\n",
    "device_type = \"GPU\" if physical_gpus else \"CPU\"\n",
    "\n",
    "# Print which device type will likely be used.\n",
    "print(\"Running primarily on:\", device_type)\n",
    "\n",
    "# Create a small synthetic dataset for profiling.\n",
    "num_samples = 2048\n",
    "feature_dim = 32\n",
    "\n",
    "# Build random features and labels tensors.\n",
    "features = tf.random.normal((num_samples, feature_dim))\n",
    "\n",
    "# Use a simple binary label pattern.\n",
    "labels = tf.cast(tf.reduce_sum(features, axis=1) > 0, tf.int32)\n",
    "\n",
    "# Validate shapes before building datasets.\n",
    "assert features.shape[0] == labels.shape[0]\n",
    "\n",
    "# Define a simple Keras model for demonstration.\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(feature_dim,)),\n",
    "    tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "])\n",
    "\n",
    "# Compile model with basic optimizer and loss.\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Create a deliberately slow input pipeline.\n",
    "slow_ds = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "\n",
    "# Add an expensive Python sleep to each element.\n",
    "slow_ds = slow_ds.map(\n",
    "    lambda x, y: (tf.py_function(\n",
    "        func=lambda z: (time.sleep(0.001), z)[1],\n",
    "        inp=[x], Tout=tf.float32\n",
    "    ), y)\n",
    ")\n",
    "\n",
    "# Ensure the shape information is preserved for the slow pipeline.\n",
    "def _set_shapes(x, y):\n",
    "    x.set_shape((feature_dim,))\n",
    "    return x, y\n",
    "\n",
    "slow_ds = slow_ds.map(_set_shapes)\n",
    "\n",
    "# Batch the slow dataset with a small batch size.\n",
    "slow_ds = slow_ds.batch(32).prefetch(1)\n",
    "\n",
    "# Create a fast, well optimized input pipeline.\n",
    "fast_ds = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "\n",
    "# Shuffle, batch, cache, and prefetch efficiently.\n",
    "fast_ds = fast_ds.shuffle(512).batch(32).cache().prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Define a helper function to time one training epoch.\n",
    "def time_one_epoch(dataset, description):\n",
    "    # Warm up the model with one silent step.\n",
    "    for batch_x, batch_y in dataset.take(1):\n",
    "        _ = model.train_on_batch(batch_x, batch_y)\n",
    "\n",
    "    # Start wall clock timer for the epoch.\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Run exactly one epoch with verbose disabled.\n",
    "    history = model.fit(\n",
    "        dataset,\n",
    "        epochs=1,\n",
    "        verbose=0,\n",
    "        steps_per_epoch=num_samples // 32,\n",
    "    )\n",
    "\n",
    "    # Compute elapsed time in seconds.\n",
    "    elapsed = time.time() - start_time\n",
    "\n",
    "    # Extract final loss and accuracy safely.\n",
    "    final_loss = float(history.history[\"loss\"][ -1])\n",
    "\n",
    "    final_acc = float(history.history[\"accuracy\"][ -1])\n",
    "\n",
    "    # Print a concise summary line.\n",
    "    print(\n",
    "        f\"{description}: time={elapsed:.3f}s, \"\n",
    "        f\"loss={final_loss:.3f}, acc={final_acc:.3f}\"\n",
    "    )\n",
    "\n",
    "# Run timing with the slow input pipeline.\n",
    "time_one_epoch(slow_ds, \"Slow pipeline epoch\")\n",
    "\n",
    "# Run timing with the fast input pipeline.\n",
    "time_one_epoch(fast_ds, \"Fast pipeline epoch\")\n",
    "\n",
    "# Explain how this relates to profiler bottlenecks.\n",
    "print(\"Notice how input design changes total step time.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898a8e30",
   "metadata": {},
   "source": [
    "## **3. Numerical Stability Essentials**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420af7d9",
   "metadata": {},
   "source": [
    "### **3.1. NaN and Inf Detection**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cc6843",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_10/Lecture_B/image_03_01.jpg?v=1769608465\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* NaNs and Infs arise from invalid computations\n",
    ">* They spread, break training, and signal instability\n",
    "\n",
    ">* Monitor loss and metrics for sudden anomalies\n",
    ">* Use tools and checks to catch nonfinite values\n",
    "\n",
    ">* Trace where NaNs first appear during training\n",
    ">* Link causes to fixes like normalization, clipping\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a504a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - NaN and Inf Detection\n",
    "\n",
    "# This script shows NaN and Inf detection basics.\n",
    "# It uses TensorFlow tensors and a tiny model.\n",
    "# Focus is on safe checks during training.\n",
    "\n",
    "# Optional TensorFlow install for some environments.\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required modules from TensorFlow.\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "tf.random.set_seed(7)\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Create a small tensor with safe finite values.\n",
    "safe_tensor = tf.constant([1.0, 2.0, 3.0], dtype=tf.float32)\n",
    "\n",
    "# Create tensors that will contain Inf and NaN values.\n",
    "inf_tensor = tf.constant([1.0, tf.float32.max], dtype=tf.float32)\n",
    "\n",
    "# Use a division that produces infinity safely.\n",
    "inf_result = inf_tensor * tf.constant(2.0, dtype=tf.float32)\n",
    "\n",
    "# Create a tensor that will produce NaN values.\n",
    "zero_tensor = tf.constant([0.0, 0.0], dtype=tf.float32)\n",
    "\n",
    "# Use invalid division to intentionally create NaNs.\n",
    "nan_result = zero_tensor / zero_tensor\n",
    "\n",
    "# Define a helper function to summarize bad values.\n",
    "def summarize_bad_values(x, name):\n",
    "    # Ensure tensor is float type before checks.\n",
    "    x = tf.cast(x, tf.float32)\n",
    "\n",
    "    # Build boolean masks for NaN and Inf values.\n",
    "    nan_mask = tf.math.is_nan(x)\n",
    "\n",
    "    # Detect both positive and negative infinities.\n",
    "    inf_mask = tf.math.is_inf(x)\n",
    "\n",
    "    # Count how many NaN and Inf values appear.\n",
    "    nan_count = tf.reduce_sum(tf.cast(nan_mask, tf.int32))\n",
    "\n",
    "    # Count infinite values using integer casting.\n",
    "    inf_count = tf.reduce_sum(tf.cast(inf_mask, tf.int32))\n",
    "\n",
    "    # Print a short summary line for this tensor.\n",
    "    print(name, \"NaNs:\", int(nan_count), \"Infs:\", int(inf_count))\n",
    "\n",
    "# Show that the safe tensor has no bad values.\n",
    "summarize_bad_values(safe_tensor, \"safe_tensor\")\n",
    "\n",
    "# Show that the Inf tensor now contains infinities.\n",
    "summarize_bad_values(inf_result, \"inf_result\")\n",
    "\n",
    "# Show that the NaN tensor now contains NaN values.\n",
    "summarize_bad_values(nan_result, \"nan_result\")\n",
    "\n",
    "# Build a tiny model that can easily overflow.\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(1,)),\n",
    "    tf.keras.layers.Dense(1, use_bias=False)\n",
    "])\n",
    "\n",
    "# Manually set a very large weight to cause overflow.\n",
    "model.layers[0].set_weights([\n",
    "    tf.constant([[1e20]], dtype=tf.float32)\n",
    "])\n",
    "\n",
    "# Create a small batch with moderate input values.\n",
    "inputs = tf.constant([[2.0], [3.0]], dtype=tf.float32)\n",
    "\n",
    "# Define a simple mean squared error loss function.\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "# Use a basic optimizer with a small learning rate.\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "# Perform one training step inside GradientTape.\n",
    "with tf.GradientTape() as tape:\n",
    "    # Forward pass through the unstable model.\n",
    "    preds = model(inputs, training=True)\n",
    "\n",
    "    # Target values are small and finite.\n",
    "    targets = tf.zeros_like(preds)\n",
    "\n",
    "    # Compute the loss between predictions and targets.\n",
    "    loss = loss_fn(targets, preds)\n",
    "\n",
    "# Compute gradients of loss with respect to weights.\n",
    "grads = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "# Check predictions for NaN and Inf values.\n",
    "summarize_bad_values(preds, \"predictions\")\n",
    "\n",
    "# Check gradients for NaN and Inf values.\n",
    "summarize_bad_values(grads[0], \"gradients\")\n",
    "\n",
    "# Only apply gradients if they are all finite.\n",
    "if tf.reduce_all(tf.math.is_finite(grads[0])):\n",
    "    # Apply gradients safely when values are finite.\n",
    "    optimizer.apply_gradients([(grads[0], model.trainable_variables[0])])\n",
    "else:\n",
    "    # Warn that update is skipped due to bad values.\n",
    "    print(\"Skipped update due to NaN or Inf gradients.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda2766f",
   "metadata": {},
   "source": [
    "### **3.2. Gradient Clipping Basics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fe90c7",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_10/Lecture_B/image_03_02.jpg?v=1769608506\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Exploding gradients destabilize training and cause overflows\n",
    ">* Gradient clipping limits update size, preventing unstable jumps\n",
    "\n",
    ">* Clip by value limits each gradient component\n",
    ">* Clip by norm rescales overall gradient magnitude\n",
    "\n",
    ">* Tune clipping thresholds carefully to avoid slowdown\n",
    ">* Start moderate, monitor metrics, combine with other stabilizers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a32b962",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Gradient Clipping Basics\n",
    "\n",
    "# This script shows basic gradient clipping usage.\n",
    "# It compares training with and without gradient clipping.\n",
    "# It uses a tiny synthetic regression dataset.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required libraries safely.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Select device preference based on availability.\n",
    "physical_gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "if physical_gpus:\n",
    "    device_type = \"GPU\"\n",
    "else:\n",
    "    device_type = \"CPU\"\n",
    "\n",
    "# Print which device type will likely be used.\n",
    "print(\"Running on device type:\", device_type)\n",
    "\n",
    "# Create a tiny synthetic regression dataset.\n",
    "num_samples = 256\n",
    "x_data = np.random.uniform(-2.0, 2.0, size=(num_samples, 1))\n",
    "noise = np.random.normal(0.0, 0.5, size=(num_samples, 1))\n",
    "y_data = 3.0 * x_data + 2.0 + noise\n",
    "\n",
    "# Convert numpy arrays to TensorFlow datasets.\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x_data, y_data))\n",
    "dataset = dataset.shuffle(buffer_size=num_samples, seed=seed_value)\n",
    "dataset = dataset.batch(32)\n",
    "\n",
    "# Define a helper function to build a simple model.\n",
    "def build_model(use_clipping):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(1,)),\n",
    "        tf.keras.layers.Dense(32, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(32, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(1),\n",
    "    ])\n",
    "    if use_clipping:\n",
    "        optimizer = tf.keras.optimizers.SGD(\n",
    "            learning_rate=1.0,\n",
    "            clipnorm=1.0,\n",
    "        )\n",
    "    else:\n",
    "        optimizer = tf.keras.optimizers.SGD(\n",
    "            learning_rate=1.0,\n",
    "        )\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=\"mse\",\n",
    "        metrics=[\"mae\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Build models with and without gradient clipping.\n",
    "model_no_clip = build_model(use_clipping=False)\n",
    "model_clip = build_model(use_clipping=True)\n",
    "\n",
    "# Train both models briefly with silent training logs.\n",
    "history_no_clip = model_no_clip.fit(\n",
    "    dataset,\n",
    "    epochs=5,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "history_clip = model_clip.fit(\n",
    "    dataset,\n",
    "    epochs=5,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Extract final losses for comparison.\n",
    "final_loss_no_clip = history_no_clip.history[\"loss\"][-1]\n",
    "final_loss_clip = history_clip.history[\"loss\"][-1]\n",
    "\n",
    "# Print a short summary comparing both training runs.\n",
    "print(\"Final MSE without clipping:\", round(final_loss_no_clip, 4))\n",
    "print(\"Final MSE with clipping:\", round(final_loss_clip, 4))\n",
    "print(\"Note how clipping stabilizes updates with large learning rate.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ca2aeb",
   "metadata": {},
   "source": [
    "### **3.3. Stable Learning Rate Choices**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e20a38",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_10/Lecture_B/image_03_03.jpg?v=1769608602\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Learning rate controls update speed and stability\n",
    ">* Too high or low harms smooth loss descent\n",
    "\n",
    ">* Use adaptive or scheduled learning rates for stability\n",
    ">* Monitor loss curves and adjust rate to prevent divergence\n",
    "\n",
    ">* Learning rate must match batch size, optimizer\n",
    ">* Retune learning rate carefully, monitor for instability\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02595a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Stable Learning Rate Choices\n",
    "\n",
    "# This script shows stable learning rates.\n",
    "# It compares safe and unsafe learning rates.\n",
    "# Use it to observe loss behavior.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required libraries safely.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set deterministic random seeds.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version briefly.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Select device preference if available.\n",
    "physical_gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "if physical_gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(physical_gpus[0], True)\n",
    "    except Exception as e:\n",
    "        print(\"GPU config warning, using default.\")\n",
    "\n",
    "# Create a tiny synthetic regression dataset.\n",
    "num_samples = 256\n",
    "x_data = np.linspace(-1.0, 1.0, num_samples).astype(\"float32\")\n",
    "noise = 0.1 * np.random.randn(num_samples).astype(\"float32\")\n",
    "\n",
    "# Generate targets with simple linear relation.\n",
    "y_data = 3.0 * x_data + 0.5 + noise\n",
    "x_data = x_data.reshape(-1, 1)\n",
    "y_data = y_data.reshape(-1, 1)\n",
    "\n",
    "# Validate shapes before training.\n",
    "assert x_data.shape == (num_samples, 1)\n",
    "assert y_data.shape == (num_samples, 1)\n",
    "\n",
    "# Build a simple dense regression model.\n",
    "def build_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(1,)),\n",
    "        tf.keras.layers.Dense(8, activation=\"tanh\"),\n",
    "        tf.keras.layers.Dense(1),\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Prepare dataset object for efficient training.\n",
    "batch_size = 32\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x_data, y_data))\n",
    "dataset = dataset.shuffle(num_samples, seed=seed_value)\n",
    "dataset = dataset.batch(batch_size)\n",
    "\n",
    "# Define two learning rates to compare.\n",
    "stable_lr = 0.01\n",
    "unstable_lr = 1.0\n",
    "\n",
    "# Create two models with identical initialization.\n",
    "model_stable = build_model()\n",
    "model_unstable = build_model()\n",
    "model_unstable.set_weights(model_stable.get_weights())\n",
    "\n",
    "# Compile models with mean squared error loss.\n",
    "model_stable.compile(\n",
    "    optimizer=tf.keras.optimizers.SGD(learning_rate=stable_lr),\n",
    "    loss=\"mse\",\n",
    ")\n",
    "model_unstable.compile(\n",
    "    optimizer=tf.keras.optimizers.SGD(learning_rate=unstable_lr),\n",
    "    loss=\"mse\",\n",
    ")\n",
    "\n",
    "# Train both models briefly with silent logs.\n",
    "epochs = 15\n",
    "history_stable = model_stable.fit(\n",
    "    dataset,\n",
    "    epochs=epochs,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "history_unstable = model_unstable.fit(\n",
    "    dataset,\n",
    "    epochs=epochs,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Extract loss histories for inspection.\n",
    "loss_stable = history_stable.history[\"loss\"]\n",
    "loss_unstable = history_unstable.history[\"loss\"]\n",
    "\n",
    "# Print a compact comparison table.\n",
    "print(\"\\nEpoch  Stable_LR_loss  Unstable_LR_loss\")\n",
    "for i in range(epochs):\n",
    "    ls = float(loss_stable[i])\n",
    "    lu = float(loss_unstable[i])\n",
    "    print(f\"{i+1:5d}  {ls:14.6f}  {lu:16.6f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24efebf6",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Performance and Debug**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7594f0",
   "metadata": {},
   "source": [
    "\n",
    "In this lecture, you learned to:\n",
    "- Enable and configure mixed precision training in TensorFlow 2.20.0 to leverage modern GPUs and TPUs. \n",
    "- Use TensorFlow profiling tools to identify performance bottlenecks in models and input pipelines. \n",
    "- Diagnose and mitigate common numerical and stability issues such as NaNs and exploding gradients. \n",
    "\n",
    "<font color='yellow'>Congratulations on completing this course!</font>"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
