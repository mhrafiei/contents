{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f51a6c5",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Performance and Debug**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbf0add",
   "metadata": {},
   "source": [
    ">Last update: 20260127.\n",
    "    \n",
    "By the end of this Lecture, you will be able to:\n",
    "- Enable and configure mixed precision training in TensorFlow 2.20.0 to leverage modern GPUs and TPUs. \n",
    "- Use TensorFlow profiling tools to identify performance bottlenecks in models and input pipelines. \n",
    "- Diagnose and mitigate common numerical and stability issues such as NaNs and exploding gradients. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8c279c",
   "metadata": {},
   "source": [
    "## **1. Mixed Precision Setup**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c411949",
   "metadata": {},
   "source": [
    "### **1.1. Mixed Precision Policy**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9166fed",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_10/Lecture_B/image_01_01.jpg?v=1769567582\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Policy chooses dtypes for different tensor operations\n",
    ">* Balances speed and stability using low and full precision\n",
    "\n",
    ">* Policy assigns tensors to suitable precisions\n",
    ">* Keeps weights high precision to ensure stable convergence\n",
    "\n",
    ">* Mixed precision cuts training time and memory\n",
    ">* Makes large, complex experiments feasible and stable\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e706c7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Mixed Precision Policy\n",
    "\n",
    "# This script demonstrates TensorFlow mixed precision policy.\n",
    "# It focuses on simple clear beginner friendly concepts.\n",
    "# Run cells in order inside Google Colab environment.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries safely.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "random.seed(7)\n",
    "np.random.seed(7)\n",
    "\n",
    "# Import TensorFlow and mixed precision utilities.\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "# Print TensorFlow version and device information.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"GPU available:\", bool(tf.config.list_physical_devices(\"GPU\")))\n",
    "\n",
    "# Define a helper function to describe tensor dtypes.\n",
    "def describe_tensor(name, tensor):\n",
    "    print(f\"{name} shape={tensor.shape} dtype={tensor.dtype}\")\n",
    "\n",
    "# Show default global mixed precision policy.\n",
    "current_policy = mixed_precision.global_policy()\n",
    "print(\"Default policy:\", current_policy)\n",
    "\n",
    "# Enable mixed float16 policy for performance.\n",
    "policy = mixed_precision.Policy(\"mixed_float16\")\n",
    "mixed_precision.set_global_policy(policy)\n",
    "\n",
    "# Confirm that the global policy has been updated.\n",
    "print(\"New global policy:\", mixed_precision.global_policy())\n",
    "\n",
    "# Create a tiny dense model using Keras Sequential.\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=(16,)),\n",
    "    layers.Dense(32, activation=\"relu\"),\n",
    "    layers.Dense(16, activation=\"relu\"),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "# Show layer compute and variable dtypes under policy.\n",
    "for layer in model.layers:\n",
    "    print(\"Layer:\", layer.name,\n",
    "          \"compute:\", layer.compute_dtype,\n",
    "          \"variable:\", layer.dtype)\n",
    "\n",
    "# Build the model by calling it on dummy data.\n",
    "dummy_input = tf.ones((4, 16), dtype=tf.float32)\n",
    "output = model(dummy_input)\n",
    "\n",
    "# Describe input and output tensor dtypes.\n",
    "describe_tensor(\"Dummy input\", dummy_input)\n",
    "describe_tensor(\"Model output\", output)\n",
    "\n",
    "# Compile model with an optimizer that supports loss scaling.\n",
    "optimizer = mixed_precision.LossScaleOptimizer(\n",
    "    tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    ")\n",
    "model.compile(optimizer=optimizer, loss=\"mse\")\n",
    "\n",
    "# Create a tiny synthetic regression dataset.\n",
    "x_train = np.random.randn(32, 16).astype(\"float32\")\n",
    "y_train = np.random.randn(32, 1).astype(\"float32\")\n",
    "\n",
    "# Validate dataset shapes before training.\n",
    "assert x_train.shape[0] == y_train.shape[0]\n",
    "assert x_train.shape[1] == 16\n",
    "\n",
    "# Train briefly with silent verbose setting.\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=2,\n",
    "                    batch_size=8,\n",
    "                    verbose=0)\n",
    "\n",
    "# Print final loss to confirm successful training.\n",
    "print(\"Final training loss:\", float(history.history[\"loss\"][-1]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa77dab7",
   "metadata": {},
   "source": [
    "### **1.2. Loss Scaling Essentials**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5ea26a",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_10/Lecture_B/image_01_02.jpg?v=1769567623\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Loss scaling prevents tiny gradients from underflowing in half-precision\n",
    ">* It multiplies loss, then rescales gradients after backprop\n",
    "\n",
    ">* Fixed scaling uses one constant factor; risks overflow\n",
    ">* Dynamic scaling auto-adjusts factor, improving stability\n",
    "\n",
    ">* Recognize training symptoms of bad loss scaling\n",
    ">* Adjust scaling to avoid underflow, overflow, wasted compute\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6556b07f",
   "metadata": {},
   "source": [
    "### **1.3. Future Ready Hardware**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431ec180",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_10/Lecture_B/image_01_03.jpg?v=1769567839\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Mixed precision matches software to accelerator hardware features\n",
    ">* Unlocks tensor core speedups for diverse deep models\n",
    "\n",
    ">* Hardware is shifting toward many precision formats\n",
    ">* Flexible mixed precision lets models exploit future accelerators\n",
    "\n",
    ">* Mixed precision unifies training across diverse hardware\n",
    ">* Consistent numerics ease validation and future upgrades\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d090efbd",
   "metadata": {},
   "source": [
    "## **2. TensorFlow Profiling Essentials**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d458b98",
   "metadata": {},
   "source": [
    "### **2.1. TensorBoard Profiler Setup**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327ea5d9",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_10/Lecture_B/image_02_01.jpg?v=1769567859\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Profiler links training runs to visual dashboards\n",
    ">* Shows detailed timing to guide performance decisions\n",
    "\n",
    ">* Configure training, logs, and TensorBoard for profiling\n",
    ">* Use structured logs to compare runs and share\n",
    "\n",
    ">* Configure TensorBoard securely for remote training environments\n",
    ">* Standardize versions and scripts for reliable profiling\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5be69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - TensorBoard Profiler Setup\n",
    "\n",
    "# This script shows TensorBoard Profiler setup.\n",
    "# It runs a tiny model with profiling enabled.\n",
    "# Use it in Colab to explore performance traces.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "# Import TensorFlow and TensorBoard utilities.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "SEED_VALUE = 42\n",
    "random.seed(SEED_VALUE)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED_VALUE)\n",
    "\n",
    "# Set TensorFlow random seed deterministically.\n",
    "tf.random.set_seed(SEED_VALUE)\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Detect available devices and choose strategy.\n",
    "physical_gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "if physical_gpus:\n",
    "    strategy = tf.distribute.OneDeviceStrategy(\"/GPU:0\")\n",
    "else:\n",
    "    strategy = tf.distribute.OneDeviceStrategy(\"/CPU:0\")\n",
    "\n",
    "# Prepare a small synthetic dataset for profiling.\n",
    "num_samples = 512\n",
    "input_dim = 32\n",
    "num_classes = 10\n",
    "\n",
    "# Create random features and labels tensors.\n",
    "features = tf.random.normal((num_samples, input_dim))\n",
    "labels = tf.random.uniform((num_samples,), 0, num_classes, dtype=tf.int32)\n",
    "\n",
    "# Validate shapes before building dataset.\n",
    "assert features.shape[0] == labels.shape[0]\n",
    "\n",
    "# Build a tf.data pipeline with batching.\n",
    "batch_size = 32\n",
    "dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "\n",
    "# Shuffle and batch the dataset deterministically.\n",
    "dataset = dataset.shuffle(num_samples, seed=SEED_VALUE)\n",
    "\n",
    "dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Create a log directory for TensorBoard Profiler.\n",
    "base_log_dir = \"logs_profiler_demo\"\n",
    "run_id = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# Combine base directory and run identifier.\n",
    "log_dir = os.path.join(base_log_dir, run_id)\n",
    "\n",
    "# Explain where logs will be written.\n",
    "print(\"Profiler logs directory:\", log_dir)\n",
    "\n",
    "# Build a simple model inside distribution strategy.\n",
    "with strategy.scope():\n",
    "    model = keras.Sequential([\n",
    "        layers.Input(shape=(input_dim,)),\n",
    "        layers.Dense(64, activation=\"relu\"),\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ])\n",
    "\n",
    "    # Compile the model with a basic optimizer.\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "# Create a TensorBoard callback with profiling enabled.\n",
    "tb_callback = keras.callbacks.TensorBoard(\n",
    "    log_dir=log_dir,\n",
    "    histogram_freq=0,\n",
    "    write_graph=False,\n",
    "    write_images=False,\n",
    "    profile_batch=\"2,4\",\n",
    ")\n",
    "\n",
    "# Train briefly to generate profiling traces.\n",
    "with strategy.scope():\n",
    "    history = model.fit(\n",
    "        dataset,\n",
    "        epochs=2,\n",
    "        steps_per_epoch=8,\n",
    "        callbacks=[tb_callback],\n",
    "        verbose=0,\n",
    "    )\n",
    "\n",
    "# Print a short summary of training results.\n",
    "final_loss = history.history[\"loss\"][-1]\n",
    "final_acc = history.history[\"accuracy\"][-1]\n",
    "\n",
    "# Show metrics and instructions to open TensorBoard.\n",
    "print(\"Final loss:\", round(float(final_loss), 4))\n",
    "print(\"Final accuracy:\", round(float(final_acc), 4))\n",
    "print(\"To view profiler, run in Colab:\")\n",
    "print(\"%load_ext tensorboard\")\n",
    "print(\"%tensorboard --logdir\", base_log_dir)\n",
    "\n",
    "# Confirm that profiling files exist in directory.\n",
    "print(\"Log directory exists:\", os.path.isdir(log_dir))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29af29cc",
   "metadata": {},
   "source": [
    "### **2.2. Reading Trace Timelines**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfd265a",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_10/Lecture_B/image_02_02.jpg?v=1769567912\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Timeline shows operations across CPU and accelerators\n",
    ">* Block lengths show duration; gaps reveal idle waiting\n",
    "\n",
    ">* Tell compute, input, and overhead regions apart\n",
    ">* Link patterns to metrics to spot bottlenecks\n",
    "\n",
    ">* Use timelines to spot subtle recurring bottlenecks\n",
    ">* Interpret visual cues to guide targeted optimizations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f7fc64",
   "metadata": {},
   "source": [
    "### **2.3. Finding Performance Bottlenecks**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94730af",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_10/Lecture_B/image_02_03.jpg?v=1769567927\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Use timelines to spot idle accelerators and waste\n",
    ">* Differentiate input bottlenecks from compute-bound models\n",
    "\n",
    ">* Profiler ranks ops and pipeline by runtime\n",
    ">* Helps locate and optimize the true hot spots\n",
    "\n",
    ">* Check overlap between data, CPU, and accelerator\n",
    ">* Use profiler to remove stalls and blocking work\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf35273",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Finding Performance Bottlenecks\n",
    "\n",
    "# This script shows TensorFlow profiling basics.\n",
    "# We create a slow input pipeline on purpose.\n",
    "# Then we compare slow and optimized pipeline traces.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Import TensorFlow and set seeds.\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Print TensorFlow version once.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Set global random seeds for reproducibility.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# Configure TensorFlow random seed deterministically.\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Detect available device type for information.\n",
    "physical_gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "\n",
    "# Print a short device availability message.\n",
    "print(\"GPUs available:\", len(physical_gpus))\n",
    "\n",
    "# Load MNIST dataset from Keras utilities.\n",
    "(x_train, y_train), _ = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Reduce dataset size for quick profiling.\n",
    "x_train = x_train[:2000]\n",
    "y_train = y_train[:2000]\n",
    "\n",
    "# Normalize and add channel dimension.\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_train = np.expand_dims(x_train, axis=-1)\n",
    "\n",
    "# Validate shapes before building datasets.\n",
    "print(\"Train shape:\", x_train.shape, y_train.shape)\n",
    "\n",
    "# Define a simple convolutional model.\n",
    "def create_model():\n",
    "    inputs = tf.keras.Input(shape=(28, 28, 1))\n",
    "    x = tf.keras.layers.Conv2D(16, 3, activation=\"relu\")(inputs)\n",
    "    x = tf.keras.layers.MaxPool2D()(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dense(32, activation=\"relu\")(x)\n",
    "    outputs = tf.keras.layers.Dense(10, activation=\"softmax\")(x)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "# Create a deliberately slow preprocessing function.\n",
    "def slow_preprocess(image, label):\n",
    "    image = tf.image.resize(image, (40, 40))\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_brightness(image, max_delta=0.3)\n",
    "    image = tf.image.central_crop(image, central_fraction=0.7)\n",
    "    image = tf.image.resize(image, (28, 28))\n",
    "    image = tf.image.per_image_standardization(image)\n",
    "    image = tf.py_function(lambda x: x, [image], Tout=tf.float32)\n",
    "    image.set_shape((28, 28, 1))\n",
    "    return image, label\n",
    "\n",
    "\n",
    "# Create a faster preprocessing function.\n",
    "def fast_preprocess(image, label):\n",
    "    image = tf.image.resize(image, (28, 28))\n",
    "    image = tf.clip_by_value(image, 0.0, 1.0)\n",
    "    return image, label\n",
    "\n",
    "\n",
    "# Build a slow input pipeline without optimizations.\n",
    "def make_slow_dataset(batch_size):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    ds = ds.shuffle(512, seed=seed_value)\n",
    "    ds = ds.map(slow_preprocess, num_parallel_calls=None)\n",
    "    ds = ds.batch(batch_size)\n",
    "    ds = ds.prefetch(1)\n",
    "    return ds\n",
    "\n",
    "\n",
    "# Build a faster input pipeline with optimizations.\n",
    "def make_fast_dataset(batch_size):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    ds = ds.shuffle(512, seed=seed_value)\n",
    "    ds = ds.map(\n",
    "        fast_preprocess,\n",
    "        num_parallel_calls=tf.data.AUTOTUNE,\n",
    "    )\n",
    "    ds = ds.batch(batch_size)\n",
    "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "\n",
    "# Create log directory for TensorBoard profiling.\n",
    "log_root = \"./tf_profile_logs\"\n",
    "os.makedirs(log_root, exist_ok=True)\n",
    "\n",
    "\n",
    "# Helper function to train with profiler enabled.\n",
    "def train_with_profiler(dataset, run_name):\n",
    "    model = create_model()\n",
    "    run_logdir = os.path.join(log_root, run_name)\n",
    "    tb_callback = tf.keras.callbacks.TensorBoard(\n",
    "        log_dir=run_logdir,\n",
    "        histogram_freq=0,\n",
    "        write_graph=False,\n",
    "        write_images=False,\n",
    "        profile_batch=\"2,4\",\n",
    "    )\n",
    "    start = time.time()\n",
    "    model.fit(\n",
    "        dataset,\n",
    "        epochs=1,\n",
    "        steps_per_epoch=20,\n",
    "        verbose=0,\n",
    "        callbacks=[tb_callback],\n",
    "    )\n",
    "    end = time.time()\n",
    "    return end - start, run_logdir\n",
    "\n",
    "\n",
    "# Create slow and fast datasets with same batch size.\n",
    "batch_size = 64\n",
    "slow_ds = make_slow_dataset(batch_size)\n",
    "fast_ds = make_fast_dataset(batch_size)\n",
    "\n",
    "# Train with slow pipeline and profile selected batches.\n",
    "slow_time, slow_logdir = train_with_profiler(slow_ds, \"slow_input\")\n",
    "\n",
    "# Train with fast pipeline and profile selected batches.\n",
    "fast_time, fast_logdir = train_with_profiler(fast_ds, \"fast_input\")\n",
    "\n",
    "# Print timing comparison for both runs.\n",
    "print(\"Slow pipeline time (s):\", round(slow_time, 3))\n",
    "print(\"Fast pipeline time (s):\", round(fast_time, 3))\n",
    "\n",
    "# Explain where to open TensorBoard traces.\n",
    "print(\"Slow run logs:\", slow_logdir)\n",
    "print(\"Fast run logs:\", fast_logdir)\n",
    "\n",
    "# Final message summarizing bottleneck investigation.\n",
    "print(\"Use TensorBoard profiler to inspect idle gaps.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef5647b",
   "metadata": {},
   "source": [
    "## **3. Training Stability Essentials**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48ea4e7",
   "metadata": {},
   "source": [
    "### **3.1. NaN and Inf Detection**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05122785",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_10/Lecture_B/image_03_01.jpg?v=1769568017\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* NaN and Inf arise from numerical instability\n",
    ">* They disrupt training and waste significant compute time\n",
    "\n",
    ">* Watch loss and metrics for sudden spikes\n",
    ">* Add checks that stop training when NaNs appear\n",
    "\n",
    ">* Use NaN timing to infer likely causes\n",
    ">* Log data and stats to locate failing operation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6b08ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - NaN and Inf Detection\n",
    "\n",
    "# This script shows NaN and Inf detection basics.\n",
    "# It uses TensorFlow tensors and a tiny model.\n",
    "# Focus on safe checks during and after training.\n",
    "\n",
    "# Install TensorFlow if needed in your environment.\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required modules from TensorFlow.\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Create a small tensor with a zero value.\n",
    "base_tensor = tf.constant([1.0, 0.0, -1.0], dtype=tf.float32)\n",
    "\n",
    "# Intentionally create Inf by dividing by zero.\n",
    "inf_tensor = base_tensor / base_tensor\n",
    "\n",
    "# Intentionally create NaN using invalid logarithm.\n",
    "nan_tensor = tf.math.log(tf.constant([-1.0, 0.0, 1.0]))\n",
    "\n",
    "# Define a helper function to summarize bad values.\n",
    "def summarize_bad_values(tensor, name):\n",
    "    # Ensure tensor is a TensorFlow tensor.\n",
    "    tensor = tf.convert_to_tensor(tensor)\n",
    "\n",
    "    # Count NaN values inside the tensor.\n",
    "    nan_mask = tf.math.is_nan(tensor)\n",
    "    nan_count = tf.reduce_sum(tf.cast(nan_mask, tf.int32))\n",
    "\n",
    "    # Count Inf values inside the tensor.\n",
    "    inf_mask = tf.math.is_inf(tensor)\n",
    "    inf_count = tf.reduce_sum(tf.cast(inf_mask, tf.int32))\n",
    "\n",
    "    # Print a short summary line for this tensor.\n",
    "    print(f\"{name}: NaNs={int(nan_count)}, Infs={int(inf_count)}\")\n",
    "\n",
    "# Show NaN and Inf counts for our tensors.\n",
    "summarize_bad_values(inf_tensor, \"inf_tensor\")\n",
    "\n",
    "# Show NaN and Inf counts for the log tensor.\n",
    "summarize_bad_values(nan_tensor, \"nan_tensor\")\n",
    "\n",
    "# Build a tiny model that can become unstable.\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(1,)),\n",
    "    tf.keras.layers.Dense(8, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# Compile with a deliberately high learning rate.\n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=5.0),\n",
    "              loss=\"mse\")\n",
    "\n",
    "# Create a tiny synthetic regression dataset.\n",
    "x = tf.linspace(-1.0, 1.0, 16)\n",
    "\n",
    "# Define targets with a simple linear relationship.\n",
    "y = 3.0 * x + 0.5\n",
    "\n",
    "# Confirm shapes are as expected before training.\n",
    "assert x.shape == y.shape\n",
    "\n",
    "# Train for a few epochs with silent logging.\n",
    "history = model.fit(x, y, epochs=10, verbose=0)\n",
    "\n",
    "# Convert loss history to a tensor for checking.\n",
    "loss_tensor = tf.convert_to_tensor(history.history[\"loss\"])\n",
    "\n",
    "# Summarize NaN and Inf in the loss history.\n",
    "summarize_bad_values(loss_tensor, \"training_loss\")\n",
    "\n",
    "# Print final loss value for quick inspection.\n",
    "print(\"Final loss value:\", float(loss_tensor[-1]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264762ba",
   "metadata": {},
   "source": [
    "### **3.2. Gradient Clipping Basics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1f6e23",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_10/Lecture_B/image_03_02.jpg?v=1769568050\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Exploding gradients can destabilize and break training\n",
    ">* Gradient clipping caps gradient size to maintain stability\n",
    "\n",
    ">* Clip by value limits individual gradient components\n",
    ">* Clip by global norm rescales overall gradient magnitude\n",
    "\n",
    ">* Choose clipping thresholds that balance stability, learning\n",
    ">* Monitor training signals and combine with other regularization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1492377c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Gradient Clipping Basics\n",
    "\n",
    "# This script shows gradient clipping basics.\n",
    "# It compares training with and without clipping.\n",
    "# Use it to observe stability and loss behavior.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required libraries safely.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Select device preferring GPU when available.\n",
    "physical_gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "if physical_gpus:\n",
    "    device_name = \"GPU\"\n",
    "else:\n",
    "    device_name = \"CPU\"\n",
    "\n",
    "# Inform which device will likely be used.\n",
    "print(\"Running on device type:\", device_name)\n",
    "\n",
    "# Load a small subset of MNIST digits.\n",
    "(x_train, y_train), _ = tf.keras.datasets.mnist.load_data()\n",
    "subset_size = 2000\n",
    "x_train = x_train[:subset_size]\n",
    "y_train = y_train[:subset_size]\n",
    "\n",
    "# Normalize images to the range zero one.\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_train = np.expand_dims(x_train, axis=-1)\n",
    "\n",
    "# Validate shapes before building models.\n",
    "print(\"Training data shape:\", x_train.shape)\n",
    "print(\"Training labels shape:\", y_train.shape)\n",
    "\n",
    "# Create a simple model factory function.\n",
    "def create_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(28, 28, 1)),\n",
    "        tf.keras.layers.Conv2D(16, 3, activation=\"relu\"),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(32, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(10, activation=\"softmax\"),\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Build model without gradient clipping.\n",
    "model_no_clip = create_model()\n",
    "optimizer_no_clip = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "model_no_clip.compile(\n",
    "    optimizer=optimizer_no_clip,\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Build model with gradient clipping by global norm.\n",
    "clip_norm_value = 1.0\n",
    "optimizer_clip = tf.keras.optimizers.Adam(\n",
    "    learning_rate=0.01,\n",
    "    clipnorm=clip_norm_value,\n",
    ")\n",
    "model_clip = create_model()\n",
    "model_clip.compile(\n",
    "    optimizer=optimizer_clip,\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Prepare a small dataset pipeline for speed.\n",
    "batch_size = 128\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "dataset = dataset.shuffle(buffer_size=subset_size, seed=seed_value)\n",
    "dataset = dataset.batch(batch_size)\n",
    "\n",
    "# Train both models briefly without verbose logs.\n",
    "print(\"Training models for three epochs each.\")\n",
    "history_no_clip = model_no_clip.fit(\n",
    "    dataset,\n",
    "    epochs=3,\n",
    "    verbose=0,\n",
    ")\n",
    "history_clip = model_clip.fit(\n",
    "    dataset,\n",
    "    epochs=3,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Extract final losses and accuracies for comparison.\n",
    "final_loss_no_clip = history_no_clip.history[\"loss\"][-1]\n",
    "final_acc_no_clip = history_no_clip.history[\"accuracy\"][-1]\n",
    "final_loss_clip = history_clip.history[\"loss\"][-1]\n",
    "final_acc_clip = history_clip.history[\"accuracy\"][-1]\n",
    "\n",
    "# Print a short comparison of results.\n",
    "print(\"No clipping final loss:\", round(float(final_loss_no_clip), 4))\n",
    "print(\"No clipping final accuracy:\", round(float(final_acc_no_clip), 4))\n",
    "print(\"Clipping final loss:\", round(float(final_loss_clip), 4))\n",
    "print(\"Clipping final accuracy:\", round(float(final_acc_clip), 4))\n",
    "\n",
    "# Show configured gradient clipping hyperparameter.\n",
    "print(\"Used global norm clip value:\", clip_norm_value)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49258d07",
   "metadata": {},
   "source": [
    "### **3.3. Tuning Learning Dynamics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a97982f",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_10/Lecture_B/image_03_03.jpg?v=1769568115\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Training stability depends on balanced learning dynamics\n",
    ">* Use tuned learning rate schedules, warmup, decay\n",
    "\n",
    ">* Optimizer choice and settings strongly affect stability\n",
    ">* Tune adaptive optimizer hyperparameters using training feedback\n",
    "\n",
    ">* Regularization and batch size strongly affect stability\n",
    ">* Jointly tune them, monitor metrics, refine iteratively\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4caf3c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Tuning Learning Dynamics\n",
    "\n",
    "# This script shows stable learning dynamics.\n",
    "# We compare two learning rates for stability.\n",
    "# Watch loss curves and gradient norms carefully.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required libraries safely.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version in one line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Select device preferring GPU when available.\n",
    "physical_gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "if physical_gpus:\n",
    "    device_name = \"/GPU:0\"\n",
    "else:\n",
    "    device_name = \"/CPU:0\"\n",
    "\n",
    "# Create a tiny synthetic regression dataset.\n",
    "num_samples = 256\n",
    "x_data = np.linspace(-2.0, 2.0, num_samples).astype(\"float32\")\n",
    "noise = 0.1 * np.random.randn(num_samples).astype(\"float32\")\n",
    "\n",
    "# Generate targets with a simple nonlinear relationship.\n",
    "y_data = 3.0 * x_data ** 2 + 0.5 * x_data + noise\n",
    "\n",
    "# Expand dimensions to match dense layer expectations.\n",
    "x_data = np.expand_dims(x_data, axis=-1)\n",
    "y_data = np.expand_dims(y_data, axis=-1)\n",
    "\n",
    "# Validate shapes before building datasets.\n",
    "assert x_data.shape == (num_samples, 1)\n",
    "assert y_data.shape == (num_samples, 1)\n",
    "\n",
    "# Build a small tf.data.Dataset for training.\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_data, y_data))\n",
    "train_ds = train_ds.shuffle(256, seed=seed_value).batch(32)\n",
    "\n",
    "# Define a simple regression model factory.\n",
    "def create_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(1,)),\n",
    "        tf.keras.layers.Dense(16, activation=\"tanh\"),\n",
    "        tf.keras.layers.Dense(16, activation=\"tanh\"),\n",
    "        tf.keras.layers.Dense(1),\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Create two models for different learning rates.\n",
    "model_stable = create_model()\n",
    "model_unstable = create_model()\n",
    "\n",
    "# Define mean squared error loss function.\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "# Create optimizers with different learning rates.\n",
    "optimizer_stable = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "optimizer_unstable = tf.keras.optimizers.Adam(learning_rate=0.5)\n",
    "\n",
    "# Prepare lists to store loss and gradient norms.\n",
    "stable_losses = []\n",
    "unstable_losses = []\n",
    "stable_grad_norms = []\n",
    "unstable_grad_norms = []\n",
    "\n",
    "# Define one training step using GradientTape.\n",
    "def train_step(model, optimizer, x_batch, y_batch):\n",
    "    with tf.GradientTape() as tape:\n",
    "        preds = model(x_batch, training=True)\n",
    "        loss = loss_fn(y_batch, preds)\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    grad_norm = tf.linalg.global_norm(grads)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return loss, grad_norm\n",
    "\n",
    "# Run a few epochs to compare dynamics.\n",
    "num_epochs = 5\n",
    "with tf.device(device_name):\n",
    "    for epoch in range(num_epochs):\n",
    "        for x_batch, y_batch in train_ds:\n",
    "            loss_s, norm_s = train_step(\n",
    "                model_stable, optimizer_stable, x_batch, y_batch\n",
    "            )\n",
    "            loss_u, norm_u = train_step(\n",
    "                model_unstable, optimizer_unstable, x_batch, y_batch\n",
    "            )\n",
    "            stable_losses.append(float(loss_s.numpy()))\n",
    "            unstable_losses.append(float(loss_u.numpy()))\n",
    "            stable_grad_norms.append(float(norm_s.numpy()))\n",
    "            unstable_grad_norms.append(float(norm_u.numpy()))\n",
    "\n",
    "# Compute simple statistics for both configurations.\n",
    "stable_loss_last = stable_losses[-1]\n",
    "unstable_loss_last = unstable_losses[-1]\n",
    "\n",
    "# Safely compute maximum gradient norms.\n",
    "max_stable_grad = max(stable_grad_norms)\n",
    "max_unstable_grad = max(unstable_grad_norms)\n",
    "\n",
    "# Print a compact comparison summary.\n",
    "print(\"Stable lr=0.01 final loss:\", round(stable_loss_last, 4))\n",
    "print(\"Unstable lr=0.5 final loss:\", round(unstable_loss_last, 4))\n",
    "print(\"Stable lr=0.01 max grad norm:\", round(max_stable_grad, 4))\n",
    "print(\"Unstable lr=0.5 max grad norm:\", round(max_unstable_grad, 4))\n",
    "\n",
    "# Show a few early and late loss values.\n",
    "print(\"First three stable losses:\", [round(v, 4) for v in stable_losses[:3]])\n",
    "print(\"First three unstable losses:\", [round(v, 4) for v in unstable_losses[:3]])\n",
    "print(\"Last three stable losses:\", [round(v, 4) for v in stable_losses[-3:]])\n",
    "print(\"Last three unstable losses:\", [round(v, 4) for v in unstable_losses[-3:]])\n",
    "\n",
    "# Indicate whether unstable run shows warning signs.\n",
    "print(\"Unstable run has exploding gradients:\", max_unstable_grad > 50.0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef838e3",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Performance and Debug**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6481d51b",
   "metadata": {},
   "source": [
    "\n",
    "In this lecture, you learned to:\n",
    "- Enable and configure mixed precision training in TensorFlow 2.20.0 to leverage modern GPUs and TPUs. \n",
    "- Use TensorFlow profiling tools to identify performance bottlenecks in models and input pipelines. \n",
    "- Diagnose and mitigate common numerical and stability issues such as NaNs and exploding gradients. \n",
    "\n",
    "<font color='yellow'>Congratulations on completing this course!</font>"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
