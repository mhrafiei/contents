{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5b24f72",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Transfer Learning**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75ffe0d",
   "metadata": {},
   "source": [
    ">Last update: 20260126.\n",
    "    \n",
    "By the end of this Lecture, you will be able to:\n",
    "- Load and adapt pretrained CNN backbones from tf.keras.applications for new classification tasks. \n",
    "- Configure layer freezing and unfreezing strategies to balance feature reuse and task-specific learning. \n",
    "- Implement a staged training schedule for fine-tuning and evaluate its impact on performance. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0319d8df",
   "metadata": {},
   "source": [
    "## **1. Working With Pretrained Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64309a1d",
   "metadata": {},
   "source": [
    "### **1.1. Using tf.keras.applications**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c412e8",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_06/Lecture_B/image_01_01.jpg?v=1769414305\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Use ready-made pretrained CNNs from keras applications\n",
    ">* Reuse rich learned features instead of training scratch\n",
    "\n",
    ">* Choose weights, heads, and input size options\n",
    ">* Use backbone features, add custom task-specific layers\n",
    "\n",
    ">* Pretrained backbones plug into larger models easily\n",
    ">* Modularity enables multi-task heads and rapid experimentation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161792dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Using tf.keras.applications\n",
    "\n",
    "# This script shows basic transfer learning usage.\n",
    "# We use tf.keras.applications for pretrained models.\n",
    "# All steps are small and beginner friendly.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Import TensorFlow and Keras modules.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Load CIFAR10 dataset from Keras datasets.\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Select a small subset for quick demonstration.\n",
    "train_samples = 1000\n",
    "test_samples = 200\n",
    "x_train = x_train[:train_samples]\n",
    "y_train = y_train[:train_samples]\n",
    "\n",
    "# Slice test data to keep runtime small.\n",
    "x_test = x_test[:test_samples]\n",
    "y_test = y_test[:test_samples]\n",
    "\n",
    "# Normalize pixel values to range zero one.\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_test = x_test.astype(\"float32\") / 255.0\n",
    "\n",
    "# Define target image size for backbone.\n",
    "img_height, img_width = 96, 96\n",
    "\n",
    "# Resize images to match backbone expectations.\n",
    "x_train_resized = tf.image.resize(x_train, (img_height, img_width))\n",
    "x_test_resized = tf.image.resize(x_test, (img_height, img_width))\n",
    "\n",
    "# Confirm resized shapes before building model.\n",
    "print(\"Train batch shape:\", x_train_resized.shape)\n",
    "print(\"Test batch shape:\", x_test_resized.shape)\n",
    "\n",
    "# Choose number of target classes here.\n",
    "num_classes = 10\n",
    "\n",
    "# Create input layer matching resized images.\n",
    "inputs = keras.Input(shape=(img_height, img_width, 3))\n",
    "\n",
    "# Use preprocessing layer for MobileNetV2.\n",
    "preprocess = keras.applications.mobilenet_v2.preprocess_input\n",
    "x = layers.Lambda(preprocess)(inputs)\n",
    "\n",
    "# Load MobileNetV2 backbone without top classifier.\n",
    "base_model = keras.applications.MobileNetV2(\n",
    "    input_shape=(img_height, img_width, 3),\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\",\n",
    ")\n",
    "\n",
    "# Freeze backbone weights for initial training.\n",
    "base_model.trainable = False\n",
    "\n",
    "# Pass preprocessed inputs through backbone.\n",
    "features = base_model(x, training=False)\n",
    "\n",
    "# Apply global pooling to reduce feature maps.\n",
    "x = layers.GlobalAveragePooling2D()(features)\n",
    "\n",
    "# Add small dense layer for better representation.\n",
    "x = layers.Dense(64, activation=\"relu\")(x)\n",
    "\n",
    "# Add final classification layer for CIFAR10.\n",
    "outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "# Build full transfer learning model.\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Compile model with simple optimizer and loss.\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Train only new head while backbone is frozen.\n",
    "history_frozen = model.fit(\n",
    "    x_train_resized,\n",
    "    y_train,\n",
    "    epochs=2,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Evaluate frozen model on small test subset.\n",
    "loss_frozen, acc_frozen = model.evaluate(\n",
    "    x_test_resized,\n",
    "    y_test,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Unfreeze some deeper layers for fine tuning.\n",
    "for layer in base_model.layers[-20:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Recompile with lower learning rate for safety.\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Fine tune model briefly with unfrozen layers.\n",
    "history_finetune = model.fit(\n",
    "    x_train_resized,\n",
    "    y_train,\n",
    "    epochs=1,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Evaluate fine tuned model on test subset.\n",
    "loss_finetune, acc_finetune = model.evaluate(\n",
    "    x_test_resized,\n",
    "    y_test,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Print concise summary of both evaluation stages.\n",
    "print(\"Frozen backbone accuracy:\", round(acc_frozen, 4))\n",
    "print(\"Fine tuned accuracy:\", round(acc_finetune, 4))\n",
    "print(\"Backbone trainable layers:\", sum(l.trainable for l in base_model.layers))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59dd9ba9",
   "metadata": {},
   "source": [
    "### **1.2. Input Preprocessing Essentials**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac29612",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_06/Lecture_B/image_01_02.jpg?v=1769414364\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Match image size, channels, and scaling exactly\n",
    ">* Mismatched preprocessing breaks features and hurts transfer\n",
    "\n",
    ">* Models expect specific pixel ranges and normalization\n",
    ">* Wrong scaling harms activations, stability, and accuracy\n",
    "\n",
    ">* Match backbone preprocessing, then add domain tweaks\n",
    ">* Keep the entire preprocessing pipeline consistent everywhere\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a6b776",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Input Preprocessing Essentials\n",
    "\n",
    "# This script shows essential input preprocessing concepts.\n",
    "# We compare raw and model specific preprocessing for images.\n",
    "# Focus is on beginner friendly TensorFlow vision workflows.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Select a small pretrained backbone from applications.\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "\n",
    "# Load CIFAR10 dataset for simple image examples.\n",
    "(x_train, y_train), _ = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Take a tiny subset to keep runtime small.\n",
    "x_sample = x_train[:8]\n",
    "y_sample = y_train[:8]\n",
    "\n",
    "# Check basic shape to ensure expected dimensions.\n",
    "print(\"Original sample shape:\", x_sample.shape)\n",
    "\n",
    "# Define target size expected by MobileNetV2.\n",
    "target_height, target_width = 224, 224\n",
    "\n",
    "# Create a simple resizing layer for images.\n",
    "resize_layer = tf.keras.layers.Resizing(\n",
    "    target_height, target_width\n",
    ")\n",
    "\n",
    "# Convert sample images to float32 for preprocessing.\n",
    "x_sample_float = x_sample.astype(\"float32\")\n",
    "\n",
    "# Resize images to match backbone input size.\n",
    "x_resized = resize_layer(x_sample_float)\n",
    "\n",
    "# Confirm resized shape before further processing.\n",
    "print(\"Resized sample shape:\", x_resized.shape)\n",
    "\n",
    "# Show raw pixel range before any normalization.\n",
    "print(\"Raw min, max:\", x_resized.numpy().min(), x_resized.numpy().max())\n",
    "\n",
    "# Apply MobileNetV2 specific preprocessing function.\n",
    "x_preprocessed = preprocess_input(x_resized)\n",
    "\n",
    "# Show new pixel range after model preprocessing.\n",
    "print(\"Preprocessed min, max:\", x_preprocessed.numpy().min(), x_preprocessed.numpy().max())\n",
    "\n",
    "# Build a minimal model using the pretrained backbone.\n",
    "base_model = MobileNetV2(\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\",\n",
    "    input_shape=(target_height, target_width, 3)\n",
    ")\n",
    "\n",
    "# Freeze backbone to focus on preprocessing effects.\n",
    "base_model.trainable = False\n",
    "\n",
    "# Add simple pooling and classification head.\n",
    "inputs = tf.keras.Input(shape=(target_height, target_width, 3))\n",
    "x = base_model(inputs, training=False)\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "outputs = tf.keras.layers.Dense(10, activation=\"softmax\")(x)\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "# Compile model with simple configuration.\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Run a forward pass with correctly preprocessed images.\n",
    "correct_preds = model.predict(x_preprocessed, verbose=0)\n",
    "\n",
    "# Run a forward pass with only resized raw images.\n",
    "raw_scaled = x_resized / 255.0\n",
    "raw_preds = model.predict(raw_scaled, verbose=0)\n",
    "\n",
    "# Compute average confidence for both preprocessing choices.\n",
    "correct_conf = float(np.mean(np.max(correct_preds, axis=1)))\n",
    "raw_conf = float(np.mean(np.max(raw_preds, axis=1)))\n",
    "\n",
    "# Print a short comparison of average confidences.\n",
    "print(\"Avg confidence with correct preprocessing:\", round(correct_conf, 4))\n",
    "print(\"Avg confidence with raw scaled images:\", round(raw_conf, 4))\n",
    "\n",
    "# Final line prints reminder about matching preprocessing expectations.\n",
    "print(\"Remember to match size and normalization to backbone.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5ab278",
   "metadata": {},
   "source": [
    "### **1.3. Selecting CNN Backbones**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba390de1",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_06/Lecture_B/image_01_03.jpg?v=1769414408\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Choose backbones balancing accuracy, speed, and memory\n",
    ">* Match model size to task needs and hardware\n",
    "\n",
    ">* Check if pretrained features match your domain\n",
    ">* Prefer backbones proven on similar specialized data\n",
    "\n",
    ">* Match architecture details to training and deployment\n",
    ">* Consider connections, efficiency, and input resolution trade-offs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad82aa54",
   "metadata": {},
   "source": [
    "## **2. Layer Freezing Techniques**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6e1955",
   "metadata": {},
   "source": [
    "### **2.1. Controlling Layer Trainability**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237c8f73",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_06/Lecture_B/image_02_01.jpg?v=1769414438\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Choose which pretrained layers can still learn\n",
    ">* Freeze generic features, adapt deeper layers to task\n",
    "\n",
    ">* Freezing layers cuts compute cost and stabilizes features\n",
    ">* Selective unfreezing adds domain-specific details while preserving generality\n",
    "\n",
    ">* Match freezing depth to task and data\n",
    ">* Treat trainability as gradual, tuning layers for adaptation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37747166",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Controlling Layer Trainability\n",
    "\n",
    "# This script shows controlling layer trainability.\n",
    "# We use a small pretrained backbone example.\n",
    "# Focus on freezing and unfreezing convolutional layers.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "random.seed(7)\n",
    "np.random.seed(7)\n",
    "\n",
    "# Import TensorFlow and Keras modules.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Select device based on GPU availability.\n",
    "physical_gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "if physical_gpus:\n",
    "    device_type = \"GPU\"\n",
    "else:\n",
    "    device_type = \"CPU\"\n",
    "\n",
    "# Print which device type will be used.\n",
    "print(\"Using device type:\", device_type)\n",
    "\n",
    "# Load CIFAR10 dataset from Keras datasets.\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Confirm dataset shapes before subsampling.\n",
    "print(\"Train shape:\", x_train.shape, y_train.shape)\n",
    "\n",
    "# Select a small subset for quick training.\n",
    "subset_size = 2000\n",
    "x_train_small = x_train[:subset_size]\n",
    "y_train_small = y_train[:subset_size]\n",
    "\n",
    "# Normalize pixel values to range zero one.\n",
    "x_train_small = x_train_small.astype(\"float32\") / 255.0\n",
    "x_test_small = x_test.astype(\"float32\") / 255.0\n",
    "\n",
    "# Convert labels to integer vectors.\n",
    "y_train_small = y_train_small.flatten()\n",
    "y_test_small = y_test.flatten()\n",
    "\n",
    "# Define input shape for the model.\n",
    "input_shape = (32, 32, 3)\n",
    "\n",
    "# Create a simple pretrained like backbone.\n",
    "backbone_input = keras.Input(shape=input_shape)\n",
    "\n",
    "# Add convolutional feature extractor layers.\n",
    "x = keras.layers.Conv2D(32, (3, 3), activation=\"relu\")(\n",
    "    backbone_input\n",
    ")\n",
    "\n",
    "# Add a second convolutional block layer.\n",
    "x = keras.layers.Conv2D(64, (3, 3), activation=\"relu\")(\n",
    "    x\n",
    ")\n",
    "\n",
    "# Add pooling to reduce spatial dimensions.\n",
    "x = keras.layers.MaxPooling2D(pool_size=(2, 2))(\n",
    "    x\n",
    ")\n",
    "\n",
    "# Add another convolutional block layer.\n",
    "x = keras.layers.Conv2D(128, (3, 3), activation=\"relu\")(\n",
    "    x\n",
    ")\n",
    "\n",
    "# Add global average pooling for features.\n",
    "features = keras.layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "# Build the backbone model object.\n",
    "backbone = keras.Model(backbone_input, features, name=\"toy_backbone\")\n",
    "\n",
    "# Pretend backbone is pretrained and freeze.\n",
    "for layer in backbone.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Show how many layers are trainable now.\n",
    "trainable_count = np.sum([layer.trainable for layer in backbone.layers])\n",
    "print(\"Trainable layers after freeze:\", int(trainable_count))\n",
    "\n",
    "# Create new classification head for CIFAR10.\n",
    "head_input = keras.Input(shape=input_shape)\n",
    "\n",
    "# Pass images through the frozen backbone.\n",
    "h = backbone(head_input, training=False)\n",
    "\n",
    "# Add a small dense classification head.\n",
    "h = keras.layers.Dense(64, activation=\"relu\")(h)\n",
    "outputs = keras.layers.Dense(10, activation=\"softmax\")(h)\n",
    "\n",
    "# Build the full transfer learning model.\n",
    "model = keras.Model(head_input, outputs, name=\"transfer_model\")\n",
    "\n",
    "# Compile model with simple optimizer.\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Train only the new head while backbone frozen.\n",
    "history_frozen = model.fit(\n",
    "    x_train_small,\n",
    "    y_train_small,\n",
    "    epochs=2,\n",
    "    batch_size=64,\n",
    "    verbose=0,\n",
    "    validation_split=0.1,\n",
    ")\n",
    "\n",
    "# Evaluate model performance with frozen backbone.\n",
    "loss_frozen, acc_frozen = model.evaluate(\n",
    "    x_test_small[:1000],\n",
    "    y_test_small[:1000],\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Unfreeze last convolutional block for fine tuning.\n",
    "for layer in backbone.layers[-3:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Recompile with lower learning rate now.\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Train again with partially trainable backbone.\n",
    "history_unfrozen = model.fit(\n",
    "    x_train_small,\n",
    "    y_train_small,\n",
    "    epochs=2,\n",
    "    batch_size=64,\n",
    "    verbose=0,\n",
    "    validation_split=0.1,\n",
    ")\n",
    "\n",
    "# Evaluate model performance after unfreezing.\n",
    "loss_unfrozen, acc_unfrozen = model.evaluate(\n",
    "    x_test_small[:1000],\n",
    "    y_test_small[:1000],\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Print concise comparison of both training stages.\n",
    "print(\"Frozen backbone accuracy:\", round(acc_frozen, 4))\n",
    "print(\"Unfrozen block accuracy:\", round(acc_unfrozen, 4))\n",
    "print(\"Trainable layers after unfreeze:\", np.sum([layer.trainable for layer in backbone.layers]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5599d0e",
   "metadata": {},
   "source": [
    "### **2.2. Training Classifier Head Only**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8845914b",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_06/Lecture_B/image_02_02.jpg?v=1769414490\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Freeze pretrained backbone as fixed feature extractor\n",
    ">* Train small head for new task efficiently\n",
    "\n",
    ">* Works best when tasks share visual patterns\n",
    ">* Gives strong, fast baseline and comparison point\n",
    "\n",
    ">* Head-only training regularizes and reduces overfitting\n",
    ">* Use as baseline before progressively unfreezing layers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77d0fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Training Classifier Head Only\n",
    "\n",
    "# This script shows training classifier head only.\n",
    "# We freeze a pretrained backbone and train top layers.\n",
    "# This demonstrates conservative transfer learning strategy.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required libraries for TensorFlow and NumPy.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set deterministic seeds for reproducible behavior.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version in one concise line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Select device preference based on GPU availability.\n",
    "physical_gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "if physical_gpus:\n",
    "    device_name = \"GPU\"\n",
    "else:\n",
    "    device_name = \"CPU\"\n",
    "\n",
    "# Print which device type will likely be used.\n",
    "print(\"Using device type:\", device_name)\n",
    "\n",
    "# Load CIFAR10 dataset using Keras utilities.\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Normalize pixel values to range zero to one.\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_test = x_test.astype(\"float32\") / 255.0\n",
    "\n",
    "# Reduce dataset size for faster demonstration.\n",
    "train_samples = 2000\n",
    "test_samples = 500\n",
    "x_train_small = x_train[:train_samples]\n",
    "y_train_small = y_train[:train_samples]\n",
    "\n",
    "# Create small test subset for quick evaluation.\n",
    "x_test_small = x_test[:test_samples]\n",
    "y_test_small = y_test[:test_samples]\n",
    "\n",
    "# Validate shapes to avoid unexpected broadcasting.\n",
    "print(\"Train subset shape:\", x_train_small.shape)\n",
    "print(\"Test subset shape:\", x_test_small.shape)\n",
    "\n",
    "# Define image size expected by MobileNetV2 backbone.\n",
    "img_height, img_width = 96, 96\n",
    "num_classes = 10\n",
    "\n",
    "# Resize images using TensorFlow image operations.\n",
    "resize_layer = tf.keras.layers.Resizing(img_height, img_width)\n",
    "x_train_resized = resize_layer(x_train_small)\n",
    "x_test_resized = resize_layer(x_test_small)\n",
    "\n",
    "# Confirm resized shapes before building model.\n",
    "print(\"Resized train shape:\", x_train_resized.shape)\n",
    "\n",
    "# Create base model from pretrained MobileNetV2.\n",
    "base_model = tf.keras.applications.MobileNetV2(\n",
    "    input_shape=(img_height, img_width, 3),\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\",\n",
    ")\n",
    "\n",
    "# Freeze all backbone layers to act as feature extractor.\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Show how many layers are frozen in backbone.\n",
    "print(\"Backbone layers frozen:\", len(base_model.layers))\n",
    "\n",
    "# Create input layer matching resized image shape.\n",
    "inputs = tf.keras.Input(shape=(img_height, img_width, 3))\n",
    "\n",
    "# Apply preprocessing specific to MobileNetV2.\n",
    "preprocessed = tf.keras.applications.mobilenet_v2.preprocess_input(inputs)\n",
    "\n",
    "# Pass preprocessed images through frozen backbone.\n",
    "features = base_model(preprocessed, training=False)\n",
    "\n",
    "# Pool spatial dimensions to single feature vector.\n",
    "pooled = tf.keras.layers.GlobalAveragePooling2D()(features)\n",
    "\n",
    "# Add small dense layer as classifier head.\n",
    "outputs = tf.keras.layers.Dense(num_classes, activation=\"softmax\")(pooled)\n",
    "\n",
    "# Build final model combining backbone and head.\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Confirm only classifier head parameters are trainable.\n",
    "trainable_count = np.sum([np.prod(v.shape) for v in model.trainable_weights])\n",
    "non_trainable_count = np.sum([np.prod(v.shape) for v in model.non_trainable_weights])\n",
    "\n",
    "# Print parameter counts for clarity and comparison.\n",
    "print(\"Trainable params (head only):\", int(trainable_count))\n",
    "print(\"Non-trainable params (backbone):\", int(non_trainable_count))\n",
    "\n",
    "# Compile model with simple optimizer and loss.\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Train only classifier head using small subset.\n",
    "history = model.fit(\n",
    "    x_train_resized,\n",
    "    y_train_small,\n",
    "    epochs=3,\n",
    "    batch_size=32,\n",
    "    validation_split=0.1,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Evaluate performance on small test subset.\n",
    "loss, acc = model.evaluate(\n",
    "    x_test_resized,\n",
    "    y_test_small,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Print concise summary of evaluation results.\n",
    "print(\"Test loss with frozen backbone:\", round(float(loss), 4))\n",
    "print(\"Test accuracy with frozen backbone:\", round(float(acc), 4))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5c28af",
   "metadata": {},
   "source": [
    "### **2.3. Progressive Layer Unfreezing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffb5c8a",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_06/Lecture_B/image_02_03.jpg?v=1769414556\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Start training with backbone frozen, head trainable\n",
    ">* Gradually unfreeze deeper layers to avoid forgetting\n",
    "\n",
    ">* Different CNN layers capture increasingly abstract features\n",
    ">* Unfreeze higher layers first; keep generic features\n",
    "\n",
    ">* Adjust learning rates when unfreezing deeper layers\n",
    ">* Track validation metrics to balance stability and adaptation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83965a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Progressive Layer Unfreezing\n",
    "\n",
    "# This script demonstrates progressive layer unfreezing.\n",
    "# We use a small pretrained CNN on CIFAR10 images.\n",
    "# Focus on freezing and unfreezing backbone layers.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required libraries safely.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version briefly.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Detect available device type.\n",
    "device_type = tf.config.list_physical_devices(\"GPU\")\n",
    "print(\"Using GPU:\" if device_type else \"Using CPU:\", bool(device_type))\n",
    "\n",
    "# Load CIFAR10 dataset from keras datasets.\n",
    "(cifar_x_train, cifar_y_train), (cifar_x_test, cifar_y_test) = (\n",
    "    tf.keras.datasets.cifar10.load_data()\n",
    ")\n",
    "\n",
    "# Use a very small subset for quick training.\n",
    "train_samples = 200\n",
    "test_samples = 100\n",
    "x_train = cifar_x_train[:train_samples]\n",
    "y_train = cifar_y_train[:train_samples]\n",
    "\n",
    "# Slice test subset for evaluation.\n",
    "x_test = cifar_x_test[:test_samples]\n",
    "y_test = cifar_y_test[:test_samples]\n",
    "\n",
    "# Normalize images to range zero one.\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_test = x_test.astype(\"float32\") / 255.0\n",
    "\n",
    "# Validate shapes before building model.\n",
    "print(\"Train shape:\", x_train.shape, y_train.shape)\n",
    "print(\"Test shape:\", x_test.shape, y_test.shape)\n",
    "\n",
    "# Define image size expected by backbone.\n",
    "img_height, img_width = 96, 96\n",
    "num_classes = 10\n",
    "\n",
    "# Resize images using tf.image.resize.\n",
    "resize_layer = tf.keras.layers.Resizing(img_height, img_width)\n",
    "x_train_resized = resize_layer(x_train)\n",
    "x_test_resized = resize_layer(x_test)\n",
    "\n",
    "# Confirm resized shapes are correct.\n",
    "print(\"Resized train shape:\", x_train_resized.shape)\n",
    "print(\"Resized test shape:\", x_test_resized.shape)\n",
    "\n",
    "# Create a small pretrained backbone model.\n",
    "base_model = tf.keras.applications.MobileNetV2(\n",
    "    input_shape=(img_height, img_width, 3),\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\",\n",
    ")\n",
    "\n",
    "# Freeze all backbone layers initially.\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add global pooling and dense classifier.\n",
    "inputs = tf.keras.Input(shape=(img_height, img_width, 3))\n",
    "x = tf.keras.applications.mobilenet_v2.preprocess_input(inputs)\n",
    "x = base_model(x, training=False)\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "outputs = tf.keras.layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "# Build the full transfer learning model.\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Compile model for initial head training.\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Train only classifier head with frozen backbone.\n",
    "history_head = model.fit(\n",
    "    x_train_resized,\n",
    "    y_train,\n",
    "    epochs=2,\n",
    "    batch_size=32,\n",
    "    validation_data=(x_test_resized, y_test),\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Evaluate after head training phase.\n",
    "loss_head, acc_head = model.evaluate(\n",
    "    x_test_resized,\n",
    "    y_test,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Progressively unfreeze top backbone layers.\n",
    "for layer in base_model.layers[-20:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Recompile with lower learning rate for fine tuning.\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Train again with partially unfrozen backbone.\n",
    "history_fine = model.fit(\n",
    "    x_train_resized,\n",
    "    y_train,\n",
    "    epochs=2,\n",
    "    batch_size=32,\n",
    "    validation_data=(x_test_resized, y_test),\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Evaluate after progressive unfreezing.\n",
    "loss_fine, acc_fine = model.evaluate(\n",
    "    x_test_resized,\n",
    "    y_test,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Print concise comparison of accuracies.\n",
    "print(\"Accuracy after head training:\", round(acc_head, 4))\n",
    "print(\"Accuracy after fine tuning:\", round(acc_fine, 4))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593a9b06",
   "metadata": {},
   "source": [
    "## **3. Staged Fine Tuning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14464fd",
   "metadata": {},
   "source": [
    "### **3.1. Two Stage Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7f2158",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_06/Lecture_B/image_03_01.jpg?v=1769414618\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Freeze backbone, train only new classification head\n",
    ">* Stabilizes training, preserves features, saves data\n",
    "\n",
    ">* Unfreeze backbone layers and train model end-to-end\n",
    ">* Refine features for new domain, boosting performance\n",
    "\n",
    ">* Two stages balance speed, resources, and risk\n",
    ">* Compare metrics to judge benefits of deeper fine tuning\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9138d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Two Stage Training\n",
    "\n",
    "# This script demonstrates two stage training.\n",
    "# We use a small pretrained CNN backbone.\n",
    "# We keep runtime short and outputs minimal.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Import tensorflow and keras utilities.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Detect available device type for information.\n",
    "physical_gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "print(\"GPUs available:\", len(physical_gpus))\n",
    "\n",
    "# Load CIFAR10 dataset from keras datasets.\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Use a small subset for quick demonstration.\n",
    "train_samples = 2000\n",
    "test_samples = 1000\n",
    "\n",
    "# Slice the dataset to the chosen subset.\n",
    "x_train_small = x_train[:train_samples]\n",
    "y_train_small = y_train[:train_samples]\n",
    "\n",
    "# Slice the test dataset subset similarly.\n",
    "x_test_small = x_test[:test_samples]\n",
    "y_test_small = y_test[:test_samples]\n",
    "\n",
    "# Normalize images to range zero one.\n",
    "x_train_small = x_train_small.astype(\"float32\") / 255.0\n",
    "x_test_small = x_test_small.astype(\"float32\") / 255.0\n",
    "\n",
    "# Confirm shapes are as expected.\n",
    "print(\"Train subset shape:\", x_train_small.shape)\n",
    "print(\"Test subset shape:\", x_test_small.shape)\n",
    "\n",
    "# Define image size expected by backbone.\n",
    "img_height, img_width = 32, 32\n",
    "num_classes = 10\n",
    "\n",
    "# Resize images to match backbone input.\n",
    "resize_layer = keras.Sequential([\n",
    "    layers.Resizing(96, 96),\n",
    "])\n",
    "\n",
    "# Apply resizing to training subset.\n",
    "x_train_resized = resize_layer(x_train_small)\n",
    "x_test_resized = resize_layer(x_test_small)\n",
    "\n",
    "# Validate resized shapes before modeling.\n",
    "print(\"Resized train shape:\", x_train_resized.shape)\n",
    "\n",
    "# Choose a small pretrained backbone model.\n",
    "backbone = keras.applications.MobileNetV2(\n",
    "    input_shape=(96, 96, 3),\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\",\n",
    ")\n",
    "\n",
    "# Freeze backbone initially for stage one.\n",
    "backbone.trainable = False\n",
    "print(\"Backbone trainable stage1:\", backbone.trainable)\n",
    "\n",
    "# Build classification head on top of backbone.\n",
    "inputs = keras.Input(shape=(96, 96, 3))\n",
    "x = backbone(inputs, training=False)\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "# Create the full transfer learning model.\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "# Compile model for first training stage.\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Train only the classification head first.\n",
    "history_stage1 = model.fit(\n",
    "    x_train_resized,\n",
    "    y_train_small,\n",
    "    epochs=2,\n",
    "    batch_size=64,\n",
    "    validation_split=0.2,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Evaluate performance after first stage.\n",
    "loss1, acc1 = model.evaluate(\n",
    "    x_test_resized,\n",
    "    y_test_small,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Print concise metrics for stage one.\n",
    "print(\"Stage1 test accuracy:\", round(float(acc1), 4))\n",
    "\n",
    "# Unfreeze some backbone layers for stage two.\n",
    "backbone.trainable = True\n",
    "for layer in backbone.layers[:80]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Confirm backbone is now partially trainable.\n",
    "trainable_count = np.sum([layer.trainable for layer in backbone.layers])\n",
    "print(\"Backbone trainable layers:\", int(trainable_count))\n",
    "\n",
    "# Recompile with lower learning rate for fine tuning.\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Train both head and selected backbone layers.\n",
    "history_stage2 = model.fit(\n",
    "    x_train_resized,\n",
    "    y_train_small,\n",
    "    epochs=2,\n",
    "    batch_size=64,\n",
    "    validation_split=0.2,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Evaluate performance after second stage.\n",
    "loss2, acc2 = model.evaluate(\n",
    "    x_test_resized,\n",
    "    y_test_small,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Print concise metrics for stage two.\n",
    "print(\"Stage2 test accuracy:\", round(float(acc2), 4))\n",
    "\n",
    "# Show simple comparison of both training stages.\n",
    "print(\"Accuracy gain:\", round(float(acc2 - acc1), 4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda7eb59",
   "metadata": {},
   "source": [
    "### **3.2. Gentle Backbone Learning Rates**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5565f673",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_06/Lecture_B/image_03_02.jpg?v=1769414687\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Backbone needs very small learning rate\n",
    ">* Preserve learned features while gently adapting to task\n",
    "\n",
    ">* Use smaller learning rate for pretrained backbone\n",
    ">* Faster head, slower backbone improves stability, generalization\n",
    "\n",
    ">* Start with tiny learning rates when unfreezing\n",
    ">* Adjust or decay rates to improve stable generalization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d6398e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Gentle Backbone Learning Rates\n",
    "\n",
    "# This script shows gentle backbone learning rates.\n",
    "# We fine tune a pretrained CNN with staged training.\n",
    "# Focus on safe small learning rates for backbones.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required libraries for TensorFlow training.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set deterministic seeds for reproducible behavior.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version in one concise line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Detect available device type for information only.\n",
    "physical_gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "print(\"GPUs available:\", len(physical_gpus))\n",
    "\n",
    "# Load CIFAR10 dataset from keras built in source.\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Select a small subset for quick demonstration.\n",
    "train_samples = 2000\n",
    "test_samples = 500\n",
    "x_train = x_train[:train_samples]\n",
    "\n",
    "# Slice labels and test images to chosen subset.\n",
    "y_train = y_train[:train_samples]\n",
    "x_test = x_test[:test_samples]\n",
    "y_test = y_test[:test_samples]\n",
    "\n",
    "# Normalize images to float range zero one.\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_test = x_test.astype(\"float32\") / 255.0\n",
    "\n",
    "# Resize images to match MobileNetV2 expected size.\n",
    "resize_layer = tf.keras.layers.Resizing(160, 160)\n",
    "x_train_resized = resize_layer(x_train)\n",
    "\n",
    "# Apply same resizing operation to test images.\n",
    "x_test_resized = resize_layer(x_test)\n",
    "\n",
    "# Confirm shapes are as expected before modeling.\n",
    "print(\"Train shape:\", x_train_resized.shape)\n",
    "print(\"Test shape:\", x_test_resized.shape)\n",
    "\n",
    "# Define number of target classes for CIFAR10.\n",
    "num_classes = 10\n",
    "\n",
    "# Load MobileNetV2 backbone without top classifier.\n",
    "backbone = tf.keras.applications.MobileNetV2(\n",
    "    input_shape=(160, 160, 3),\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\",\n",
    ")\n",
    "\n",
    "# Freeze backbone initially for head only training.\n",
    "backbone.trainable = False\n",
    "\n",
    "# Build classification head on top of backbone.\n",
    "inputs = tf.keras.Input(shape=(160, 160, 3))\n",
    "scaled = tf.keras.applications.mobilenet_v2.preprocess_input(inputs)\n",
    "\n",
    "# Pass scaled inputs through pretrained backbone network.\n",
    "features = backbone(scaled, training=False)\n",
    "pooled = tf.keras.layers.GlobalAveragePooling2D()(features)\n",
    "\n",
    "# Add small dense layer for extra capacity.\n",
    "hidden = tf.keras.layers.Dense(64, activation=\"relu\")(pooled)\n",
    "outputs = tf.keras.layers.Dense(num_classes, activation=\"softmax\")(hidden)\n",
    "\n",
    "# Create full model combining backbone and head.\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Compile model for stage one with higher rate.\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Train only classification head for few epochs.\n",
    "history_stage1 = model.fit(\n",
    "    x_train_resized,\n",
    "    y_train,\n",
    "    epochs=2,\n",
    "    batch_size=64,\n",
    "    validation_split=0.2,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Unfreeze backbone for gentle fine tuning stage.\n",
    "backbone.trainable = True\n",
    "\n",
    "# Set gentle learning rate for entire model backbone.\n",
    "backbone_learning_rate = 1e-5\n",
    "\n",
    "# Use smaller rate to avoid destroying pretrained features.\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=backbone_learning_rate),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Train whole model briefly with gentle backbone rate.\n",
    "history_stage2 = model.fit(\n",
    "    x_train_resized,\n",
    "    y_train,\n",
    "    epochs=2,\n",
    "    batch_size=64,\n",
    "    validation_split=0.2,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Evaluate model performance after staged fine tuning.\n",
    "loss, accuracy = model.evaluate(\n",
    "    x_test_resized,\n",
    "    y_test,\n",
    "    batch_size=64,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Print concise summary of both training stages.\n",
    "print(\"Stage1 final val acc:\", history_stage1.history[\"val_accuracy\"][-1])\n",
    "print(\"Stage2 final val acc:\", history_stage2.history[\"val_accuracy\"][-1])\n",
    "\n",
    "# Show test accuracy after gentle backbone fine tuning.\n",
    "print(\"Test accuracy after fine tuning:\", accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a4b73c",
   "metadata": {},
   "source": [
    "### **3.3. Overfitting Monitoring Strategies**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72240d14",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_06/Lecture_B/image_03_03.jpg?v=1769414796\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Use a realistic validation set and metrics\n",
    ">* Watch validation loss diverging from training as warning\n",
    "\n",
    ">* Track per-class metrics to detect uneven learning\n",
    ">* Use confusion patterns to spot bias, adjust training\n",
    "\n",
    ">* Use validation-based early stopping and checkpoints\n",
    ">* Stress-test checkpoints to ensure real-world robustness\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9467a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Overfitting Monitoring Strategies\n",
    "\n",
    "# This script shows staged fine tuning monitoring.\n",
    "# We use a tiny CNN with transfer learning.\n",
    "# We monitor validation loss to detect overfitting.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required libraries safely.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version briefly.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Detect available device type.\n",
    "device_type = tf.config.list_physical_devices(\"GPU\")\n",
    "print(\"Using GPU:\" if device_type else \"Using CPU only\")\n",
    "\n",
    "# Load CIFAR10 dataset from keras datasets.\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Use small subset for quick demonstration.\n",
    "train_samples = 2000\n",
    "test_samples = 500\n",
    "x_train = x_train[:train_samples]\n",
    "y_train = y_train[:train_samples]\n",
    "\n",
    "# Slice test set for validation usage.\n",
    "x_val = x_test[:test_samples]\n",
    "y_val = y_test[:test_samples]\n",
    "\n",
    "# Normalize images to zero one range.\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_val = x_val.astype(\"float32\") / 255.0\n",
    "\n",
    "# Validate shapes before building model.\n",
    "print(\"Train shape:\", x_train.shape, y_train.shape)\n",
    "print(\"Val shape:\", x_val.shape, y_val.shape)\n",
    "\n",
    "# Load small pretrained backbone from applications.\n",
    "base_model = tf.keras.applications.MobileNetV2(\n",
    "    input_shape=(96, 96, 3), include_top=False, weights=\"imagenet\"\n",
    ")\n",
    "\n",
    "# Freeze backbone initially for head training.\n",
    "base_model.trainable = False\n",
    "\n",
    "# Build preprocessing and resizing layers.\n",
    "inputs = tf.keras.Input(shape=(32, 32, 3))\n",
    "resize = tf.keras.layers.Resizing(96, 96)(inputs)\n",
    "preprocessed = tf.keras.applications.mobilenet_v2.preprocess_input(resize)\n",
    "\n",
    "# Add backbone and pooling layer.\n",
    "features = base_model(preprocessed, training=False)\n",
    "pooled = tf.keras.layers.GlobalAveragePooling2D()(features)\n",
    "\n",
    "# Add small classifier head for CIFAR10.\n",
    "outputs = tf.keras.layers.Dense(10, activation=\"softmax\")(pooled)\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "# Compile model for first training stage.\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Define early stopping callback for monitoring.\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=2, restore_best_weights=True,\n",
    ")\n",
    "\n",
    "# Train only classifier head first stage.\n",
    "history_stage1 = model.fit(\n",
    "    x_train, y_train, epochs=3, batch_size=64,\n",
    "    validation_data=(x_val, y_val), verbose=0, callbacks=[early_stop],\n",
    ")\n",
    "\n",
    "# Record best validation loss from stage one.\n",
    "best_val_loss_stage1 = min(history_stage1.history[\"val_loss\"])\n",
    "print(\"Stage1 best val_loss:\", round(best_val_loss_stage1, 4))\n",
    "\n",
    "# Unfreeze top layers of backbone for fine tuning.\n",
    "for layer in base_model.layers[-20:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Recompile with lower learning rate for stability.\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Define new early stopping for second stage.\n",
    "early_stop_stage2 = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=2, restore_best_weights=True,\n",
    ")\n",
    "\n",
    "# Train second stage with partial unfreezing.\n",
    "history_stage2 = model.fit(\n",
    "    x_train, y_train, epochs=5, batch_size=64,\n",
    "    validation_data=(x_val, y_val), verbose=0,\n",
    "    callbacks=[early_stop_stage2],\n",
    ")\n",
    "\n",
    "# Get best validation loss from second stage.\n",
    "best_val_loss_stage2 = min(history_stage2.history[\"val_loss\"])\n",
    "print(\"Stage2 best val_loss:\", round(best_val_loss_stage2, 4))\n",
    "\n",
    "# Decide if fine tuning improved generalization.\n",
    "if best_val_loss_stage2 < best_val_loss_stage1:\n",
    "    decision_message = \"Fine tuning helped, keep new weights.\"\n",
    "else:\n",
    "    decision_message = \"Fine tuning hurt, prefer earlier checkpoint.\"\n",
    "\n",
    "# Print concise monitoring summary lines.\n",
    "print(\"Monitoring summary:\")\n",
    "print(\"Stage1 epochs:\", len(history_stage1.history[\"loss\"]))\n",
    "print(\"Stage2 epochs:\", len(history_stage2.history[\"loss\"]))\n",
    "print(\"Decision:\", decision_message)\n",
    "\n",
    "# Evaluate final model on validation subset.\n",
    "val_loss, val_acc = model.evaluate(\n",
    "    x_val, y_val, verbose=0,\n",
    ")\n",
    "print(\"Final val_loss and val_accuracy:\", round(val_loss, 4), round(val_acc, 4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51876fa4",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Transfer Learning**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79807d2",
   "metadata": {},
   "source": [
    "\n",
    "In this lecture, you learned to:\n",
    "- Load and adapt pretrained CNN backbones from tf.keras.applications for new classification tasks. \n",
    "- Configure layer freezing and unfreezing strategies to balance feature reuse and task-specific learning. \n",
    "- Implement a staged training schedule for fine-tuning and evaluate its impact on performance. \n",
    "\n",
    "In the next Module (Module 7), we will go over 'NLP with TensorFlow'"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
