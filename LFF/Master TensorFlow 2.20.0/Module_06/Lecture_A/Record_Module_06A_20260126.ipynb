{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2113e27e",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**CNN Fundamentals**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff023d4",
   "metadata": {},
   "source": [
    ">Last update: 20260126.\n",
    "    \n",
    "By the end of this Lecture, you will be able to:\n",
    "- Construct basic CNN architectures using Conv2D, MaxPooling2D, and Dense layers in tf.keras. \n",
    "- Prepare image datasets with tf.data and image preprocessing utilities for CNN training. \n",
    "- Train and evaluate a CNN on a small image dataset, interpreting accuracy and loss metrics. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b571adf",
   "metadata": {},
   "source": [
    "## **1. Convolution Layer Basics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c486299",
   "metadata": {},
   "source": [
    "### **1.1. Understanding Filters and Strides**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa982bc",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_06/Lecture_A/image_01_01.jpg?v=1769409644\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Filters slide over images detecting local patterns\n",
    ">* Many filters create deep feature maps for representation\n",
    "\n",
    ">* Stride controls filter movement and feature resolution\n",
    ">* Larger strides reduce detail but save computation\n",
    "\n",
    ">* Filters and strides balance detail and efficiency\n",
    ">* More filters, small strides capture rich, useful patterns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e287bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Understanding Filters and Strides\n",
    "\n",
    "# This script explains filters and strides visually.\n",
    "# It uses TensorFlow to build simple Conv2D layers.\n",
    "# We compare outputs for different filters and strides.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required libraries safely.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set deterministic random seeds.\n",
    "seed_value = 7\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version briefly.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Create a tiny 6x6 grayscale image.\n",
    "base_image = np.array(\n",
    "    [\n",
    "        [0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 1, 1, 0, 0],\n",
    "        [0, 0, 1, 1, 0, 0],\n",
    "        [0, 0, 1, 1, 0, 0],\n",
    "        [0, 0, 1, 1, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0],\n",
    "    ],\n",
    "    dtype=\"float32\",\n",
    ")\n",
    "\n",
    "# Expand dimensions to match Conv2D input.\n",
    "image_tensor = base_image[np.newaxis, ..., np.newaxis]\n",
    "\n",
    "# Validate the image tensor shape.\n",
    "print(\"Input image shape:\", image_tensor.shape)\n",
    "\n",
    "# Define a vertical edge detection filter.\n",
    "vertical_filter = np.array(\n",
    "    [\n",
    "        [-1.0, 1.0, 0.0],\n",
    "        [-1.0, 1.0, 0.0],\n",
    "        [-1.0, 1.0, 0.0],\n",
    "    ],\n",
    "    dtype=\"float32\",\n",
    ")\n",
    "\n",
    "# Define a horizontal edge detection filter.\n",
    "horizontal_filter = np.array(\n",
    "    [\n",
    "        [-1.0, -1.0, -1.0],\n",
    "        [1.0, 1.0, 1.0],\n",
    "        [0.0, 0.0, 0.0],\n",
    "    ],\n",
    "    dtype=\"float32\",\n",
    ")\n",
    "\n",
    "# Stack filters to create two filter kernels.\n",
    "kernel_stack = np.stack([vertical_filter, horizontal_filter], axis=-1)\n",
    "\n",
    "# Reshape kernel to match Conv2D kernel shape.\n",
    "kernel_tensor = kernel_stack[..., np.newaxis, :]\n",
    "\n",
    "# Confirm kernel tensor shape for safety.\n",
    "print(\"Kernel tensor shape:\", kernel_tensor.shape)\n",
    "\n",
    "# Build a Conv2D layer with stride one.\n",
    "conv_stride_one = tf.keras.layers.Conv2D(\n",
    "    filters=2,\n",
    "    kernel_size=(3, 3),\n",
    "    strides=(1, 1),\n",
    "    padding=\"valid\",\n",
    "    use_bias=False,\n",
    ")\n",
    "\n",
    "# Build a Conv2D layer with stride two.\n",
    "conv_stride_two = tf.keras.layers.Conv2D(\n",
    "    filters=2,\n",
    "    kernel_size=(3, 3),\n",
    "    strides=(2, 2),\n",
    "    padding=\"valid\",\n",
    "    use_bias=False,\n",
    ")\n",
    "\n",
    "# Call layers once to build weights.\n",
    "_ = conv_stride_one(image_tensor)\n",
    "_ = conv_stride_two(image_tensor)\n",
    "\n",
    "# Assign our custom kernel to both layers.\n",
    "conv_stride_one.set_weights([kernel_tensor])\n",
    "conv_stride_two.set_weights([kernel_tensor])\n",
    "\n",
    "# Apply convolution with stride one.\n",
    "output_one = conv_stride_one(image_tensor).numpy()\n",
    "\n",
    "# Apply convolution with stride two.\n",
    "output_two = conv_stride_two(image_tensor).numpy()\n",
    "\n",
    "# Print shapes to show stride effect.\n",
    "print(\"Output shape stride 1:\", output_one.shape)\n",
    "print(\"Output shape stride 2:\", output_two.shape)\n",
    "\n",
    "# Print small feature map for first filter.\n",
    "print(\"Feature map filter 1 stride 1:\")\n",
    "print(output_one[0, :, :, 0])\n",
    "\n",
    "# Print small feature map for first filter stride two.\n",
    "print(\"Feature map filter 1 stride 2:\")\n",
    "print(output_two[0, :, :, 0])\n",
    "\n",
    "# Final confirmation line summarizing lesson.\n",
    "print(\"More filters learn patterns, larger strides reduce resolution.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ced2ce4",
   "metadata": {},
   "source": [
    "### **1.2. Padding and Kernel Choices**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59beaf98",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_06/Lecture_A/image_01_02.jpg?v=1769409721\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Kernel size controls local versus global patterns\n",
    ">* Padding preserves edge information and output size\n",
    "\n",
    ">* Valid padding shrinks feature maps, emphasizing compression\n",
    ">* Same padding keeps size, preserving spatial detail\n",
    "\n",
    ">* Small kernels with same padding grow receptive field\n",
    ">* Kernel and padding choices trade detail for context\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3d422c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Padding and Kernel Choices\n",
    "\n",
    "# This script shows padding and kernel basics.\n",
    "# It compares Conv2D outputs with different settings.\n",
    "# It uses a tiny random image batch example.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required modules from TensorFlow.\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set a deterministic random seed value.\n",
    "tf.random.set_seed(7)\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Create a tiny batch of random grayscale images.\n",
    "images = tf.random.uniform((2, 8, 8, 1))\n",
    "\n",
    "# Confirm the input tensor shape is correct.\n",
    "print(\"Input shape:\", images.shape)\n",
    "\n",
    "# Build a Conv2D layer with same padding.\n",
    "conv_same = tf.keras.layers.Conv2D(\n",
    "    filters=1,\n",
    "    kernel_size=(3, 3),\n",
    "    padding=\"same\",\n",
    ")\n",
    "\n",
    "# Build a Conv2D layer with valid padding.\n",
    "conv_valid = tf.keras.layers.Conv2D(\n",
    "    filters=1,\n",
    "    kernel_size=(3, 3),\n",
    "    padding=\"valid\",\n",
    ")\n",
    "\n",
    "# Apply same padding convolution to the images.\n",
    "output_same = conv_same(images)\n",
    "\n",
    "# Apply valid padding convolution to the images.\n",
    "output_valid = conv_valid(images)\n",
    "\n",
    "# Show output shapes for both padding choices.\n",
    "print(\"Same padding output shape:\", output_same.shape)\n",
    "\n",
    "# Show how valid padding reduces spatial dimensions.\n",
    "print(\"Valid padding output shape:\", output_valid.shape)\n",
    "\n",
    "# Build a Conv2D layer with larger kernel size.\n",
    "conv_large_kernel = tf.keras.layers.Conv2D(\n",
    "    filters=1,\n",
    "    kernel_size=(5, 5),\n",
    "    padding=\"same\",\n",
    ")\n",
    "\n",
    "# Apply the larger kernel convolution to images.\n",
    "output_large_kernel = conv_large_kernel(images)\n",
    "\n",
    "# Confirm shape stays same with same padding.\n",
    "print(\"Large kernel same padding shape:\", output_large_kernel.shape)\n",
    "\n",
    "# Compare parameter counts for small and large kernels.\n",
    "params_small = conv_same.count_params()\n",
    "\n",
    "# Compute parameter count for the larger kernel layer.\n",
    "params_large = conv_large_kernel.count_params()\n",
    "\n",
    "# Print parameter counts to show kernel size effect.\n",
    "print(\"Params small kernel:\", params_small)\n",
    "\n",
    "# Print parameter counts for the larger kernel.\n",
    "print(\"Params large kernel:\", params_large)\n",
    "\n",
    "# End by printing a short explanatory summary line.\n",
    "print(\"Same keeps size, valid shrinks; larger kernels add parameters.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb008299",
   "metadata": {},
   "source": [
    "### **1.3. CNN Activation Functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611ae7e6",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_06/Lecture_A/image_01_03.jpg?v=1769409753\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Activation functions add nonlinearity so CNNs learn\n",
    ">* They gate feature strength, building complex visual concepts\n",
    "\n",
    ">* ReLU is default, fast, and avoids vanishing gradients\n",
    ">* It creates sparse, focused activations highlighting key patterns\n",
    "\n",
    ">* ReLU has alternatives like leaky and parametric\n",
    ">* Activation choice and placement strongly affect feature separation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3242b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - CNN Activation Functions\n",
    "\n",
    "# This script demonstrates CNN activation functions.\n",
    "# It uses TensorFlow to build tiny models.\n",
    "# Focus on ReLU and sigmoid layer behaviors.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required libraries safely.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Create a small batch of fake image data.\n",
    "batch_size = 4\n",
    "height, width, channels = 8, 8, 1\n",
    "images = np.random.randn(batch_size, height, width, channels).astype(\"float32\")\n",
    "\n",
    "# Confirm the image batch shape is correct.\n",
    "print(\"Image batch shape:\", images.shape)\n",
    "\n",
    "# Build a simple CNN block with ReLU.\n",
    "relu_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(\n",
    "        filters=2,\n",
    "        kernel_size=(3, 3),\n",
    "        activation=\"relu\",\n",
    "        input_shape=(height, width, channels),\n",
    "    )\n",
    "])\n",
    "\n",
    "# Build a similar CNN block with sigmoid.\n",
    "sigmoid_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(\n",
    "        filters=2,\n",
    "        kernel_size=(3, 3),\n",
    "        activation=\"sigmoid\",\n",
    "        input_shape=(height, width, channels),\n",
    "    )\n",
    "])\n",
    "\n",
    "# Run a forward pass through the ReLU model.\n",
    "relu_output = relu_model(images)\n",
    "\n",
    "# Run a forward pass through the sigmoid model.\n",
    "sigmoid_output = sigmoid_model(images)\n",
    "\n",
    "# Check that output shapes match expectations.\n",
    "print(\"ReLU output shape:\", relu_output.shape)\n",
    "print(\"Sigmoid output shape:\", sigmoid_output.shape)\n",
    "\n",
    "# Compute simple statistics for ReLU activations.\n",
    "relu_min = float(tf.reduce_min(relu_output).numpy())\n",
    "relu_max = float(tf.reduce_max(relu_output).numpy())\n",
    "relu_mean = float(tf.reduce_mean(relu_output).numpy())\n",
    "\n",
    "# Compute simple statistics for sigmoid activations.\n",
    "sig_min = float(tf.reduce_min(sigmoid_output).numpy())\n",
    "sig_max = float(tf.reduce_max(sigmoid_output).numpy())\n",
    "sig_mean = float(tf.reduce_mean(sigmoid_output).numpy())\n",
    "\n",
    "# Print summary statistics for both activations.\n",
    "print(\"ReLU activations min max mean:\", relu_min, relu_max, relu_mean)\n",
    "print(\"Sigmoid activations min max mean:\", sig_min, sig_max, sig_mean)\n",
    "\n",
    "# Show how many ReLU outputs are exactly zero.\n",
    "zero_count = int(tf.math.count_nonzero(relu_output == 0.0).numpy())\n",
    "print(\"Number of exact zeros in ReLU output:\", zero_count)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99be5750",
   "metadata": {},
   "source": [
    "## **2. Pooling and Flattening**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b75c268",
   "metadata": {},
   "source": [
    "### **2.1. Pooling Layer Basics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1afbdd",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_06/Lecture_A/image_02_01.jpg?v=1769409789\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Pooling summarizes and compresses spatial feature information\n",
    ">* Max pooling keeps strongest activations, reducing resolution\n",
    "\n",
    ">* Pooling makes features less sensitive to position\n",
    ">* It improves robustness to shifts, distortions, and noise\n",
    "\n",
    ">* Pooling shrinks feature maps, saving compute and memory\n",
    ">* Balances speed and detail, reducing overfitting risk\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055c4a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Pooling Layer Basics\n",
    "\n",
    "# This script demonstrates basic pooling concepts.\n",
    "# It uses TensorFlow to build simple layers.\n",
    "# Focus is on pooling and flattening behavior.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import TensorFlow and NumPy for this demo.\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Print TensorFlow version for reproducibility.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Set random seeds for deterministic behavior.\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Create a small fake image batch tensor.\n",
    "input_batch = tf.constant(\n",
    "    np.arange(1, 17, dtype=np.float32).reshape((1, 4, 4, 1))\n",
    ")\n",
    "\n",
    "# Confirm the input shape is as expected.\n",
    "print(\"Input batch shape:\", input_batch.shape)\n",
    "\n",
    "# Print the input values in a clear layout.\n",
    "print(\"Input values (4x4 image):\")\n",
    "print(tf.reshape(input_batch, (4, 4)))\n",
    "\n",
    "# Define a max pooling layer with 2x2 window.\n",
    "max_pool = tf.keras.layers.MaxPooling2D(\n",
    "    pool_size=(2, 2), strides=(2, 2)\n",
    ")\n",
    "\n",
    "# Apply max pooling to the input batch.\n",
    "pooled_output = max_pool(input_batch)\n",
    "\n",
    "# Show pooled output shape and values.\n",
    "print(\"Pooled shape:\", pooled_output.shape)\n",
    "print(\"Pooled values (2x2 result):\")\n",
    "print(tf.reshape(pooled_output, (2, 2)))\n",
    "\n",
    "# Define an average pooling layer for comparison.\n",
    "avg_pool = tf.keras.layers.AveragePooling2D(\n",
    "    pool_size=(2, 2), strides=(2, 2)\n",
    ")\n",
    "\n",
    "# Apply average pooling to the same input.\n",
    "avg_output = avg_pool(input_batch)\n",
    "\n",
    "# Show average pooled shape and values.\n",
    "print(\"Average pooled shape:\", avg_output.shape)\n",
    "print(\"Average pooled values (2x2):\")\n",
    "print(tf.reshape(avg_output, (2, 2)))\n",
    "\n",
    "# Define a flatten layer to go to Dense.\n",
    "flatten_layer = tf.keras.layers.Flatten()\n",
    "\n",
    "# Flatten the max pooled output tensor.\n",
    "flattened = flatten_layer(pooled_output)\n",
    "\n",
    "# Show flattened shape and values for clarity.\n",
    "print(\"Flattened shape:\", flattened.shape)\n",
    "print(\"Flattened values:\", flattened.numpy())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d4ef76",
   "metadata": {},
   "source": [
    "### **2.2. Flatten vs GlobalAveragePooling2D**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e311b6",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_06/Lecture_A/image_02_02.jpg?v=1769409820\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Flattening keeps every spatial activation for classification\n",
    ">* Global average pooling summarizes each feature map overall\n",
    "\n",
    ">* Flattening creates huge vectors, many parameters, overfitting\n",
    ">* Global average pooling reduces parameters, improves generalization\n",
    "\n",
    ">* Flatten keeps spatial detail but less robust\n",
    ">* Global pooling adds invariance and guides preprocessing choices\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e8a2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Flatten vs GlobalAveragePooling2D\n",
    "\n",
    "# This script compares Flatten and GlobalAveragePooling2D.\n",
    "# It uses a tiny CNN on MNIST images.\n",
    "# Focus is on dataset prep and pooling choices.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Import TensorFlow and Keras layers.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Check and report available physical devices.\n",
    "devices = tf.config.list_physical_devices()\n",
    "print(\"Available devices count:\", len(devices))\n",
    "\n",
    "# Load MNIST dataset from Keras datasets.\n",
    "(mnist_x_train, mnist_y_train), _ = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Select a small subset for quick demonstration.\n",
    "subset_size = 4000\n",
    "mnist_x_train = mnist_x_train[:subset_size]\n",
    "mnist_y_train = mnist_y_train[:subset_size]\n",
    "\n",
    "# Normalize pixel values to range [0,1].\n",
    "mnist_x_train = mnist_x_train.astype(\"float32\") / 255.0\n",
    "\n",
    "# Add channel dimension for CNN input.\n",
    "mnist_x_train = np.expand_dims(mnist_x_train, axis=-1)\n",
    "\n",
    "# Confirm input shape is as expected.\n",
    "print(\"Train subset shape:\", mnist_x_train.shape)\n",
    "\n",
    "# Create a tf.data Dataset from numpy arrays.\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (mnist_x_train, mnist_y_train)\n",
    ")\n",
    "\n",
    "# Shuffle dataset with a safe buffer size.\n",
    "train_ds = train_ds.shuffle(buffer_size=subset_size, seed=42)\n",
    "\n",
    "# Define a simple preprocessing function.\n",
    "def preprocess(image, label):\n",
    "    image = tf.image.resize(image, size=(28, 28))\n",
    "    return image, label\n",
    "\n",
    "# Map preprocessing over the dataset.\n",
    "train_ds = train_ds.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "# Batch and prefetch for efficient input pipeline.\n",
    "batch_size = 64\n",
    "train_ds = train_ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Build a small shared convolutional base.\n",
    "inputs = keras.Input(shape=(28, 28, 1))\n",
    "x = layers.Conv2D(16, (3, 3), activation=\"relu\")(inputs)\n",
    "x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "# Add another convolution and pooling layer.\n",
    "x = layers.Conv2D(32, (3, 3), activation=\"relu\")(x)\n",
    "feature_maps = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "# Create Flatten branch for classification.\n",
    "flat_branch = layers.Flatten()(feature_maps)\n",
    "flat_output = layers.Dense(10, activation=\"softmax\")(flat_branch)\n",
    "\n",
    "# Create GlobalAveragePooling2D branch for classification.\n",
    "gap_branch = layers.GlobalAveragePooling2D()(feature_maps)\n",
    "gap_output = layers.Dense(10, activation=\"softmax\")(gap_branch)\n",
    "\n",
    "# Build two separate models sharing the same base.\n",
    "model_flat = keras.Model(inputs=inputs, outputs=flat_output)\n",
    "model_gap = keras.Model(inputs=inputs, outputs=gap_output)\n",
    "\n",
    "# Compile both models with same optimizer and loss.\n",
    "model_flat.compile(\n",
    "    optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Compile the global average pooling model similarly.\n",
    "model_gap.compile(\n",
    "    optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Train Flatten model briefly with silent verbose.\n",
    "history_flat = model_flat.fit(train_ds, epochs=1, verbose=0)\n",
    "\n",
    "# Train GlobalAveragePooling2D model briefly.\n",
    "history_gap = model_gap.fit(train_ds, epochs=1, verbose=0)\n",
    "\n",
    "# Get parameter counts for both models.\n",
    "params_flat = model_flat.count_params()\n",
    "params_gap = model_gap.count_params()\n",
    "\n",
    "# Extract final training accuracy values.\n",
    "acc_flat = history_flat.history[\"accuracy\"][-1]\n",
    "acc_gap = history_gap.history[\"accuracy\"][-1]\n",
    "\n",
    "# Print concise comparison of parameter counts.\n",
    "print(\"Flatten model parameters:\", params_flat)\n",
    "print(\"GAP model parameters:\", params_gap)\n",
    "\n",
    "# Print concise comparison of training accuracies.\n",
    "print(\"Flatten model train accuracy:\", round(float(acc_flat), 4))\n",
    "print(\"GAP model train accuracy:\", round(float(acc_gap), 4))\n",
    "\n",
    "# Show feature map and pooled shapes for intuition.\n",
    "print(\"Feature maps shape example:\", feature_maps.shape)\n",
    "print(\"Flatten output length:\", flat_branch.shape[-1])\n",
    "print(\"GAP output length:\", gap_branch.shape[-1])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faae69ff",
   "metadata": {},
   "source": [
    "### **2.3. Managing Feature Map Size**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1118ea",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_06/Lecture_A/image_02_03.jpg?v=1769409863\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Feature map size affects memory and computation\n",
    ">* Balance downsampling to keep detail yet efficiency\n",
    "\n",
    ">* Control feature maps using input size and layers\n",
    ">* Balance downsampling speed with preserving important details\n",
    "\n",
    ">* Standardize image sizes and batches during preprocessing\n",
    ">* Plan layer downsizing to balance efficiency and detail\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867bc3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Managing Feature Map Size\n",
    "\n",
    "# This script shows managing CNN feature map size.\n",
    "# We use MNIST images with simple preprocessing steps.\n",
    "# We inspect shapes after pooling and flattening.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required libraries safely.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Load MNIST dataset from tf.keras datasets.\n",
    "(mnist_x_train, mnist_y_train), _ = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Select a small subset for quick processing.\n",
    "subset_size = 512\n",
    "mnist_x_train = mnist_x_train[:subset_size]\n",
    "mnist_y_train = mnist_y_train[:subset_size]\n",
    "\n",
    "# Add channel dimension and convert to float.\n",
    "mnist_x_train = mnist_x_train[..., np.newaxis].astype(\"float32\")\n",
    "\n",
    "# Normalize pixel values to range zero one.\n",
    "mnist_x_train = mnist_x_train / 255.0\n",
    "\n",
    "# Define target image size for the pipeline.\n",
    "img_height, img_width = 28, 28\n",
    "\n",
    "# Create a tf.data dataset from numpy arrays.\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (mnist_x_train, mnist_y_train)\n",
    ")\n",
    "\n",
    "# Define a simple preprocessing function.\n",
    "def preprocess(image, label):\n",
    "    # Ensure image has expected shape.\n",
    "    image = tf.ensure_shape(image, (28, 28, 1))\n",
    "    # Resize image to target spatial size.\n",
    "    image = tf.image.resize(image, (img_height, img_width))\n",
    "    return image, label\n",
    "\n",
    "# Apply preprocessing and batching to dataset.\n",
    "train_ds = train_ds.map(preprocess).batch(32)\n",
    "\n",
    "# Take one small batch for shape inspection.\n",
    "for batch_images, batch_labels in train_ds.take(1):\n",
    "    input_batch = batch_images\n",
    "\n",
    "# Print original batch and image shapes.\n",
    "print(\"Input batch shape:\", input_batch.shape)\n",
    "print(\"Single image shape:\", input_batch[0].shape)\n",
    "\n",
    "# Build a small CNN to track feature sizes.\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(img_height, img_width, 1)),\n",
    "    tf.keras.layers.Conv2D(8, (3, 3), padding=\"same\", activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Conv2D(16, (3, 3), padding=\"same\", activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "# Call the model once to define its input tensor.\n",
    "_ = model(input_batch)\n",
    "\n",
    "# Run a forward pass to observe feature shapes.\n",
    "feature_extractor = tf.keras.Model(\n",
    "    inputs=model.layers[0].input,\n",
    "    outputs=[\n",
    "        model.layers[1].output,\n",
    "        model.layers[2].output,\n",
    "        model.layers[3].output,\n",
    "        model.layers[4].output,\n",
    "        model.layers[5].output,\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Compute intermediate feature maps for one batch.\n",
    "conv1_out, pool1_out, conv2_out, pool2_out, flat_out = feature_extractor(\n",
    "    input_batch\n",
    ")\n",
    "\n",
    "# Print shapes after each important transformation.\n",
    "print(\"After first conv shape:\", conv1_out.shape)\n",
    "print(\"After first pool shape:\", pool1_out.shape)\n",
    "print(\"After second conv shape:\", conv2_out.shape)\n",
    "print(\"After second pool shape:\", pool2_out.shape)\n",
    "print(\"After flatten shape:\", flat_out.shape)\n",
    "\n",
    "# Show how many features remain after flattening.\n",
    "print(\"Flattened feature vector length:\", flat_out.shape[-1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e85166",
   "metadata": {},
   "source": [
    "## **3. Building Image Pipelines**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4665c04",
   "metadata": {},
   "source": [
    "### **3.1. Loading Images From Directories**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55a2c6a",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_06/Lecture_A/image_03_01.jpg?v=1769409968\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Load images from labeled folders into tensors\n",
    ">* Ensure consistent shapes and easy dataset swapping\n",
    "\n",
    ">* Resize images and assign consistent numeric labels\n",
    ">* Carefully split data to avoid validation leakage\n",
    "\n",
    ">* Use batched, shuffled pipelines for training images\n",
    ">* Disable shuffling for stable, trustworthy evaluation metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17a2222",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Loading Images From Directories\n",
    "\n",
    "# This script shows loading images from directories.\n",
    "# It uses TensorFlow image utilities for beginners.\n",
    "# It keeps output short and training very small.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import pathlib\n",
    "import random\n",
    "\n",
    "# Import TensorFlow and Keras utilities.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "\n",
    "# Set TensorFlow random seed for determinism.\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Detect available device type for information.\n",
    "physical_gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "\n",
    "# Print whether GPU is available or only CPU.\n",
    "print(\"GPU available:\", bool(physical_gpus))\n",
    "\n",
    "# Load CIFAR10 dataset from Keras datasets.\n",
    "(cifar_images, cifar_labels), _ = keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Confirm dataset shapes before further processing.\n",
    "print(\"CIFAR10 images shape:\", cifar_images.shape)\n",
    "\n",
    "# Choose a tiny subset for quick demonstration.\n",
    "subset_size = 60\n",
    "cifar_images = cifar_images[:subset_size]\n",
    "\n",
    "# Slice labels to match the image subset.\n",
    "cifar_labels = cifar_labels[:subset_size].flatten()\n",
    "\n",
    "# Validate subset shapes after slicing.\n",
    "print(\"Subset images shape:\", cifar_images.shape)\n",
    "\n",
    "# Create a base directory for image folders.\n",
    "base_dir = pathlib.Path(\"cifar10_small_dir\")\n",
    "\n",
    "# Remove existing directory tree if it exists.\n",
    "if base_dir.exists():\n",
    "    for child in base_dir.rglob(\"*\"):\n",
    "        if child.is_file():\n",
    "            child.unlink()\n",
    "\n",
    "# Recreate the base directory cleanly.\n",
    "if base_dir.exists():\n",
    "    for child in sorted(base_dir.glob(\"*\")):\n",
    "        if child.is_dir():\n",
    "            for sub in child.glob(\"*\"):\n",
    "                if sub.is_file():\n",
    "                    sub.unlink()\n",
    "\n",
    "# Ensure base directory exists for saving.\n",
    "base_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Define class names for first three classes.\n",
    "class_names = [\"airplane\", \"automobile\", \"bird\"]\n",
    "\n",
    "# Create subdirectories for train and validation.\n",
    "train_dir = base_dir / \"train\"\n",
    "val_dir = base_dir / \"val\"\n",
    "\n",
    "# Make train and validation directories.\n",
    "train_dir.mkdir(exist_ok=True)\n",
    "val_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Create class subfolders inside train directory.\n",
    "for name in class_names:\n",
    "    (train_dir / name).mkdir(exist_ok=True)\n",
    "\n",
    "# Create class subfolders inside validation directory.\n",
    "for name in class_names:\n",
    "    (val_dir / name).mkdir(exist_ok=True)\n",
    "\n",
    "# Separate indices for selected three classes.\n",
    "selected_indices = [i for i, y in enumerate(cifar_labels)\n",
    "                    if int(y) in [0, 1, 2]]\n",
    "\n",
    "# Limit to small number of images per class.\n",
    "max_per_class = 10\n",
    "\n",
    "# Track counts per class for balancing.\n",
    "class_counts = {0: 0, 1: 0, 2: 0}\n",
    "\n",
    "# Iterate indices and save images into folders.\n",
    "for idx in selected_indices:\n",
    "    label = int(cifar_labels[idx])\n",
    "    if class_counts[label] >= max_per_class:\n",
    "        continue\n",
    "    class_counts[label] += 1\n",
    "    split = \"train\" if class_counts[label] <= 7 else \"val\"\n",
    "    folder = train_dir if split == \"train\" else val_dir\n",
    "    class_name = class_names[label]\n",
    "    img_array = cifar_images[idx]\n",
    "    img_path = folder / class_name / f\"img_{idx}.png\"\n",
    "    tf.keras.utils.save_img(str(img_path), img_array)\n",
    "\n",
    "# Print how many images saved per class.\n",
    "print(\"Saved per class:\", class_counts)\n",
    "\n",
    "# Define common image size for loading.\n",
    "img_height = 32\n",
    "img_width = 32\n",
    "\n",
    "# Create training dataset from directory.\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    directory=train_dir,\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"int\",\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    seed=seed_value,\n",
    "    validation_split=None,\n",
    ")\n",
    "\n",
    "# Create validation dataset from directory.\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    directory=val_dir,\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"int\",\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    seed=seed_value,\n",
    "    validation_split=None,\n",
    ")\n",
    "\n",
    "# Cache and prefetch datasets for performance.\n",
    "autotune = tf.data.AUTOTUNE\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=autotune)\n",
    "\n",
    "# Apply same optimization to validation dataset.\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=autotune)\n",
    "\n",
    "# Build a tiny CNN model for demonstration.\n",
    "model = keras.Sequential([\n",
    "    layers.Rescaling(1.0 / 255.0, input_shape=(img_height, img_width, 3)),\n",
    "    layers.Conv2D(16, (3, 3), activation=\"relu\"),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(32, (3, 3), activation=\"relu\"),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(32, activation=\"relu\"),\n",
    "    layers.Dense(len(class_names), activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "# Compile model with simple optimizer and loss.\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Train model briefly with silent verbose setting.\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=3,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Evaluate model on validation dataset silently.\n",
    "val_loss, val_acc = model.evaluate(val_ds, verbose=0)\n",
    "\n",
    "# Print final validation loss and accuracy.\n",
    "print(\"Validation loss:\", round(float(val_loss), 4))\n",
    "\n",
    "# Print validation accuracy rounded for readability.\n",
    "print(\"Validation accuracy:\", round(float(val_acc), 4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fb35a4",
   "metadata": {},
   "source": [
    "### **3.2. Image Rescaling and Augmentation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f313ebe",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_06/Lecture_A/image_03_02.jpg?v=1769410086\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Rescale pixel values to a consistent range\n",
    ">* Rescaling stabilizes training and highlights meaningful patterns\n",
    "\n",
    ">* Augmentation creates varied, realistic versions of images\n",
    ">* Model learns robust features, improving generalization performance\n",
    "\n",
    ">* Match rescaling and augmentation to task, metrics\n",
    ">* Tune augmentation strength by watching accuracy, loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff999902",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Image Rescaling and Augmentation\n",
    "\n",
    "# This script shows image rescaling and augmentation.\n",
    "# It uses TensorFlow to build simple pipelines.\n",
    "# Focus on visualizing effects not heavy training.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# Import TensorFlow and check version.\n",
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Detect available device type for information.\n",
    "physical_gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "print(\"GPUs available:\", len(physical_gpus))\n",
    "\n",
    "# Load MNIST dataset for simple image examples.\n",
    "(x_train, y_train), _ = tf.keras.datasets.mnist.load_data()\n",
    "print(\"Original train shape:\", x_train.shape)\n",
    "\n",
    "# Select a small subset for quick processing.\n",
    "subset_size = 8\n",
    "x_small = x_train[:subset_size]\n",
    "y_small = y_train[:subset_size]\n",
    "\n",
    "# Expand grayscale images to have channel dimension.\n",
    "x_small = np.expand_dims(x_small, axis=-1)\n",
    "print(\"Subset shape with channel:\", x_small.shape)\n",
    "\n",
    "# Normalize pixel values to range zero one.\n",
    "x_rescaled = x_small.astype(\"float32\") / 255.0\n",
    "print(\"Rescaled min max:\", x_rescaled.min(), x_rescaled.max())\n",
    "\n",
    "# Create a tf.data dataset from rescaled images.\n",
    "ds_base = tf.data.Dataset.from_tensor_slices((x_rescaled, y_small))\n",
    "ds_base = ds_base.batch(4)\n",
    "\n",
    "# Define simple augmentation layer using Keras.\n",
    "augmentation_layer = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "    tf.keras.layers.RandomRotation(0.1)\n",
    "])\n",
    "\n",
    "# Apply augmentation only on training batches.\n",
    "augmented_examples = []\n",
    "for batch_images, _ in ds_base.take(1):\n",
    "    augmented_batch = augmentation_layer(batch_images, training=True)\n",
    "    augmented_examples.append(augmented_batch)\n",
    "\n",
    "# Convert one augmented batch to numpy array.\n",
    "augmented_batch_np = augmented_examples[0].numpy()\n",
    "print(\"Augmented batch shape:\", augmented_batch_np.shape)\n",
    "\n",
    "# Verify rescaling is preserved after augmentation.\n",
    "print(\"Augmented min max:\", augmented_batch_np.min(), augmented_batch_np.max())\n",
    "\n",
    "# Show first original and augmented pixel center values.\n",
    "center_index = 14\n",
    "orig_center = x_rescaled[0, center_index, center_index, 0]\n",
    "aug_center = augmented_batch_np[0, center_index, center_index, 0]\n",
    "print(\"Original center pixel:\", float(orig_center))\n",
    "print(\"Augmented center pixel:\", float(aug_center))\n",
    "\n",
    "# Build a simple CNN model for demonstration.\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(28, 28, 1)),\n",
    "    tf.keras.layers.Conv2D(8, (3, 3), activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "# Compile model with basic optimizer and loss.\n",
    "model.compile(optimizer=\"adam\",\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# Prepare augmented training dataset pipeline.\n",
    "ds_train = tf.data.Dataset.from_tensor_slices((x_rescaled, y_small))\n",
    "ds_train = ds_train.shuffle(subset_size, seed=seed_value)\n",
    "ds_train = ds_train.map(lambda img, label: (augmentation_layer(img,\n",
    "                                                               training=True),\n",
    "                                            label))\n",
    "\n",
    "# Batch and prefetch for efficient training.\n",
    "ds_train = ds_train.batch(4).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Train briefly with silent verbose setting.\n",
    "history = model.fit(ds_train, epochs=2, verbose=0)\n",
    "\n",
    "# Print final training loss and accuracy metrics.\n",
    "final_loss = history.history[\"loss\"][-1]\n",
    "final_acc = history.history[\"accuracy\"][-1]\n",
    "print(\"Final training loss:\", round(final_loss, 4))\n",
    "print(\"Final training accuracy:\", round(final_acc, 4))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ed3861",
   "metadata": {},
   "source": [
    "### **3.3. Batching and Shuffling**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7df85d",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_06/Lecture_A/image_03_03.jpg?v=1769410125\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Batching splits data; batch size controls updates\n",
    ">* Batch size trades memory, speed, and generalization\n",
    "\n",
    ">* Shuffle images each epoch to randomize order\n",
    ">* Prevents sequence bias and makes batches representative\n",
    "\n",
    ">* Small, shuffled batches cause natural metric fluctuations\n",
    ">* Watch long-term trends to detect overfitting issues\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69dca42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Batching and Shuffling\n",
    "\n",
    "# This script shows batching and shuffling basics.\n",
    "# It uses a tiny MNIST subset for clarity.\n",
    "# We train a small CNN with silent logging.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Import TensorFlow and Keras utilities.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Select device based on GPU availability.\n",
    "physical_gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "if physical_gpus:\n",
    "    device_name = \"GPU\"\n",
    "else:\n",
    "    device_name = \"CPU\"\n",
    "\n",
    "# Inform which device will likely be used.\n",
    "print(\"Using device:\", device_name)\n",
    "\n",
    "# Load MNIST dataset from Keras datasets.\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Reduce dataset size for faster demonstration.\n",
    "train_samples = 4000\n",
    "test_samples = 1000\n",
    "\n",
    "# Slice arrays to keep only small subsets.\n",
    "x_train = x_train[:train_samples]\n",
    "y_train = y_train[:train_samples]\n",
    "\n",
    "# Slice test arrays similarly for quick evaluation.\n",
    "x_test = x_test[:test_samples]\n",
    "y_test = y_test[:test_samples]\n",
    "\n",
    "# Normalize pixel values to range [0, 1].\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_test = x_test.astype(\"float32\") / 255.0\n",
    "\n",
    "# Add channel dimension for Conv2D layers.\n",
    "x_train = np.expand_dims(x_train, axis=-1)\n",
    "x_test = np.expand_dims(x_test, axis=-1)\n",
    "\n",
    "# Confirm shapes are as expected.\n",
    "print(\"Train shape:\", x_train.shape)\n",
    "print(\"Test shape:\", x_test.shape)\n",
    "\n",
    "# Define batch size hyperparameter.\n",
    "batch_size = 64\n",
    "\n",
    "# Create base tf.data dataset from tensors.\n",
    "base_train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "\n",
    "# Demonstrate dataset without shuffling first.\n",
    "no_shuffle_ds = base_train_ds.batch(batch_size)\n",
    "\n",
    "# Take one batch and inspect label distribution.\n",
    "no_shuffle_batch = next(iter(no_shuffle_ds))\n",
    "print(\"First batch labels no shuffle:\", no_shuffle_batch[1][:10].numpy())\n",
    "\n",
    "# Now create shuffled and batched dataset.\n",
    "shuffle_buffer = train_samples\n",
    "shuffled_ds = base_train_ds.shuffle(shuffle_buffer, seed=SEED)\n",
    "\n",
    "# Batch after shuffling to mix classes.\n",
    "shuffled_batched_ds = shuffled_ds.batch(batch_size)\n",
    "\n",
    "# Inspect first shuffled batch label distribution.\n",
    "shuffled_batch = next(iter(shuffled_batched_ds))\n",
    "print(\"First batch labels shuffled:\", shuffled_batch[1][:10].numpy())\n",
    "\n",
    "# Prefetch to overlap preprocessing and training.\n",
    "train_ds = shuffled_batched_ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Prepare a small batched test dataset.\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "\n",
    "# Batch and prefetch test dataset for efficiency.\n",
    "test_ds = test_ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Build a simple CNN model for classification.\n",
    "model = keras.Sequential([\n",
    "    layers.Conv2D(16, (3, 3), activation=\"relu\", input_shape=(28, 28, 1)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(32, (3, 3), activation=\"relu\"),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation=\"relu\"),\n",
    "    layers.Dense(10, activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "# Compile model with optimizer, loss, and metrics.\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Train model briefly with silent verbose setting.\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=3,\n",
    "    validation_data=test_ds,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Extract final training and validation metrics.\n",
    "final_train_loss = history.history[\"loss\"][-1]\n",
    "final_train_acc = history.history[\"accuracy\"][-1]\n",
    "\n",
    "# Extract final validation loss and accuracy.\n",
    "final_val_loss = history.history[\"val_loss\"][-1]\n",
    "final_val_acc = history.history[\"val_accuracy\"][-1]\n",
    "\n",
    "# Print concise summary of training results.\n",
    "print(\"Final train loss:\", round(float(final_train_loss), 4))\n",
    "print(\"Final train accuracy:\", round(float(final_train_acc), 4))\n",
    "print(\"Final val loss:\", round(float(final_val_loss), 4))\n",
    "print(\"Final val accuracy:\", round(float(final_val_acc), 4))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1790f1c8",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**CNN Fundamentals**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62502f73",
   "metadata": {},
   "source": [
    "\n",
    "In this lecture, you learned to:\n",
    "- Construct basic CNN architectures using Conv2D, MaxPooling2D, and Dense layers in tf.keras. \n",
    "- Prepare image datasets with tf.data and image preprocessing utilities for CNN training. \n",
    "- Train and evaluate a CNN on a small image dataset, interpreting accuracy and loss metrics. \n",
    "\n",
    "In the next Lecture (Lecture B), we will go over 'Transfer Learning'"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
