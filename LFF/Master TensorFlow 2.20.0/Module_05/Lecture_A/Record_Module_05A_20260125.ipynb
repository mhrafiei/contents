{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef32f84d",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**tf.data Basics**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac5c56d",
   "metadata": {},
   "source": [
    ">Last update: 20260125.\n",
    "    \n",
    "By the end of this Lecture, you will be able to:\n",
    "- Create tf.data.Dataset pipelines from tensors, NumPy arrays, and TFRecord files. \n",
    "- Apply common tf.data transformations such as map, batch, shuffle, and prefetch. \n",
    "- Debug dataset shapes and types to ensure compatibility with Keras models. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c33ce5",
   "metadata": {},
   "source": [
    "## **1. Building TensorFlow Datasets**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc56d614",
   "metadata": {},
   "source": [
    "### **1.1. Creating Datasets from Tensors**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea2bc8c",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_05/Lecture_A/image_01_01.jpg?v=1769401250\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Wrap in-memory tensors as simple datasets\n",
    ">* Enable easy experimentation and later scalable pipelines\n",
    "\n",
    ">* Aligned tensor dimensions control dataset element pairing\n",
    ">* Consistent leading dimension keeps examples coherent for models\n",
    "\n",
    ">* Datasets expose shape and dtype problems early\n",
    ">* In-memory datasets validate data matches model expectations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72a27fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Creating Datasets from Tensors\n",
    "\n",
    "# This script shows tf.data basics with tensors.\n",
    "# You will create simple datasets from memory.\n",
    "# Shapes and dtypes are checked for safety.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import TensorFlow and NumPy for this lesson.\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Set seeds for deterministic behavior here.\n",
    "tf.random.set_seed(7)\n",
    "np.random.seed(7)\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Create small NumPy arrays for features and labels.\n",
    "features_np = np.array([[1.0], [2.0], [3.0], [4.0]], dtype=np.float32)\n",
    "labels_np = np.array([[0.0], [0.0], [1.0], [1.0]], dtype=np.float32)\n",
    "\n",
    "# Validate that leading dimensions of arrays match.\n",
    "assert features_np.shape[0] == labels_np.shape[0]\n",
    "\n",
    "# Convert NumPy arrays to TensorFlow tensors.\n",
    "features_tf = tf.convert_to_tensor(features_np, dtype=tf.float32)\n",
    "labels_tf = tf.convert_to_tensor(labels_np, dtype=tf.float32)\n",
    "\n",
    "# Show basic information about created tensors.\n",
    "print(\"Features tensor shape:\", features_tf.shape)\n",
    "print(\"Labels tensor shape:\", labels_tf.shape)\n",
    "\n",
    "# Build dataset directly from a tuple of tensors.\n",
    "base_dataset = tf.data.Dataset.from_tensor_slices((features_tf, labels_tf))\n",
    "\n",
    "# Inspect one raw element from the base dataset.\n",
    "for element in base_dataset.take(1):\n",
    "    print(\"One raw element:\", element)\n",
    "\n",
    "# Define a simple mapping function for scaling features.\n",
    "def scale_features(feature, label):\n",
    "    scaled_feature = feature / tf.constant(4.0, dtype=tf.float32)\n",
    "    return scaled_feature, label\n",
    "\n",
    "# Apply map, shuffle, batch, and prefetch transformations.\n",
    "train_dataset = (base_dataset\n",
    "                 .map(scale_features)\n",
    "                 .shuffle(buffer_size=4, seed=7)\n",
    "                 .batch(2)\n",
    "                 .prefetch(tf.data.AUTOTUNE))\n",
    "\n",
    "# Inspect shapes and dtypes from the transformed dataset.\n",
    "for batch_features, batch_labels in train_dataset.take(1):\n",
    "    print(\"Batch features shape:\", batch_features.shape)\n",
    "    print(\"Batch labels shape:\", batch_labels.shape)\n",
    "    print(\"Batch features dtype:\", batch_features.dtype)\n",
    "\n",
    "# Build a tiny Keras model compatible with dataset.\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(1,)),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "# Compile model with simple optimizer and loss.\n",
    "model.compile(optimizer=\"adam\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# Train model briefly using the dataset pipeline.\n",
    "history = model.fit(train_dataset,\n",
    "                    epochs=5,\n",
    "                    verbose=0)\n",
    "\n",
    "# Evaluate model on the same small dataset.\n",
    "loss, acc = model.evaluate(train_dataset, verbose=0)\n",
    "\n",
    "# Print final loss and accuracy in one line.\n",
    "print(\"Final loss and accuracy:\", float(loss), float(acc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2434c78a",
   "metadata": {},
   "source": [
    "### **1.2. TFRecord Input Pipelines**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f173ba3",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_05/Lecture_A/image_01_02.jpg?v=1769401293\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* TFRecord stores many serialized training examples efficiently\n",
    ">* Stream large datasets from disk for scalable training\n",
    "\n",
    ">* First read raw serialized examples from TFRecords\n",
    ">* Then parse strings into typed, shaped tensors\n",
    "\n",
    ">* Parsed TFRecords act like regular tensor datasets\n",
    ">* Supports scaling, sharding, and distributed training efficiently\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e3600f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - TFRecord Input Pipelines\n",
    "\n",
    "# This script shows basic TFRecord input pipelines.\n",
    "# It creates tiny TFRecord files and parses them.\n",
    "# Use this as a gentle TensorFlow dataset introduction.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import pathlib\n",
    "\n",
    "# Import TensorFlow and check version.\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "random.seed(7)\n",
    "tf.random.set_seed(7)\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Create a small temporary directory for TFRecords.\n",
    "base_dir = pathlib.Path(\"tfrecord_demo\")\n",
    "base_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Define a helper to create one Example proto.\n",
    "def make_example(features_tensor, label_int):\n",
    "    feature_dict = {\n",
    "        \"features\": tf.train.Feature(\n",
    "            float_list=tf.train.FloatList(value=features_tensor)\n",
    "        ),\n",
    "        \"label\": tf.train.Feature(\n",
    "            int64_list=tf.train.Int64List(value=[label_int])\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    example_proto = tf.train.Example(\n",
    "        features=tf.train.Features(feature=feature_dict)\n",
    "    )\n",
    "    return example_proto\n",
    "\n",
    "# Write a tiny TFRecord file with few examples.\n",
    "def write_tfrecord_file(path, num_examples, feature_dim):\n",
    "    with tf.io.TFRecordWriter(str(path)) as writer:\n",
    "        for i in range(num_examples):\n",
    "            features = [float(i)] * feature_dim\n",
    "            label = i % 2\n",
    "            example = make_example(features, label)\n",
    "            writer.write(example.SerializeToString())\n",
    "\n",
    "# Define file paths for two small shards.\n",
    "file_one = base_dir / \"data_part_1.tfrecord\"\n",
    "file_two = base_dir / \"data_part_2.tfrecord\"\n",
    "\n",
    "# Create two shards with few examples each.\n",
    "write_tfrecord_file(file_one, num_examples=4, feature_dim=3)\n",
    "write_tfrecord_file(file_two, num_examples=4, feature_dim=3)\n",
    "\n",
    "# List TFRecord files as string paths.\n",
    "file_pattern = str(base_dir / \"data_part_*.tfrecord\")\n",
    "files_dataset = tf.data.Dataset.list_files(file_pattern, shuffle=False)\n",
    "\n",
    "# Read records from multiple files using interleave.\n",
    "raw_dataset = files_dataset.interleave(\n",
    "    lambda f: tf.data.TFRecordDataset(f),\n",
    "    cycle_length=2,\n",
    "    num_parallel_calls=tf.data.AUTOTUNE,\n",
    ")\n",
    "\n",
    "# Define the feature description for parsing.\n",
    "feature_description = {\n",
    "    \"features\": tf.io.FixedLenFeature([3], tf.float32),\n",
    "    \"label\": tf.io.FixedLenFeature([], tf.int64),\n",
    "}\n",
    "\n",
    "# Create a parsing function for serialized examples.\n",
    "def parse_example(serialized_example):\n",
    "    parsed = tf.io.parse_single_example(\n",
    "        serialized_example,\n",
    "        feature_description,\n",
    "    )\n",
    "    features = parsed[\"features\"]\n",
    "    label = parsed[\"label\"]\n",
    "    return features, label\n",
    "\n",
    "# Map parsing over the raw dataset.\n",
    "parsed_dataset = raw_dataset.map(\n",
    "    parse_example,\n",
    "    num_parallel_calls=tf.data.AUTOTUNE,\n",
    ")\n",
    "\n",
    "# Shuffle, batch, and prefetch for efficiency.\n",
    "final_dataset = parsed_dataset.shuffle(8).batch(2).prefetch(1)\n",
    "\n",
    "# Inspect one batch to understand shapes and types.\n",
    "for batch_features, batch_labels in final_dataset.take(1):\n",
    "    print(\"Batch features shape:\", batch_features.shape)\n",
    "    print(\"Batch labels shape:\", batch_labels.shape)\n",
    "    print(\"Batch features dtype:\", batch_features.dtype)\n",
    "    print(\"Batch labels dtype:\", batch_labels.dtype)\n",
    "\n",
    "# Show the actual numeric values for clarity.\n",
    "print(\"Example batch features:\", batch_features.numpy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b053868",
   "metadata": {},
   "source": [
    "### **1.3. Listing Dataset Files**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9002af2",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_05/Lecture_A/image_01_03.jpg?v=1769401331\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Create datasets from file path lists only\n",
    ">* Stream, order, and label files efficiently during training\n",
    "\n",
    ">* Use filename patterns to automatically list files\n",
    ">* Treat paths as dynamic, filterable views of storage\n",
    "\n",
    ">* File-path datasets feed parsers for each file\n",
    ">* Separation simplifies debugging, storage changes, and scaling\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca14d0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Listing Dataset Files\n",
    "\n",
    "# This script shows how to list dataset files.\n",
    "# We focus on tf.data and file patterns.\n",
    "# Run cells sequentially to follow the flow.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import TensorFlow and supporting modules.\n",
    "import os\n",
    "import pathlib\n",
    "import tensorflow as tf\n",
    "\n",
    "# Print TensorFlow version for quick verification.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Create a small temporary root directory.\n",
    "root_dir = pathlib.Path(\"demo_data_root\")\n",
    "root_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Define two subdirectories to mimic class folders.\n",
    "class_names = [\"cats\", \"dogs\"]\n",
    "for name in class_names:\n",
    "    (root_dir / name).mkdir(exist_ok=True)\n",
    "\n",
    "# Create a helper function to write tiny text files.\n",
    "def write_dummy_file(path: pathlib.Path, text: str) -> None:\n",
    "    path.write_text(text, encoding=\"utf-8\")\n",
    "\n",
    "# Populate each folder with a few tiny files.\n",
    "for label in class_names:\n",
    "    for index in range(3):\n",
    "        file_path = root_dir / label / f\"sample_{index}.txt\"\n",
    "        write_dummy_file(file_path, f\"Dummy {label} file {index}.\")\n",
    "\n",
    "# Show the directory tree using pathlib iteration.\n",
    "all_paths = sorted(root_dir.rglob(\"*.txt\"))\n",
    "print(\"Number of created files:\", len(all_paths))\n",
    "\n",
    "# Print a few example paths for orientation.\n",
    "for path in all_paths[:4]:\n",
    "    print(\"Example file:\", path)\n",
    "\n",
    "# Build a file pattern that matches all text files.\n",
    "pattern_all = str(root_dir / \"*\" / \"*.txt\")\n",
    "print(\"Pattern for all files:\", pattern_all)\n",
    "\n",
    "# Create a dataset from the file pattern.\n",
    "files_ds_all = tf.data.Dataset.list_files(pattern_all, shuffle=False)\n",
    "\n",
    "# Inspect the first few elements from the dataset.\n",
    "print(\"Listing all dataset file paths:\")\n",
    "for path_tensor in files_ds_all.take(4):\n",
    "    print(\"Path element:\", path_tensor.numpy().decode(\"utf-8\"))\n",
    "\n",
    "# Build a pattern that only matches cat files.\n",
    "pattern_cats = str(root_dir / \"cats\" / \"*.txt\")\n",
    "print(\"Pattern for cat files:\", pattern_cats)\n",
    "\n",
    "# Create a dataset that lists only cat file paths.\n",
    "files_ds_cats = tf.data.Dataset.list_files(pattern_cats, shuffle=False)\n",
    "\n",
    "# Confirm that only cat paths are included.\n",
    "print(\"Listing only cat file paths:\")\n",
    "for path_tensor in files_ds_cats.take(3):\n",
    "    print(\"Cat path:\", path_tensor.numpy().decode(\"utf-8\"))\n",
    "\n",
    "# Show the dtype and shape of a dataset element.\n",
    "first_path = next(iter(files_ds_all))\n",
    "print(\"Element dtype:\", first_path.dtype, \"shape:\", first_path.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e20a638",
   "metadata": {},
   "source": [
    "## **2. Core Dataset Transformations**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbaa857",
   "metadata": {},
   "source": [
    "### **2.1. Mapping for Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd00a36",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_05/Lecture_A/image_02_01.jpg?v=1769401370\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Map applies preprocessing function to each example\n",
    ">* Transforms raw stored data into model-ready batches\n",
    "\n",
    ">* Mapping centralizes complex, domain-specific preprocessing steps\n",
    ">* Ensures every dataset example is processed consistently\n",
    "\n",
    ">* Parallel mapping boosts preprocessing speed and throughput\n",
    ">* Fused with batching, prefetching for scalable pipelines\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda28677",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Mapping for Preprocessing\n",
    "\n",
    "# This script shows tf.data mapping preprocessing.\n",
    "# It focuses on simple numeric feature scaling.\n",
    "# You can run it directly inside Google Colab.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required TensorFlow module.\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set global random seed for determinism.\n",
    "tf.random.set_seed(7)\n",
    "\n",
    "# Print TensorFlow version in one line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Create small numeric feature tensor.\n",
    "features = tf.constant([[1.0, 10.0], [2.0, 20.0], [3.0, 30.0]])\n",
    "\n",
    "# Create small numeric label tensor.\n",
    "labels = tf.constant([[0.0], [1.0], [0.0]])\n",
    "\n",
    "# Validate shapes before building dataset.\n",
    "assert features.shape[0] == labels.shape[0]\n",
    "\n",
    "# Build dataset from tensor slices.\n",
    "base_ds = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "\n",
    "# Define preprocessing mapping function.\n",
    "def preprocess_example(feature, label):\n",
    "    # Compute feature mean and standard deviation.\n",
    "    mean = tf.reduce_mean(feature)\n",
    "    std = tf.math.reduce_std(feature)\n",
    "\n",
    "    # Avoid division by zero using epsilon.\n",
    "    eps = tf.constant(1e-6, dtype=feature.dtype)\n",
    "\n",
    "    # Normalize features to zero mean unit variance.\n",
    "    norm_feature = (feature - mean) / (std + eps)\n",
    "\n",
    "    # Return normalized feature and original label.\n",
    "    return norm_feature, label\n",
    "\n",
    "# Apply mapping with num_parallel_calls autotune.\n",
    "mapped_ds = base_ds.map(preprocess_example, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "# Shuffle dataset with small buffer size.\n",
    "shuffled_ds = mapped_ds.shuffle(buffer_size=3, seed=7)\n",
    "\n",
    "# Batch dataset into small batches.\n",
    "batched_ds = shuffled_ds.batch(2, drop_remainder=False)\n",
    "\n",
    "# Prefetch to overlap preprocessing and consumption.\n",
    "final_ds = batched_ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Iterate over final dataset and inspect shapes.\n",
    "for batch_features, batch_labels in final_ds:\n",
    "    print(\"Batch features shape:\", batch_features.shape)\n",
    "    print(\"Batch labels shape:\", batch_labels.shape)\n",
    "    print(\"First batch features:\", batch_features.numpy())\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2511f8",
   "metadata": {},
   "source": [
    "### **2.2. Efficient Batching Techniques**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c16858",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_05/Lecture_A/image_02_02.jpg?v=1769401402\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Batching groups many examples for each step\n",
    ">* Improves hardware efficiency and stabilizes training gradients\n",
    "\n",
    ">* Batch size trades speed, memory, and updates\n",
    ">* Treat batch size as tunable, hardware-aware hyperparameter\n",
    "\n",
    ">* Decide how to handle leftover small batches\n",
    ">* Order batching and preprocessing to balance efficiency\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f107adb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Efficient Batching Techniques\n",
    "\n",
    "# This script demonstrates efficient batching techniques.\n",
    "# It uses tf.data to build simple datasets.\n",
    "# Focus on batch size and drop remainder behavior.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required TensorFlow module.\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set a deterministic global random seed.\n",
    "tf.random.set_seed(7)\n",
    "\n",
    "# Print TensorFlow version briefly.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Create a small tensor of feature values.\n",
    "features = tf.range(start=0, limit=12, delta=1, dtype=tf.int32)\n",
    "\n",
    "# Create matching labels as simple multiples.\n",
    "labels = features * 10\n",
    "\n",
    "# Stack features and labels into a dataset.\n",
    "base_ds = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "\n",
    "# Show dataset element shape and type.\n",
    "for elem in base_ds.take(1):\n",
    "    print(\"Single element shapes:\", elem[0].shape, elem[1].shape)\n",
    "\n",
    "# Define a small helper to describe batches.\n",
    "def describe_batches(ds, name):\n",
    "    print(\"\\n\", name)\n",
    "    for batch in ds:\n",
    "        x, y = batch\n",
    "        print(\"batch shape:\", x.shape, \"labels shape:\", y.shape)\n",
    "\n",
    "# Create batches without dropping the remainder.\n",
    "no_drop_ds = base_ds.batch(batch_size=5, drop_remainder=False)\n",
    "\n",
    "# Describe batches when keeping the final smaller batch.\n",
    "describe_batches(no_drop_ds, \"No drop_remainder (keep last small batch)\")\n",
    "\n",
    "# Create batches while dropping the final smaller batch.\n",
    "drop_ds = base_ds.batch(batch_size=5, drop_remainder=True)\n",
    "\n",
    "# Describe batches when forcing all batches equal sized.\n",
    "describe_batches(drop_ds, \"With drop_remainder=True (fixed batch shapes)\")\n",
    "\n",
    "# Prefetch to overlap data preparation and consumption.\n",
    "prefetch_ds = drop_ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "# Take one batch to confirm shapes stay consistent.\n",
    "for batch in prefetch_ds.take(1):\n",
    "    x_batch, y_batch = batch\n",
    "    print(\"\\nPrefetched batch shape:\", x_batch.shape, y_batch.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70756999",
   "metadata": {},
   "source": [
    "### **2.3. Efficient Shuffle and Repeat**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80ff3be",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_05/Lecture_A/image_02_03.jpg?v=1769401435\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Shuffle randomizes example order to avoid patterns\n",
    ">* Repeat cycles data for many randomized epochs\n",
    "\n",
    ">* Balance shuffle buffer size with memory limits\n",
    ">* Place repeat after shuffle for varied epochs\n",
    "\n",
    ">* Shuffle, then repeat, then batch, then prefetch\n",
    ">* Respect sequence boundaries while optimizing randomness and efficiency\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2cbd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Efficient Shuffle and Repeat\n",
    "\n",
    "# This script demonstrates efficient shuffle and repeat.\n",
    "# It uses a tiny synthetic dataset for clarity.\n",
    "# Run all cells sequentially inside Google Colab.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import TensorFlow and NumPy for data handling.\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Set deterministic seeds for reproducible shuffling.\n",
    "np.random.seed(7)\n",
    "tf.random.set_seed(7)\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Create a small NumPy array of feature values.\n",
    "features = np.arange(12, dtype=np.float32).reshape(6, 2)\n",
    "\n",
    "# Create simple labels equal to row indices.\n",
    "labels = np.arange(6, dtype=np.int32)\n",
    "\n",
    "# Validate shapes before building the dataset.\n",
    "assert features.shape[0] == labels.shape[0]\n",
    "\n",
    "# Build a base dataset from tensors.\n",
    "base_ds = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "\n",
    "# Define a tiny preprocessing map function.\n",
    "def scale_features(x, y):\n",
    "    x = x / 10.0\n",
    "    return x, y\n",
    "\n",
    "# Apply the map transformation to scale features.\n",
    "mapped_ds = base_ds.map(scale_features)\n",
    "\n",
    "# Build a pipeline with shuffle then repeat then batch.\n",
    "shuffle_then_repeat = (\n",
    "    mapped_ds.shuffle(buffer_size=6, seed=7, reshuffle_each_iteration=True)\n",
    "    .repeat(2)\n",
    "    .batch(3)\n",
    ")\n",
    "\n",
    "# Build a pipeline with repeat then shuffle then batch.\n",
    "repeat_then_shuffle = (\n",
    "    mapped_ds.repeat(2)\n",
    "    .shuffle(buffer_size=6, seed=7, reshuffle_each_iteration=True)\n",
    "    .batch(3)\n",
    ")\n",
    "\n",
    "# Helper function to collect one epoch of batches.\n",
    "def collect_batches(ds, num_batches):\n",
    "    batches = []\n",
    "    for batch_index, (x_batch, y_batch) in enumerate(ds):\n",
    "        batches.append((batch_index, y_batch.numpy().tolist()))\n",
    "        if batch_index + 1 >= num_batches:\n",
    "            break\n",
    "    return batches\n",
    "\n",
    "# Collect a few batches from shuffle then repeat pipeline.\n",
    "str_shuffle_repeat = collect_batches(shuffle_then_repeat, num_batches=4)\n",
    "\n",
    "# Collect a few batches from repeat then shuffle pipeline.\n",
    "str_repeat_shuffle = collect_batches(repeat_then_shuffle, num_batches=4)\n",
    "\n",
    "# Print results to compare ordering patterns.\n",
    "print(\"\\nBatches with shuffle().repeat():\")\n",
    "for idx, labels_list in str_shuffle_repeat:\n",
    "    print(\"batch\", idx, \"labels\", labels_list)\n",
    "\n",
    "# Print second configuration ordering for contrast.\n",
    "print(\"\\nBatches with repeat().shuffle():\")\n",
    "for idx, labels_list in str_repeat_shuffle:\n",
    "    print(\"batch\", idx, \"labels\", labels_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7684b3",
   "metadata": {},
   "source": [
    "## **3. Efficient Dataset Inspection**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beda45e9",
   "metadata": {},
   "source": [
    "### **3.1. Prefetching for Overlap**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c1a12b",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_05/Lecture_A/image_03_01.jpg?v=1769401480\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Prefetching overlaps data loading with model computation\n",
    ">* Ensure prefetched batches keep consistent shapes and types\n",
    "\n",
    ">* Prefetch keeps the data conveyor belt full\n",
    ">* Verify shapes and dtypes stay unchanged with prefetch\n",
    "\n",
    ">* Place prefetch after map, shuffle, batch\n",
    ">* Disable prefetch to trace shape, type bugs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac16db02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Prefetching for Overlap\n",
    "\n",
    "# This script shows tf.data prefetching basics.\n",
    "# We focus on shapes types and Keras compatibility.\n",
    "# Run cells to inspect batches before and after prefetch.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import TensorFlow and NumPy for this example.\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Set deterministic seeds for reproducible behavior.\n",
    "tf.random.set_seed(7)\n",
    "np.random.seed(7)\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Create small synthetic feature and label arrays.\n",
    "features = np.random.rand(12, 4).astype(\"float32\")\n",
    "labels = np.random.randint(0, 2, size=(12, 1)).astype(\"int32\")\n",
    "\n",
    "# Validate shapes before building the dataset.\n",
    "assert features.shape[0] == labels.shape[0]\n",
    "\n",
    "# Build a dataset from the NumPy arrays.\n",
    "base_ds = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "\n",
    "# Shuffle with fixed buffer and seed for stability.\n",
    "shuffled_ds = base_ds.shuffle(buffer_size=12, seed=7)\n",
    "\n",
    "# Batch the dataset to match model expectations.\n",
    "batched_ds = shuffled_ds.batch(4, drop_remainder=True)\n",
    "\n",
    "# Inspect one batch before adding prefetch.\n",
    "for batch_features, batch_labels in batched_ds.take(1):\n",
    "    print(\"Before prefetch shapes:\", batch_features.shape, batch_labels.shape)\n",
    "    print(\"Before prefetch dtypes:\", batch_features.dtype, batch_labels.dtype)\n",
    "\n",
    "# Add prefetch to overlap input work and compute.\n",
    "prefetched_ds = batched_ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Inspect one batch after adding prefetch.\n",
    "for p_features, p_labels in prefetched_ds.take(1):\n",
    "    print(\"After prefetch shapes:\", p_features.shape, p_labels.shape)\n",
    "    print(\"After prefetch dtypes:\", p_features.dtype, p_labels.dtype)\n",
    "\n",
    "# Define a tiny Keras model matching feature shape.\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(4,)),\n",
    "    tf.keras.layers.Dense(4, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "])\n",
    "\n",
    "# Compile the model with simple settings.\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")\n",
    "\n",
    "# Run a very short silent training step.\n",
    "history = model.fit(prefetched_ds, epochs=1, verbose=0)\n",
    "\n",
    "# Confirm that training completed without shape issues.\n",
    "print(\"Training finished with prefetching and stable shapes.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f744f6",
   "metadata": {},
   "source": [
    "### **3.2. Parallel Mapping Control**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eaeb2b7",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_05/Lecture_A/image_03_02.jpg?v=1769401516\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Parallel map controls speed and debugging difficulty\n",
    ">* Lower parallelism simplifies tracing shapes and dtypes\n",
    "\n",
    ">* Complex, resource-heavy map functions need careful parallelism\n",
    ">* Lower parallelism simplifies debugging, then safely increase\n",
    "\n",
    ">* Less parallelism gives clearer, ordered debugging logs\n",
    ">* Controlled mapping ensures consistent shapes across collaborators\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47743ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Parallel Mapping Control\n",
    "\n",
    "# This script demonstrates parallel mapping control.\n",
    "# It focuses on dataset shapes and types debugging.\n",
    "# Run cells sequentially in a fresh Colab runtime.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import TensorFlow and NumPy for this example.\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Print TensorFlow version for reproducibility.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Set global random seeds for deterministic behavior.\n",
    "tf.random.set_seed(7)\n",
    "np.random.seed(7)\n",
    "\n",
    "# Create small NumPy arrays for features and labels.\n",
    "features_np = np.random.rand(8, 4).astype(\"float32\")\n",
    "labels_np = np.random.randint(0, 2, size=(8, 1)).astype(\"int32\")\n",
    "\n",
    "# Build a dataset from the NumPy arrays.\n",
    "base_ds = tf.data.Dataset.from_tensor_slices((features_np, labels_np))\n",
    "\n",
    "# Define a mapping function with shape and type logging.\n",
    "def debug_map(example_features, example_labels):\n",
    "    # Print shapes and dtypes for one example only.\n",
    "    tf.print(\"map input shapes:\", tf.shape(example_features),\n",
    "             tf.shape(example_labels))\n",
    "    tf.print(\"map input dtypes:\", example_features.dtype,\n",
    "             example_labels.dtype)\n",
    "\n",
    "    # Add a simple transformation to features.\n",
    "    scaled_features = example_features * tf.constant(2.0, dtype=tf.float32)\n",
    "\n",
    "    # Ensure labels are int32 for model compatibility.\n",
    "    safe_labels = tf.cast(example_labels, tf.int32)\n",
    "\n",
    "    # Print shapes after transformation for verification.\n",
    "    tf.print(\"map output shapes:\", tf.shape(scaled_features),\n",
    "             tf.shape(safe_labels))\n",
    "\n",
    "    return scaled_features, safe_labels\n",
    "\n",
    "# Create a dataset with sequential mapping for debugging.\n",
    "debug_ds = base_ds.map(debug_map, num_parallel_calls=1)\n",
    "\n",
    "# Batch and prefetch to match simple Keras model input.\n",
    "debug_ds = debug_ds.batch(4).prefetch(1)\n",
    "\n",
    "# Inspect one batch to confirm shapes and types.\n",
    "for batch_features, batch_labels in debug_ds.take(1):\n",
    "    print(\"Batch features shape:\", batch_features.shape)\n",
    "    print(\"Batch labels shape:\", batch_labels.shape)\n",
    "\n",
    "# Build a tiny Keras model matching the dataset shapes.\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(4,)),\n",
    "    tf.keras.layers.Dense(4, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "])\n",
    "\n",
    "# Compile the model with simple binary classification settings.\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# Train briefly with verbose zero to avoid long logs.\n",
    "history = model.fit(debug_ds, epochs=1, verbose=0)\n",
    "\n",
    "# Now enable parallel mapping for performance after debugging.\n",
    "fast_ds = base_ds.map(debug_map, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "# Batch and prefetch the faster dataset for training.\n",
    "fast_ds = fast_ds.batch(4).prefetch(1)\n",
    "\n",
    "# Train again briefly on the faster parallel dataset.\n",
    "final_history = model.fit(fast_ds, epochs=1, verbose=0)\n",
    "\n",
    "# Print final batch shapes to confirm compatibility remains.\n",
    "for f_batch, l_batch in fast_ds.take(1):\n",
    "    print(\"Fast batch features shape:\", f_batch.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0ddbf4",
   "metadata": {},
   "source": [
    "### **3.3. Inspecting element_spec**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d75428a",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_05/Lecture_A/image_03_03.jpg?v=1769401561\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* element_spec shows dataset structure, shapes, dtypes\n",
    ">* Use it to ensure model-compatible inputs, outputs\n",
    "\n",
    ">* Transformations change dataset shapes and structures\n",
    ">* Inspect element_spec after steps to catch mismatches\n",
    "\n",
    ">* Element spec defines a contract with models\n",
    ">* Regular checks prevent late, hard-to-debug errors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a31feff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Inspecting element_spec\n",
    "\n",
    "# This script shows how to inspect element_spec.\n",
    "# It focuses on dataset shapes and types debugging.\n",
    "# Run cells in order to follow the explanation.\n",
    "\n",
    "# Optional TensorFlow install for some environments.\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import TensorFlow and NumPy for this example.\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Set a deterministic random seed for reproducibility.\n",
    "tf.random.set_seed(7)\n",
    "\n",
    "# Print TensorFlow version in a compact single line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Create small NumPy arrays for features and labels.\n",
    "features_np = np.arange(12, dtype=np.float32).reshape((6, 2))\n",
    "\n",
    "# Create integer labels aligned with the features.\n",
    "labels_np = np.array([0, 1, 0, 1, 0, 1], dtype=np.int32)\n",
    "\n",
    "# Build a dataset from the NumPy feature and label arrays.\n",
    "base_ds = tf.data.Dataset.from_tensor_slices((features_np, labels_np))\n",
    "\n",
    "# Print the element_spec of the base dataset.\n",
    "print(\"Base element_spec:\", base_ds.element_spec)\n",
    "\n",
    "# Shuffle the dataset with a small buffer size.\n",
    "shuffled_ds = base_ds.shuffle(buffer_size=6, seed=7)\n",
    "\n",
    "# Batch the shuffled dataset into small batches.\n",
    "batched_ds = shuffled_ds.batch(batch_size=2, drop_remainder=False)\n",
    "\n",
    "# Prefetch to overlap preprocessing and model execution.\n",
    "final_ds = batched_ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "# Print the element_spec after batching and prefetching.\n",
    "print(\"Final element_spec:\", final_ds.element_spec)\n",
    "\n",
    "# Take one batch from the final dataset for inspection.\n",
    "for batch_features, batch_labels in final_ds.take(1):\n",
    "    # Print shapes and dtypes for the batch tensors.\n",
    "    print(\"Batch features shape:\", batch_features.shape)\n",
    "    print(\"Batch features dtype:\", batch_features.dtype)\n",
    "    print(\"Batch labels shape:\", batch_labels.shape)\n",
    "    print(\"Batch labels dtype:\", batch_labels.dtype)\n",
    "\n",
    "# Define a simple Keras model compatible with the dataset.\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(2,)),\n",
    "    tf.keras.layers.Dense(4, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "])\n",
    "\n",
    "# Compile the model with a binary classification configuration.\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")\n",
    "\n",
    "# Run a tiny silent training step to confirm compatibility.\n",
    "model.fit(final_ds, epochs=1, verbose=0, steps_per_epoch=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aedff56",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**tf.data Basics**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9078c9",
   "metadata": {},
   "source": [
    "\n",
    "In this lecture, you learned to:\n",
    "- Create tf.data.Dataset pipelines from tensors, NumPy arrays, and TFRecord files. \n",
    "- Apply common tf.data transformations such as map, batch, shuffle, and prefetch. \n",
    "- Debug dataset shapes and types to ensure compatibility with Keras models. \n",
    "\n",
    "In the next Lecture (Lecture B), we will go over 'Advanced Pipelines'"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
