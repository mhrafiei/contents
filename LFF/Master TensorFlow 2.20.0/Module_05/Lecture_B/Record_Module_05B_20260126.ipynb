{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7f6c40f",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Advanced Pipelines**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b509bcb",
   "metadata": {},
   "source": [
    ">Last update: 20260126.\n",
    "    \n",
    "By the end of this Lecture, you will be able to:\n",
    "- Optimize tf.data pipelines using caching, interleave, and parallelism settings for large datasets. \n",
    "- Configure input pipelines that work efficiently with distributed training strategies. \n",
    "- Diagnose and resolve common tf.data performance issues such as slow startup or CPU bottlenecks. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc50dfa",
   "metadata": {},
   "source": [
    "## **1. Interleave and Cache**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea852166",
   "metadata": {},
   "source": [
    "### **1.1. Efficient File Interleaving**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91a0a7a",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_05/Lecture_B/image_01_01.jpg?v=1769404835\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Interleaving reads multiple files concurrently to hide latency\n",
    ">* This keeps batches flowing and hardware fully utilized\n",
    "\n",
    ">* Balance number of files and prefetch depth\n",
    ">* Tune interleaving empirically to avoid stalls, overload\n",
    "\n",
    ">* Use medium-sized shards to avoid file overhead\n",
    ">* Interleave shards to balance load and improve mixing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42a7050",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Efficient File Interleaving\n",
    "\n",
    "# This script demonstrates efficient file interleaving.\n",
    "# We compare sequential and interleaved tf.data pipelines.\n",
    "# Focus on throughput not model training details.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required TensorFlow and system modules.\n",
    "import os\n",
    "import time\n",
    "import tempfile\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set a global random seed for deterministic behavior.\n",
    "tf.random.set_seed(7)\n",
    "\n",
    "# Print TensorFlow version in one concise line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Create a temporary directory for our tiny dataset.\n",
    "base_dir = tempfile.mkdtemp(prefix=\"interleave_demo_\")\n",
    "\n",
    "# Define simple helper to create small text files.\n",
    "def create_text_file(path, num_lines, delay_factor):\n",
    "    with tf.io.gfile.GFile(path, \"w\") as f:\n",
    "        for i in range(num_lines):\n",
    "            f.write(f\"{delay_factor},{i}\\n\")\n",
    "\n",
    "# Generate a few small files with different artificial delays.\n",
    "num_files = 4\n",
    "lines_per_file = 20\n",
    "\n",
    "# Store file paths for later dataset creation.\n",
    "file_paths = []\n",
    "for idx in range(num_files):\n",
    "    file_path = os.path.join(base_dir, f\"file_{idx}.txt\")\n",
    "    create_text_file(file_path, lines_per_file, idx + 1)\n",
    "    file_paths.append(file_path)\n",
    "\n",
    "# Convert Python list of paths into a TensorFlow constant.\n",
    "file_paths_tensor = tf.constant(file_paths)\n",
    "\n",
    "# Define a parser that simulates slow and fast files.\n",
    "def parse_line_with_delay(line):\n",
    "    parts = tf.strings.split(line, \",\")\n",
    "    delay_factor = tf.strings.to_number(parts[0], tf.float32)\n",
    "    value = tf.strings.to_number(parts[1], tf.float32)\n",
    "    busy_wait_steps = tf.cast(delay_factor * 2000, tf.int32)\n",
    "    _ = tf.range(busy_wait_steps)\n",
    "    return value\n",
    "\n",
    "# Build a sequential pipeline reading one file at a time.\n",
    "def build_sequential_dataset():\n",
    "    ds_files = tf.data.Dataset.from_tensor_slices(file_paths_tensor)\n",
    "    ds_lines = ds_files.flat_map(\n",
    "        lambda path: tf.data.TextLineDataset(path)\n",
    "    )\n",
    "    ds_values = ds_lines.map(parse_line_with_delay, num_parallel_calls=1)\n",
    "    ds_values = ds_values.batch(16).prefetch(1)\n",
    "    return ds_values\n",
    "\n",
    "# Build an interleaved pipeline reading several files concurrently.\n",
    "def build_interleaved_dataset():\n",
    "    ds_files = tf.data.Dataset.from_tensor_slices(file_paths_tensor)\n",
    "    ds_interleaved = ds_files.interleave(\n",
    "        lambda path: tf.data.TextLineDataset(path),\n",
    "        cycle_length=4,\n",
    "        block_length=4,\n",
    "        num_parallel_calls=tf.data.AUTOTUNE,\n",
    "    )\n",
    "    ds_values = ds_interleaved.map(\n",
    "        parse_line_with_delay,\n",
    "        num_parallel_calls=tf.data.AUTOTUNE,\n",
    "    )\n",
    "    ds_values = ds_values.cache().batch(16).prefetch(1)\n",
    "    return ds_values\n",
    "\n",
    "# Utility to time how long it takes to iterate a dataset.\n",
    "def time_dataset(dataset, label):\n",
    "    start = time.time()\n",
    "    total_batches = 0\n",
    "    total_elements = 0\n",
    "    for batch in dataset:\n",
    "        total_batches += 1\n",
    "        total_elements += int(batch.shape[0])\n",
    "    duration = time.time() - start\n",
    "    print(\n",
    "        f\"{label}: {total_batches} batches, {total_elements} elements, {duration:.3f}s\",\n",
    "    )\n",
    "\n",
    "# Build both datasets and validate shapes before timing.\n",
    "sequential_ds = build_sequential_dataset()\n",
    "interleaved_ds = build_interleaved_dataset()\n",
    "\n",
    "# Take one batch from each dataset to confirm shapes.\n",
    "first_seq_batch = next(iter(sequential_ds))\n",
    "first_int_batch = next(iter(interleaved_ds))\n",
    "print(\"Sequential batch shape:\", first_seq_batch.shape)\n",
    "print(\"Interleaved batch shape:\", first_int_batch.shape)\n",
    "\n",
    "# Time full pass through each dataset to compare throughput.\n",
    "time_dataset(sequential_ds, \"Sequential pipeline\")\n",
    "time_dataset(interleaved_ds, \"Interleaved cached pipeline\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0f7907",
   "metadata": {},
   "source": [
    "### **1.2. Caching Memory vs Disk**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e41299",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_05/Lecture_B/image_01_02.jpg?v=1769404919\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* In-memory caching speeds up repeated dataset passes\n",
    ">* Must balance speed gains against limited RAM resources\n",
    "\n",
    ">* Disk caching is slower than RAM, but scalable\n",
    ">* Stores preprocessed data snapshot reusable across runs\n",
    "\n",
    ">* Choose cache type based on measured bottlenecks\n",
    ">* Combine memory, disk, and selective caching over time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a90c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Caching Memory vs Disk\n",
    "\n",
    "# This script compares memory and disk caching.\n",
    "# It uses a tiny synthetic image dataset.\n",
    "# Focus on tf data cache performance basics.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import time\n",
    "import pathlib\n",
    "\n",
    "# Import tensorflow and check version.\n",
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Set deterministic random seeds.\n",
    "SEED = 42\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# Define small synthetic image dataset size.\n",
    "NUM_IMAGES = 256\n",
    "IMAGE_SHAPE = (28, 28, 1)\n",
    "\n",
    "# Create a function to generate synthetic images.\n",
    "def make_synthetic_dataset(num_images, image_shape):\n",
    "    images = tf.random.uniform(\n",
    "        shape=(num_images,) + image_shape,\n",
    "        minval=0.0,\n",
    "        maxval=1.0,\n",
    "        dtype=tf.float32,\n",
    "        seed=SEED,\n",
    "    )\n",
    "\n",
    "    labels = tf.zeros((num_images,), dtype=tf.int32)\n",
    "\n",
    "    ds = tf.data.Dataset.from_tensor_slices((images, labels))\n",
    "    return ds\n",
    "\n",
    "# Create base dataset once.\n",
    "base_ds = make_synthetic_dataset(NUM_IMAGES, IMAGE_SHAPE)\n",
    "\n",
    "# Define a simple preprocessing function.\n",
    "@tf.function\n",
    "def preprocess(image, label):\n",
    "    image = tf.image.flip_left_right(image)\n",
    "    image = tf.image.random_brightness(image, 0.1, seed=SEED)\n",
    "    image = tf.image.per_image_standardization(image)\n",
    "    return image, label\n",
    "\n",
    "# Wrap preprocessing for tf data map.\n",
    "def map_preprocess(image, label):\n",
    "    image, label = preprocess(image, label)\n",
    "    return image, label\n",
    "\n",
    "# Define a helper to build a pipeline.\n",
    "def build_pipeline(cache_path=None, use_memory=True):\n",
    "    ds = base_ds.shuffle(NUM_IMAGES, seed=SEED)\n",
    "    ds = ds.map(map_preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    if use_memory:\n",
    "        ds = ds.cache()\n",
    "    else:\n",
    "        ds = ds.cache(cache_path)\n",
    "\n",
    "    ds = ds.batch(32)\n",
    "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "# Define a helper to time one full epoch.\n",
    "def time_one_epoch(dataset, label):\n",
    "    start = time.time()\n",
    "    num_batches = 0\n",
    "    for batch in dataset:\n",
    "        num_batches += 1\n",
    "    end = time.time()\n",
    "    print(label, \"batches:\", num_batches, \"time:\", round(end - start, 3))\n",
    "\n",
    "# Prepare a temporary cache directory path.\n",
    "cache_dir = pathlib.Path(\"./tf_cache_example\")\n",
    "cache_dir.mkdir(exist_ok=True)\n",
    "cache_file = str(cache_dir / \"synthetic_cache.tfdata\")\n",
    "\n",
    "# Build memory cached pipeline.\n",
    "memory_ds = build_pipeline(use_memory=True)\n",
    "\n",
    "# Build disk cached pipeline.\n",
    "disk_ds = build_pipeline(cache_path=cache_file, use_memory=False)\n",
    "\n",
    "# Time first and second epoch for memory cache.\n",
    "print(\"\\nMemory cache first epoch (fills cache):\")\n",
    "time_one_epoch(memory_ds, \"memory_epoch1\")\n",
    "\n",
    "print(\"Memory cache second epoch (uses RAM):\")\n",
    "time_one_epoch(memory_ds, \"memory_epoch2\")\n",
    "\n",
    "# Time first and second epoch for disk cache.\n",
    "print(\"\\nDisk cache first epoch (writes cache file):\")\n",
    "time_one_epoch(disk_ds, \"disk_epoch1\")\n",
    "\n",
    "print(\"Disk cache second epoch (reads from disk):\")\n",
    "time_one_epoch(disk_ds, \"disk_epoch2\")\n",
    "\n",
    "# Clean up small cache file if it exists.\n",
    "if os.path.exists(cache_file):\n",
    "    os.remove(cache_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfd57e5",
   "metadata": {},
   "source": [
    "### **1.3. Randomness Performance Tradeoffs**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1397ae04",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_05/Lecture_B/image_01_03.jpg?v=1769404959\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Interleaving and parallel reads increase data randomness\n",
    ">* Caching too early freezes order, hurting generalization\n",
    "\n",
    ">* Stronger randomness increases CPU, memory, and I O\n",
    ">* Tuning caching and shuffling balances speed and diversity\n",
    "\n",
    ">* Place caching and interleaving carefully to balance randomness\n",
    ">* Start with high randomness, then tune for speed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949767e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Randomness Performance Tradeoffs\n",
    "\n",
    "# This script compares randomness and performance tradeoffs.\n",
    "# It uses a tiny synthetic dataset with tf.data pipelines.\n",
    "# Focus on interleave shuffle cache and timing behavior.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Import tensorflow and check version.\n",
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "\n",
    "# Create a small synthetic dataset of file identifiers.\n",
    "num_files = 6\n",
    "examples_per_file = 20\n",
    "\n",
    "# Build a list of fake file ids.\n",
    "file_ids = list(range(num_files))\n",
    "print(\"Number of fake files:\", len(file_ids))\n",
    "\n",
    "# Define a function that simulates reading a file.\n",
    "def read_fake_file(file_id):\n",
    "    # Create a range of example indices per file.\n",
    "    ds = tf.data.Dataset.range(examples_per_file)\n",
    "    # Tag each example with its file id.\n",
    "    ds = ds.map(lambda x: (file_id, x), num_parallel_calls=1)\n",
    "    return ds\n",
    "\n",
    "# Helper to build a pipeline with given options.\n",
    "def make_pipeline(shuffle_files, interleave_parallel, cache_before_shuffle):\n",
    "    # Start from dataset of file ids.\n",
    "    ds = tf.data.Dataset.from_tensor_slices(file_ids)\n",
    "    # Optionally shuffle file order.\n",
    "    if shuffle_files:\n",
    "        ds = ds.shuffle(buffer_size=num_files, seed=SEED)\n",
    "\n",
    "    # Interleave examples from multiple files.\n",
    "    ds = ds.interleave(\n",
    "        lambda fid: read_fake_file(fid),\n",
    "        cycle_length=interleave_parallel,\n",
    "        num_parallel_calls=interleave_parallel,\n",
    "        deterministic=True,\n",
    "    )\n",
    "\n",
    "    # Optionally cache before shuffling examples.\n",
    "    if cache_before_shuffle:\n",
    "        ds = ds.cache()\n",
    "\n",
    "    # Shuffle individual examples for randomness.\n",
    "    ds = ds.shuffle(\n",
    "        buffer_size=num_files * examples_per_file,\n",
    "        seed=SEED,\n",
    "        reshuffle_each_iteration=True,\n",
    "    )\n",
    "\n",
    "    # Batch a few examples for inspection.\n",
    "    ds = ds.batch(8, drop_remainder=False)\n",
    "    return ds\n",
    "\n",
    "# Helper to run one epoch and measure time.\n",
    "def run_epoch(ds, label, max_batches=5):\n",
    "    # Record start time for this configuration.\n",
    "    start = time.time()\n",
    "    first_batches = []\n",
    "\n",
    "    # Iterate over a few batches only.\n",
    "    for batch_index, batch in enumerate(ds):\n",
    "        if batch_index >= max_batches:\n",
    "            break\n",
    "        first_batches.append(batch[0])\n",
    "\n",
    "    # Compute elapsed time in milliseconds.\n",
    "    elapsed = (time.time() - start) * 1000.0\n",
    "\n",
    "    # Print summary with first batch source files.\n",
    "    file_ids_batch = [int(b[0].numpy()) for b in first_batches]\n",
    "    print(label, \"time_ms=\", round(elapsed, 2), \"first_files=\", file_ids_batch)\n",
    "\n",
    "# Build three pipelines with different tradeoffs.\n",
    "pipe_strong_random = make_pipeline(\n",
    "    shuffle_files=True,\n",
    "    interleave_parallel=4,\n",
    "    cache_before_shuffle=False,\n",
    ")\n",
    "\n",
    "pipe_cached_early = make_pipeline(\n",
    "    shuffle_files=True,\n",
    "    interleave_parallel=4,\n",
    "    cache_before_shuffle=True,\n",
    ")\n",
    "\n",
    "pipe_low_parallel = make_pipeline(\n",
    "    shuffle_files=False,\n",
    "    interleave_parallel=1,\n",
    "    cache_before_shuffle=False,\n",
    ")\n",
    "\n",
    "# Run two epochs to compare randomness and speed.\n",
    "for epoch in range(2):\n",
    "    print(\"\\nEpoch\", epoch)\n",
    "    run_epoch(pipe_strong_random, \"strong_random\", max_batches=3)\n",
    "    run_epoch(pipe_cached_early, \"cached_early\", max_batches=3)\n",
    "    run_epoch(pipe_low_parallel, \"low_parallel\", max_batches=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644f5687",
   "metadata": {},
   "source": [
    "## **2. Parallel Input Optimization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19b1e16",
   "metadata": {},
   "source": [
    "### **2.1. Automatic Parallel Calls**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3975fd",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_05/Lecture_B/image_02_01.jpg?v=1769405007\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Automatically runs preprocessing on many elements concurrently\n",
    ">* Keeps multiple devices busy by overlapping data preparation\n",
    "\n",
    ">* Balances throughput and resource use across replicas\n",
    ">* Adapts parallelism to CPU capacity and workload\n",
    "\n",
    ">* Same pipeline scales automatically from single to multi-device\n",
    ">* System tunes parallelism, improving utilization and consistency\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27b5254",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Automatic Parallel Calls\n",
    "\n",
    "# This script shows automatic parallel calls simply.\n",
    "# We build a small tf.data pipeline for images.\n",
    "# Then we compare auto and manual parallel calls.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Print TensorFlow version briefly.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Detect available devices for context.\n",
    "physical_gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "print(\"GPUs available:\", len(physical_gpus))\n",
    "\n",
    "# Create a simple distributed strategy safely.\n",
    "if len(physical_gpus) > 1:\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "else:\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "\n",
    "# Set global random seeds deterministically.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Load MNIST dataset using Keras helper.\n",
    "(mnist_x_train, mnist_y_train), _ = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Select a small subset for quick runtime.\n",
    "subset_size = 4096\n",
    "mnist_x_train = mnist_x_train[:subset_size]\n",
    "mnist_y_train = mnist_y_train[:subset_size]\n",
    "\n",
    "# Validate shapes before building dataset.\n",
    "assert mnist_x_train.shape[0] == mnist_y_train.shape[0]\n",
    "\n",
    "# Define a simple preprocessing function.\n",
    "\n",
    "\n",
    "def preprocess(image, label):\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    image = tf.expand_dims(image, axis=-1)\n",
    "    image = tf.image.resize(image, (32, 32))\n",
    "    image = tf.image.random_flip_left_right(image, seed=seed_value)\n",
    "    return image, label\n",
    "\n",
    "# Build a base dataset from tensors.\n",
    "base_ds = tf.data.Dataset.from_tensor_slices((mnist_x_train, mnist_y_train))\n",
    "\n",
    "# Shuffle and batch the dataset elements.\n",
    "base_ds = base_ds.shuffle(buffer_size=subset_size, seed=seed_value)\n",
    "base_ds = base_ds.batch(64)\n",
    "\n",
    "# Cache to avoid repeated decoding work.\n",
    "base_ds = base_ds.cache()\n",
    "\n",
    "# Create dataset with automatic parallel calls.\n",
    "auto_ds = base_ds.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "auto_ds = auto_ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Create dataset with manual single parallel call.\n",
    "manual_ds = base_ds.map(preprocess, num_parallel_calls=1)\n",
    "manual_ds = manual_ds.prefetch(1)\n",
    "\n",
    "# Define a simple model inside strategy scope.\n",
    "with strategy.scope():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(32, 32, 1)),\n",
    "        tf.keras.layers.Conv2D(16, 3, activation=\"relu\"),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(10, activation=\"softmax\"),\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "# Function to time one short training run.\n",
    "\n",
    "\n",
    "def time_training(dataset, description):\n",
    "    start = time.time()\n",
    "    history = model.fit(\n",
    "        dataset,\n",
    "        epochs=1,\n",
    "        steps_per_epoch=20,\n",
    "        verbose=0,\n",
    "    )\n",
    "    end = time.time()\n",
    "    duration = end - start\n",
    "    final_acc = history.history[\"accuracy\"][0]\n",
    "    print(description, \"time:\", round(duration, 3), \"sec\",\n",
    "          \"acc:\", round(float(final_acc), 3))\n",
    "\n",
    "# Warm up model once to stabilize timing.\n",
    "_ = model.fit(auto_ds.take(5), epochs=1, verbose=0)\n",
    "\n",
    "# Time training with manual parallel calls.\n",
    "time_training(manual_ds, \"Manual parallel calls\")\n",
    "\n",
    "# Time training with automatic parallel calls.\n",
    "time_training(auto_ds, \"Automatic parallel calls\")\n",
    "\n",
    "# Show one batch shape to confirm pipeline.\n",
    "for batch_images, batch_labels in auto_ds.take(1):\n",
    "    print(\"Batch images shape:\", batch_images.shape,\n",
    "          \"Batch labels shape:\", batch_labels.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07089050",
   "metadata": {},
   "source": [
    "### **2.2. Parallel Mapping Strategies**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f98f936",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_05/Lecture_B/image_02_02.jpg?v=1769405104\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Parallel mapping spreads heavy preprocessing across resources\n",
    ">* Proper parallelism keeps GPUs TPUs busy continuously\n",
    "\n",
    ">* Match parallelism to preprocessing cost and variability\n",
    ">* Scale parallel mapping with devices to meet demand\n",
    "\n",
    ">* Balance parallel mapping with other pipeline stages\n",
    ">* Profile workloads and tune parallelism for each system\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e2736e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Parallel Mapping Strategies\n",
    "\n",
    "# This script shows parallel mapping strategies.\n",
    "# It compares sequential and parallel map performance.\n",
    "# It uses a tiny synthetic image dataset.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required libraries safely.\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Import tensorflow and check version.\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set global random seeds for determinism.\n",
    "SEED_VALUE = 42\n",
    "random.seed(SEED_VALUE)\n",
    "\n",
    "# Set numpy and tensorflow seeds deterministically.\n",
    "import numpy as np\n",
    "np.random.seed(SEED_VALUE)\n",
    "\n",
    "tf.random.set_seed(SEED_VALUE)\n",
    "\n",
    "# Print tensorflow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Detect available devices for brief context.\n",
    "physical_gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "\n",
    "# Print a compact device summary line.\n",
    "print(\"GPUs available:\", len(physical_gpus))\n",
    "\n",
    "# Define tiny synthetic image dataset parameters.\n",
    "NUM_IMAGES = 2048\n",
    "IMAGE_HEIGHT = 28\n",
    "\n",
    "# Define remaining image shape parameters.\n",
    "IMAGE_WIDTH = 28\n",
    "NUM_CHANNELS = 1\n",
    "\n",
    "# Create small random image array deterministically.\n",
    "images = np.random.randint(\n",
    "    0,\n",
    "    256,\n",
    "    size=(NUM_IMAGES, IMAGE_HEIGHT, IMAGE_WIDTH, NUM_CHANNELS),\n",
    "    dtype=np.uint8,\n",
    ")\n",
    "\n",
    "# Create simple labels for completeness.\n",
    "labels = np.random.randint(\n",
    "    0,\n",
    "    10,\n",
    "    size=(NUM_IMAGES,),\n",
    "    dtype=np.int32,\n",
    ")\n",
    "\n",
    "# Wrap numpy arrays into a tf.data dataset.\n",
    "base_ds = tf.data.Dataset.from_tensor_slices((images, labels))\n",
    "\n",
    "# Shuffle and batch to mimic training input.\n",
    "base_ds = base_ds.shuffle(NUM_IMAGES, seed=SEED_VALUE)\n",
    "\n",
    "# Define global batch size for distributed training.\n",
    "GLOBAL_BATCH_SIZE = 128\n",
    "\n",
    "# Batch dataset with drop remainder for stability.\n",
    "base_ds = base_ds.batch(GLOBAL_BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "# Define an expensive preprocessing function.\n",
    "def heavy_preprocess(image, label):\n",
    "    # Cast image to float and normalize.\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "\n",
    "    # Apply random flip for augmentation.\n",
    "    image = tf.image.random_flip_left_right(image, seed=SEED_VALUE)\n",
    "\n",
    "    # Apply random brightness jitter.\n",
    "    image = tf.image.random_brightness(image, max_delta=0.1)\n",
    "\n",
    "    # Simulate extra CPU work with convolution.\n",
    "    kernel = tf.ones((3, 3, NUM_CHANNELS, NUM_CHANNELS)) / 9.0\n",
    "\n",
    "    image = tf.nn.conv2d(\n",
    "        image,\n",
    "        kernel,\n",
    "        strides=1,\n",
    "        padding=\"SAME\",\n",
    "    )\n",
    "\n",
    "    # Return processed image and original label.\n",
    "    return image, label\n",
    "\n",
    "# Build dataset with sequential mapping configuration.\n",
    "seq_ds = base_ds.map(\n",
    "    heavy_preprocess,\n",
    "    num_parallel_calls=1,\n",
    ")\n",
    "\n",
    "# Prefetch to overlap mapping and training.\n",
    "seq_ds = seq_ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Build dataset with parallel mapping configuration.\n",
    "par_ds = base_ds.map(\n",
    "    heavy_preprocess,\n",
    "    num_parallel_calls=tf.data.AUTOTUNE,\n",
    ")\n",
    "\n",
    "# Prefetch for the parallel dataset as well.\n",
    "par_ds = par_ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Define a simple function to time one epoch.\n",
    "def time_one_epoch(dataset, num_steps):\n",
    "    # Record start time using time module.\n",
    "    start = time.time()\n",
    "\n",
    "    # Iterate fixed number of steps defensively.\n",
    "    step_count = 0\n",
    "\n",
    "    for batch_images, batch_labels in dataset:\n",
    "        # Validate batch shapes before using.\n",
    "        if batch_images.shape[0] != GLOBAL_BATCH_SIZE:\n",
    "            break\n",
    "\n",
    "        # Simple lightweight computation per batch.\n",
    "        _ = tf.reduce_mean(batch_images)\n",
    "\n",
    "        step_count += 1\n",
    "\n",
    "        if step_count >= num_steps:\n",
    "            break\n",
    "\n",
    "    # Compute elapsed time in seconds.\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "    # Return elapsed time and actual steps.\n",
    "    return elapsed, step_count\n",
    "\n",
    "# Determine safe number of steps for timing.\n",
    "TOTAL_STEPS = int(NUM_IMAGES / GLOBAL_BATCH_SIZE)\n",
    "\n",
    "# Limit steps to keep runtime very small.\n",
    "MEASURE_STEPS = min(TOTAL_STEPS, 10)\n",
    "\n",
    "# Warm up both datasets briefly before timing.\n",
    "_ = next(iter(seq_ds))\n",
    "_ = next(iter(par_ds))\n",
    "\n",
    "# Time sequential mapping dataset throughput.\n",
    "seq_time, seq_steps = time_one_epoch(seq_ds, MEASURE_STEPS)\n",
    "\n",
    "# Time parallel mapping dataset throughput.\n",
    "par_time, par_steps = time_one_epoch(par_ds, MEASURE_STEPS)\n",
    "\n",
    "# Compute images per second for sequential mapping.\n",
    "seq_images_per_sec = (seq_steps * GLOBAL_BATCH_SIZE) / max(seq_time, 1e-6)\n",
    "\n",
    "# Compute images per second for parallel mapping.\n",
    "par_images_per_sec = (par_steps * GLOBAL_BATCH_SIZE) / max(par_time, 1e-6)\n",
    "\n",
    "# Print concise comparison of both strategies.\n",
    "print(\"Sequential map steps:\", seq_steps, \"time:\", round(seq_time, 3))\n",
    "\n",
    "# Print parallel mapping timing information.\n",
    "print(\"Parallel map steps:\", par_steps, \"time:\", round(par_time, 3))\n",
    "\n",
    "# Print throughput numbers for both configurations.\n",
    "print(\"Sequential images/sec:\", int(seq_images_per_sec))\n",
    "\n",
    "# Print parallel throughput to highlight improvement.\n",
    "print(\"Parallel images/sec:\", int(par_images_per_sec))\n",
    "\n",
    "# Show simple speedup factor for quick intuition.\n",
    "print(\"Parallel speedup factor:\", round(par_images_per_sec / seq_images_per_sec, 2))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11e78d1",
   "metadata": {},
   "source": [
    "### **2.3. Accelerating GPU and TPU**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb23f10b",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_05/Lecture_B/image_02_03.jpg?v=1769405145\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Keep GPUs and TPUs constantly fed with data\n",
    ">* Overlap host preprocessing with training across all replicas\n",
    "\n",
    ">* Match parallelism and prefetching to replica demand\n",
    ">* Use stronger host side pipelines, especially on TPUs\n",
    "\n",
    ">* Slow input pipelines leave accelerators idle, underused\n",
    ">* Profile and tune parallelism to unlock linear scaling\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733c594b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Accelerating GPU and TPU\n",
    "\n",
    "# This script shows parallel input optimization.\n",
    "# We focus on accelerators and distributed strategies.\n",
    "# All examples are small and beginner friendly.\n",
    "\n",
    "# # Uncomment if TensorFlow is not already installed.\n",
    "# !pip install -q tensorflow==2.20.0.\n",
    "\n",
    "# Import required modules for TensorFlow usage.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set deterministic seeds for reproducible behavior.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Detect available devices and possible accelerators.\n",
    "physical_gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "physical_tpus = tf.config.list_physical_devices(\"TPU\")\n",
    "print(\"GPUs:\", len(physical_gpus), \"TPUs:\", len(physical_tpus))\n",
    "\n",
    "# Choose a distribution strategy for demonstration.\n",
    "if physical_tpus:\n",
    "    resolver = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n",
    "    strategy = tf.distribute.TPUStrategy(resolver)\n",
    "elif physical_gpus:\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "else:\n",
    "    strategy = tf.distribute.OneDeviceStrategy(\"/cpu:0\")\n",
    "\n",
    "# Report the number of replicas in the strategy.\n",
    "num_replicas = strategy.num_replicas_in_sync\n",
    "print(\"Replicas in sync:\", num_replicas)\n",
    "\n",
    "# Load a small subset of MNIST image data.\n",
    "(x_train, y_train), _ = tf.keras.datasets.mnist.load_data()\n",
    "subset_size = 6000\n",
    "x_train = x_train[:subset_size]\n",
    "y_train = y_train[:subset_size]\n",
    "\n",
    "# Normalize and add channel dimension for images.\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_train = np.expand_dims(x_train, axis=-1)\n",
    "print(\"Train subset shape:\", x_train.shape)\n",
    "\n",
    "# Define a simple preprocessing function for dataset.\n",
    "def preprocess(image, label):\n",
    "    image = tf.image.resize(image, (28, 28))\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    return image, label\n",
    "\n",
    "# Create a base tf.data dataset from numpy arrays.\n",
    "base_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "base_ds = base_ds.shuffle(buffer_size=subset_size, seed=seed_value)\n",
    "\n",
    "# Compute global and per replica batch sizes.\n",
    "global_batch_size = 128\n",
    "per_replica_batch = global_batch_size // num_replicas\n",
    "if global_batch_size % num_replicas != 0:\n",
    "    per_replica_batch = max(1, per_replica_batch)\n",
    "\n",
    "# Build an optimized pipeline with parallel mapping.\n",
    "num_parallel_calls = tf.data.AUTOTUNE\n",
    "optimized_ds = base_ds.map(preprocess, num_parallel_calls=num_parallel_calls)\n",
    "optimized_ds = optimized_ds.cache()\n",
    "\n",
    "# Use interleave to simulate multiple file shards.\n",
    "optimized_ds = optimized_ds.interleave(\n",
    "    lambda x, y: tf.data.Dataset.from_tensors((x, y)),\n",
    "    cycle_length=4,\n",
    "    num_parallel_calls=num_parallel_calls,\n",
    ")\n",
    "\n",
    "# Batch and prefetch to overlap host and device work.\n",
    "optimized_ds = optimized_ds.batch(global_batch_size, drop_remainder=True)\n",
    "optimized_ds = optimized_ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Distribute the dataset for the chosen strategy.\n",
    "dist_dataset = strategy.experimental_distribute_dataset(optimized_ds)\n",
    "\n",
    "# Define a very small convolutional model.\n",
    "def create_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(16, (3, 3), activation=\"relu\", input_shape=(28, 28, 1)),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(32, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(10, activation=\"softmax\"),\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Create and compile the model inside strategy scope.\n",
    "with strategy.scope():\n",
    "    model = create_model()\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "# Train briefly to show pipeline and accelerator usage.\n",
    "history = model.fit(dist_dataset, epochs=1, steps_per_epoch=20, verbose=0)\n",
    "\n",
    "# Print final training metrics from the short run.\n",
    "final_loss = history.history[\"loss\"][-1]\n",
    "final_acc = history.history[\"accuracy\"][-1]\n",
    "print(\"Final loss:\", round(float(final_loss), 4))\n",
    "print(\"Final accuracy:\", round(float(final_acc), 4))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78d4c8a",
   "metadata": {},
   "source": [
    "## **3. Debugging Data Pipelines**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc50337b",
   "metadata": {},
   "source": [
    "### **3.1. Quick Dataset Inspection**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ea451f",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_05/Lecture_B/image_03_01.jpg?v=1769405184\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Quickly iterate a few batches, checking outputs\n",
    ">* Watch startup delays to spot expensive initialization work\n",
    "\n",
    ">* Inspect element structure to avoid unnecessary work\n",
    ">* Simplify data formats and precompute repeated metadata\n",
    "\n",
    ">* Time a few batches versus training step\n",
    ">* Use timing patterns to locate pipeline bottlenecks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7261fdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Quick Dataset Inspection\n",
    "\n",
    "# This script inspects a tf.data pipeline quickly.\n",
    "# It focuses on timing and element structure.\n",
    "# Use it to spot simple performance issues.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Set deterministic random seeds.\n",
    "random.seed(7)\n",
    "os.environ[\"PYTHONHASHSEED\"] = \"7\"\n",
    "\n",
    "# Import TensorFlow and check version.\n",
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Create a small synthetic image dataset.\n",
    "num_samples = 64\n",
    "image_height = 28\n",
    "\n",
    "# Validate simple positive sizes.\n",
    "assert num_samples > 0 and image_height > 0\n",
    "\n",
    "# Build random image and label tensors.\n",
    "images = tf.random.uniform(\n",
    "    shape=(num_samples, image_height, image_height, 1)\n",
    ")\n",
    "labels = tf.random.uniform(\n",
    "    shape=(num_samples,), maxval=10, dtype=tf.int32\n",
    ")\n",
    "\n",
    "# Wrap tensors in a tf.data Dataset.\n",
    "base_ds = tf.data.Dataset.from_tensor_slices((images, labels))\n",
    "\n",
    "# Define a simple preprocessing function.\n",
    "def preprocess(example_image, example_label):\n",
    "    image = tf.cast(example_image, tf.float32) / 255.0\n",
    "    label = tf.cast(example_label, tf.int32)\n",
    "    return image, label\n",
    "\n",
    "\n",
    "# Apply preprocessing and batching.\n",
    "batch_size = 8\n",
    "inspected_ds = base_ds.map(preprocess).batch(batch_size)\n",
    "\n",
    "# Take a few batches for quick inspection.\n",
    "num_inspect_batches = 3\n",
    "\n",
    "# Time how long it takes to get batches.\n",
    "start_time = time.time()\n",
    "first_batch_time = None\n",
    "\n",
    "# Iterate over a small number of batches.\n",
    "for batch_index, batch in enumerate(inspected_ds.take(num_inspect_batches)):\n",
    "    batch_images, batch_labels = batch\n",
    "    now = time.time()\n",
    "    if batch_index == 0:\n",
    "        first_batch_time = now - start_time\n",
    "    print(\n",
    "        \"Batch\", batch_index,\n",
    "        \"shape\", batch_images.shape,\n",
    "        \"dtype\", batch_images.dtype\n",
    "    )\n",
    "    print(\n",
    "        \"Labels shape\", batch_labels.shape,\n",
    "        \"dtype\", batch_labels.dtype\n",
    "    )\n",
    "\n",
    "# Compute total elapsed time for inspected batches.\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "# Print simple timing summary for inspection.\n",
    "print(\"First batch seconds:\", round(first_batch_time, 4))\n",
    "print(\"Total seconds for batches:\", round(total_time, 4))\n",
    "print(\"Average seconds per batch:\", round(total_time / num_inspect_batches, 4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24f6bc3",
   "metadata": {},
   "source": [
    "### **3.2. Profiling Input Performance**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef1208a",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_05/Lecture_B/image_03_02.jpg?v=1769405239\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Treat the input pipeline as measurable system\n",
    ">* Compare startup behavior versus steady-state throughput patterns\n",
    "\n",
    ">* Time isolated pipelines over fixed batch counts\n",
    ">* Use timings to locate bottlenecks and startup overheads\n",
    "\n",
    ">* Track CPU, GPU, I O, and prefetch overlap\n",
    ">* Correlate timings to confirm real bottleneck improvements\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59db1b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Profiling Input Performance\n",
    "\n",
    "# This script profiles tf.data input performance simply.\n",
    "# It compares slow and optimized pipelines for beginners.\n",
    "# Focus on timing batches and checking device utilization.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries safely.\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Import TensorFlow and check version.\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "random.seed(7)\n",
    "\n",
    "# Set TensorFlow random seed deterministically.\n",
    "tf.random.set_seed(7)\n",
    "\n",
    "# Print TensorFlow version in one concise line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Detect available physical devices for context.\n",
    "physical_devices = tf.config.list_physical_devices()\n",
    "\n",
    "# Print number of detected devices briefly.\n",
    "print(\"Detected devices:\", len(physical_devices))\n",
    "\n",
    "# Create a small synthetic dataset for profiling.\n",
    "num_examples = 2000\n",
    "\n",
    "# Build a simple tensor of integer ids.\n",
    "ids = tf.range(num_examples, dtype=tf.int32)\n",
    "\n",
    "# Define a function simulating expensive preprocessing.\n",
    "def slow_preprocess(x: tf.Tensor) -> tf.Tensor:\n",
    "    x = tf.cast(x, tf.float32)\n",
    "    for _ in range(5):\n",
    "        x = tf.math.sqrt(x + 1.0)\n",
    "    return x\n",
    "\n",
    "# Define a function simulating lighter preprocessing.\n",
    "def fast_preprocess(x: tf.Tensor) -> tf.Tensor:\n",
    "    x = tf.cast(x, tf.float32)\n",
    "    return x * 0.5 + 1.0\n",
    "\n",
    "# Helper function to time iteration over some batches.\n",
    "def time_dataset(ds: tf.data.Dataset, num_batches: int) -> float:\n",
    "    start = time.time()\n",
    "    count = 0\n",
    "    for batch in ds.take(num_batches):\n",
    "        _ = tf.reduce_sum(batch)\n",
    "        count += 1\n",
    "    end = time.time()\n",
    "    if count == 0:\n",
    "        return 0.0\n",
    "    return (end - start) / float(count)\n",
    "\n",
    "# Build a deliberately slow input pipeline.\n",
    "slow_ds = tf.data.Dataset.from_tensor_slices(ids)\n",
    "\n",
    "# Add slow mapping without parallel calls.\n",
    "slow_ds = slow_ds.map(slow_preprocess)\n",
    "\n",
    "# Batch the slow dataset with small batch size.\n",
    "slow_ds = slow_ds.batch(32)\n",
    "\n",
    "# Prefetch minimally to show limited overlap.\n",
    "slow_ds = slow_ds.prefetch(1)\n",
    "\n",
    "# Build an optimized pipeline using parallelism.\n",
    "opt_ds = tf.data.Dataset.from_tensor_slices(ids)\n",
    "\n",
    "# Map with fast preprocessing and autotune parallelism.\n",
    "opt_ds = opt_ds.map(\n",
    "    fast_preprocess,\n",
    "    num_parallel_calls=tf.data.AUTOTUNE,\n",
    ")\n",
    "\n",
    "# Batch with same size for fair comparison.\n",
    "opt_ds = opt_ds.batch(32)\n",
    "\n",
    "# Prefetch aggressively using autotune setting.\n",
    "opt_ds = opt_ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Warm up both datasets to reduce startup noise.\n",
    "_ = next(iter(slow_ds.take(1)))\n",
    "\n",
    "# Warm up optimized dataset similarly.\n",
    "_ = next(iter(opt_ds.take(1)))\n",
    "\n",
    "# Choose number of batches for timing runs.\n",
    "num_timing_batches = 50\n",
    "\n",
    "# Time the slow pipeline average batch latency.\n",
    "slow_latency = time_dataset(slow_ds, num_timing_batches)\n",
    "\n",
    "# Time the optimized pipeline average batch latency.\n",
    "opt_latency = time_dataset(opt_ds, num_timing_batches)\n",
    "\n",
    "# Compute examples per second for slow pipeline.\n",
    "slow_eps = 32.0 / slow_latency if slow_latency > 0 else 0.0\n",
    "\n",
    "# Compute examples per second for optimized pipeline.\n",
    "opt_eps = 32.0 / opt_latency if opt_latency > 0 else 0.0\n",
    "\n",
    "# Print concise timing summary for both pipelines.\n",
    "print(\"Slow pipeline avg batch seconds:\", round(slow_latency, 4))\n",
    "\n",
    "# Print optimized pipeline timing for comparison.\n",
    "print(\"Optimized pipeline avg batch seconds:\", round(opt_latency, 4))\n",
    "\n",
    "# Print throughput in examples per second values.\n",
    "print(\"Slow pipeline examples per second:\", int(slow_eps))\n",
    "\n",
    "# Print optimized throughput for quick inspection.\n",
    "print(\"Optimized pipeline examples per second:\", int(opt_eps))\n",
    "\n",
    "# Build a tiny model to observe input interaction.\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(1,)),\n",
    "    tf.keras.layers.Dense(8, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1),\n",
    "])\n",
    "\n",
    "# Compile model with simple optimizer and loss.\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "\n",
    "# Create labels tensor matching dataset batches.\n",
    "labels = tf.cast(ids[:num_examples], tf.float32)\n",
    "\n",
    "# Zip features and labels for training dataset.\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((ids, labels))\n",
    "\n",
    "# Apply slow preprocessing only to features here.\n",
    "train_slow = train_ds.map(\n",
    "    lambda x, y: (slow_preprocess(x), y),\n",
    ")\n",
    "\n",
    "# Batch and prefetch slow training dataset.\n",
    "train_slow = train_slow.batch(32).prefetch(1)\n",
    "\n",
    "# Apply fast preprocessing for optimized training dataset.\n",
    "train_opt = train_ds.map(\n",
    "    lambda x, y: (fast_preprocess(x), y),\n",
    "    num_parallel_calls=tf.data.AUTOTUNE,\n",
    ")\n",
    "\n",
    "# Batch and prefetch optimized training dataset.\n",
    "train_opt = train_opt.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Train briefly on slow pipeline and time steps.\n",
    "start_slow_train = time.time()\n",
    "model.fit(train_slow, epochs=1, verbose=0)\n",
    "end_slow_train = time.time()\n",
    "\n",
    "# Train briefly on optimized pipeline and time steps.\n",
    "start_opt_train = time.time()\n",
    "model.fit(train_opt, epochs=1, verbose=0)\n",
    "end_opt_train = time.time()\n",
    "\n",
    "# Compute per epoch durations for both pipelines.\n",
    "slow_epoch_time = end_slow_train - start_slow_train\n",
    "\n",
    "# Compute optimized epoch duration similarly.\n",
    "opt_epoch_time = end_opt_train - start_opt_train\n",
    "\n",
    "# Print concise training time comparison summary.\n",
    "print(\"Slow pipeline epoch seconds:\", round(slow_epoch_time, 3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175dda12",
   "metadata": {},
   "source": [
    "### **3.3. Handling data order determinism**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59031ab0",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_05/Lecture_B/image_03_03.jpg?v=1769405356\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Deterministic order aids reproducibility and debugging reliability\n",
    ">* Relaxed order boosts parallelism and overall throughput\n",
    "\n",
    ">* Strict ordering can stall pipelines and GPUs\n",
    ">* Relaxed determinism boosts parallelism, hiding slow reads\n",
    "\n",
    ">* Choose deterministic mode for debugging and comparisons\n",
    ">* Switch to non-deterministic mode for faster training\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9d864e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Handling data order determinism\n",
    "\n",
    "# This script shows deterministic versus nondeterministic pipelines.\n",
    "# It focuses on tf.data order and performance tradeoffs.\n",
    "# Run cells to compare behavior and printed element orders.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required modules safely.\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Print TensorFlow version briefly.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Create a small base dataset of integers.\n",
    "base_values = tf.range(start=0, limit=8, delta=1)\n",
    "\n",
    "# Define an artificial slow mapping function.\n",
    "@tf.function\n",
    "\n",
    "\n",
    "def slow_square(x):\n",
    "    # Add tiny sleep using tf.py_function.\n",
    "    def _sleep_and_square(v):\n",
    "        time.sleep(0.01 + float(v % 3) * 0.005)\n",
    "        return np.int32(v * v)\n",
    "\n",
    "    # Wrap Python function for TensorFlow.\n",
    "    y = tf.py_function(_sleep_and_square, [x], Tout=tf.int32)\n",
    "    # Ensure shape information is preserved.\n",
    "    y.set_shape(x.shape)\n",
    "    return y\n",
    "\n",
    "# Build a deterministic pipeline with parallel map.\n",
    "def make_deterministic_dataset():\n",
    "    # Start from base tensor dataset.\n",
    "    ds = tf.data.Dataset.from_tensor_slices(base_values)\n",
    "    # Map with parallel calls and deterministic order.\n",
    "    ds = ds.map(\n",
    "        slow_square,\n",
    "        num_parallel_calls=tf.data.AUTOTUNE,\n",
    "        deterministic=True,\n",
    "    )\n",
    "    # Batch elements for efficiency.\n",
    "    ds = ds.batch(4)\n",
    "    return ds\n",
    "\n",
    "# Build a nondeterministic pipeline with parallel map.\n",
    "def make_nondeterministic_dataset():\n",
    "    # Start from same base tensor dataset.\n",
    "    ds = tf.data.Dataset.from_tensor_slices(base_values)\n",
    "    # Map with parallel calls and relaxed order.\n",
    "    ds = ds.map(\n",
    "        slow_square,\n",
    "        num_parallel_calls=tf.data.AUTOTUNE,\n",
    "        deterministic=False,\n",
    "    )\n",
    "    # Batch elements for efficiency.\n",
    "    ds = ds.batch(4)\n",
    "    return ds\n",
    "\n",
    "# Helper to iterate once and collect batches.\n",
    "def run_once(dataset, label):\n",
    "    # Record start time for simple timing.\n",
    "    start = time.time()\n",
    "    batches = []\n",
    "    for batch in dataset:\n",
    "        batches.append(batch.numpy())\n",
    "    duration = time.time() - start\n",
    "    # Print label, order, and duration.\n",
    "    print(label, \"batches:\", batches)\n",
    "    print(label, \"time_sec:\", round(duration, 3))\n",
    "\n",
    "# Run deterministic pipeline and observe order.\n",
    "run_once(make_deterministic_dataset(), \"deterministic\")\n",
    "\n",
    "# Run nondeterministic pipeline and observe order.\n",
    "run_once(make_nondeterministic_dataset(), \"nondeterministic\")\n",
    "\n",
    "# Run nondeterministic pipeline again to show variability.\n",
    "run_once(make_nondeterministic_dataset(), \"nondeterministic_run2\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90ffd0d",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Advanced Pipelines**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6953e31",
   "metadata": {},
   "source": [
    "\n",
    "In this lecture, you learned to:\n",
    "- Optimize tf.data pipelines using caching, interleave, and parallelism settings for large datasets. \n",
    "- Configure input pipelines that work efficiently with distributed training strategies. \n",
    "- Diagnose and resolve common tf.data performance issues such as slow startup or CPU bottlenecks. \n",
    "\n",
    "In the next Module (Module 6), we will go over 'Computer Vision'"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
