{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3889b50",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Using model.fit**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d6a651",
   "metadata": {},
   "source": [
    ">Last update: 20260125.\n",
    "    \n",
    "By the end of this Lecture, you will be able to:\n",
    "- Configure model.fit with appropriate batch sizes, epochs, and validation strategies for a given dataset. \n",
    "- Use Keras callbacks to monitor training, implement early stopping, and save model checkpoints. \n",
    "- Interpret training and validation curves to diagnose underfitting and overfitting. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c362248e",
   "metadata": {},
   "source": [
    "## **1. Configuring model fit**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2166552",
   "metadata": {},
   "source": [
    "### **1.1. Batch size and epochs**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca4cccb",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_04/Lecture_A/image_01_01.jpg?v=1769392054\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Batch size affects update frequency, stability, memory\n",
    ">* Epochs balance learning time, underfitting, overfitting risk\n",
    "\n",
    ">* Batch size is limited by hardware memory\n",
    ">* Tune batch size for speed and generalization\n",
    "\n",
    ">* Epochs depend on task complexity and difficulty\n",
    ">* Use validation curves to stop training at optimum\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1f95aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Batch size and epochs\n",
    "\n",
    "# This script shows batch size and epochs.\n",
    "# It uses a tiny MNIST subset dataset.\n",
    "# It keeps training fast and output short.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Import TensorFlow and Keras utilities.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Load MNIST dataset from Keras datasets.\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize pixel values to range zero one.\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_test = x_test.astype(\"float32\") / 255.0\n",
    "\n",
    "# Add channel dimension for convolution layers.\n",
    "x_train = np.expand_dims(x_train, axis=-1)\n",
    "x_test = np.expand_dims(x_test, axis=-1)\n",
    "\n",
    "# Use a small subset to keep training quick.\n",
    "train_samples = 4000\n",
    "val_samples = 1000\n",
    "x_train_small = x_train[:train_samples]\n",
    "y_train_small = y_train[:train_samples]\n",
    "\n",
    "# Create a validation split from training subset.\n",
    "x_val_small = x_train[train_samples:train_samples + val_samples]\n",
    "y_val_small = y_train[train_samples:train_samples + val_samples]\n",
    "\n",
    "# Validate shapes before building the model.\n",
    "print(\"Train subset shape:\", x_train_small.shape)\n",
    "print(\"Validation subset shape:\", x_val_small.shape)\n",
    "\n",
    "# Build a simple sequential CNN model.\n",
    "model = keras.Sequential([\n",
    "    layers.Conv2D(16, (3, 3), activation=\"relu\", input_shape=(28, 28, 1)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(32, activation=\"relu\"),\n",
    "    layers.Dense(10, activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "# Compile model with optimizer and loss.\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Define two different batch sizes for comparison.\n",
    "small_batch_size = 16\n",
    "large_batch_size = 128\n",
    "epochs = 5\n",
    "\n",
    "# Train with small batch size and fixed epochs.\n",
    "history_small = model.fit(\n",
    "    x_train_small,\n",
    "    y_train_small,\n",
    "    batch_size=small_batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=(x_val_small, y_val_small),\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Train again with larger batch size and same epochs.\n",
    "history_large = model.fit(\n",
    "    x_train_small,\n",
    "    y_train_small,\n",
    "    batch_size=large_batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=(x_val_small, y_val_small),\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Helper function to summarize final metrics.\n",
    "def summarize_run(name, history):\n",
    "    train_loss = history.history[\"loss\"][-1]\n",
    "    train_acc = history.history[\"accuracy\"][-1]\n",
    "    val_loss = history.history[\"val_loss\"][-1]\n",
    "    val_acc = history.history[\"val_accuracy\"][-1]\n",
    "    print(\n",
    "        f\"{name} -> loss: {train_loss:.3f}, acc: {train_acc:.3f}, \"\n",
    "        f\"val_loss: {val_loss:.3f}, val_acc: {val_acc:.3f}\"\n",
    "    )\n",
    "\n",
    "# Print a short explanation header line.\n",
    "print(\"Comparing small and large batch sizes after training.\")\n",
    "\n",
    "# Show results for small batch size training.\n",
    "summarize_run(\"Small batch size\", history_small)\n",
    "\n",
    "# Show results for large batch size training.\n",
    "summarize_run(\"Large batch size\", history_large)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da0713e",
   "metadata": {},
   "source": [
    "### **1.2. Choosing Validation Inputs**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52203a3e",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_04/Lecture_A/image_01_02.jpg?v=1769392097\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Validation data must be separate from training\n",
    ">* Make validation realistic to estimate real performance\n",
    "\n",
    ">* Choose validation method based on dataset structure\n",
    ">* Respect ordering, mimic deployment to avoid misleading scores\n",
    "\n",
    ">* Match preprocessing, avoid random validation augmentations\n",
    ">* Preserve real class balance for stable feedback\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715ccc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Choosing Validation Inputs\n",
    "\n",
    "# This script shows choosing validation inputs.\n",
    "# It compares validation_split and explicit validation_data.\n",
    "# It uses a tiny MNIST subset for speed.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required libraries safely.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Select device preference based on availability.\n",
    "physical_gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "if physical_gpus:\n",
    "    device_type = \"GPU\"\n",
    "else:\n",
    "    device_type = \"CPU\"\n",
    "\n",
    "# Print which device type will likely be used.\n",
    "print(\"Using device type:\", device_type)\n",
    "\n",
    "# Load MNIST dataset from Keras datasets.\n",
    "(mnist_x_train, mnist_y_train), _ = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize pixel values to the range zero one.\n",
    "mnist_x_train = mnist_x_train.astype(\"float32\") / 255.0\n",
    "\n",
    "# Add channel dimension for convolutional layers.\n",
    "mnist_x_train = np.expand_dims(mnist_x_train, axis=-1)\n",
    "\n",
    "# Confirm shapes are as expected before splitting.\n",
    "print(\"Training data shape:\", mnist_x_train.shape)\n",
    "\n",
    "# Use a small subset to keep runtime short.\n",
    "subset_size = 6000\n",
    "mnist_x_train = mnist_x_train[:subset_size]\n",
    "mnist_y_train = mnist_y_train[:subset_size]\n",
    "\n",
    "# Verify subset sizes are consistent and safe.\n",
    "print(\"Subset size:\", mnist_x_train.shape[0])\n",
    "\n",
    "# Build a simple convolutional classification model.\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(8, (3, 3), activation=\"relu\",\n",
    "                           input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(32, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "# Compile the model with standard settings.\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Train using validation_split on shuffled iid data.\n",
    "history_split = model.fit(\n",
    "    mnist_x_train,\n",
    "    mnist_y_train,\n",
    "    batch_size=64,\n",
    "    epochs=3,\n",
    "    validation_split=0.2,\n",
    "    shuffle=True,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Manually create explicit validation_data arrays.\n",
    "val_size = int(0.2 * subset_size)\n",
    "explicit_x_val = mnist_x_train[-val_size:]\n",
    "explicit_y_val = mnist_y_train[-val_size:]\n",
    "\n",
    "# Use the remaining data strictly for training.\n",
    "explicit_x_train = mnist_x_train[:-val_size]\n",
    "explicit_y_train = mnist_y_train[:-val_size]\n",
    "\n",
    "# Confirm explicit split sizes are consistent.\n",
    "print(\"Explicit train size:\", explicit_x_train.shape[0])\n",
    "\n",
    "# Rebuild a fresh model for fair comparison.\n",
    "model_explicit = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(8, (3, 3), activation=\"relu\",\n",
    "                           input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(32, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "# Compile the second model with same settings.\n",
    "model_explicit.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Train using explicit validation_data argument.\n",
    "history_explicit = model_explicit.fit(\n",
    "    explicit_x_train,\n",
    "    explicit_y_train,\n",
    "    batch_size=64,\n",
    "    epochs=3,\n",
    "    validation_data=(explicit_x_val, explicit_y_val),\n",
    "    shuffle=False,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Extract final metrics from both training runs.\n",
    "final_split_val_acc = history_split.history[\"val_accuracy\"][-1]\n",
    "final_explicit_val_acc = history_explicit.history[\"val_accuracy\"][-1]\n",
    "\n",
    "# Print concise comparison of validation strategies.\n",
    "print(\"Final val_accuracy with validation_split:\",\n",
    "      round(float(final_split_val_acc), 4))\n",
    "\n",
    "# Show validation accuracy using explicit validation_data.\n",
    "print(\"Final val_accuracy with explicit data:\",\n",
    "      round(float(final_explicit_val_acc), 4))\n",
    "\n",
    "# Explain why explicit validation helps structured data.\n",
    "print(\"Use validation_split only for well shuffled iid data.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e1e371",
   "metadata": {},
   "source": [
    "### **1.3. Shuffling and Class Weights**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99461df4",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_04/Lecture_A/image_01_03.jpg?v=1769392135\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Shuffle data so batches reflect overall distribution\n",
    ">* Prevents order-based overfitting and stabilizes training\n",
    "\n",
    ">* Imbalanced datasets make models ignore rare classes\n",
    ">* Class weights and shuffling amplify rare examples\n",
    "\n",
    ">* Shuffle whole sequences while preserving internal order\n",
    ">* Use class weights so rare sequences matter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202e5b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Shuffling and Class Weights\n",
    "\n",
    "# This script shows shuffling and class weights.\n",
    "# It uses a tiny imbalanced dataset example.\n",
    "# It runs quickly with minimal printed output.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required libraries safely.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version briefly.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Load MNIST dataset from Keras.\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Reduce dataset size for quick training.\n",
    "x_train = x_train[:6000]\n",
    "y_train = y_train[:6000]\n",
    "\n",
    "# Create binary labels to induce imbalance.\n",
    "minority_class = 1\n",
    "binary_labels = (y_train == minority_class).astype(\"int32\")\n",
    "\n",
    "# Check class counts for imbalance.\n",
    "unique, counts = np.unique(binary_labels, return_counts=True)\n",
    "print(\"Class counts:\", dict(zip(unique, counts)))\n",
    "\n",
    "# Normalize images and add channel dimension.\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_train = np.expand_dims(x_train, axis=-1)\n",
    "\n",
    "# Validate shapes before building model.\n",
    "print(\"Train shape:\", x_train.shape, binary_labels.shape)\n",
    "\n",
    "# Build a small sequential model.\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(28, 28, 1)),\n",
    "    tf.keras.layers.Conv2D(8, (3, 3), activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(16, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "])\n",
    "\n",
    "# Compile model with binary crossentropy loss.\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Compute simple class weights for imbalance.\n",
    "neg, pos = counts\n",
    "weight_for_0 = (1.0 / neg) * (neg + pos) / 2.0\n",
    "weight_for_1 = (1.0 / pos) * (neg + pos) / 2.0\n",
    "class_weights = {0: weight_for_0, 1: weight_for_1}\n",
    "\n",
    "# Print class weights for inspection.\n",
    "print(\"Class weights:\", class_weights)\n",
    "\n",
    "# Train with shuffling enabled and class weights.\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    binary_labels,\n",
    "    epochs=3,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    class_weight=class_weights,\n",
    "    validation_split=0.2,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Extract final training and validation metrics.\n",
    "final_loss = history.history[\"loss\"][-1]\n",
    "final_val_loss = history.history[\"val_loss\"][-1]\n",
    "final_acc = history.history[\"accuracy\"][-1]\n",
    "final_val_acc = history.history[\"val_accuracy\"][-1]\n",
    "\n",
    "# Print concise summary of training results.\n",
    "print(\"Final loss and val_loss:\", round(final_loss, 3), round(final_val_loss, 3))\n",
    "print(\"Final acc and val_acc:\", round(final_acc, 3), round(final_val_acc, 3))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41792252",
   "metadata": {},
   "source": [
    "## **2. Keras Training Callbacks**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b262c8b9",
   "metadata": {},
   "source": [
    "### **2.1. Tuning Early Stopping**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc9608c",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_04/Lecture_A/image_02_01.jpg?v=1769392206\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Early stopping halts training before serious overfitting\n",
    ">* Balances enough learning time against memorizing noise\n",
    "\n",
    ">* Choose metric, patience, and minimum meaningful improvement\n",
    ">* Adjust settings to handle plateaus and noise\n",
    "\n",
    ">* Tune early stopping iteratively using training curves\n",
    ">* Balance compute cost, metric noise, and performance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1690d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Tuning Early Stopping\n",
    "\n",
    "# This script demonstrates tuning early stopping.\n",
    "# It uses a tiny MNIST subset for speed.\n",
    "# It keeps training output short and clear.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required libraries safely.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Select device based on GPU availability.\n",
    "physical_gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "if physical_gpus:\n",
    "    device_name = \"GPU\"\n",
    "else:\n",
    "    device_name = \"CPU\"\n",
    "\n",
    "# Print which device will be mainly used.\n",
    "print(\"Using device:\", device_name)\n",
    "\n",
    "# Load MNIST dataset from Keras.\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize images to range zero one.\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_test = x_test.astype(\"float32\") / 255.0\n",
    "\n",
    "# Add channel dimension for Conv2D.\n",
    "x_train = np.expand_dims(x_train, axis=-1)\n",
    "x_test = np.expand_dims(x_test, axis=-1)\n",
    "\n",
    "# Validate shapes before training.\n",
    "assert x_train.shape[0] == y_train.shape[0]\n",
    "assert x_test.shape[0] == y_test.shape[0]\n",
    "\n",
    "# Use a small subset for quick training.\n",
    "train_samples = 800\n",
    "val_samples = 200\n",
    "x_train_small = x_train[:train_samples]\n",
    "y_train_small = y_train[:train_samples]\n",
    "\n",
    "# Create validation split from training subset.\n",
    "x_val_small = x_train[train_samples:train_samples + val_samples]\n",
    "y_val_small = y_train[train_samples:train_samples + val_samples]\n",
    "\n",
    "# Build a simple convolutional model.\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(\n",
    "        16,\n",
    "        (3, 3),\n",
    "        activation=\"relu\",\n",
    "        input_shape=(28, 28, 1),\n",
    "    ),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(32, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "# Compile model with suitable settings.\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Define a baseline early stopping callback.\n",
    "baseline_es = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=2,\n",
    "    min_delta=0.001,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "# Define a stricter early stopping callback.\n",
    "strict_es = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=1,\n",
    "    min_delta=0.01,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "# Train model with baseline early stopping.\n",
    "history_baseline = model.fit(\n",
    "    x_train_small,\n",
    "    y_train_small,\n",
    "    validation_data=(x_val_small, y_val_small),\n",
    "    epochs=20,\n",
    "    batch_size=64,\n",
    "    callbacks=[baseline_es],\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Record number of epochs actually run.\n",
    "epochs_baseline = len(history_baseline.history[\"loss\"])\n",
    "\n",
    "# Reinitialize model weights for fair comparison.\n",
    "model_strict = tf.keras.models.clone_model(model)\n",
    "model_strict.build(input_shape=(None, 28, 28, 1))\n",
    "model_strict.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Train model with stricter early stopping.\n",
    "history_strict = model_strict.fit(\n",
    "    x_train_small,\n",
    "    y_train_small,\n",
    "    validation_data=(x_val_small, y_val_small),\n",
    "    epochs=20,\n",
    "    batch_size=64,\n",
    "    callbacks=[strict_es],\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Record number of epochs for strict setting.\n",
    "epochs_strict = len(history_strict.history[\"loss\"])\n",
    "\n",
    "# Evaluate both models on shared test subset.\n",
    "small_test_samples = 500\n",
    "x_test_small = x_test[:small_test_samples]\n",
    "y_test_small = y_test[:small_test_samples]\n",
    "\n",
    "# Evaluate baseline early stopping model.\n",
    "baseline_eval = model.evaluate(\n",
    "    x_test_small,\n",
    "    y_test_small,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Evaluate strict early stopping model.\n",
    "strict_eval = model_strict.evaluate(\n",
    "    x_test_small,\n",
    "    y_test_small,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Print concise comparison of both strategies.\n",
    "print(\"Baseline ES epochs:\", epochs_baseline)\n",
    "print(\"Strict ES epochs:\", epochs_strict)\n",
    "print(\"Baseline ES test loss:\", round(baseline_eval[0], 4))\n",
    "print(\"Baseline ES test acc:\", round(baseline_eval[1], 4))\n",
    "print(\"Strict ES test loss:\", round(strict_eval[0], 4))\n",
    "print(\"Strict ES test acc:\", round(strict_eval[1], 4))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469a17c1",
   "metadata": {},
   "source": [
    "### **2.2. Checkpoint Paths and Formats**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdf33dc",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_04/Lecture_A/image_02_02.jpg?v=1769392286\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Organize checkpoints in clear, separate experiment folders\n",
    ">* Use descriptive filenames to track training progress\n",
    "\n",
    ">* Choose formats by what they store\n",
    ">* Full-model formats ease sharing and deployment\n",
    "\n",
    ">* Organize checkpoints by project, configs, and logs\n",
    ">* Keep key checkpoints to ensure reproducible, auditable models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74099f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Checkpoint Paths and Formats\n",
    "\n",
    "# This script shows simple Keras checkpoint usage.\n",
    "# It focuses on paths and file formats.\n",
    "# Run cells sequentially in a Colab notebook.\n",
    "\n",
    "# Install TensorFlow if not already available.\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import pathlib\n",
    "import random\n",
    "\n",
    "# Import TensorFlow and Keras utilities.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Set deterministic random seeds.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "\n",
    "# Set TensorFlow random seed value.\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version briefly.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Load MNIST dataset from Keras.\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Use a small subset for quick training.\n",
    "train_samples = 4000\n",
    "x_train = x_train[:train_samples]\n",
    "\n",
    "y_train = y_train[:train_samples]\n",
    "\n",
    "# Normalize pixel values to zero one.\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "\n",
    "# Add channel dimension for Conv2D.\n",
    "x_train = x_train[..., tf.newaxis]\n",
    "\n",
    "# Confirm shapes are as expected.\n",
    "assert x_train.shape[0] == train_samples\n",
    "\n",
    "# Create a simple sequential model.\n",
    "model = keras.Sequential([\n",
    "    layers.Conv2D(8, (3, 3), activation=\"relu\", input_shape=(28, 28, 1)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(10, activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "# Compile model with basic settings.\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Create a base directory for checkpoints.\n",
    "base_dir = pathlib.Path(\"checkpoints_example\")\n",
    "base_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Define an experiment specific subdirectory.\n",
    "experiment_dir = base_dir / \"mnist_small_run\"\n",
    "experiment_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Show the resolved experiment directory.\n",
    "print(\"Checkpoint directory:\", experiment_dir.resolve())\n",
    "\n",
    "# Define a filename pattern with epoch and val accuracy.\n",
    "ckpt_pattern = \"epoch_{epoch:02d}_valacc_{val_accuracy:.3f}\"\n",
    "\n",
    "# Create full checkpoint path with Keras format.\n",
    "ckpt_path = str(experiment_dir / (ckpt_pattern + \".keras\"))\n",
    "\n",
    "# Configure ModelCheckpoint callback for best model.\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=ckpt_path,\n",
    "    monitor=\"val_accuracy\",\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False,\n",
    "    mode=\"max\",\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Configure EarlyStopping to avoid long training.\n",
    "early_stop_cb = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_accuracy\",\n",
    "    patience=2,\n",
    "    restore_best_weights=True,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Train model briefly with validation split.\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=5,\n",
    "    batch_size=64,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[checkpoint_cb, early_stop_cb],\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# List checkpoint files created in directory.\n",
    "ckpt_files = sorted(experiment_dir.glob(\"*.keras\"))\n",
    "\n",
    "# Print short summary of checkpoint files.\n",
    "print(\"Number of checkpoint files:\", len(ckpt_files))\n",
    "\n",
    "# Print each checkpoint filename clearly.\n",
    "for path in ckpt_files:\n",
    "    print(\"Saved checkpoint:\", path.name)\n",
    "\n",
    "# Load best model from last checkpoint file.\n",
    "if ckpt_files:\n",
    "    best_model_path = str(ckpt_files[-1])\n",
    "else:\n",
    "    best_model_path = None\n",
    "\n",
    "# Safely load model if a checkpoint exists.\n",
    "if best_model_path is not None:\n",
    "    loaded_model = keras.models.load_model(best_model_path)\n",
    "else:\n",
    "    loaded_model = model\n",
    "\n",
    "# Evaluate loaded model on small test subset.\n",
    "x_test_small = x_test[:1000].astype(\"float32\") / 255.0\n",
    "\n",
    "# Add channel dimension for test images.\n",
    "x_test_small = x_test_small[..., tf.newaxis]\n",
    "\n",
    "y_test_small = y_test[:1000]\n",
    "\n",
    "# Confirm shapes before evaluation.\n",
    "assert x_test_small.shape[0] == y_test_small.shape[0]\n",
    "\n",
    "# Evaluate silently and print concise results.\n",
    "loss, acc = loaded_model.evaluate(\n",
    "    x_test_small,\n",
    "    y_test_small,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Show accuracy from loaded checkpoint model.\n",
    "print(\"Loaded checkpoint test accuracy:\", round(acc, 3))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbd37d9",
   "metadata": {},
   "source": [
    "### **2.3. TensorBoard Training Logs**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98444811",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_04/Lecture_A/image_02_03.jpg?v=1769392330\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* TensorBoard logs metrics over time during training\n",
    ">* Interactive dashboards reveal learning progress and stability\n",
    "\n",
    ">* Log many metrics and visualize multiple runs\n",
    ">* Compare runs to find faster, better generalization\n",
    "\n",
    ">* Visualize why early stopping and checkpoints trigger\n",
    ">* Use curves to tune hyperparameters and callbacks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29609f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - TensorBoard Training Logs\n",
    "\n",
    "# This script shows TensorBoard training logs.\n",
    "# It uses a tiny model and dataset.\n",
    "# Run it in Google Colab for practice.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required libraries safely.\n",
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "# Import TensorFlow and Keras.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "np.random.seed(7)\n",
    "tf.random.set_seed(7)\n",
    "\n",
    "# Print TensorFlow version in one line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Choose device based on GPU availability.\n",
    "physical_gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "if physical_gpus:\n",
    "    device_name = \"GPU\"\n",
    "else:\n",
    "    device_name = \"CPU\"\n",
    "\n",
    "# Print which device will be mainly used.\n",
    "print(\"Using device:\", device_name)\n",
    "\n",
    "# Load MNIST dataset from Keras.\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Use a small subset for quick training.\n",
    "train_samples = 4000\n",
    "test_samples = 1000\n",
    "\n",
    "# Slice the dataset to the chosen size.\n",
    "x_train = x_train[:train_samples]\n",
    "y_train = y_train[:train_samples]\n",
    "\n",
    "# Slice the test set similarly.\n",
    "x_test = x_test[:test_samples]\n",
    "y_test = y_test[:test_samples]\n",
    "\n",
    "# Normalize images to the range [0,1].\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_test = x_test.astype(\"float32\") / 255.0\n",
    "\n",
    "# Add channel dimension for Conv2D.\n",
    "x_train = np.expand_dims(x_train, axis=-1)\n",
    "x_test = np.expand_dims(x_test, axis=-1)\n",
    "\n",
    "# Validate shapes before building model.\n",
    "print(\"Train shape:\", x_train.shape, y_train.shape)\n",
    "\n",
    "# Build a simple CNN model.\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Conv2D(8, (3, 3), activation=\"relu\", input_shape=(28, 28, 1)),\n",
    "    keras.layers.MaxPooling2D((2, 2)),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(32, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "# Compile the model with basic settings.\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Create a base log directory for TensorBoard.\n",
    "base_log_dir = \"logs_tf_callbacks\"\n",
    "os.makedirs(base_log_dir, exist_ok=True)\n",
    "\n",
    "# Build a unique run directory using timestamp.\n",
    "run_id = datetime.datetime.now().strftime(\"run_%Y%m%d_%H%M%S\")\n",
    "log_dir = os.path.join(base_log_dir, run_id)\n",
    "\n",
    "# Create the TensorBoard callback object.\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(\n",
    "    log_dir=log_dir,\n",
    "    histogram_freq=0,\n",
    "    write_graph=True,\n",
    "    write_images=False,\n",
    ")\n",
    "\n",
    "# Create an early stopping callback.\n",
    "early_stop_cb = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=2,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "# Create a model checkpoint callback.\n",
    "checkpoint_path = os.path.join(base_log_dir, \"best_model.keras\")\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    monitor=\"val_loss\",\n",
    "    save_best_only=True,\n",
    ")\n",
    "\n",
    "# Train the model with callbacks attached.\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=5,\n",
    "    batch_size=64,\n",
    "    callbacks=[tensorboard_cb, early_stop_cb, checkpoint_cb],\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Evaluate the model silently on test data.\n",
    "loss, acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "# Print a short training summary.\n",
    "print(\"TensorBoard logs saved to:\", log_dir)\n",
    "\n",
    "# Print where the best model checkpoint is stored.\n",
    "print(\"Best model checkpoint:\", checkpoint_path)\n",
    "\n",
    "# Print final test performance metrics.\n",
    "print(\"Test loss:\", round(float(loss), 4), \"Test accuracy:\", round(float(acc), 4))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03f7fdd",
   "metadata": {},
   "source": [
    "## **3. Monitoring Training Curves**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831e44c6",
   "metadata": {},
   "source": [
    "### **3.1. Inspecting History Objects**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8fbcc2",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_04/Lecture_A/image_03_01.jpg?v=1769392369\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* History object logs metrics for every epoch\n",
    ">* It reveals training and validation performance over time\n",
    "\n",
    ">* Metrics store per-epoch training and validation values\n",
    ">* Comparing sequences reveals progress and early overfitting\n",
    "\n",
    ">* View metric histories as learning trajectories over epochs\n",
    ">* Use trajectories to spot underfitting, overfitting, and stagnation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9443260b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Inspecting History Objects\n",
    "\n",
    "# This script shows how to inspect History objects.\n",
    "# It focuses on training and validation metric trajectories.\n",
    "# Use it to connect curves with underfitting and overfitting.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Import TensorFlow and Keras utilities.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Print TensorFlow version briefly.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Load MNIST dataset from Keras datasets.\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Use a small subset to keep runtime low.\n",
    "train_samples = 4000\n",
    "test_samples = 1000\n",
    "x_train = x_train[:train_samples]\n",
    "y_train = y_train[:train_samples]\n",
    "\n",
    "# Reduce test set size for quick evaluation.\n",
    "x_test = x_test[:test_samples]\n",
    "y_test = y_test[:test_samples]\n",
    "\n",
    "# Normalize pixel values to range zero one.\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_test = x_test.astype(\"float32\") / 255.0\n",
    "\n",
    "# Add channel dimension for convolutional layers.\n",
    "x_train = np.expand_dims(x_train, axis=-1)\n",
    "x_test = np.expand_dims(x_test, axis=-1)\n",
    "\n",
    "# Validate shapes before building the model.\n",
    "print(\"Train shape:\", x_train.shape, y_train.shape)\n",
    "print(\"Test shape:\", x_test.shape, y_test.shape)\n",
    "\n",
    "# Build a small convolutional neural network.\n",
    "model = keras.Sequential([\n",
    "    layers.Conv2D(16, (3, 3), activation=\"relu\", input_shape=(28, 28, 1)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(32, activation=\"relu\"),\n",
    "    layers.Dense(10, activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "# Compile the model with accuracy metric.\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Train the model quietly and capture the History.\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=8,\n",
    "    batch_size=64,\n",
    "    validation_split=0.2,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Access the history dictionary from the History object.\n",
    "history_dict = history.history\n",
    "\n",
    "# Print available metric keys stored per epoch.\n",
    "print(\"History keys:\", list(history_dict.keys()))\n",
    "\n",
    "# Extract training and validation accuracy sequences.\n",
    "train_acc = history_dict.get(\"accuracy\", [])\n",
    "val_acc = history_dict.get(\"val_accuracy\", [])\n",
    "\n",
    "# Extract training and validation loss sequences.\n",
    "train_loss = history_dict.get(\"loss\", [])\n",
    "val_loss = history_dict.get(\"val_loss\", [])\n",
    "\n",
    "# Print first three epochs to inspect trajectories.\n",
    "for epoch in range(3):\n",
    "    ta = float(train_acc[epoch])\n",
    "    va = float(val_acc[epoch])\n",
    "    tl = float(train_loss[epoch])\n",
    "    vl = float(val_loss[epoch])\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}: acc={ta:.3f}, val_acc={va:.3f}, \"\n",
    "        f\"loss={tl:.3f}, val_loss={vl:.3f}\"\n",
    "    )\n",
    "\n",
    "# Print final epoch metrics to compare with early epochs.\n",
    "last = len(train_acc) - 1\n",
    "print(\n",
    "    f\"Final epoch {last+1}: acc={train_acc[last]:.3f}, \"\n",
    "    f\"val_acc={val_acc[last]:.3f}\"\n",
    ")\n",
    "\n",
    "# Import matplotlib for a single compact plot.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a simple plot of loss curves over epochs.\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.plot(train_loss, label=\"train_loss\")\n",
    "plt.plot(val_loss, label=\"val_loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training vs validation loss trajectory\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f45c70e",
   "metadata": {},
   "source": [
    "### **3.2. Visualizing Training Metrics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68d88be",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_04/Lecture_A/image_03_02.jpg?v=1769392413\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Plot loss and accuracy across training epochs\n",
    ">* Use curves to spot learning patterns and issues\n",
    "\n",
    ">* Plot key training and validation metrics together\n",
    ">* Use curves to spot plateaus and overtraining\n",
    "\n",
    ">* Compare runs to see hyperparameter effects visually\n",
    ">* Use curves as a dashboard for overfitting\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1696fd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Visualizing Training Metrics\n",
    "\n",
    "# This script visualizes training and validation metrics.\n",
    "# It uses a tiny MNIST subset for speed.\n",
    "# Focus is on reading training curves.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Import TensorFlow and Keras utilities.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Select device based on GPU availability.\n",
    "physical_gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "use_gpu = bool(physical_gpus)\n",
    "print(\"Using GPU:\", use_gpu)\n",
    "\n",
    "# Load MNIST dataset from Keras datasets.\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Reduce dataset size for quick demonstration.\n",
    "train_samples = 4000\n",
    "test_samples = 1000\n",
    "x_train = x_train[:train_samples]\n",
    "y_train = y_train[:train_samples]\n",
    "\n",
    "# Slice test data to a small subset.\n",
    "x_test = x_test[:test_samples]\n",
    "y_test = y_test[:test_samples]\n",
    "\n",
    "# Normalize images to range zero to one.\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_test = x_test.astype(\"float32\") / 255.0\n",
    "\n",
    "# Add channel dimension for convolutional layers.\n",
    "x_train = np.expand_dims(x_train, axis=-1)\n",
    "x_test = np.expand_dims(x_test, axis=-1)\n",
    "\n",
    "# Validate shapes before building model.\n",
    "print(\"Train shape:\", x_train.shape, y_train.shape)\n",
    "print(\"Test shape:\", x_test.shape, y_test.shape)\n",
    "\n",
    "# Build a small convolutional neural network.\n",
    "model = keras.Sequential([\n",
    "    layers.Conv2D(16, (3, 3), activation=\"relu\", input_shape=(28, 28, 1)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(32, activation=\"relu\"),\n",
    "    layers.Dense(10, activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "# Compile model with suitable loss and metrics.\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Train model with validation split and silent logs.\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=8,\n",
    "    batch_size=64,\n",
    "    validation_split=0.2,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Extract history dictionary for plotting.\n",
    "history_dict = history.history\n",
    "\n",
    "# Import matplotlib for plotting curves.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a new figure for loss curves.\n",
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "# Plot training and validation loss over epochs.\n",
    "plt.plot(history.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"val_loss\")\n",
    "\n",
    "# Label axes and add legend for clarity.\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training vs Validation Loss\")\n",
    "plt.legend()\n",
    "\n",
    "# Display the plot to visualize learning.\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeeb4d72",
   "metadata": {},
   "source": [
    "### **3.3. Spotting Overfitting Trends**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b348f6",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_04/Lecture_A/image_03_03.jpg?v=1769392450\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Overfitting means memorizing training data, not generalizing\n",
    ">* Training improves while validation worsens, revealing overfitting\n",
    "\n",
    ">* Ignore single noisy spikes in validation metrics\n",
    ">* Overfitting shows sustained rising validation loss, plateaued accuracy\n",
    "\n",
    ">* Use divergence to stop or adjust training\n",
    ">* Apply regularization or simplification to prevent overfitting\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846d7b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Spotting Overfitting Trends\n",
    "\n",
    "# This script shows overfitting using training curves.\n",
    "# It trains a small model on MNIST digits.\n",
    "# Then it plots training and validation loss trends.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required libraries safely.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Select device preference based on availability.\n",
    "physical_gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "if physical_gpus:\n",
    "    device_type = \"GPU\"\n",
    "else:\n",
    "    device_type = \"CPU\"\n",
    "print(\"Using device type:\", device_type)\n",
    "\n",
    "# Load MNIST dataset from Keras utilities.\n",
    "(mnist_x_train, mnist_y_train), (mnist_x_test, mnist_y_test) = (\n",
    "    tf.keras.datasets.mnist.load_data()\n",
    ")\n",
    "\n",
    "# Confirm dataset shapes before subsampling.\n",
    "print(\"Train shape:\", mnist_x_train.shape, \"Test shape:\", mnist_x_test.shape)\n",
    "\n",
    "# Reduce dataset size to speed up training.\n",
    "train_samples = 4000\n",
    "val_samples = 2000\n",
    "x_train_small = mnist_x_train[:train_samples]\n",
    "y_train_small = mnist_y_train[:train_samples]\n",
    "\n",
    "# Create a validation split from training data.\n",
    "x_val_small = mnist_x_train[train_samples:train_samples + val_samples]\n",
    "y_val_small = mnist_y_train[train_samples:train_samples + val_samples]\n",
    "\n",
    "# Normalize pixel values to range zero one.\n",
    "x_train_small = x_train_small.astype(\"float32\") / 255.0\n",
    "x_val_small = x_val_small.astype(\"float32\") / 255.0\n",
    "\n",
    "# Add channel dimension for convolutional layers.\n",
    "x_train_small = np.expand_dims(x_train_small, axis=-1)\n",
    "x_val_small = np.expand_dims(x_val_small, axis=-1)\n",
    "\n",
    "# Verify shapes after preprocessing steps.\n",
    "print(\"Train small shape:\", x_train_small.shape)\n",
    "print(\"Val small shape:\", x_val_small.shape)\n",
    "\n",
    "# Build a small convolutional model.\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(\n",
    "        16,\n",
    "        (3, 3),\n",
    "        activation=\"relu\",\n",
    "        input_shape=(28, 28, 1),\n",
    "    ),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "# Compile model with suitable loss and optimizer.\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Train for many epochs to encourage overfitting.\n",
    "history = model.fit(\n",
    "    x_train_small,\n",
    "    y_train_small,\n",
    "    epochs=40,\n",
    "    batch_size=64,\n",
    "    validation_data=(x_val_small, y_val_small),\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Extract loss curves from training history.\n",
    "train_loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "\n",
    "# Print final training and validation losses.\n",
    "print(\"Final train loss:\", round(train_loss[-1], 4))\n",
    "print(\"Final val loss:\", round(val_loss[-1], 4))\n",
    "\n",
    "# Print simple message about overfitting trend.\n",
    "if val_loss[-1] > min(val_loss):\n",
    "    print(\"Validation loss increased after its best epoch.\")\n",
    "else:\n",
    "    print(\"Validation loss did not clearly increase.\")\n",
    "\n",
    "# Import matplotlib for plotting curves.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a figure for loss curves.\n",
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "# Plot training loss across epochs.\n",
    "plt.plot(train_loss, label=\"Train loss\")\n",
    "\n",
    "# Plot validation loss across epochs.\n",
    "plt.plot(val_loss, label=\"Val loss\")\n",
    "\n",
    "# Add labels and legend for clarity.\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training vs validation loss over epochs\")\n",
    "plt.legend()\n",
    "\n",
    "# Display the plot to visually inspect overfitting.\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89874605",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Using model.fit**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2391c18f",
   "metadata": {},
   "source": [
    "\n",
    "In this lecture, you learned to:\n",
    "- Configure model.fit with appropriate batch sizes, epochs, and validation strategies for a given dataset. \n",
    "- Use Keras callbacks to monitor training, implement early stopping, and save model checkpoints. \n",
    "- Interpret training and validation curves to diagnose underfitting and overfitting. \n",
    "\n",
    "In the next Lecture (Lecture B), we will go over 'Custom Training Loops'"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
