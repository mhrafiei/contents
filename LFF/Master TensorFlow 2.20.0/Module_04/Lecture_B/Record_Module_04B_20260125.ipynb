{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb799ddc",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Custom Training Loops**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6bce7b",
   "metadata": {},
   "source": [
    ">Last update: 20260125.\n",
    "    \n",
    "By the end of this Lecture, you will be able to:\n",
    "- Implement a per-batch training step using tf.GradientTape and a Keras model. \n",
    "- Wrap training steps with tf.function to improve performance while preserving debuggability. \n",
    "- Track custom metrics within a training loop and reset them appropriately each epoch. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0471b9f8",
   "metadata": {},
   "source": [
    "## **1. Per Batch Train Step**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c8cb99",
   "metadata": {},
   "source": [
    "### **1.1. Model Forward Pass**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8c1364",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_04/Lecture_B/image_01_01.jpg?v=1769396583\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Forward pass feeds each batch through the model\n",
    ">* Transforms inputs to predictions; later steps assess quality\n",
    "\n",
    ">* Forward pass is recorded for automatic differentiation\n",
    ">* Tracing links inputs, weights, predictions for gradients\n",
    "\n",
    ">* Training forward pass uses training-specific layer behavior\n",
    ">* Ensures realistic learning, stable statistics, differentiable predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f1b4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Model Forward Pass\n",
    "\n",
    "# This script demonstrates a simple model forward pass.\n",
    "# It focuses on per batch predictions during training.\n",
    "# We keep outputs small and explanations beginner friendly.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries safely.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Import TensorFlow and Keras submodules.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Set deterministic random seeds for reproducibility.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Select device preference based on GPU availability.\n",
    "physical_gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "if physical_gpus:\n",
    "    device_name = \"GPU\"\n",
    "else:\n",
    "    device_name = \"CPU\"\n",
    "\n",
    "# Print which device type will likely be used.\n",
    "print(\"Running on device type:\", device_name)\n",
    "\n",
    "# Load MNIST dataset using Keras helper.\n",
    "(x_train, y_train), _ = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Select a very small subset for quick demonstration.\n",
    "subset_size = 64\n",
    "x_train_small = x_train[:subset_size]\n",
    "y_train_small = y_train[:subset_size]\n",
    "\n",
    "# Normalize images to float32 in range zero one.\n",
    "x_train_small = x_train_small.astype(\"float32\") / 255.0\n",
    "\n",
    "# Add channel dimension to match model expectations.\n",
    "x_train_small = np.expand_dims(x_train_small, axis=-1)\n",
    "\n",
    "# Validate input shape before building the model.\n",
    "print(\"Input batch shape:\", x_train_small.shape)\n",
    "\n",
    "# Build a simple sequential Keras model.\n",
    "model = keras.Sequential([\n",
    "    layers.Input(shape=(28, 28, 1)),\n",
    "    layers.Conv2D(8, (3, 3), activation=\"relu\"),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(10, activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "# Compile model to attach loss and optimizer.\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Take one small batch for the forward pass.\n",
    "batch_size = 16\n",
    "x_batch = x_train_small[:batch_size]\n",
    "y_batch = y_train_small[:batch_size]\n",
    "\n",
    "# Confirm batch shapes before passing to the model.\n",
    "print(\"Batch images shape:\", x_batch.shape)\n",
    "print(\"Batch labels shape:\", y_batch.shape)\n",
    "\n",
    "# Define a single forward pass using GradientTape.\n",
    "@tf.function\n",
    "def forward_pass_step(inputs):\n",
    "    # Record operations for automatic differentiation.\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(inputs, training=True)\n",
    "    return predictions\n",
    "\n",
    "# Run the forward pass on one training batch.\n",
    "pred_batch = forward_pass_step(x_batch)\n",
    "\n",
    "# Check that predictions have expected shape.\n",
    "print(\"Predictions shape:\", pred_batch.shape)\n",
    "\n",
    "# Convert predictions to numpy for simple inspection.\n",
    "pred_batch_np = pred_batch.numpy()\n",
    "\n",
    "# Print predictions for the first three examples.\n",
    "for i in range(3):\n",
    "    class_probs = pred_batch_np[i]\n",
    "    predicted_class = int(np.argmax(class_probs))\n",
    "    true_class = int(y_batch[i])\n",
    "    print(\n",
    "        \"Example\",\n",
    "        i,\n",
    "        \"true:\",\n",
    "        true_class,\n",
    "        \"pred:\",\n",
    "        predicted_class,\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b0d680",
   "metadata": {},
   "source": [
    "### **1.2. Batch Loss Calculation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d8ea38",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_04/Lecture_B/image_01_02.jpg?v=1769396662\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Compare predictions to targets using a loss\n",
    ">* Aggregate example losses into one scalar batch loss\n",
    "\n",
    ">* Loss choice depends on task and labels\n",
    ">* Loss is differentiable scalar used for gradients\n",
    "\n",
    ">* Loss choice shapes model focus and behavior\n",
    ">* Monitoring decreasing batch loss shows learning progress\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826fa748",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Batch Loss Calculation\n",
    "\n",
    "# This script shows batch loss calculation clearly.\n",
    "# It uses TensorFlow for a tiny demo model.\n",
    "# Focus is on per batch loss with GradientTape.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required standard libraries safely.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Import TensorFlow and Keras submodules.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "\n",
    "# Set NumPy and TensorFlow seeds also.\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Choose device automatically based on availability.\n",
    "physical_gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "\n",
    "# Inform which device type will be used.\n",
    "if physical_gpus:\n",
    "    device_type = \"GPU\"\n",
    "else:\n",
    "    device_type = \"CPU\"\n",
    "\n",
    "# Print selected device type briefly.\n",
    "print(\"Using device type:\", device_type)\n",
    "\n",
    "# Create a tiny synthetic regression dataset.\n",
    "num_samples = 64\n",
    "input_dim = 3\n",
    "\n",
    "# Generate random input features with small values.\n",
    "features = np.random.randn(num_samples, input_dim).astype(\"float32\")\n",
    "\n",
    "# Define true weights and bias for targets.\n",
    "true_w = np.array([[2.0], [-1.0], [0.5]], dtype=\"float32\")\n",
    "\n",
    "# Compute noiseless targets using matrix multiplication.\n",
    "targets = features @ true_w + 0.3\n",
    "\n",
    "# Add small Gaussian noise to targets.\n",
    "noise = 0.05 * np.random.randn(num_samples, 1).astype(\"float32\")\n",
    "\n",
    "# Final noisy targets for training batches.\n",
    "targets = targets + noise\n",
    "\n",
    "# Wrap data into a tf.data.Dataset object.\n",
    "dataset = tf.data.Dataset.from_tensor_slices((features, targets))\n",
    "\n",
    "# Shuffle and batch the dataset for training.\n",
    "batch_size = 8\n",
    "dataset = dataset.shuffle(buffer_size=num_samples).batch(batch_size)\n",
    "\n",
    "# Build a simple Keras regression model.\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Input(shape=(input_dim,)),\n",
    "    keras.layers.Dense(4, activation=\"relu\"),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# Create a mean squared error loss instance.\n",
    "loss_fn = keras.losses.MeanSquaredError()\n",
    "\n",
    "# Create an optimizer for parameter updates.\n",
    "optimizer = keras.optimizers.SGD(learning_rate=0.1)\n",
    "\n",
    "# Define a metric to track mean batch loss.\n",
    "train_loss_metric = keras.metrics.Mean(name=\"train_loss\")\n",
    "\n",
    "# Define one training step using GradientTape.\n",
    "@tf.function\n",
    "def train_step(batch_x, batch_y):\n",
    "    # Validate shapes before forward pass.\n",
    "    tf.debugging.assert_rank(batch_x, 2)\n",
    "    tf.debugging.assert_rank(batch_y, 2)\n",
    "\n",
    "    # Record operations for automatic differentiation.\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(batch_x, training=True)\n",
    "\n",
    "        # Compute scalar batch loss from predictions.\n",
    "        batch_loss = loss_fn(batch_y, predictions)\n",
    "\n",
    "    # Compute gradients of loss with respect to weights.\n",
    "    gradients = tape.gradient(batch_loss, model.trainable_variables)\n",
    "\n",
    "    # Apply gradients to update model parameters.\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    # Update running metric with current batch loss.\n",
    "    train_loss_metric.update_state(batch_loss)\n",
    "\n",
    "    # Return scalar batch loss for optional inspection.\n",
    "    return batch_loss\n",
    "\n",
    "# Run a tiny custom training loop for two epochs.\n",
    "num_epochs = 2\n",
    "for epoch in range(num_epochs):\n",
    "    # Reset metric state at the start of each epoch.\n",
    "    train_loss_metric.reset_state()\n",
    "\n",
    "    # Iterate over small number of batches.\n",
    "    for step, (batch_x, batch_y) in enumerate(dataset):\n",
    "        batch_loss_value = train_step(batch_x, batch_y)\n",
    "\n",
    "        # Print first batch loss each epoch only.\n",
    "        if step == 0:\n",
    "            print(\n",
    "                f\"Epoch {epoch + 1}, first batch loss:\",\n",
    "                float(batch_loss_value)\n",
    "            )\n",
    "\n",
    "    # Print mean loss across all batches this epoch.\n",
    "    print(\n",
    "        f\"Epoch {epoch + 1}, mean batch loss:\",\n",
    "        float(train_loss_metric.result())\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb23e61b",
   "metadata": {},
   "source": [
    "### **1.3. Gradient Calculation and Update**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dee34aa",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_04/Lecture_B/image_01_03.jpg?v=1769396699\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Gradients show how each weight affects loss\n",
    ">* Autodiff backpropagates to produce optimizer-ready gradients\n",
    "\n",
    ">* Optimizer pairs gradients with weights and updates\n",
    ">* Small gradient-based steps gradually improve model predictions\n",
    "\n",
    ">* Manage gradient size and target correct variables\n",
    ">* Stable gradients give smoother training and better models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f194cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Gradient Calculation and Update\n",
    "\n",
    "# This script shows gradient calculation and updates.\n",
    "# It uses tf.GradientTape with a simple model.\n",
    "# Focus on one batch training step implementation.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required TensorFlow and NumPy modules.\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Set deterministic seeds for reproducible behavior.\n",
    "tf.random.set_seed(7)\n",
    "np.random.seed(7)\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Select device string based on GPU availability.\n",
    "physical_gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "if physical_gpus:\n",
    "    device_name = \"GPU available\"\n",
    "else:\n",
    "    device_name = \"GPU not available\"\n",
    "\n",
    "# Show which type of device will be used.\n",
    "print(\"Device status:\", device_name)\n",
    "\n",
    "# Create tiny synthetic input data for demonstration.\n",
    "inputs = np.linspace(-1.0, 1.0, num=8, dtype=np.float32)\n",
    "inputs = inputs.reshape(-1, 1)\n",
    "\n",
    "# Create simple target values using a linear rule.\n",
    "targets = 3.0 * inputs + 0.5\n",
    "\n",
    "# Wrap arrays as TensorFlow tensors with float32.\n",
    "inputs_tf = tf.convert_to_tensor(inputs, dtype=tf.float32)\n",
    "\n",
    "# Ensure targets tensor has matching shape.\n",
    "targets_tf = tf.convert_to_tensor(targets, dtype=tf.float32)\n",
    "\n",
    "# Confirm shapes are compatible for training.\n",
    "assert inputs_tf.shape == targets_tf.shape\n",
    "\n",
    "# Build a tiny Keras model with one Dense layer.\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(1, input_shape=(1,))\n",
    "])\n",
    "\n",
    "# Create a mean squared error loss function.\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "# Create a simple SGD optimizer with small learning rate.\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)\n",
    "\n",
    "# Define one training step using GradientTape.\n",
    "@tf.function\n",
    "def train_step(x_batch, y_batch):\n",
    "    # Record operations for automatic differentiation.\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(x_batch, training=True)\n",
    "        loss_value = loss_fn(y_batch, predictions)\n",
    "\n",
    "    # Compute gradients with respect to trainable variables.\n",
    "    gradients = tape.gradient(loss_value, model.trainable_variables)\n",
    "\n",
    "    # Optionally clip gradients to avoid extreme values.\n",
    "    clipped_gradients = [\n",
    "        tf.clip_by_norm(g, clip_norm=1.0) for g in gradients\n",
    "    ]\n",
    "\n",
    "    # Apply gradients to update model parameters.\n",
    "    optimizer.apply_gradients(\n",
    "        zip(clipped_gradients, model.trainable_variables)\n",
    "    )\n",
    "\n",
    "    # Return scalar loss and gradient norms for inspection.\n",
    "    grad_norms = [tf.norm(g) for g in clipped_gradients]\n",
    "    return loss_value, grad_norms\n",
    "\n",
    "# Run a few manual training steps on the same batch.\n",
    "num_steps = 3\n",
    "for step in range(num_steps):\n",
    "    loss_value, grad_norms = train_step(inputs_tf, targets_tf)\n",
    "    grad_norms_np = [float(g.numpy()) for g in grad_norms]\n",
    "    print(\n",
    "        \"Step\", step, \"loss:\", float(loss_value.numpy()),\n",
    "        \"grad_norms:\", grad_norms_np\n",
    "    )\n",
    "\n",
    "# Show final predictions after gradient updates.\n",
    "final_preds = model(inputs_tf, training=False)\n",
    "print(\"Final predictions:\", final_preds.numpy().reshape(-1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8ce499",
   "metadata": {},
   "source": [
    "## **2. Mastering tf function**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a722d3",
   "metadata": {},
   "source": [
    "### **2.1. Decorating the Training Step**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6c6fd1",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_04/Lecture_B/image_02_01.jpg?v=1769396737\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* tf.function compiles training steps into fast graphs\n",
    ">* Reduces Python overhead, speeding many training iterations\n",
    "\n",
    ">* Define training_step to take tensors, return tensors\n",
    ">* Call compiled training_step inside simple Python loops\n",
    "\n",
    ">* Balance graph performance with training transparency and debugging\n",
    ">* Keep core math in graph, control in Python\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c8462d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Decorating the Training Step\n",
    "\n",
    "# This script shows tf function on training steps.\n",
    "# It uses a tiny model and dataset subset.\n",
    "# Focus is decorating training step for speed.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required TensorFlow modules.\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "tf.random.set_seed(7)\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Select device string based on GPU availability.\n",
    "physical_gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "\n",
    "# Choose device name for information only.\n",
    "if physical_gpus:\n",
    "    device_name = \"GPU\"\n",
    "else:\n",
    "    device_name = \"CPU\"\n",
    "\n",
    "# Print which device type is selected.\n",
    "print(\"Using device type:\", device_name)\n",
    "\n",
    "# Load MNIST dataset from Keras datasets.\n",
    "(mnist_x_train, mnist_y_train), _ = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize images to float32 in range zero one.\n",
    "mnist_x_train = mnist_x_train.astype(\"float32\") / 255.0\n",
    "\n",
    "# Add channel dimension to images for Conv2D.\n",
    "mnist_x_train = mnist_x_train[..., tf.newaxis]\n",
    "\n",
    "# Take a small subset for quick demonstration.\n",
    "subset_size = 2048\n",
    "\n",
    "# Slice the subset safely from training data.\n",
    "train_images = mnist_x_train[:subset_size]\n",
    "\n",
    "# Slice corresponding labels for the subset.\n",
    "train_labels = mnist_y_train[:subset_size]\n",
    "\n",
    "# Validate shapes before building dataset.\n",
    "assert train_images.shape[0] == subset_size\n",
    "\n",
    "# Create tf.data dataset from tensors.\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
    "\n",
    "# Shuffle and batch the dataset for training.\n",
    "train_ds = train_ds.shuffle(1024, seed=7).batch(64)\n",
    "\n",
    "# Build a simple sequential convolutional model.\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(8, (3, 3), activation=\"relu\", input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(32, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "# Define loss function for sparse classification.\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Define optimizer with a small learning rate.\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# Define a metric to track mean training loss.\n",
    "train_loss_metric = tf.keras.metrics.Mean(name=\"train_loss\")\n",
    "\n",
    "# Define a metric to track training accuracy.\n",
    "train_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy(name=\"train_acc\")\n",
    "\n",
    "# Define one training step without decoration first.\n",
    "def plain_train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(images, training=True)\n",
    "        loss_value = loss_fn(labels, logits)\n",
    "    gradients = tape.gradient(loss_value, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    train_loss_metric.update_state(loss_value)\n",
    "    train_acc_metric.update_state(labels, logits)\n",
    "\n",
    "# Decorate the training step using tf function.\n",
    "@tf.function\n",
    "def graph_train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(images, training=True)\n",
    "        loss_value = loss_fn(labels, logits)\n",
    "    gradients = tape.gradient(loss_value, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    train_loss_metric.update_state(loss_value)\n",
    "    train_acc_metric.update_state(labels, logits)\n",
    "\n",
    "# Run one warmup epoch using plain training step.\n",
    "for batch_images, batch_labels in train_ds:\n",
    "    plain_train_step(batch_images, batch_labels)\n",
    "\n",
    "# Read metrics after warmup epoch.\n",
    "warmup_loss = float(train_loss_metric.result().numpy())\n",
    "\n",
    "# Read accuracy metric after warmup epoch.\n",
    "warmup_acc = float(train_acc_metric.result().numpy())\n",
    "\n",
    "# Reset metrics before tf function training epoch.\n",
    "train_loss_metric.reset_state()\n",
    "\n",
    "# Reset accuracy metric for a clean second epoch.\n",
    "train_acc_metric.reset_state()\n",
    "\n",
    "# Run one epoch using the tf function training step.\n",
    "for batch_images, batch_labels in train_ds:\n",
    "    graph_train_step(batch_images, batch_labels)\n",
    "\n",
    "# Read metrics after tf function epoch.\n",
    "wrapped_loss = float(train_loss_metric.result().numpy())\n",
    "\n",
    "# Read accuracy metric after tf function epoch.\n",
    "wrapped_acc = float(train_acc_metric.result().numpy())\n",
    "\n",
    "# Print a short comparison of both epochs.\n",
    "print(\"Warmup epoch loss and accuracy:\", round(warmup_loss, 4), round(warmup_acc, 4))\n",
    "\n",
    "# Print metrics for the tf function decorated epoch.\n",
    "print(\"tf.function epoch loss and accuracy:\", round(wrapped_loss, 4), round(wrapped_acc, 4))\n",
    "\n",
    "# Print a final note about tf function usage.\n",
    "print(\"Training step decorated with tf.function executed successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6c9be7",
   "metadata": {},
   "source": [
    "### **2.2. Tracing vs Retracing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f46e4e",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_04/Lecture_B/image_02_02.jpg?v=1769396782\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Tracing builds an optimized graph from Python code\n",
    ">* Later calls reuse this graph for faster training\n",
    "\n",
    ">* Retracing happens when inputs or control flow change\n",
    ">* Frequent retracing adds overhead and reduces performance\n",
    "\n",
    ">* Keep inputs consistent to avoid frequent retracing\n",
    ">* Stable graphs give fast training and easy debugging\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37aaf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Tracing vs Retracing\n",
    "\n",
    "# This script demonstrates tracing versus retracing simply.\n",
    "# We use a tiny model and custom tf function steps.\n",
    "# Focus on input shapes and dtypes affecting tracing.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import TensorFlow and NumPy libraries.\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "tf.random.set_seed(7)\n",
    "np.random.seed(7)\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Select device string based on GPU availability.\n",
    "device = \"GPU\" if tf.config.list_physical_devices(\"GPU\") else \"CPU\"\n",
    "\n",
    "# Print which device type will be used.\n",
    "print(\"Running on device type:\", device)\n",
    "\n",
    "# Create a simple dense model for demonstration.\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(4,)),\n",
    "    tf.keras.layers.Dense(3, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# Create a simple optimizer instance.\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)\n",
    "\n",
    "# Define a basic mean squared error loss.\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "# Define a tf function with fixed input signature.\n",
    "@tf.function(input_signature=[\n",
    "    tf.TensorSpec(shape=(None, 4), dtype=tf.float32),\n",
    "    tf.TensorSpec(shape=(None, 1), dtype=tf.float32)\n",
    "])\n",
    "\n",
    "def train_step_fixed(x_batch, y_batch):\n",
    "    # Record operations for automatic differentiation.\n",
    "    with tf.GradientTape() as tape:\n",
    "        preds = model(x_batch, training=True)\n",
    "        loss = loss_fn(y_batch, preds)\n",
    "    # Compute gradients and apply them.\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "# Define a tf function without fixed signature.\n",
    "@tf.function\n",
    "\n",
    "def train_step_flexible(x_batch, y_batch):\n",
    "    # Same body but no explicit input signature.\n",
    "    with tf.GradientTape() as tape:\n",
    "        preds = model(x_batch, training=True)\n",
    "        loss = loss_fn(y_batch, preds)\n",
    "    # Apply gradients to update model weights.\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "# Create helper to build random batches with given shape.\n",
    "def make_batch(batch_size, feature_dim):\n",
    "    # Create random features and targets with correct shapes.\n",
    "    x = np.random.randn(batch_size, feature_dim).astype(\"float32\")\n",
    "    y = np.random.randn(batch_size, 1).astype(\"float32\")\n",
    "    return x, y\n",
    "\n",
    "# Prepare two batches with same shape for fixed function.\n",
    "x1, y1 = make_batch(batch_size=8, feature_dim=4)\n",
    "\n",
    "# Call fixed function twice to reuse traced graph.\n",
    "loss1 = train_step_fixed(x1, y1)\n",
    "loss2 = train_step_fixed(x1, y1)\n",
    "\n",
    "# Print losses to show normal execution reuse.\n",
    "print(\"Fixed signature losses:\", float(loss1), float(loss2))\n",
    "\n",
    "# Prepare batches with different batch sizes for flexible.\n",
    "x2, y2 = make_batch(batch_size=4, feature_dim=4)\n",
    "\n",
    "# Call flexible function with first batch size.\n",
    "loss3 = train_step_flexible(x1, y1)\n",
    "\n",
    "# Call flexible function with different batch size.\n",
    "loss4 = train_step_flexible(x2, y2)\n",
    "\n",
    "# Print losses to highlight potential retracing behavior.\n",
    "print(\"Flexible signature losses:\", float(loss3), float(loss4))\n",
    "\n",
    "# Show final model prediction shape for confirmation.\n",
    "print(\"Final prediction shape:\", model(x1).shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3446d9b8",
   "metadata": {},
   "source": [
    "### **2.3. Debugging Autograph Issues**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8492b39",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_04/Lecture_B/image_02_03.jpg?v=1769396819\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Autograph rewrites Python control flow into graphs\n",
    ">* Graph execution can break prints and dynamic patterns\n",
    "\n",
    ">* Disable tf.function, test pieces in eager\n",
    ">* Reenable graph, simplify code, use tf logging\n",
    "\n",
    ">* Read tracing errors to spot incompatible Python patterns\n",
    ">* Refactor to tensor operations and use TF logging\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e2e03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Debugging Autograph Issues\n",
    "\n",
    "# This script demonstrates debugging TensorFlow Autograph issues.\n",
    "# It compares eager and tf function training steps safely.\n",
    "# It keeps prints minimal while showing key behaviors.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required TensorFlow module.\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set deterministic random seeds.\n",
    "tf.random.set_seed(7)\n",
    "\n",
    "# Print TensorFlow version briefly.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Select available device type string.\n",
    "device_type = \"GPU\" if tf.config.list_physical_devices(\"GPU\") else \"CPU\"\n",
    "\n",
    "# Print selected device type.\n",
    "print(\"Using device type:\", device_type)\n",
    "\n",
    "# Create a tiny synthetic regression dataset.\n",
    "features = tf.random.normal(shape=(16, 3))\n",
    "\n",
    "# Create targets as simple linear combination.\n",
    "targets = tf.reduce_sum(features, axis=1, keepdims=True)\n",
    "\n",
    "# Validate dataset shapes defensively.\n",
    "assert features.shape == (16, 3)\n",
    "\n",
    "# Validate target shape defensively.\n",
    "assert targets.shape == (16, 1)\n",
    "\n",
    "# Build a small Keras model.\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(3,)),\n",
    "    tf.keras.layers.Dense(4, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1),\n",
    "])\n",
    "\n",
    "# Create an optimizer instance.\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)\n",
    "\n",
    "# Define a simple loss function.\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "# Define a metric to track mean loss.\n",
    "train_metric = tf.keras.metrics.Mean(name=\"train_loss\")\n",
    "\n",
    "# Define an eager training step for clarity.\n",
    "def train_step_eager(x_batch, y_batch):\n",
    "    with tf.GradientTape() as tape:\n",
    "        preds = model(x_batch, training=True)\n",
    "        loss = loss_fn(y_batch, preds)\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    train_metric.update_state(loss)\n",
    "    return loss\n",
    "\n",
    "# Define a tf function training step with autograph.\n",
    "@tf.function\n",
    "def train_step_graph(x_batch, y_batch):\n",
    "    tf.debugging.assert_shapes([(x_batch, (None, 3)), (y_batch, (None, 1))])\n",
    "    with tf.GradientTape() as tape:\n",
    "        preds = model(x_batch, training=True)\n",
    "        loss = loss_fn(y_batch, preds)\n",
    "        tf.print(\"Inside graph loss:\", loss)\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    train_metric.update_state(loss)\n",
    "    return loss\n",
    "\n",
    "# Run one eager step to verify behavior.\n",
    "loss_eager = train_step_eager(features, targets)\n",
    "\n",
    "# Print eager mode loss value.\n",
    "print(\"Eager step loss:\", float(loss_eager))\n",
    "\n",
    "# Reset metric before graph execution.\n",
    "train_metric.reset_state()\n",
    "\n",
    "# Run one graph step to compare behavior.\n",
    "loss_graph = train_step_graph(features, targets)\n",
    "\n",
    "# Print graph mode loss value.\n",
    "print(\"Graph step loss:\", float(loss_graph))\n",
    "\n",
    "# Show metric value after graph step.\n",
    "print(\"Tracked metric loss:\", float(train_metric.result()))\n",
    "\n",
    "# Final confirmation message about script completion.\n",
    "print(\"Finished Autograph debugging demonstration.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733ea925",
   "metadata": {},
   "source": [
    "## **3. Metrics in Training Loops**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ead27f2",
   "metadata": {},
   "source": [
    "### **3.1. Using tf.keras.metrics.Metric**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62519456",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_04/Lecture_B/image_03_01.jpg?v=1769396892\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Metric objects track performance across many batches\n",
    ">* They give flexible, custom metrics for any workflow\n",
    "\n",
    ">* Metric objects act like reusable measurement gauges\n",
    ">* They centralize statistics, simplifying code and reducing bugs\n",
    "\n",
    ">* Define and reuse multiple metrics across training phases\n",
    ">* Swap or extend metrics without changing loop structure\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd7d089",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Using tf.keras.metrics.Metric\n",
    "\n",
    "# This script shows metrics in custom loops.\n",
    "# It uses TensorFlow 2.20.0 with small data.\n",
    "# Focus is on tf.keras.metrics.Metric usage.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required libraries safely.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set deterministic seeds for reproducibility.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version briefly.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Select device preferring GPU when available.\n",
    "physical_gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "if physical_gpus:\n",
    "    device_name = \"GPU\"\n",
    "else:\n",
    "    device_name = \"CPU\"\n",
    "print(\"Using device type:\", device_name)\n",
    "\n",
    "# Load MNIST dataset from Keras datasets.\n",
    "(mnist_x_train, mnist_y_train), _ = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Reduce dataset size for quick demonstration.\n",
    "train_images = mnist_x_train[:2000].astype(\"float32\") / 255.0\n",
    "train_labels = mnist_y_train[:2000].astype(\"int32\")\n",
    "\n",
    "# Add channel dimension to images.\n",
    "train_images = np.expand_dims(train_images, axis=-1)\n",
    "\n",
    "# Validate shapes before building dataset.\n",
    "assert train_images.shape[0] == train_labels.shape[0]\n",
    "assert train_images.ndim == 4 and train_labels.ndim == 1\n",
    "\n",
    "# Create small tf.data.Dataset for training.\n",
    "batch_size = 64\n",
    "dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
    "\n",
    "# Shuffle and batch the dataset.\n",
    "dataset = dataset.shuffle(buffer_size=2000, seed=seed_value)\n",
    "dataset = dataset.batch(batch_size)\n",
    "\n",
    "# Build a simple sequential classification model.\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(28, 28, 1)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "# Define optimizer and loss function.\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "# Create metric objects for loss and accuracy.\n",
    "train_loss_metric = tf.keras.metrics.Mean(name=\"train_loss\")\n",
    "train_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "    name=\"train_accuracy\"\n",
    ")\n",
    "\n",
    "# Define one training step using GradientTape.\n",
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(images, training=True)\n",
    "        loss_value = loss_fn(labels, predictions)\n",
    "    gradients = tape.gradient(loss_value, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    # Update metric states with current batch.\n",
    "    train_loss_metric.update_state(loss_value)\n",
    "    train_acc_metric.update_state(labels, predictions)\n",
    "\n",
    "# Set number of epochs for demonstration.\n",
    "num_epochs = 3\n",
    "\n",
    "# Run custom training loop with metrics.\n",
    "for epoch in range(num_epochs):\n",
    "    # Reset metric states at epoch start.\n",
    "    train_loss_metric.reset_state()\n",
    "    train_acc_metric.reset_state()\n",
    "\n",
    "    # Iterate over batches in dataset.\n",
    "    for batch_images, batch_labels in dataset:\n",
    "        train_step(batch_images, batch_labels)\n",
    "\n",
    "    # Read metric results after epoch.\n",
    "    epoch_loss = train_loss_metric.result().numpy()\n",
    "    epoch_acc = train_acc_metric.result().numpy()\n",
    "\n",
    "    # Print concise epoch summary line.\n",
    "    print(\n",
    "        f\"Epoch {epoch + 1}: loss={epoch_loss:.4f}, accuracy={epoch_acc:.4f}\"\n",
    "    )\n",
    "\n",
    "# Show final metric values after training.\n",
    "final_loss = train_loss_metric.result().numpy()\n",
    "final_acc = train_acc_metric.result().numpy()\n",
    "print(\"Final tracked loss:\", round(float(final_loss), 4))\n",
    "print(\"Final tracked accuracy:\", round(float(final_acc), 4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cc63cc",
   "metadata": {},
   "source": [
    "### **3.2. Updating And Reading Metrics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99a92f2",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_04/Lecture_B/image_03_02.jpg?v=1769396940\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Treat each metric as a stateful calculator\n",
    ">* Update metrics every batch with predictions and loss\n",
    "\n",
    ">* Updating and reading metrics are separate actions\n",
    ">* Read metrics at checkpoints; reading doesnâ€™t change state\n",
    "\n",
    ">* Update all metrics every training batch consistently\n",
    ">* Read metrics at epoch end for reliable snapshots\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c433c8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Updating And Reading Metrics\n",
    "\n",
    "# This script shows metrics in custom loops.\n",
    "# It focuses on updating and reading metrics.\n",
    "# Run cells to observe metric behavior clearly.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required libraries for TensorFlow training.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set seeds for reproducible behavior in this demo.\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version in a compact single line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Select device preference based on GPU availability.\n",
    "physical_gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "if physical_gpus:\n",
    "    device_type = \"GPU\"\n",
    "else:\n",
    "    device_type = \"CPU\"\n",
    "\n",
    "# Print which device type will likely be used.\n",
    "print(\"Running on device type:\", device_type)\n",
    "\n",
    "# Create a tiny synthetic classification dataset.\n",
    "num_samples = 256\n",
    "num_features = 20\n",
    "num_classes = 3\n",
    "\n",
    "# Generate random input features with normal distribution.\n",
    "X = np.random.randn(num_samples, num_features).astype(\"float32\")\n",
    "\n",
    "# Generate random integer labels for classification.\n",
    "y_int = np.random.randint(num_classes, size=(num_samples,))\n",
    "\n",
    "# Convert integer labels to one hot encoded vectors.\n",
    "y_onehot = tf.one_hot(y_int, depth=num_classes)\n",
    "\n",
    "# Wrap arrays into a tf.data.Dataset pipeline.\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X, y_onehot))\n",
    "\n",
    "# Shuffle and batch the dataset for training.\n",
    "batch_size = 32\n",
    "dataset = dataset.shuffle(buffer_size=num_samples, seed=seed_value)\n",
    "\n",
    "# Batch and prefetch for efficient small training loop.\n",
    "dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Build a simple Keras model for demonstration.\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(num_features,)),\n",
    "    tf.keras.layers.Dense(16, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(num_classes, activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "# Define an optimizer and a loss function.\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "# Create metric objects for loss and accuracy.\n",
    "train_loss_metric = tf.keras.metrics.Mean(name=\"train_loss\")\n",
    "train_acc_metric = tf.keras.metrics.CategoricalAccuracy(\n",
    "    name=\"train_accuracy\"\n",
    ")\n",
    "\n",
    "# Define one training step using GradientTape and metrics.\n",
    "@tf.function\n",
    "def train_step(batch_x, batch_y):\n",
    "    # Record operations for automatic differentiation.\n",
    "    with tf.GradientTape() as tape:\n",
    "        preds = model(batch_x, training=True)\n",
    "        loss_value = loss_fn(batch_y, preds)\n",
    "\n",
    "    # Compute gradients of loss with respect to weights.\n",
    "    grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "\n",
    "    # Apply gradients to update model parameters.\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    # Update loss metric with current batch loss value.\n",
    "    train_loss_metric.update_state(loss_value)\n",
    "\n",
    "    # Update accuracy metric with labels and predictions.\n",
    "    train_acc_metric.update_state(batch_y, preds)\n",
    "\n",
    "    # Return scalar loss for optional external uses.\n",
    "    return loss_value\n",
    "\n",
    "# Validate dataset shapes before starting training.\n",
    "for sample_x, sample_y in dataset.take(1):\n",
    "    assert sample_x.shape[1] == num_features\n",
    "    assert sample_y.shape[1] == num_classes\n",
    "\n",
    "# Set number of epochs small for quick demonstration.\n",
    "num_epochs = 3\n",
    "\n",
    "# Run custom training loop with metric tracking.\n",
    "for epoch in range(num_epochs):\n",
    "    # Reset metric states at the start of each epoch.\n",
    "    train_loss_metric.reset_state()\n",
    "    train_acc_metric.reset_state()\n",
    "\n",
    "    # Iterate over batches and perform training steps.\n",
    "    for batch_x, batch_y in dataset:\n",
    "        _ = train_step(batch_x, batch_y)\n",
    "\n",
    "    # Read metric results once after processing all batches.\n",
    "    epoch_loss = train_loss_metric.result().numpy()\n",
    "    epoch_acc = train_acc_metric.result().numpy()\n",
    "\n",
    "    # Print a concise summary line for this epoch.\n",
    "    print(\n",
    "        f\"Epoch {epoch + 1}: loss={epoch_loss:.4f}, \"\n",
    "        f\"accuracy={epoch_acc:.4f}\"\n",
    "    )\n",
    "\n",
    "# Show final metric values after last epoch training.\n",
    "final_loss = float(train_loss_metric.result().numpy())\n",
    "final_acc = float(train_acc_metric.result().numpy())\n",
    "print(\"Final tracked loss:\", round(final_loss, 4), \"accuracy:\", round(final_acc, 4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d68626",
   "metadata": {},
   "source": [
    "### **3.3. Resetting Metrics Each Epoch**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79a1247",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.jsdelivr.net/gh/mhrafiei/contents@main/LFF/Master TensorFlow 2.20.0/Module_04/Lecture_B/image_03_03.jpg?v=1769397004\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    ">* Metrics should reflect only the current epoch\n",
    ">* Reset each epoch to see true performance trends\n",
    "\n",
    ">* Reset all metrics at each epoch start\n",
    ">* Otherwise metrics accumulate and hide true improvements\n",
    "\n",
    ">* Per-epoch resets improve monitoring and debugging decisions\n",
    ">* They reveal trends, anomalies, and overfitting accurately\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7199ab42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Code - Resetting Metrics Each Epoch\n",
    "\n",
    "# This script shows metrics resetting each epoch.\n",
    "# It uses a tiny model and dataset subset.\n",
    "# Focus on clear metric behavior in loops.\n",
    "\n",
    "# !pip install tensorflow==2.20.0.\n",
    "\n",
    "# Import required modules for TensorFlow training.\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set seeds for reproducible training behavior.\n",
    "seed_value = 7\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Print TensorFlow version in one short line.\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Select device preference based on GPU availability.\n",
    "physical_gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "if physical_gpus:\n",
    "    device_name = \"GPU\"\n",
    "else:\n",
    "    device_name = \"CPU\"\n",
    "\n",
    "# Print which device type will likely be used.\n",
    "print(\"Running training on device type:\", device_name)\n",
    "\n",
    "# Load MNIST dataset using Keras utilities.\n",
    "(mnist_x_train, mnist_y_train), _ = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize images to float32 values between zero and one.\n",
    "mnist_x_train = mnist_x_train.astype(\"float32\") / 255.0\n",
    "\n",
    "# Add channel dimension to match Conv2D expectations.\n",
    "mnist_x_train = np.expand_dims(mnist_x_train, axis=-1)\n",
    "\n",
    "# Use a very small subset for quick demonstration.\n",
    "subset_size = 512\n",
    "mnist_x_train = mnist_x_train[:subset_size]\n",
    "mnist_y_train = mnist_y_train[:subset_size]\n",
    "\n",
    "# Validate shapes to avoid unexpected training issues.\n",
    "assert mnist_x_train.shape[0] == subset_size\n",
    "assert mnist_y_train.shape[0] == subset_size\n",
    "\n",
    "# Create a tf.data.Dataset for efficient batching.\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (mnist_x_train, mnist_y_train)\n",
    ")\n",
    "\n",
    "# Shuffle lightly and batch into small groups.\n",
    "train_ds = train_ds.shuffle(buffer_size=1024, seed=seed_value)\n",
    "train_ds = train_ds.batch(64)\n",
    "\n",
    "# Build a simple sequential convolutional model.\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(\n",
    "        8, (3, 3), activation=\"relu\", input_shape=(28, 28, 1)\n",
    "    ),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(32, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "# Define optimizer and loss function for classification.\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "# Create metric objects for loss and accuracy tracking.\n",
    "train_loss_metric = tf.keras.metrics.Mean(name=\"train_loss\")\n",
    "train_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "    name=\"train_accuracy\"\n",
    ")\n",
    "\n",
    "# Define one training step using GradientTape mechanics.\n",
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(images, training=True)\n",
    "        loss_value = loss_fn(labels, predictions)\n",
    "    gradients = tape.gradient(loss_value, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    train_loss_metric.update_state(loss_value)\n",
    "    train_acc_metric.update_state(labels, predictions)\n",
    "\n",
    "# Set number of epochs small for quick execution.\n",
    "num_epochs = 3\n",
    "\n",
    "# Run custom training loop with metric resets each epoch.\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss_metric.reset_state()\n",
    "    train_acc_metric.reset_state()\n",
    "    for batch_images, batch_labels in train_ds:\n",
    "        train_step(batch_images, batch_labels)\n",
    "    epoch_loss = train_loss_metric.result().numpy()\n",
    "    epoch_acc = train_acc_metric.result().numpy()\n",
    "    print(\n",
    "        f\"Epoch {epoch + 1}: loss={epoch_loss:.4f}, accuracy={epoch_acc:.4f}\"\n",
    "    )\n",
    "\n",
    "# Show that metrics are independent by printing final values.\n",
    "print(\"Final epoch loss and accuracy reflect last epoch only.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ff0b91",
   "metadata": {},
   "source": [
    "# <font color=\"#418FDE\" size=\"6.5\" uppercase>**Custom Training Loops**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b41819",
   "metadata": {},
   "source": [
    "\n",
    "In this lecture, you learned to:\n",
    "- Implement a per-batch training step using tf.GradientTape and a Keras model. \n",
    "- Wrap training steps with tf.function to improve performance while preserving debuggability. \n",
    "- Track custom metrics within a training loop and reset them appropriately each epoch. \n",
    "\n",
    "In the next Module (Module 5), we will go over 'Data Pipelines'"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
